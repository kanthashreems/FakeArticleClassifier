evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle0.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1366 words.
Number of 4-grams hit = 1355  (99.19%)
Number of 3-grams hit = 9  (0.66%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle1.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1620 words.
Number of 4-grams hit = 1609  (99.32%)
Number of 3-grams hit = 9  (0.56%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle2.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 591 words.
Number of 4-grams hit = 584  (98.82%)
Number of 3-grams hit = 5  (0.85%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle3.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 661 words.
Number of 4-grams hit = 654  (98.94%)
Number of 3-grams hit = 5  (0.76%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle4.out
Perplexity = 3.06, Entropy = 1.61 bits
Computation based on 425 words.
Number of 4-grams hit = 420  (98.82%)
Number of 3-grams hit = 3  (0.71%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle5.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 957 words.
Number of 4-grams hit = 949  (99.16%)
Number of 3-grams hit = 5  (0.52%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle6.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 317 words.
Number of 4-grams hit = 314  (99.05%)
Number of 3-grams hit = 1  (0.32%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle7.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 665 words.
Number of 4-grams hit = 659  (99.10%)
Number of 3-grams hit = 4  (0.60%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle8.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 528 words.
Number of 4-grams hit = 524  (99.24%)
Number of 3-grams hit = 2  (0.38%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle9.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 345 words.
Number of 4-grams hit = 339  (98.26%)
Number of 3-grams hit = 4  (1.16%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle10.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 327 words.
Number of 4-grams hit = 321  (98.17%)
Number of 3-grams hit = 4  (1.22%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle11.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 325 words.
Number of 4-grams hit = 320  (98.46%)
Number of 3-grams hit = 3  (0.92%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle12.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 326 words.
Number of 4-grams hit = 320  (98.16%)
Number of 3-grams hit = 4  (1.23%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle13.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 488 words.
Number of 4-grams hit = 482  (98.77%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle14.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 523 words.
Number of 4-grams hit = 520  (99.43%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle15.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 543 words.
Number of 4-grams hit = 536  (98.71%)
Number of 3-grams hit = 4  (0.74%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle16.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 400 words.
Number of 4-grams hit = 397  (99.25%)
Number of 3-grams hit = 1  (0.25%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle17.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 722 words.
Number of 4-grams hit = 716  (99.17%)
Number of 3-grams hit = 3  (0.42%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle18.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 445 words.
Number of 4-grams hit = 440  (98.88%)
Number of 3-grams hit = 3  (0.67%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle19.out
Perplexity = 4.11, Entropy = 2.04 bits
Computation based on 560 words.
Number of 4-grams hit = 549  (98.04%)
Number of 3-grams hit = 8  (1.43%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle20.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 386 words.
Number of 4-grams hit = 381  (98.70%)
Number of 3-grams hit = 3  (0.78%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle21.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 500 words.
Number of 4-grams hit = 493  (98.60%)
Number of 3-grams hit = 5  (1.00%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle22.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 387 words.
Number of 4-grams hit = 384  (99.22%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle23.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 378 words.
Number of 4-grams hit = 371  (98.15%)
Number of 3-grams hit = 5  (1.32%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle24.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 599 words.
Number of 4-grams hit = 594  (99.17%)
Number of 3-grams hit = 3  (0.50%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle25.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 413 words.
Number of 4-grams hit = 405  (98.06%)
Number of 3-grams hit = 6  (1.45%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle26.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1649 words.
Number of 4-grams hit = 1638  (99.33%)
Number of 3-grams hit = 9  (0.55%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle27.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 424 words.
Number of 4-grams hit = 420  (99.06%)
Number of 3-grams hit = 2  (0.47%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle28.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1511 words.
Number of 4-grams hit = 1500  (99.27%)
Number of 3-grams hit = 7  (0.46%)
Number of 2-grams hit = 3  (0.20%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle29.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 346 words.
Number of 4-grams hit = 343  (99.13%)
Number of 3-grams hit = 1  (0.29%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle30.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 523 words.
Number of 4-grams hit = 518  (99.04%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle31.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 481 words.
Number of 4-grams hit = 476  (98.96%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle32.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 516 words.
Number of 4-grams hit = 510  (98.84%)
Number of 3-grams hit = 4  (0.78%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle33.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 476 words.
Number of 4-grams hit = 471  (98.95%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle34.out
Perplexity = 3.00, Entropy = 1.59 bits
Computation based on 326 words.
Number of 4-grams hit = 323  (99.08%)
Number of 3-grams hit = 1  (0.31%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle35.out
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 402 words.
Number of 4-grams hit = 398  (99.00%)
Number of 3-grams hit = 2  (0.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle36.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1088 words.
Number of 4-grams hit = 1076  (98.90%)
Number of 3-grams hit = 10  (0.92%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle37.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 604 words.
Number of 4-grams hit = 595  (98.51%)
Number of 3-grams hit = 6  (0.99%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle38.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 444 words.
Number of 4-grams hit = 440  (99.10%)
Number of 3-grams hit = 2  (0.45%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle39.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 2871 words.
Number of 4-grams hit = 2844  (99.06%)
Number of 3-grams hit = 24  (0.84%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle40.out
Perplexity = 4.33, Entropy = 2.12 bits
Computation based on 442 words.
Number of 4-grams hit = 432  (97.74%)
Number of 3-grams hit = 7  (1.58%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle41.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1060 words.
Number of 4-grams hit = 1054  (99.43%)
Number of 3-grams hit = 3  (0.28%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle42.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 591 words.
Number of 4-grams hit = 582  (98.48%)
Number of 3-grams hit = 6  (1.02%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle43.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 490 words.
Number of 4-grams hit = 482  (98.37%)
Number of 3-grams hit = 5  (1.02%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle44.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 549 words.
Number of 4-grams hit = 546  (99.45%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle45.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 502 words.
Number of 4-grams hit = 497  (99.00%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle46.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 436 words.
Number of 4-grams hit = 431  (98.85%)
Number of 3-grams hit = 3  (0.69%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle47.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 2914 words.
Number of 4-grams hit = 2893  (99.28%)
Number of 3-grams hit = 18  (0.62%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle48.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 400 words.
Number of 4-grams hit = 393  (98.25%)
Number of 3-grams hit = 3  (0.75%)
Number of 2-grams hit = 3  (0.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle49.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 395 words.
Number of 4-grams hit = 390  (98.73%)
Number of 3-grams hit = 3  (0.76%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle50.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 563 words.
Number of 4-grams hit = 556  (98.76%)
Number of 3-grams hit = 4  (0.71%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle51.out
Perplexity = 3.24, Entropy = 1.69 bits
Computation based on 1184 words.
Number of 4-grams hit = 1178  (99.49%)
Number of 3-grams hit = 4  (0.34%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle52.out
Perplexity = 3.54, Entropy = 1.83 bits
Computation based on 401 words.
Number of 4-grams hit = 395  (98.50%)
Number of 3-grams hit = 3  (0.75%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle53.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 521 words.
Number of 4-grams hit = 516  (99.04%)
Number of 3-grams hit = 3  (0.58%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle54.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1201 words.
Number of 4-grams hit = 1188  (98.92%)
Number of 3-grams hit = 11  (0.92%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle55.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 578 words.
Number of 4-grams hit = 572  (98.96%)
Number of 3-grams hit = 4  (0.69%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle56.out
Perplexity = 4.10, Entropy = 2.04 bits
Computation based on 988 words.
Number of 4-grams hit = 973  (98.48%)
Number of 3-grams hit = 11  (1.11%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle57.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1691 words.
Number of 4-grams hit = 1679  (99.29%)
Number of 3-grams hit = 9  (0.53%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle58.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 416 words.
Number of 4-grams hit = 413  (99.28%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle59.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 231 words.
Number of 4-grams hit = 225  (97.40%)
Number of 3-grams hit = 3  (1.30%)
Number of 2-grams hit = 2  (0.87%)
Number of 1-grams hit = 1  (0.43%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle60.out
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 354 words.
Number of 4-grams hit = 348  (98.31%)
Number of 3-grams hit = 4  (1.13%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle61.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 530 words.
Number of 4-grams hit = 521  (98.30%)
Number of 3-grams hit = 7  (1.32%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle62.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 1398 words.
Number of 4-grams hit = 1384  (99.00%)
Number of 3-grams hit = 11  (0.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle63.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 1066 words.
Number of 4-grams hit = 1059  (99.34%)
Number of 3-grams hit = 5  (0.47%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle64.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 4550 words.
Number of 4-grams hit = 4515  (99.23%)
Number of 3-grams hit = 28  (0.62%)
Number of 2-grams hit = 6  (0.13%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle65.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 697 words.
Number of 4-grams hit = 687  (98.57%)
Number of 3-grams hit = 7  (1.00%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle66.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 514 words.
Number of 4-grams hit = 510  (99.22%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle67.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 484 words.
Number of 4-grams hit = 481  (99.38%)
Number of 3-grams hit = 1  (0.21%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle68.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1206 words.
Number of 4-grams hit = 1199  (99.42%)
Number of 3-grams hit = 4  (0.33%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle69.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 407 words.
Number of 4-grams hit = 404  (99.26%)
Number of 3-grams hit = 1  (0.25%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle70.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 1071 words.
Number of 4-grams hit = 1058  (98.79%)
Number of 3-grams hit = 9  (0.84%)
Number of 2-grams hit = 3  (0.28%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle71.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 591 words.
Number of 4-grams hit = 585  (98.98%)
Number of 3-grams hit = 3  (0.51%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle72.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1423 words.
Number of 4-grams hit = 1411  (99.16%)
Number of 3-grams hit = 9  (0.63%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle73.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 566 words.
Number of 4-grams hit = 562  (99.29%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle74.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 901 words.
Number of 4-grams hit = 890  (98.78%)
Number of 3-grams hit = 9  (1.00%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle75.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1288 words.
Number of 4-grams hit = 1278  (99.22%)
Number of 3-grams hit = 8  (0.62%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle76.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1515 words.
Number of 4-grams hit = 1502  (99.14%)
Number of 3-grams hit = 10  (0.66%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle77.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 322 words.
Number of 4-grams hit = 313  (97.20%)
Number of 3-grams hit = 5  (1.55%)
Number of 2-grams hit = 3  (0.93%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle78.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 1223 words.
Number of 4-grams hit = 1212  (99.10%)
Number of 3-grams hit = 6  (0.49%)
Number of 2-grams hit = 4  (0.33%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle79.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1163 words.
Number of 4-grams hit = 1151  (98.97%)
Number of 3-grams hit = 8  (0.69%)
Number of 2-grams hit = 3  (0.26%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle80.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 412 words.
Number of 4-grams hit = 405  (98.30%)
Number of 3-grams hit = 5  (1.21%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle81.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 4418 words.
Number of 4-grams hit = 4378  (99.09%)
Number of 3-grams hit = 35  (0.79%)
Number of 2-grams hit = 4  (0.09%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle82.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 510 words.
Number of 4-grams hit = 503  (98.63%)
Number of 3-grams hit = 5  (0.98%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle83.out
Perplexity = 4.11, Entropy = 2.04 bits
Computation based on 476 words.
Number of 4-grams hit = 469  (98.53%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 2  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle84.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 431 words.
Number of 4-grams hit = 428  (99.30%)
Number of 3-grams hit = 1  (0.23%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle85.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1317 words.
Number of 4-grams hit = 1308  (99.32%)
Number of 3-grams hit = 6  (0.46%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle86.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 309 words.
Number of 4-grams hit = 306  (99.03%)
Number of 3-grams hit = 1  (0.32%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle87.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 676 words.
Number of 4-grams hit = 663  (98.08%)
Number of 3-grams hit = 9  (1.33%)
Number of 2-grams hit = 3  (0.44%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle88.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1583 words.
Number of 4-grams hit = 1572  (99.31%)
Number of 3-grams hit = 7  (0.44%)
Number of 2-grams hit = 3  (0.19%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle89.out
Perplexity = 4.36, Entropy = 2.13 bits
Computation based on 414 words.
Number of 4-grams hit = 406  (98.07%)
Number of 3-grams hit = 5  (1.21%)
Number of 2-grams hit = 2  (0.48%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle90.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 555 words.
Number of 4-grams hit = 546  (98.38%)
Number of 3-grams hit = 7  (1.26%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle91.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 371 words.
Number of 4-grams hit = 364  (98.11%)
Number of 3-grams hit = 5  (1.35%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle92.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1134 words.
Number of 4-grams hit = 1128  (99.47%)
Number of 3-grams hit = 4  (0.35%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle93.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 745 words.
Number of 4-grams hit = 736  (98.79%)
Number of 3-grams hit = 7  (0.94%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle94.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1395 words.
Number of 4-grams hit = 1381  (99.00%)
Number of 3-grams hit = 10  (0.72%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle95.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 546 words.
Number of 4-grams hit = 540  (98.90%)
Number of 3-grams hit = 4  (0.73%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle96.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 439 words.
Number of 4-grams hit = 432  (98.41%)
Number of 3-grams hit = 4  (0.91%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle97.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 601 words.
Number of 4-grams hit = 593  (98.67%)
Number of 3-grams hit = 5  (0.83%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle98.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 391 words.
Number of 4-grams hit = 386  (98.72%)
Number of 3-grams hit = 2  (0.51%)
Number of 2-grams hit = 2  (0.51%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle99.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 497 words.
Number of 4-grams hit = 492  (98.99%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle100.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 513 words.
Number of 4-grams hit = 505  (98.44%)
Number of 3-grams hit = 6  (1.17%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle101.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 554 words.
Number of 4-grams hit = 550  (99.28%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle102.out
Perplexity = 3.99, Entropy = 1.99 bits
Computation based on 448 words.
Number of 4-grams hit = 440  (98.21%)
Number of 3-grams hit = 6  (1.34%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle103.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 536 words.
Number of 4-grams hit = 533  (99.44%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle104.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 6983 words.
Number of 4-grams hit = 6926  (99.18%)
Number of 3-grams hit = 47  (0.67%)
Number of 2-grams hit = 9  (0.13%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle105.out
Perplexity = 4.49, Entropy = 2.17 bits
Computation based on 485 words.
Number of 4-grams hit = 476  (98.14%)
Number of 3-grams hit = 7  (1.44%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle106.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 875 words.
Number of 4-grams hit = 870  (99.43%)
Number of 3-grams hit = 3  (0.34%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle107.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 529 words.
Number of 4-grams hit = 522  (98.68%)
Number of 3-grams hit = 4  (0.76%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle108.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 568 words.
Number of 4-grams hit = 562  (98.94%)
Number of 3-grams hit = 3  (0.53%)
Number of 2-grams hit = 2  (0.35%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle109.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 1223 words.
Number of 4-grams hit = 1204  (98.45%)
Number of 3-grams hit = 16  (1.31%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle110.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 981 words.
Number of 4-grams hit = 974  (99.29%)
Number of 3-grams hit = 5  (0.51%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle111.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 1359 words.
Number of 4-grams hit = 1355  (99.71%)
Number of 3-grams hit = 2  (0.15%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle112.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 355 words.
Number of 4-grams hit = 351  (98.87%)
Number of 3-grams hit = 2  (0.56%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle113.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 6382 words.
Number of 4-grams hit = 6317  (98.98%)
Number of 3-grams hit = 53  (0.83%)
Number of 2-grams hit = 11  (0.17%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle114.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 552 words.
Number of 4-grams hit = 549  (99.46%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle115.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 698 words.
Number of 4-grams hit = 694  (99.43%)
Number of 3-grams hit = 2  (0.29%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle116.out
Perplexity = 4.20, Entropy = 2.07 bits
Computation based on 1647 words.
Number of 4-grams hit = 1621  (98.42%)
Number of 3-grams hit = 20  (1.21%)
Number of 2-grams hit = 5  (0.30%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle117.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 557 words.
Number of 4-grams hit = 551  (98.92%)
Number of 3-grams hit = 3  (0.54%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle118.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 667 words.
Number of 4-grams hit = 662  (99.25%)
Number of 3-grams hit = 3  (0.45%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle119.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 424 words.
Number of 4-grams hit = 420  (99.06%)
Number of 3-grams hit = 2  (0.47%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle120.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 608 words.
Number of 4-grams hit = 598  (98.36%)
Number of 3-grams hit = 5  (0.82%)
Number of 2-grams hit = 4  (0.66%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle121.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 7778 words.
Number of 4-grams hit = 7729  (99.37%)
Number of 3-grams hit = 43  (0.55%)
Number of 2-grams hit = 5  (0.06%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle122.out
Perplexity = 4.11, Entropy = 2.04 bits
Computation based on 353 words.
Number of 4-grams hit = 348  (98.58%)
Number of 3-grams hit = 3  (0.85%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle123.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 553 words.
Number of 4-grams hit = 547  (98.92%)
Number of 3-grams hit = 4  (0.72%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle124.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 873 words.
Number of 4-grams hit = 866  (99.20%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle125.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 525 words.
Number of 4-grams hit = 520  (99.05%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle126.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 587 words.
Number of 4-grams hit = 580  (98.81%)
Number of 3-grams hit = 5  (0.85%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle127.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1001 words.
Number of 4-grams hit = 985  (98.40%)
Number of 3-grams hit = 13  (1.30%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle128.out
Perplexity = 4.30, Entropy = 2.10 bits
Computation based on 462 words.
Number of 4-grams hit = 454  (98.27%)
Number of 3-grams hit = 6  (1.30%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle129.out
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 511 words.
Number of 4-grams hit = 502  (98.24%)
Number of 3-grams hit = 7  (1.37%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle130.out
Perplexity = 3.14, Entropy = 1.65 bits
Computation based on 1659 words.
Number of 4-grams hit = 1649  (99.40%)
Number of 3-grams hit = 8  (0.48%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle131.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 7398 words.
Number of 4-grams hit = 7356  (99.43%)
Number of 3-grams hit = 37  (0.50%)
Number of 2-grams hit = 4  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle132.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1222 words.
Number of 4-grams hit = 1215  (99.43%)
Number of 3-grams hit = 5  (0.41%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle133.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 262 words.
Number of 4-grams hit = 255  (97.33%)
Number of 3-grams hit = 5  (1.91%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle134.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 539 words.
Number of 4-grams hit = 534  (99.07%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle135.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 521 words.
Number of 4-grams hit = 517  (99.23%)
Number of 3-grams hit = 2  (0.38%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle136.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 567 words.
Number of 4-grams hit = 563  (99.29%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle137.out
Perplexity = 4.07, Entropy = 2.02 bits
Computation based on 361 words.
Number of 4-grams hit = 353  (97.78%)
Number of 3-grams hit = 5  (1.39%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle138.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 342 words.
Number of 4-grams hit = 338  (98.83%)
Number of 3-grams hit = 2  (0.58%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle139.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 298 words.
Number of 4-grams hit = 295  (98.99%)
Number of 3-grams hit = 1  (0.34%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle140.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 400 words.
Number of 4-grams hit = 392  (98.00%)
Number of 3-grams hit = 6  (1.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle141.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 467 words.
Number of 4-grams hit = 462  (98.93%)
Number of 3-grams hit = 2  (0.43%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle142.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 758 words.
Number of 4-grams hit = 745  (98.28%)
Number of 3-grams hit = 11  (1.45%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle143.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 412 words.
Number of 4-grams hit = 408  (99.03%)
Number of 3-grams hit = 2  (0.49%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle144.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 541 words.
Number of 4-grams hit = 538  (99.45%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle145.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 2107 words.
Number of 4-grams hit = 2095  (99.43%)
Number of 3-grams hit = 10  (0.47%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle146.out
Perplexity = 4.42, Entropy = 2.14 bits
Computation based on 262 words.
Number of 4-grams hit = 254  (96.95%)
Number of 3-grams hit = 4  (1.53%)
Number of 2-grams hit = 3  (1.15%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle147.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 714 words.
Number of 4-grams hit = 706  (98.88%)
Number of 3-grams hit = 5  (0.70%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle148.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 1886 words.
Number of 4-grams hit = 1866  (98.94%)
Number of 3-grams hit = 18  (0.95%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle149.out
Perplexity = 3.26, Entropy = 1.71 bits
Computation based on 612 words.
Number of 4-grams hit = 607  (99.18%)
Number of 3-grams hit = 3  (0.49%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle150.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 686 words.
Number of 4-grams hit = 673  (98.10%)
Number of 3-grams hit = 10  (1.46%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle151.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 188 words.
Number of 4-grams hit = 180  (95.74%)
Number of 3-grams hit = 5  (2.66%)
Number of 2-grams hit = 2  (1.06%)
Number of 1-grams hit = 1  (0.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle152.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1984 words.
Number of 4-grams hit = 1965  (99.04%)
Number of 3-grams hit = 16  (0.81%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle153.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 629 words.
Number of 4-grams hit = 621  (98.73%)
Number of 3-grams hit = 5  (0.79%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle154.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1160 words.
Number of 4-grams hit = 1154  (99.48%)
Number of 3-grams hit = 4  (0.34%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle155.out
Perplexity = 3.49, Entropy = 1.81 bits
Computation based on 695 words.
Number of 4-grams hit = 685  (98.56%)
Number of 3-grams hit = 7  (1.01%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle156.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 1549 words.
Number of 4-grams hit = 1531  (98.84%)
Number of 3-grams hit = 13  (0.84%)
Number of 2-grams hit = 4  (0.26%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle157.out
Perplexity = 4.30, Entropy = 2.10 bits
Computation based on 431 words.
Number of 4-grams hit = 422  (97.91%)
Number of 3-grams hit = 6  (1.39%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle158.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 623 words.
Number of 4-grams hit = 614  (98.56%)
Number of 3-grams hit = 6  (0.96%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle159.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 1362 words.
Number of 4-grams hit = 1343  (98.60%)
Number of 3-grams hit = 12  (0.88%)
Number of 2-grams hit = 6  (0.44%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle160.out
Perplexity = 4.07, Entropy = 2.03 bits
Computation based on 584 words.
Number of 4-grams hit = 576  (98.63%)
Number of 3-grams hit = 6  (1.03%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle161.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 211 words.
Number of 4-grams hit = 208  (98.58%)
Number of 3-grams hit = 1  (0.47%)
Number of 2-grams hit = 1  (0.47%)
Number of 1-grams hit = 1  (0.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle162.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1245 words.
Number of 4-grams hit = 1233  (99.04%)
Number of 3-grams hit = 9  (0.72%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle163.out
Perplexity = 2.93, Entropy = 1.55 bits
Computation based on 752 words.
Number of 4-grams hit = 747  (99.34%)
Number of 3-grams hit = 3  (0.40%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle164.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1948 words.
Number of 4-grams hit = 1930  (99.08%)
Number of 3-grams hit = 15  (0.77%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle165.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 593 words.
Number of 4-grams hit = 588  (99.16%)
Number of 3-grams hit = 3  (0.51%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle166.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 555 words.
Number of 4-grams hit = 546  (98.38%)
Number of 3-grams hit = 5  (0.90%)
Number of 2-grams hit = 3  (0.54%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle167.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1260 words.
Number of 4-grams hit = 1249  (99.13%)
Number of 3-grams hit = 7  (0.56%)
Number of 2-grams hit = 3  (0.24%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle168.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 571 words.
Number of 4-grams hit = 567  (99.30%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle169.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 551 words.
Number of 4-grams hit = 547  (99.27%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle170.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1235 words.
Number of 4-grams hit = 1230  (99.60%)
Number of 3-grams hit = 3  (0.24%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle171.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 656 words.
Number of 4-grams hit = 644  (98.17%)
Number of 3-grams hit = 9  (1.37%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle172.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 734 words.
Number of 4-grams hit = 726  (98.91%)
Number of 3-grams hit = 5  (0.68%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle173.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1110 words.
Number of 4-grams hit = 1102  (99.28%)
Number of 3-grams hit = 4  (0.36%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle174.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 493 words.
Number of 4-grams hit = 488  (98.99%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle175.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 486 words.
Number of 4-grams hit = 478  (98.35%)
Number of 3-grams hit = 5  (1.03%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle176.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 800 words.
Number of 4-grams hit = 790  (98.75%)
Number of 3-grams hit = 7  (0.88%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle177.out
Perplexity = 4.10, Entropy = 2.03 bits
Computation based on 557 words.
Number of 4-grams hit = 546  (98.03%)
Number of 3-grams hit = 9  (1.62%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle178.out
Perplexity = 4.15, Entropy = 2.05 bits
Computation based on 183 words.
Number of 4-grams hit = 177  (96.72%)
Number of 3-grams hit = 3  (1.64%)
Number of 2-grams hit = 2  (1.09%)
Number of 1-grams hit = 1  (0.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle179.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 491 words.
Number of 4-grams hit = 485  (98.78%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle180.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 473 words.
Number of 4-grams hit = 467  (98.73%)
Number of 3-grams hit = 4  (0.85%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle181.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 955 words.
Number of 4-grams hit = 949  (99.37%)
Number of 3-grams hit = 4  (0.42%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle182.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 407 words.
Number of 4-grams hit = 400  (98.28%)
Number of 3-grams hit = 4  (0.98%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle183.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 504 words.
Number of 4-grams hit = 498  (98.81%)
Number of 3-grams hit = 4  (0.79%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle184.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 5958 words.
Number of 4-grams hit = 5926  (99.46%)
Number of 3-grams hit = 28  (0.47%)
Number of 2-grams hit = 3  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle185.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1070 words.
Number of 4-grams hit = 1063  (99.35%)
Number of 3-grams hit = 5  (0.47%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle186.out
Perplexity = 3.17, Entropy = 1.67 bits
Computation based on 529 words.
Number of 4-grams hit = 526  (99.43%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle187.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 3137 words.
Number of 4-grams hit = 3117  (99.36%)
Number of 3-grams hit = 18  (0.57%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle188.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 729 words.
Number of 4-grams hit = 723  (99.18%)
Number of 3-grams hit = 4  (0.55%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle189.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 580 words.
Number of 4-grams hit = 572  (98.62%)
Number of 3-grams hit = 5  (0.86%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle190.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 421 words.
Number of 4-grams hit = 418  (99.29%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle191.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 5916 words.
Number of 4-grams hit = 5882  (99.43%)
Number of 3-grams hit = 27  (0.46%)
Number of 2-grams hit = 4  (0.07%)
Number of 1-grams hit = 3  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle192.out
Perplexity = 2.76, Entropy = 1.46 bits
Computation based on 818 words.
Number of 4-grams hit = 814  (99.51%)
Number of 3-grams hit = 2  (0.24%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle193.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 170 words.
Number of 4-grams hit = 167  (98.24%)
Number of 3-grams hit = 1  (0.59%)
Number of 2-grams hit = 1  (0.59%)
Number of 1-grams hit = 1  (0.59%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle194.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 661 words.
Number of 4-grams hit = 653  (98.79%)
Number of 3-grams hit = 6  (0.91%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle195.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 864 words.
Number of 4-grams hit = 855  (98.96%)
Number of 3-grams hit = 7  (0.81%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle196.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 730 words.
Number of 4-grams hit = 720  (98.63%)
Number of 3-grams hit = 8  (1.10%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle197.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 251 words.
Number of 4-grams hit = 248  (98.80%)
Number of 3-grams hit = 1  (0.40%)
Number of 2-grams hit = 1  (0.40%)
Number of 1-grams hit = 1  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle198.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 768 words.
Number of 4-grams hit = 759  (98.83%)
Number of 3-grams hit = 6  (0.78%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle199.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 4967 words.
Number of 4-grams hit = 4945  (99.56%)
Number of 3-grams hit = 18  (0.36%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle200.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 702 words.
Number of 4-grams hit = 695  (99.00%)
Number of 3-grams hit = 5  (0.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle201.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 507 words.
Number of 4-grams hit = 500  (98.62%)
Number of 3-grams hit = 5  (0.99%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle202.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 1262 words.
Number of 4-grams hit = 1248  (98.89%)
Number of 3-grams hit = 11  (0.87%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle203.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 5293 words.
Number of 4-grams hit = 5272  (99.60%)
Number of 3-grams hit = 17  (0.32%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle204.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 5882 words.
Number of 4-grams hit = 5839  (99.27%)
Number of 3-grams hit = 39  (0.66%)
Number of 2-grams hit = 3  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle205.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 603 words.
Number of 4-grams hit = 594  (98.51%)
Number of 3-grams hit = 7  (1.16%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle206.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 636 words.
Number of 4-grams hit = 628  (98.74%)
Number of 3-grams hit = 5  (0.79%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle207.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 779 words.
Number of 4-grams hit = 766  (98.33%)
Number of 3-grams hit = 9  (1.16%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle208.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 951 words.
Number of 4-grams hit = 938  (98.63%)
Number of 3-grams hit = 9  (0.95%)
Number of 2-grams hit = 3  (0.32%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle209.out
Perplexity = 4.13, Entropy = 2.05 bits
Computation based on 283 words.
Number of 4-grams hit = 276  (97.53%)
Number of 3-grams hit = 5  (1.77%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle210.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 886 words.
Number of 4-grams hit = 880  (99.32%)
Number of 3-grams hit = 4  (0.45%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle211.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 504 words.
Number of 4-grams hit = 500  (99.21%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle212.out
Perplexity = 4.06, Entropy = 2.02 bits
Computation based on 449 words.
Number of 4-grams hit = 441  (98.22%)
Number of 3-grams hit = 6  (1.34%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle213.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 695 words.
Number of 4-grams hit = 684  (98.42%)
Number of 3-grams hit = 8  (1.15%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle214.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 509 words.
Number of 4-grams hit = 501  (98.43%)
Number of 3-grams hit = 4  (0.79%)
Number of 2-grams hit = 3  (0.59%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle215.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 742 words.
Number of 4-grams hit = 727  (97.98%)
Number of 3-grams hit = 11  (1.48%)
Number of 2-grams hit = 3  (0.40%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle216.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 525 words.
Number of 4-grams hit = 519  (98.86%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle217.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 323 words.
Number of 4-grams hit = 317  (98.14%)
Number of 3-grams hit = 3  (0.93%)
Number of 2-grams hit = 2  (0.62%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle218.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 792 words.
Number of 4-grams hit = 782  (98.74%)
Number of 3-grams hit = 8  (1.01%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle219.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 439 words.
Number of 4-grams hit = 425  (96.81%)
Number of 3-grams hit = 11  (2.51%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle220.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 4706 words.
Number of 4-grams hit = 4676  (99.36%)
Number of 3-grams hit = 26  (0.55%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle221.out
Perplexity = 3.45, Entropy = 1.78 bits
Computation based on 1255 words.
Number of 4-grams hit = 1248  (99.44%)
Number of 3-grams hit = 5  (0.40%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle222.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 4839 words.
Number of 4-grams hit = 4787  (98.93%)
Number of 3-grams hit = 44  (0.91%)
Number of 2-grams hit = 7  (0.14%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle223.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 895 words.
Number of 4-grams hit = 890  (99.44%)
Number of 3-grams hit = 3  (0.34%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle224.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 406 words.
Number of 4-grams hit = 401  (98.77%)
Number of 3-grams hit = 3  (0.74%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle225.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 777 words.
Number of 4-grams hit = 766  (98.58%)
Number of 3-grams hit = 7  (0.90%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle226.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1314 words.
Number of 4-grams hit = 1308  (99.54%)
Number of 3-grams hit = 4  (0.30%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle227.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 1017 words.
Number of 4-grams hit = 1009  (99.21%)
Number of 3-grams hit = 5  (0.49%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle228.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 851 words.
Number of 4-grams hit = 845  (99.29%)
Number of 3-grams hit = 4  (0.47%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle229.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 518 words.
Number of 4-grams hit = 508  (98.07%)
Number of 3-grams hit = 7  (1.35%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle230.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 337 words.
Number of 4-grams hit = 334  (99.11%)
Number of 3-grams hit = 1  (0.30%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle231.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 850 words.
Number of 4-grams hit = 845  (99.41%)
Number of 3-grams hit = 2  (0.24%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle232.out
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 391 words.
Number of 4-grams hit = 385  (98.47%)
Number of 3-grams hit = 4  (1.02%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle233.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 734 words.
Number of 4-grams hit = 728  (99.18%)
Number of 3-grams hit = 4  (0.54%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle234.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1446 words.
Number of 4-grams hit = 1429  (98.82%)
Number of 3-grams hit = 12  (0.83%)
Number of 2-grams hit = 4  (0.28%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle235.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 379 words.
Number of 4-grams hit = 373  (98.42%)
Number of 3-grams hit = 3  (0.79%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle236.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 801 words.
Number of 4-grams hit = 795  (99.25%)
Number of 3-grams hit = 4  (0.50%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle237.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 423 words.
Number of 4-grams hit = 416  (98.35%)
Number of 3-grams hit = 4  (0.95%)
Number of 2-grams hit = 2  (0.47%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle238.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 339 words.
Number of 4-grams hit = 335  (98.82%)
Number of 3-grams hit = 2  (0.59%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle239.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 530 words.
Number of 4-grams hit = 524  (98.87%)
Number of 3-grams hit = 4  (0.75%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle240.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 350 words.
Number of 4-grams hit = 347  (99.14%)
Number of 3-grams hit = 1  (0.29%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle241.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 334 words.
Number of 4-grams hit = 324  (97.01%)
Number of 3-grams hit = 6  (1.80%)
Number of 2-grams hit = 3  (0.90%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle242.out
Perplexity = 3.33, Entropy = 1.73 bits
Computation based on 319 words.
Number of 4-grams hit = 316  (99.06%)
Number of 3-grams hit = 1  (0.31%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle243.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 813 words.
Number of 4-grams hit = 806  (99.14%)
Number of 3-grams hit = 4  (0.49%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle244.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 457 words.
Number of 4-grams hit = 453  (99.12%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle245.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 5533 words.
Number of 4-grams hit = 5491  (99.24%)
Number of 3-grams hit = 35  (0.63%)
Number of 2-grams hit = 6  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle246.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 304 words.
Number of 4-grams hit = 299  (98.36%)
Number of 3-grams hit = 3  (0.99%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle247.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 492 words.
Number of 4-grams hit = 487  (98.98%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle248.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 388 words.
Number of 4-grams hit = 385  (99.23%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle249.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 928 words.
Number of 4-grams hit = 918  (98.92%)
Number of 3-grams hit = 8  (0.86%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle250.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 5273 words.
Number of 4-grams hit = 5226  (99.11%)
Number of 3-grams hit = 40  (0.76%)
Number of 2-grams hit = 6  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle251.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 5439 words.
Number of 4-grams hit = 5400  (99.28%)
Number of 3-grams hit = 33  (0.61%)
Number of 2-grams hit = 5  (0.09%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle252.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 579 words.
Number of 4-grams hit = 570  (98.45%)
Number of 3-grams hit = 6  (1.04%)
Number of 2-grams hit = 2  (0.35%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle253.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 5348 words.
Number of 4-grams hit = 5307  (99.23%)
Number of 3-grams hit = 32  (0.60%)
Number of 2-grams hit = 8  (0.15%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle254.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 836 words.
Number of 4-grams hit = 830  (99.28%)
Number of 3-grams hit = 4  (0.48%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle255.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 526 words.
Number of 4-grams hit = 520  (98.86%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle256.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 837 words.
Number of 4-grams hit = 813  (97.13%)
Number of 3-grams hit = 18  (2.15%)
Number of 2-grams hit = 5  (0.60%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle257.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 6431 words.
Number of 4-grams hit = 6367  (99.00%)
Number of 3-grams hit = 51  (0.79%)
Number of 2-grams hit = 10  (0.16%)
Number of 1-grams hit = 3  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle258.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 444 words.
Number of 4-grams hit = 433  (97.52%)
Number of 3-grams hit = 9  (2.03%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle259.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 764 words.
Number of 4-grams hit = 759  (99.35%)
Number of 3-grams hit = 3  (0.39%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle260.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 7634 words.
Number of 4-grams hit = 7605  (99.62%)
Number of 3-grams hit = 24  (0.31%)
Number of 2-grams hit = 4  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle261.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 717 words.
Number of 4-grams hit = 704  (98.19%)
Number of 3-grams hit = 10  (1.39%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle262.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 721 words.
Number of 4-grams hit = 715  (99.17%)
Number of 3-grams hit = 3  (0.42%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle263.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 411 words.
Number of 4-grams hit = 404  (98.30%)
Number of 3-grams hit = 5  (1.22%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle264.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 857 words.
Number of 4-grams hit = 854  (99.65%)
Number of 3-grams hit = 1  (0.12%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle265.out
Perplexity = 3.45, Entropy = 1.78 bits
Computation based on 602 words.
Number of 4-grams hit = 598  (99.34%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle266.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 468 words.
Number of 4-grams hit = 462  (98.72%)
Number of 3-grams hit = 4  (0.85%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle267.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 6787 words.
Number of 4-grams hit = 6737  (99.26%)
Number of 3-grams hit = 42  (0.62%)
Number of 2-grams hit = 7  (0.10%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle268.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 6349 words.
Number of 4-grams hit = 6295  (99.15%)
Number of 3-grams hit = 46  (0.72%)
Number of 2-grams hit = 7  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle269.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 457 words.
Number of 4-grams hit = 453  (99.12%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle270.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 404 words.
Number of 4-grams hit = 396  (98.02%)
Number of 3-grams hit = 5  (1.24%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle271.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 7909 words.
Number of 4-grams hit = 7856  (99.33%)
Number of 3-grams hit = 49  (0.62%)
Number of 2-grams hit = 3  (0.04%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle272.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 4304 words.
Number of 4-grams hit = 4262  (99.02%)
Number of 3-grams hit = 39  (0.91%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle273.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 683 words.
Number of 4-grams hit = 675  (98.83%)
Number of 3-grams hit = 5  (0.73%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle274.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 704 words.
Number of 4-grams hit = 697  (99.01%)
Number of 3-grams hit = 5  (0.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle275.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 2091 words.
Number of 4-grams hit = 2077  (99.33%)
Number of 3-grams hit = 12  (0.57%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle276.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 506 words.
Number of 4-grams hit = 503  (99.41%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle277.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 835 words.
Number of 4-grams hit = 832  (99.64%)
Number of 3-grams hit = 1  (0.12%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle278.out
Perplexity = 4.18, Entropy = 2.06 bits
Computation based on 856 words.
Number of 4-grams hit = 845  (98.71%)
Number of 3-grams hit = 7  (0.82%)
Number of 2-grams hit = 3  (0.35%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle279.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 446 words.
Number of 4-grams hit = 440  (98.65%)
Number of 3-grams hit = 4  (0.90%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle280.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1844 words.
Number of 4-grams hit = 1826  (99.02%)
Number of 3-grams hit = 16  (0.87%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle281.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 610 words.
Number of 4-grams hit = 606  (99.34%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle282.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 779 words.
Number of 4-grams hit = 772  (99.10%)
Number of 3-grams hit = 5  (0.64%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle283.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1294 words.
Number of 4-grams hit = 1282  (99.07%)
Number of 3-grams hit = 8  (0.62%)
Number of 2-grams hit = 3  (0.23%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle284.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 1757 words.
Number of 4-grams hit = 1734  (98.69%)
Number of 3-grams hit = 18  (1.02%)
Number of 2-grams hit = 4  (0.23%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle285.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1159 words.
Number of 4-grams hit = 1147  (98.96%)
Number of 3-grams hit = 8  (0.69%)
Number of 2-grams hit = 3  (0.26%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle286.out
Perplexity = 4.27, Entropy = 2.09 bits
Computation based on 485 words.
Number of 4-grams hit = 479  (98.76%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle287.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 1778 words.
Number of 4-grams hit = 1765  (99.27%)
Number of 3-grams hit = 11  (0.62%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle288.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 635 words.
Number of 4-grams hit = 626  (98.58%)
Number of 3-grams hit = 6  (0.94%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle289.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 774 words.
Number of 4-grams hit = 765  (98.84%)
Number of 3-grams hit = 6  (0.78%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle290.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1396 words.
Number of 4-grams hit = 1382  (99.00%)
Number of 3-grams hit = 10  (0.72%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle291.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 552 words.
Number of 4-grams hit = 546  (98.91%)
Number of 3-grams hit = 4  (0.72%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle292.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 575 words.
Number of 4-grams hit = 572  (99.48%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle293.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 593 words.
Number of 4-grams hit = 589  (99.33%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle294.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 2240 words.
Number of 4-grams hit = 2231  (99.60%)
Number of 3-grams hit = 7  (0.31%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle295.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 644 words.
Number of 4-grams hit = 640  (99.38%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle296.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1670 words.
Number of 4-grams hit = 1660  (99.40%)
Number of 3-grams hit = 8  (0.48%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle297.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 550 words.
Number of 4-grams hit = 543  (98.73%)
Number of 3-grams hit = 4  (0.73%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle298.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 3737 words.
Number of 4-grams hit = 3703  (99.09%)
Number of 3-grams hit = 30  (0.80%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle299.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 3001 words.
Number of 4-grams hit = 2986  (99.50%)
Number of 3-grams hit = 13  (0.43%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle300.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 2733 words.
Number of 4-grams hit = 2706  (99.01%)
Number of 3-grams hit = 22  (0.80%)
Number of 2-grams hit = 4  (0.15%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle301.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 2305 words.
Number of 4-grams hit = 2288  (99.26%)
Number of 3-grams hit = 12  (0.52%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 2  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle302.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 698 words.
Number of 4-grams hit = 689  (98.71%)
Number of 3-grams hit = 6  (0.86%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle303.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 941 words.
Number of 4-grams hit = 932  (99.04%)
Number of 3-grams hit = 5  (0.53%)
Number of 2-grams hit = 3  (0.32%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle304.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 446 words.
Number of 4-grams hit = 437  (97.98%)
Number of 3-grams hit = 6  (1.35%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle305.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 331 words.
Number of 4-grams hit = 328  (99.09%)
Number of 3-grams hit = 1  (0.30%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle306.out
Perplexity = 2.88, Entropy = 1.53 bits
Computation based on 635 words.
Number of 4-grams hit = 628  (98.90%)
Number of 3-grams hit = 5  (0.79%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle307.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 557 words.
Number of 4-grams hit = 549  (98.56%)
Number of 3-grams hit = 6  (1.08%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle308.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 417 words.
Number of 4-grams hit = 414  (99.28%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle309.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 265 words.
Number of 4-grams hit = 261  (98.49%)
Number of 3-grams hit = 2  (0.75%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle310.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 522 words.
Number of 4-grams hit = 517  (99.04%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle311.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 497 words.
Number of 4-grams hit = 494  (99.40%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle312.out
Perplexity = 2.92, Entropy = 1.54 bits
Computation based on 651 words.
Number of 4-grams hit = 647  (99.39%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle313.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 512 words.
Number of 4-grams hit = 506  (98.83%)
Number of 3-grams hit = 3  (0.59%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle314.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 3615 words.
Number of 4-grams hit = 3587  (99.23%)
Number of 3-grams hit = 24  (0.66%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle315.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1186 words.
Number of 4-grams hit = 1178  (99.33%)
Number of 3-grams hit = 6  (0.51%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle316.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 363 words.
Number of 4-grams hit = 355  (97.80%)
Number of 3-grams hit = 5  (1.38%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle317.out
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 675 words.
Number of 4-grams hit = 668  (98.96%)
Number of 3-grams hit = 5  (0.74%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle318.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 3867 words.
Number of 4-grams hit = 3834  (99.15%)
Number of 3-grams hit = 28  (0.72%)
Number of 2-grams hit = 4  (0.10%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle319.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 641 words.
Number of 4-grams hit = 630  (98.28%)
Number of 3-grams hit = 9  (1.40%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle320.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 405 words.
Number of 4-grams hit = 402  (99.26%)
Number of 3-grams hit = 1  (0.25%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle321.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1172 words.
Number of 4-grams hit = 1160  (98.98%)
Number of 3-grams hit = 9  (0.77%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle322.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 1851 words.
Number of 4-grams hit = 1831  (98.92%)
Number of 3-grams hit = 15  (0.81%)
Number of 2-grams hit = 4  (0.22%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle323.out
Perplexity = 3.28, Entropy = 1.72 bits
Computation based on 877 words.
Number of 4-grams hit = 870  (99.20%)
Number of 3-grams hit = 4  (0.46%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle324.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 1808 words.
Number of 4-grams hit = 1800  (99.56%)
Number of 3-grams hit = 6  (0.33%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle325.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 1207 words.
Number of 4-grams hit = 1191  (98.67%)
Number of 3-grams hit = 12  (0.99%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle326.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 655 words.
Number of 4-grams hit = 648  (98.93%)
Number of 3-grams hit = 5  (0.76%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle327.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 1223 words.
Number of 4-grams hit = 1219  (99.67%)
Number of 3-grams hit = 2  (0.16%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle328.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 277 words.
Number of 4-grams hit = 274  (98.92%)
Number of 3-grams hit = 1  (0.36%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle329.out
Perplexity = 4.07, Entropy = 2.02 bits
Computation based on 445 words.
Number of 4-grams hit = 435  (97.75%)
Number of 3-grams hit = 7  (1.57%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle330.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1141 words.
Number of 4-grams hit = 1125  (98.60%)
Number of 3-grams hit = 8  (0.70%)
Number of 2-grams hit = 6  (0.53%)
Number of 1-grams hit = 2  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle331.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 2113 words.
Number of 4-grams hit = 2102  (99.48%)
Number of 3-grams hit = 8  (0.38%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle332.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 626 words.
Number of 4-grams hit = 619  (98.88%)
Number of 3-grams hit = 5  (0.80%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle333.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 1432 words.
Number of 4-grams hit = 1412  (98.60%)
Number of 3-grams hit = 17  (1.19%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle334.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1057 words.
Number of 4-grams hit = 1052  (99.53%)
Number of 3-grams hit = 3  (0.28%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle335.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1850 words.
Number of 4-grams hit = 1835  (99.19%)
Number of 3-grams hit = 12  (0.65%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle336.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1239 words.
Number of 4-grams hit = 1223  (98.71%)
Number of 3-grams hit = 12  (0.97%)
Number of 2-grams hit = 3  (0.24%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle337.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 438 words.
Number of 4-grams hit = 434  (99.09%)
Number of 3-grams hit = 2  (0.46%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle338.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 3804 words.
Number of 4-grams hit = 3774  (99.21%)
Number of 3-grams hit = 23  (0.60%)
Number of 2-grams hit = 6  (0.16%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle339.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 408 words.
Number of 4-grams hit = 400  (98.04%)
Number of 3-grams hit = 5  (1.23%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle340.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1243 words.
Number of 4-grams hit = 1232  (99.12%)
Number of 3-grams hit = 8  (0.64%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle341.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1498 words.
Number of 4-grams hit = 1484  (99.07%)
Number of 3-grams hit = 10  (0.67%)
Number of 2-grams hit = 3  (0.20%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle342.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 1604 words.
Number of 4-grams hit = 1583  (98.69%)
Number of 3-grams hit = 17  (1.06%)
Number of 2-grams hit = 3  (0.19%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle343.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 522 words.
Number of 4-grams hit = 510  (97.70%)
Number of 3-grams hit = 9  (1.72%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle344.out
Perplexity = 3.02, Entropy = 1.60 bits
Computation based on 620 words.
Number of 4-grams hit = 615  (99.19%)
Number of 3-grams hit = 3  (0.48%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle345.out
Perplexity = 3.22, Entropy = 1.68 bits
Computation based on 3421 words.
Number of 4-grams hit = 3401  (99.42%)
Number of 3-grams hit = 16  (0.47%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle346.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1725 words.
Number of 4-grams hit = 1715  (99.42%)
Number of 3-grams hit = 8  (0.46%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle347.out
Perplexity = 3.04, Entropy = 1.60 bits
Computation based on 690 words.
Number of 4-grams hit = 683  (98.99%)
Number of 3-grams hit = 5  (0.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle348.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 2288 words.
Number of 4-grams hit = 2279  (99.61%)
Number of 3-grams hit = 7  (0.31%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle349.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 2145 words.
Number of 4-grams hit = 2124  (99.02%)
Number of 3-grams hit = 19  (0.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle350.out
Perplexity = 4.31, Entropy = 2.11 bits
Computation based on 400 words.
Number of 4-grams hit = 390  (97.50%)
Number of 3-grams hit = 7  (1.75%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle351.out
Perplexity = 2.96, Entropy = 1.57 bits
Computation based on 464 words.
Number of 4-grams hit = 461  (99.35%)
Number of 3-grams hit = 1  (0.22%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle352.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 488 words.
Number of 4-grams hit = 476  (97.54%)
Number of 3-grams hit = 10  (2.05%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle353.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 471 words.
Number of 4-grams hit = 463  (98.30%)
Number of 3-grams hit = 6  (1.27%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle354.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 551 words.
Number of 4-grams hit = 545  (98.91%)
Number of 3-grams hit = 3  (0.54%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle355.out
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 448 words.
Number of 4-grams hit = 440  (98.21%)
Number of 3-grams hit = 5  (1.12%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle356.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 743 words.
Number of 4-grams hit = 737  (99.19%)
Number of 3-grams hit = 4  (0.54%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle357.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 834 words.
Number of 4-grams hit = 826  (99.04%)
Number of 3-grams hit = 4  (0.48%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle358.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1334 words.
Number of 4-grams hit = 1322  (99.10%)
Number of 3-grams hit = 10  (0.75%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle359.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1380 words.
Number of 4-grams hit = 1374  (99.57%)
Number of 3-grams hit = 4  (0.29%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle360.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 760 words.
Number of 4-grams hit = 751  (98.82%)
Number of 3-grams hit = 6  (0.79%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle361.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 1178 words.
Number of 4-grams hit = 1170  (99.32%)
Number of 3-grams hit = 5  (0.42%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle362.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 1543 words.
Number of 4-grams hit = 1523  (98.70%)
Number of 3-grams hit = 17  (1.10%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle363.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 1251 words.
Number of 4-grams hit = 1243  (99.36%)
Number of 3-grams hit = 6  (0.48%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle364.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 345 words.
Number of 4-grams hit = 339  (98.26%)
Number of 3-grams hit = 4  (1.16%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle365.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 430 words.
Number of 4-grams hit = 421  (97.91%)
Number of 3-grams hit = 7  (1.63%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle366.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 390 words.
Number of 4-grams hit = 385  (98.72%)
Number of 3-grams hit = 3  (0.77%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle367.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1013 words.
Number of 4-grams hit = 1003  (99.01%)
Number of 3-grams hit = 7  (0.69%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle368.out
Perplexity = 4.50, Entropy = 2.17 bits
Computation based on 345 words.
Number of 4-grams hit = 339  (98.26%)
Number of 3-grams hit = 4  (1.16%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle369.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 352 words.
Number of 4-grams hit = 346  (98.30%)
Number of 3-grams hit = 4  (1.14%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle370.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 1781 words.
Number of 4-grams hit = 1770  (99.38%)
Number of 3-grams hit = 8  (0.45%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle371.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 280 words.
Number of 4-grams hit = 276  (98.57%)
Number of 3-grams hit = 2  (0.71%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle372.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 456 words.
Number of 4-grams hit = 450  (98.68%)
Number of 3-grams hit = 3  (0.66%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle373.out
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 546 words.
Number of 4-grams hit = 542  (99.27%)
Number of 3-grams hit = 2  (0.37%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle374.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1668 words.
Number of 4-grams hit = 1661  (99.58%)
Number of 3-grams hit = 5  (0.30%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle375.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 1770 words.
Number of 4-grams hit = 1759  (99.38%)
Number of 3-grams hit = 9  (0.51%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle376.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 621 words.
Number of 4-grams hit = 610  (98.23%)
Number of 3-grams hit = 8  (1.29%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle377.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 809 words.
Number of 4-grams hit = 802  (99.13%)
Number of 3-grams hit = 5  (0.62%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle378.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 671 words.
Number of 4-grams hit = 666  (99.25%)
Number of 3-grams hit = 3  (0.45%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle379.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 2784 words.
Number of 4-grams hit = 2759  (99.10%)
Number of 3-grams hit = 18  (0.65%)
Number of 2-grams hit = 5  (0.18%)
Number of 1-grams hit = 2  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle380.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 377 words.
Number of 4-grams hit = 373  (98.94%)
Number of 3-grams hit = 2  (0.53%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle381.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 700 words.
Number of 4-grams hit = 696  (99.43%)
Number of 3-grams hit = 2  (0.29%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle382.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1415 words.
Number of 4-grams hit = 1395  (98.59%)
Number of 3-grams hit = 13  (0.92%)
Number of 2-grams hit = 6  (0.42%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle383.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 1120 words.
Number of 4-grams hit = 1108  (98.93%)
Number of 3-grams hit = 10  (0.89%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle384.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1098 words.
Number of 4-grams hit = 1087  (99.00%)
Number of 3-grams hit = 9  (0.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle385.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 2797 words.
Number of 4-grams hit = 2777  (99.28%)
Number of 3-grams hit = 17  (0.61%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle386.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 442 words.
Number of 4-grams hit = 435  (98.42%)
Number of 3-grams hit = 4  (0.90%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle387.out
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 1354 words.
Number of 4-grams hit = 1342  (99.11%)
Number of 3-grams hit = 8  (0.59%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle388.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 4692 words.
Number of 4-grams hit = 4668  (99.49%)
Number of 3-grams hit = 22  (0.47%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle389.out
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 1197 words.
Number of 4-grams hit = 1180  (98.58%)
Number of 3-grams hit = 13  (1.09%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle390.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1281 words.
Number of 4-grams hit = 1271  (99.22%)
Number of 3-grams hit = 8  (0.62%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle391.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 3885 words.
Number of 4-grams hit = 3852  (99.15%)
Number of 3-grams hit = 26  (0.67%)
Number of 2-grams hit = 5  (0.13%)
Number of 1-grams hit = 2  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle392.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 256 words.
Number of 4-grams hit = 252  (98.44%)
Number of 3-grams hit = 2  (0.78%)
Number of 2-grams hit = 1  (0.39%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle393.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 508 words.
Number of 4-grams hit = 504  (99.21%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle394.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 417 words.
Number of 4-grams hit = 412  (98.80%)
Number of 3-grams hit = 3  (0.72%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle395.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 332 words.
Number of 4-grams hit = 324  (97.59%)
Number of 3-grams hit = 5  (1.51%)
Number of 2-grams hit = 2  (0.60%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle396.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 3378 words.
Number of 4-grams hit = 3354  (99.29%)
Number of 3-grams hit = 20  (0.59%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle397.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 4002 words.
Number of 4-grams hit = 3976  (99.35%)
Number of 3-grams hit = 23  (0.57%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle398.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 878 words.
Number of 4-grams hit = 871  (99.20%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle399.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 3969 words.
Number of 4-grams hit = 3928  (98.97%)
Number of 3-grams hit = 33  (0.83%)
Number of 2-grams hit = 6  (0.15%)
Number of 1-grams hit = 2  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle400.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 634 words.
Number of 4-grams hit = 627  (98.90%)
Number of 3-grams hit = 5  (0.79%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle401.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 721 words.
Number of 4-grams hit = 711  (98.61%)
Number of 3-grams hit = 5  (0.69%)
Number of 2-grams hit = 4  (0.55%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle402.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 3978 words.
Number of 4-grams hit = 3926  (98.69%)
Number of 3-grams hit = 46  (1.16%)
Number of 2-grams hit = 5  (0.13%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle403.out
Perplexity = 2.96, Entropy = 1.57 bits
Computation based on 668 words.
Number of 4-grams hit = 664  (99.40%)
Number of 3-grams hit = 2  (0.30%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle404.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 228 words.
Number of 4-grams hit = 225  (98.68%)
Number of 3-grams hit = 1  (0.44%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle405.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 172 words.
Number of 4-grams hit = 169  (98.26%)
Number of 3-grams hit = 1  (0.58%)
Number of 2-grams hit = 1  (0.58%)
Number of 1-grams hit = 1  (0.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle406.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1046 words.
Number of 4-grams hit = 1042  (99.62%)
Number of 3-grams hit = 2  (0.19%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle407.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 755 words.
Number of 4-grams hit = 746  (98.81%)
Number of 3-grams hit = 6  (0.79%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle408.out
Perplexity = 4.26, Entropy = 2.09 bits
Computation based on 469 words.
Number of 4-grams hit = 459  (97.87%)
Number of 3-grams hit = 7  (1.49%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle409.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 595 words.
Number of 4-grams hit = 589  (98.99%)
Number of 3-grams hit = 4  (0.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle410.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 261 words.
Number of 4-grams hit = 255  (97.70%)
Number of 3-grams hit = 3  (1.15%)
Number of 2-grams hit = 2  (0.77%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle411.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 465 words.
Number of 4-grams hit = 460  (98.92%)
Number of 3-grams hit = 3  (0.65%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle412.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 4945 words.
Number of 4-grams hit = 4897  (99.03%)
Number of 3-grams hit = 39  (0.79%)
Number of 2-grams hit = 8  (0.16%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle413.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 4175 words.
Number of 4-grams hit = 4137  (99.09%)
Number of 3-grams hit = 30  (0.72%)
Number of 2-grams hit = 7  (0.17%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle414.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 430 words.
Number of 4-grams hit = 423  (98.37%)
Number of 3-grams hit = 5  (1.16%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle415.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 509 words.
Number of 4-grams hit = 506  (99.41%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle416.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 635 words.
Number of 4-grams hit = 627  (98.74%)
Number of 3-grams hit = 6  (0.94%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle417.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 947 words.
Number of 4-grams hit = 940  (99.26%)
Number of 3-grams hit = 4  (0.42%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle418.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 863 words.
Number of 4-grams hit = 859  (99.54%)
Number of 3-grams hit = 2  (0.23%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle419.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 581 words.
Number of 4-grams hit = 575  (98.97%)
Number of 3-grams hit = 3  (0.52%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle420.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 667 words.
Number of 4-grams hit = 660  (98.95%)
Number of 3-grams hit = 5  (0.75%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle421.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 893 words.
Number of 4-grams hit = 886  (99.22%)
Number of 3-grams hit = 5  (0.56%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle422.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 602 words.
Number of 4-grams hit = 598  (99.34%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle423.out
Perplexity = 3.69, Entropy = 1.89 bits
Computation based on 652 words.
Number of 4-grams hit = 649  (99.54%)
Number of 3-grams hit = 1  (0.15%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle424.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 582 words.
Number of 4-grams hit = 578  (99.31%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle425.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 385 words.
Number of 4-grams hit = 380  (98.70%)
Number of 3-grams hit = 3  (0.78%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle426.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 537 words.
Number of 4-grams hit = 532  (99.07%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle427.out
Perplexity = 3.93, Entropy = 1.98 bits
Computation based on 844 words.
Number of 4-grams hit = 832  (98.58%)
Number of 3-grams hit = 10  (1.18%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle428.out
Perplexity = 2.94, Entropy = 1.56 bits
Computation based on 680 words.
Number of 4-grams hit = 676  (99.41%)
Number of 3-grams hit = 2  (0.29%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle429.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 812 words.
Number of 4-grams hit = 806  (99.26%)
Number of 3-grams hit = 3  (0.37%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle430.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 699 words.
Number of 4-grams hit = 696  (99.57%)
Number of 3-grams hit = 1  (0.14%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle431.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 612 words.
Number of 4-grams hit = 602  (98.37%)
Number of 3-grams hit = 5  (0.82%)
Number of 2-grams hit = 4  (0.65%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle432.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 388 words.
Number of 4-grams hit = 381  (98.20%)
Number of 3-grams hit = 4  (1.03%)
Number of 2-grams hit = 2  (0.52%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle433.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 1055 words.
Number of 4-grams hit = 1045  (99.05%)
Number of 3-grams hit = 6  (0.57%)
Number of 2-grams hit = 3  (0.28%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle434.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 414 words.
Number of 4-grams hit = 411  (99.28%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle435.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 797 words.
Number of 4-grams hit = 793  (99.50%)
Number of 3-grams hit = 2  (0.25%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle436.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 585 words.
Number of 4-grams hit = 577  (98.63%)
Number of 3-grams hit = 6  (1.03%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle437.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1077 words.
Number of 4-grams hit = 1068  (99.16%)
Number of 3-grams hit = 6  (0.56%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle438.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 318 words.
Number of 4-grams hit = 311  (97.80%)
Number of 3-grams hit = 4  (1.26%)
Number of 2-grams hit = 2  (0.63%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle439.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 569 words.
Number of 4-grams hit = 557  (97.89%)
Number of 3-grams hit = 10  (1.76%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle440.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 753 words.
Number of 4-grams hit = 746  (99.07%)
Number of 3-grams hit = 5  (0.66%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle441.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 439 words.
Number of 4-grams hit = 430  (97.95%)
Number of 3-grams hit = 6  (1.37%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle442.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 11076 words.
Number of 4-grams hit = 11006  (99.37%)
Number of 3-grams hit = 62  (0.56%)
Number of 2-grams hit = 7  (0.06%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle443.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 9530 words.
Number of 4-grams hit = 9477  (99.44%)
Number of 3-grams hit = 47  (0.49%)
Number of 2-grams hit = 5  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle444.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 703 words.
Number of 4-grams hit = 698  (99.29%)
Number of 3-grams hit = 3  (0.43%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle445.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 556 words.
Number of 4-grams hit = 551  (99.10%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle446.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 969 words.
Number of 4-grams hit = 963  (99.38%)
Number of 3-grams hit = 4  (0.41%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle447.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 787 words.
Number of 4-grams hit = 781  (99.24%)
Number of 3-grams hit = 4  (0.51%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle448.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 346 words.
Number of 4-grams hit = 339  (97.98%)
Number of 3-grams hit = 4  (1.16%)
Number of 2-grams hit = 2  (0.58%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle449.out
Perplexity = 2.86, Entropy = 1.51 bits
Computation based on 520 words.
Number of 4-grams hit = 516  (99.23%)
Number of 3-grams hit = 2  (0.38%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle450.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 386 words.
Number of 4-grams hit = 382  (98.96%)
Number of 3-grams hit = 2  (0.52%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle451.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 615 words.
Number of 4-grams hit = 610  (99.19%)
Number of 3-grams hit = 3  (0.49%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle452.out
Perplexity = 3.06, Entropy = 1.61 bits
Computation based on 262 words.
Number of 4-grams hit = 258  (98.47%)
Number of 3-grams hit = 2  (0.76%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle453.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 679 words.
Number of 4-grams hit = 673  (99.12%)
Number of 3-grams hit = 4  (0.59%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle454.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 598 words.
Number of 4-grams hit = 583  (97.49%)
Number of 3-grams hit = 9  (1.51%)
Number of 2-grams hit = 5  (0.84%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle455.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 405 words.
Number of 4-grams hit = 400  (98.77%)
Number of 3-grams hit = 3  (0.74%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle456.out
Perplexity = 2.98, Entropy = 1.58 bits
Computation based on 719 words.
Number of 4-grams hit = 716  (99.58%)
Number of 3-grams hit = 1  (0.14%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle457.out
Perplexity = 3.26, Entropy = 1.71 bits
Computation based on 366 words.
Number of 4-grams hit = 359  (98.09%)
Number of 3-grams hit = 4  (1.09%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle458.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 299 words.
Number of 4-grams hit = 294  (98.33%)
Number of 3-grams hit = 2  (0.67%)
Number of 2-grams hit = 2  (0.67%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle459.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 752 words.
Number of 4-grams hit = 744  (98.94%)
Number of 3-grams hit = 6  (0.80%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle460.out
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 354 words.
Number of 4-grams hit = 344  (97.18%)
Number of 3-grams hit = 8  (2.26%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle461.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 440 words.
Number of 4-grams hit = 437  (99.32%)
Number of 3-grams hit = 1  (0.23%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle462.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1537 words.
Number of 4-grams hit = 1512  (98.37%)
Number of 3-grams hit = 22  (1.43%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle463.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 610 words.
Number of 4-grams hit = 605  (99.18%)
Number of 3-grams hit = 3  (0.49%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle464.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 466 words.
Number of 4-grams hit = 459  (98.50%)
Number of 3-grams hit = 5  (1.07%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle465.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1935 words.
Number of 4-grams hit = 1920  (99.22%)
Number of 3-grams hit = 12  (0.62%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle466.out
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 225 words.
Number of 4-grams hit = 218  (96.89%)
Number of 3-grams hit = 4  (1.78%)
Number of 2-grams hit = 2  (0.89%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle467.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1127 words.
Number of 4-grams hit = 1118  (99.20%)
Number of 3-grams hit = 7  (0.62%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle468.out
Perplexity = 2.99, Entropy = 1.58 bits
Computation based on 843 words.
Number of 4-grams hit = 831  (98.58%)
Number of 3-grams hit = 6  (0.71%)
Number of 2-grams hit = 5  (0.59%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle469.out
Perplexity = 2.87, Entropy = 1.52 bits
Computation based on 381 words.
Number of 4-grams hit = 378  (99.21%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle470.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 259 words.
Number of 4-grams hit = 253  (97.68%)
Number of 3-grams hit = 2  (0.77%)
Number of 2-grams hit = 3  (1.16%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle471.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 2170 words.
Number of 4-grams hit = 2149  (99.03%)
Number of 3-grams hit = 14  (0.65%)
Number of 2-grams hit = 6  (0.28%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle472.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1013 words.
Number of 4-grams hit = 1000  (98.72%)
Number of 3-grams hit = 10  (0.99%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle473.out
Perplexity = 3.17, Entropy = 1.66 bits
Computation based on 247 words.
Number of 4-grams hit = 244  (98.79%)
Number of 3-grams hit = 1  (0.40%)
Number of 2-grams hit = 1  (0.40%)
Number of 1-grams hit = 1  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle474.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 838 words.
Number of 4-grams hit = 828  (98.81%)
Number of 3-grams hit = 6  (0.72%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle475.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 588 words.
Number of 4-grams hit = 582  (98.98%)
Number of 3-grams hit = 4  (0.68%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle476.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 626 words.
Number of 4-grams hit = 619  (98.88%)
Number of 3-grams hit = 4  (0.64%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle477.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 474 words.
Number of 4-grams hit = 468  (98.73%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle478.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 547 words.
Number of 4-grams hit = 540  (98.72%)
Number of 3-grams hit = 5  (0.91%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle479.out
Perplexity = 4.42, Entropy = 2.14 bits
Computation based on 458 words.
Number of 4-grams hit = 451  (98.47%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 3  (0.66%)
Number of 1-grams hit = 2  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle480.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 749 words.
Number of 4-grams hit = 739  (98.66%)
Number of 3-grams hit = 7  (0.93%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle481.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 379 words.
Number of 4-grams hit = 372  (98.15%)
Number of 3-grams hit = 5  (1.32%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle482.out
Perplexity = 3.10, Entropy = 1.63 bits
Computation based on 542 words.
Number of 4-grams hit = 536  (98.89%)
Number of 3-grams hit = 2  (0.37%)
Number of 2-grams hit = 3  (0.55%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle483.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 1863 words.
Number of 4-grams hit = 1853  (99.46%)
Number of 3-grams hit = 8  (0.43%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle484.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1569 words.
Number of 4-grams hit = 1559  (99.36%)
Number of 3-grams hit = 7  (0.45%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle485.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 402 words.
Number of 4-grams hit = 391  (97.26%)
Number of 3-grams hit = 9  (2.24%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle486.out
Perplexity = 4.27, Entropy = 2.09 bits
Computation based on 369 words.
Number of 4-grams hit = 358  (97.02%)
Number of 3-grams hit = 8  (2.17%)
Number of 2-grams hit = 2  (0.54%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle487.out
Perplexity = 4.01, Entropy = 2.01 bits
Computation based on 462 words.
Number of 4-grams hit = 455  (98.48%)
Number of 3-grams hit = 5  (1.08%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle488.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 770 words.
Number of 4-grams hit = 767  (99.61%)
Number of 3-grams hit = 1  (0.13%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle489.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 577 words.
Number of 4-grams hit = 574  (99.48%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle490.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 533 words.
Number of 4-grams hit = 528  (99.06%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle491.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1520 words.
Number of 4-grams hit = 1503  (98.88%)
Number of 3-grams hit = 12  (0.79%)
Number of 2-grams hit = 4  (0.26%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle492.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 473 words.
Number of 4-grams hit = 467  (98.73%)
Number of 3-grams hit = 4  (0.85%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle493.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 827 words.
Number of 4-grams hit = 820  (99.15%)
Number of 3-grams hit = 5  (0.60%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle494.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 540 words.
Number of 4-grams hit = 536  (99.26%)
Number of 3-grams hit = 2  (0.37%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle495.out
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 454 words.
Number of 4-grams hit = 445  (98.02%)
Number of 3-grams hit = 6  (1.32%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle496.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 608 words.
Number of 4-grams hit = 603  (99.18%)
Number of 3-grams hit = 3  (0.49%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle497.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 711 words.
Number of 4-grams hit = 706  (99.30%)
Number of 3-grams hit = 3  (0.42%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle498.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 1824 words.
Number of 4-grams hit = 1807  (99.07%)
Number of 3-grams hit = 11  (0.60%)
Number of 2-grams hit = 5  (0.27%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle499.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 542 words.
Number of 4-grams hit = 535  (98.71%)
Number of 3-grams hit = 5  (0.92%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle500.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 486 words.
Number of 4-grams hit = 481  (98.97%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle501.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 448 words.
Number of 4-grams hit = 440  (98.21%)
Number of 3-grams hit = 5  (1.12%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle502.out
Perplexity = 3.01, Entropy = 1.59 bits
Computation based on 535 words.
Number of 4-grams hit = 532  (99.44%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle503.out
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 1109 words.
Number of 4-grams hit = 1096  (98.83%)
Number of 3-grams hit = 10  (0.90%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle504.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 658 words.
Number of 4-grams hit = 652  (99.09%)
Number of 3-grams hit = 4  (0.61%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle505.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 768 words.
Number of 4-grams hit = 758  (98.70%)
Number of 3-grams hit = 8  (1.04%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle506.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 594 words.
Number of 4-grams hit = 587  (98.82%)
Number of 3-grams hit = 5  (0.84%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle507.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1702 words.
Number of 4-grams hit = 1686  (99.06%)
Number of 3-grams hit = 13  (0.76%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle508.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 700 words.
Number of 4-grams hit = 696  (99.43%)
Number of 3-grams hit = 2  (0.29%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle509.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 383 words.
Number of 4-grams hit = 375  (97.91%)
Number of 3-grams hit = 4  (1.04%)
Number of 2-grams hit = 3  (0.78%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle510.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1032 words.
Number of 4-grams hit = 1021  (98.93%)
Number of 3-grams hit = 7  (0.68%)
Number of 2-grams hit = 3  (0.29%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle511.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 677 words.
Number of 4-grams hit = 671  (99.11%)
Number of 3-grams hit = 4  (0.59%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle512.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 2057 words.
Number of 4-grams hit = 2046  (99.47%)
Number of 3-grams hit = 8  (0.39%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle513.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 448 words.
Number of 4-grams hit = 442  (98.66%)
Number of 3-grams hit = 4  (0.89%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle514.out
Perplexity = 3.05, Entropy = 1.61 bits
Computation based on 422 words.
Number of 4-grams hit = 419  (99.29%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle515.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1070 words.
Number of 4-grams hit = 1064  (99.44%)
Number of 3-grams hit = 3  (0.28%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle516.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 440 words.
Number of 4-grams hit = 436  (99.09%)
Number of 3-grams hit = 2  (0.45%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle517.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1145 words.
Number of 4-grams hit = 1135  (99.13%)
Number of 3-grams hit = 8  (0.70%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle518.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 639 words.
Number of 4-grams hit = 634  (99.22%)
Number of 3-grams hit = 3  (0.47%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle519.out
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 592 words.
Number of 4-grams hit = 582  (98.31%)
Number of 3-grams hit = 8  (1.35%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle520.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 663 words.
Number of 4-grams hit = 659  (99.40%)
Number of 3-grams hit = 2  (0.30%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle521.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 548 words.
Number of 4-grams hit = 540  (98.54%)
Number of 3-grams hit = 6  (1.09%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle522.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1741 words.
Number of 4-grams hit = 1732  (99.48%)
Number of 3-grams hit = 7  (0.40%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle523.out
Perplexity = 4.43, Entropy = 2.15 bits
Computation based on 354 words.
Number of 4-grams hit = 343  (96.89%)
Number of 3-grams hit = 6  (1.69%)
Number of 2-grams hit = 4  (1.13%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle524.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1488 words.
Number of 4-grams hit = 1462  (98.25%)
Number of 3-grams hit = 21  (1.41%)
Number of 2-grams hit = 4  (0.27%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle525.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 2121 words.
Number of 4-grams hit = 2102  (99.10%)
Number of 3-grams hit = 16  (0.75%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle526.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 2067 words.
Number of 4-grams hit = 2057  (99.52%)
Number of 3-grams hit = 7  (0.34%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle527.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 793 words.
Number of 4-grams hit = 788  (99.37%)
Number of 3-grams hit = 3  (0.38%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle528.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 788 words.
Number of 4-grams hit = 779  (98.86%)
Number of 3-grams hit = 7  (0.89%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle529.out
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 957 words.
Number of 4-grams hit = 949  (99.16%)
Number of 3-grams hit = 4  (0.42%)
Number of 2-grams hit = 3  (0.31%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle530.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 451 words.
Number of 4-grams hit = 446  (98.89%)
Number of 3-grams hit = 3  (0.67%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle531.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1046 words.
Number of 4-grams hit = 1036  (99.04%)
Number of 3-grams hit = 5  (0.48%)
Number of 2-grams hit = 4  (0.38%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle532.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1019 words.
Number of 4-grams hit = 1011  (99.21%)
Number of 3-grams hit = 6  (0.59%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle533.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1210 words.
Number of 4-grams hit = 1200  (99.17%)
Number of 3-grams hit = 8  (0.66%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle534.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 549 words.
Number of 4-grams hit = 545  (99.27%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle535.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 375 words.
Number of 4-grams hit = 371  (98.93%)
Number of 3-grams hit = 2  (0.53%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle536.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1602 words.
Number of 4-grams hit = 1594  (99.50%)
Number of 3-grams hit = 6  (0.37%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle537.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1098 words.
Number of 4-grams hit = 1090  (99.27%)
Number of 3-grams hit = 5  (0.46%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle538.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 3722 words.
Number of 4-grams hit = 3699  (99.38%)
Number of 3-grams hit = 19  (0.51%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle539.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 581 words.
Number of 4-grams hit = 577  (99.31%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle540.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 2359 words.
Number of 4-grams hit = 2338  (99.11%)
Number of 3-grams hit = 19  (0.81%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle541.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 544 words.
Number of 4-grams hit = 539  (99.08%)
Number of 3-grams hit = 3  (0.55%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle542.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1758 words.
Number of 4-grams hit = 1752  (99.66%)
Number of 3-grams hit = 4  (0.23%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle543.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 592 words.
Number of 4-grams hit = 589  (99.49%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle544.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1399 words.
Number of 4-grams hit = 1384  (98.93%)
Number of 3-grams hit = 10  (0.71%)
Number of 2-grams hit = 4  (0.29%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle545.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 802 words.
Number of 4-grams hit = 793  (98.88%)
Number of 3-grams hit = 6  (0.75%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle546.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 753 words.
Number of 4-grams hit = 746  (99.07%)
Number of 3-grams hit = 5  (0.66%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle547.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 953 words.
Number of 4-grams hit = 941  (98.74%)
Number of 3-grams hit = 8  (0.84%)
Number of 2-grams hit = 3  (0.31%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle548.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 671 words.
Number of 4-grams hit = 658  (98.06%)
Number of 3-grams hit = 8  (1.19%)
Number of 2-grams hit = 3  (0.45%)
Number of 1-grams hit = 2  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle549.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 387 words.
Number of 4-grams hit = 380  (98.19%)
Number of 3-grams hit = 5  (1.29%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle550.out
Perplexity = 4.34, Entropy = 2.12 bits
Computation based on 204 words.
Number of 4-grams hit = 198  (97.06%)
Number of 3-grams hit = 4  (1.96%)
Number of 2-grams hit = 1  (0.49%)
Number of 1-grams hit = 1  (0.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle551.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 996 words.
Number of 4-grams hit = 985  (98.90%)
Number of 3-grams hit = 8  (0.80%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle552.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1143 words.
Number of 4-grams hit = 1135  (99.30%)
Number of 3-grams hit = 6  (0.52%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle553.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1347 words.
Number of 4-grams hit = 1334  (99.03%)
Number of 3-grams hit = 11  (0.82%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle554.out
Perplexity = 3.54, Entropy = 1.83 bits
Computation based on 4358 words.
Number of 4-grams hit = 4314  (98.99%)
Number of 3-grams hit = 35  (0.80%)
Number of 2-grams hit = 8  (0.18%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle555.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 544 words.
Number of 4-grams hit = 533  (97.98%)
Number of 3-grams hit = 7  (1.29%)
Number of 2-grams hit = 3  (0.55%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle556.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 605 words.
Number of 4-grams hit = 597  (98.68%)
Number of 3-grams hit = 5  (0.83%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle557.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 418 words.
Number of 4-grams hit = 412  (98.56%)
Number of 3-grams hit = 3  (0.72%)
Number of 2-grams hit = 2  (0.48%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle558.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 1850 words.
Number of 4-grams hit = 1837  (99.30%)
Number of 3-grams hit = 9  (0.49%)
Number of 2-grams hit = 3  (0.16%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle559.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 694 words.
Number of 4-grams hit = 684  (98.56%)
Number of 3-grams hit = 6  (0.86%)
Number of 2-grams hit = 3  (0.43%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle560.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 595 words.
Number of 4-grams hit = 589  (98.99%)
Number of 3-grams hit = 4  (0.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle561.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 362 words.
Number of 4-grams hit = 357  (98.62%)
Number of 3-grams hit = 3  (0.83%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle562.out
Perplexity = 4.24, Entropy = 2.08 bits
Computation based on 331 words.
Number of 4-grams hit = 326  (98.49%)
Number of 3-grams hit = 3  (0.91%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle563.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 524 words.
Number of 4-grams hit = 516  (98.47%)
Number of 3-grams hit = 6  (1.15%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle564.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1451 words.
Number of 4-grams hit = 1442  (99.38%)
Number of 3-grams hit = 6  (0.41%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle565.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1392 words.
Number of 4-grams hit = 1382  (99.28%)
Number of 3-grams hit = 8  (0.57%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle566.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 4543 words.
Number of 4-grams hit = 4511  (99.30%)
Number of 3-grams hit = 25  (0.55%)
Number of 2-grams hit = 5  (0.11%)
Number of 1-grams hit = 2  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle567.out
Perplexity = 4.20, Entropy = 2.07 bits
Computation based on 596 words.
Number of 4-grams hit = 588  (98.66%)
Number of 3-grams hit = 6  (1.01%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle568.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 355 words.
Number of 4-grams hit = 351  (98.87%)
Number of 3-grams hit = 2  (0.56%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle569.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 4706 words.
Number of 4-grams hit = 4674  (99.32%)
Number of 3-grams hit = 27  (0.57%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle570.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 897 words.
Number of 4-grams hit = 890  (99.22%)
Number of 3-grams hit = 5  (0.56%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle571.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 576 words.
Number of 4-grams hit = 573  (99.48%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle572.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 619 words.
Number of 4-grams hit = 612  (98.87%)
Number of 3-grams hit = 5  (0.81%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle573.out
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 279 words.
Number of 4-grams hit = 273  (97.85%)
Number of 3-grams hit = 2  (0.72%)
Number of 2-grams hit = 3  (1.08%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle574.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1179 words.
Number of 4-grams hit = 1163  (98.64%)
Number of 3-grams hit = 12  (1.02%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle575.out
Perplexity = 2.91, Entropy = 1.54 bits
Computation based on 643 words.
Number of 4-grams hit = 639  (99.38%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle576.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 489 words.
Number of 4-grams hit = 484  (98.98%)
Number of 3-grams hit = 2  (0.41%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle577.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 405 words.
Number of 4-grams hit = 400  (98.77%)
Number of 3-grams hit = 3  (0.74%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle578.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 447 words.
Number of 4-grams hit = 441  (98.66%)
Number of 3-grams hit = 3  (0.67%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle579.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 479 words.
Number of 4-grams hit = 473  (98.75%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle580.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 585 words.
Number of 4-grams hit = 581  (99.32%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle581.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 584 words.
Number of 4-grams hit = 579  (99.14%)
Number of 3-grams hit = 3  (0.51%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle582.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 547 words.
Number of 4-grams hit = 542  (99.09%)
Number of 3-grams hit = 3  (0.55%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle583.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 722 words.
Number of 4-grams hit = 718  (99.45%)
Number of 3-grams hit = 2  (0.28%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle584.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 477 words.
Number of 4-grams hit = 470  (98.53%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle585.out
Perplexity = 3.05, Entropy = 1.61 bits
Computation based on 1036 words.
Number of 4-grams hit = 1030  (99.42%)
Number of 3-grams hit = 4  (0.39%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle586.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 782 words.
Number of 4-grams hit = 774  (98.98%)
Number of 3-grams hit = 5  (0.64%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle587.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 576 words.
Number of 4-grams hit = 571  (99.13%)
Number of 3-grams hit = 3  (0.52%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle588.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 1045 words.
Number of 4-grams hit = 1036  (99.14%)
Number of 3-grams hit = 7  (0.67%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle589.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 538 words.
Number of 4-grams hit = 534  (99.26%)
Number of 3-grams hit = 2  (0.37%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle590.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1468 words.
Number of 4-grams hit = 1458  (99.32%)
Number of 3-grams hit = 8  (0.54%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle591.out
Perplexity = 3.88, Entropy = 1.95 bits
Computation based on 749 words.
Number of 4-grams hit = 745  (99.47%)
Number of 3-grams hit = 2  (0.27%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle592.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 367 words.
Number of 4-grams hit = 363  (98.91%)
Number of 3-grams hit = 2  (0.54%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle593.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 409 words.
Number of 4-grams hit = 402  (98.29%)
Number of 3-grams hit = 4  (0.98%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle594.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 653 words.
Number of 4-grams hit = 649  (99.39%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle595.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 398 words.
Number of 4-grams hit = 392  (98.49%)
Number of 3-grams hit = 3  (0.75%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle596.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1041 words.
Number of 4-grams hit = 1028  (98.75%)
Number of 3-grams hit = 9  (0.86%)
Number of 2-grams hit = 3  (0.29%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle597.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1109 words.
Number of 4-grams hit = 1103  (99.46%)
Number of 3-grams hit = 4  (0.36%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle598.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 327 words.
Number of 4-grams hit = 319  (97.55%)
Number of 3-grams hit = 5  (1.53%)
Number of 2-grams hit = 2  (0.61%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle599.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 879 words.
Number of 4-grams hit = 872  (99.20%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle600.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 5209 words.
Number of 4-grams hit = 5175  (99.35%)
Number of 3-grams hit = 29  (0.56%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle601.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1181 words.
Number of 4-grams hit = 1169  (98.98%)
Number of 3-grams hit = 8  (0.68%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle602.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 652 words.
Number of 4-grams hit = 645  (98.93%)
Number of 3-grams hit = 5  (0.77%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle603.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 94 words.
Number of 4-grams hit = 91  (96.81%)
Number of 3-grams hit = 1  (1.06%)
Number of 2-grams hit = 1  (1.06%)
Number of 1-grams hit = 1  (1.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle604.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 612 words.
Number of 4-grams hit = 605  (98.86%)
Number of 3-grams hit = 5  (0.82%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle605.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 3406 words.
Number of 4-grams hit = 3386  (99.41%)
Number of 3-grams hit = 15  (0.44%)
Number of 2-grams hit = 4  (0.12%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle606.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 978 words.
Number of 4-grams hit = 971  (99.28%)
Number of 3-grams hit = 5  (0.51%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle607.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1157 words.
Number of 4-grams hit = 1151  (99.48%)
Number of 3-grams hit = 4  (0.35%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle608.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 674 words.
Number of 4-grams hit = 667  (98.96%)
Number of 3-grams hit = 4  (0.59%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle609.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 4849 words.
Number of 4-grams hit = 4816  (99.32%)
Number of 3-grams hit = 29  (0.60%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle610.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 6480 words.
Number of 4-grams hit = 6435  (99.31%)
Number of 3-grams hit = 40  (0.62%)
Number of 2-grams hit = 4  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle611.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 856 words.
Number of 4-grams hit = 842  (98.36%)
Number of 3-grams hit = 11  (1.29%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle612.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1757 words.
Number of 4-grams hit = 1744  (99.26%)
Number of 3-grams hit = 9  (0.51%)
Number of 2-grams hit = 3  (0.17%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle613.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 4584 words.
Number of 4-grams hit = 4563  (99.54%)
Number of 3-grams hit = 17  (0.37%)
Number of 2-grams hit = 3  (0.07%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle614.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1629 words.
Number of 4-grams hit = 1614  (99.08%)
Number of 3-grams hit = 11  (0.68%)
Number of 2-grams hit = 3  (0.18%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle615.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 494 words.
Number of 4-grams hit = 485  (98.18%)
Number of 3-grams hit = 6  (1.21%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle616.out
Perplexity = 2.93, Entropy = 1.55 bits
Computation based on 827 words.
Number of 4-grams hit = 819  (99.03%)
Number of 3-grams hit = 6  (0.73%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle617.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1418 words.
Number of 4-grams hit = 1411  (99.51%)
Number of 3-grams hit = 5  (0.35%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle618.out
Perplexity = 3.45, Entropy = 1.78 bits
Computation based on 4055 words.
Number of 4-grams hit = 4016  (99.04%)
Number of 3-grams hit = 34  (0.84%)
Number of 2-grams hit = 4  (0.10%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle619.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 853 words.
Number of 4-grams hit = 840  (98.48%)
Number of 3-grams hit = 7  (0.82%)
Number of 2-grams hit = 5  (0.59%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle620.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 5770 words.
Number of 4-grams hit = 5727  (99.25%)
Number of 3-grams hit = 35  (0.61%)
Number of 2-grams hit = 7  (0.12%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle621.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 347 words.
Number of 4-grams hit = 341  (98.27%)
Number of 3-grams hit = 3  (0.86%)
Number of 2-grams hit = 2  (0.58%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle622.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 503 words.
Number of 4-grams hit = 497  (98.81%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 2  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle623.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1430 words.
Number of 4-grams hit = 1422  (99.44%)
Number of 3-grams hit = 6  (0.42%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle624.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 5610 words.
Number of 4-grams hit = 5572  (99.32%)
Number of 3-grams hit = 29  (0.52%)
Number of 2-grams hit = 7  (0.12%)
Number of 1-grams hit = 2  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle625.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 1191 words.
Number of 4-grams hit = 1173  (98.49%)
Number of 3-grams hit = 15  (1.26%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle626.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 483 words.
Number of 4-grams hit = 479  (99.17%)
Number of 3-grams hit = 2  (0.41%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle627.out
Perplexity = 3.02, Entropy = 1.59 bits
Computation based on 159 words.
Number of 4-grams hit = 156  (98.11%)
Number of 3-grams hit = 1  (0.63%)
Number of 2-grams hit = 1  (0.63%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle628.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1142 words.
Number of 4-grams hit = 1125  (98.51%)
Number of 3-grams hit = 15  (1.31%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle629.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1114 words.
Number of 4-grams hit = 1103  (99.01%)
Number of 3-grams hit = 8  (0.72%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle630.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 349 words.
Number of 4-grams hit = 346  (99.14%)
Number of 3-grams hit = 1  (0.29%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle631.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 931 words.
Number of 4-grams hit = 923  (99.14%)
Number of 3-grams hit = 6  (0.64%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle632.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 439 words.
Number of 4-grams hit = 428  (97.49%)
Number of 3-grams hit = 8  (1.82%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle633.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 579 words.
Number of 4-grams hit = 572  (98.79%)
Number of 3-grams hit = 5  (0.86%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle634.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 603 words.
Number of 4-grams hit = 598  (99.17%)
Number of 3-grams hit = 3  (0.50%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle635.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 169 words.
Number of 4-grams hit = 166  (98.22%)
Number of 3-grams hit = 1  (0.59%)
Number of 2-grams hit = 1  (0.59%)
Number of 1-grams hit = 1  (0.59%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle636.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1435 words.
Number of 4-grams hit = 1424  (99.23%)
Number of 3-grams hit = 8  (0.56%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle637.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 1118 words.
Number of 4-grams hit = 1102  (98.57%)
Number of 3-grams hit = 10  (0.89%)
Number of 2-grams hit = 5  (0.45%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle638.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 586 words.
Number of 4-grams hit = 580  (98.98%)
Number of 3-grams hit = 4  (0.68%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle639.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 561 words.
Number of 4-grams hit = 555  (98.93%)
Number of 3-grams hit = 4  (0.71%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle640.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 227 words.
Number of 4-grams hit = 224  (98.68%)
Number of 3-grams hit = 1  (0.44%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle641.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 888 words.
Number of 4-grams hit = 884  (99.55%)
Number of 3-grams hit = 2  (0.23%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle642.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 477 words.
Number of 4-grams hit = 474  (99.37%)
Number of 3-grams hit = 1  (0.21%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle643.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 522 words.
Number of 4-grams hit = 516  (98.85%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle644.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1207 words.
Number of 4-grams hit = 1198  (99.25%)
Number of 3-grams hit = 6  (0.50%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle645.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 475 words.
Number of 4-grams hit = 472  (99.37%)
Number of 3-grams hit = 1  (0.21%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle646.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 615 words.
Number of 4-grams hit = 611  (99.35%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle647.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 653 words.
Number of 4-grams hit = 645  (98.77%)
Number of 3-grams hit = 6  (0.92%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle648.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 511 words.
Number of 4-grams hit = 506  (99.02%)
Number of 3-grams hit = 3  (0.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle649.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 577 words.
Number of 4-grams hit = 573  (99.31%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle650.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 579 words.
Number of 4-grams hit = 575  (99.31%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle651.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 503 words.
Number of 4-grams hit = 494  (98.21%)
Number of 3-grams hit = 6  (1.19%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle652.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 956 words.
Number of 4-grams hit = 935  (97.80%)
Number of 3-grams hit = 16  (1.67%)
Number of 2-grams hit = 4  (0.42%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle653.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 848 words.
Number of 4-grams hit = 843  (99.41%)
Number of 3-grams hit = 3  (0.35%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle654.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 827 words.
Number of 4-grams hit = 816  (98.67%)
Number of 3-grams hit = 8  (0.97%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle655.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 547 words.
Number of 4-grams hit = 542  (99.09%)
Number of 3-grams hit = 3  (0.55%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle656.out
Perplexity = 4.26, Entropy = 2.09 bits
Computation based on 501 words.
Number of 4-grams hit = 496  (99.00%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle657.out
Perplexity = 3.10, Entropy = 1.63 bits
Computation based on 412 words.
Number of 4-grams hit = 409  (99.27%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle658.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1097 words.
Number of 4-grams hit = 1086  (99.00%)
Number of 3-grams hit = 7  (0.64%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle659.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1082 words.
Number of 4-grams hit = 1071  (98.98%)
Number of 3-grams hit = 9  (0.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle660.out
Perplexity = 2.36, Entropy = 1.24 bits
Computation based on 151 words.
Number of 4-grams hit = 148  (98.01%)
Number of 3-grams hit = 1  (0.66%)
Number of 2-grams hit = 1  (0.66%)
Number of 1-grams hit = 1  (0.66%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle661.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 1330 words.
Number of 4-grams hit = 1319  (99.17%)
Number of 3-grams hit = 7  (0.53%)
Number of 2-grams hit = 3  (0.23%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle662.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 131 words.
Number of 4-grams hit = 127  (96.95%)
Number of 3-grams hit = 2  (1.53%)
Number of 2-grams hit = 1  (0.76%)
Number of 1-grams hit = 1  (0.76%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle663.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1445 words.
Number of 4-grams hit = 1433  (99.17%)
Number of 3-grams hit = 7  (0.48%)
Number of 2-grams hit = 4  (0.28%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle664.out
Perplexity = 4.37, Entropy = 2.13 bits
Computation based on 770 words.
Number of 4-grams hit = 754  (97.92%)
Number of 3-grams hit = 12  (1.56%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle665.out
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 830 words.
Number of 4-grams hit = 819  (98.67%)
Number of 3-grams hit = 8  (0.96%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle666.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 413 words.
Number of 4-grams hit = 407  (98.55%)
Number of 3-grams hit = 4  (0.97%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle667.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1455 words.
Number of 4-grams hit = 1438  (98.83%)
Number of 3-grams hit = 13  (0.89%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle668.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 504 words.
Number of 4-grams hit = 498  (98.81%)
Number of 3-grams hit = 4  (0.79%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle669.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 524 words.
Number of 4-grams hit = 520  (99.24%)
Number of 3-grams hit = 2  (0.38%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle670.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 481 words.
Number of 4-grams hit = 475  (98.75%)
Number of 3-grams hit = 4  (0.83%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle671.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 693 words.
Number of 4-grams hit = 685  (98.85%)
Number of 3-grams hit = 5  (0.72%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle672.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 316 words.
Number of 4-grams hit = 313  (99.05%)
Number of 3-grams hit = 1  (0.32%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle673.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 336 words.
Number of 4-grams hit = 332  (98.81%)
Number of 3-grams hit = 2  (0.60%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle674.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 159 words.
Number of 4-grams hit = 156  (98.11%)
Number of 3-grams hit = 1  (0.63%)
Number of 2-grams hit = 1  (0.63%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle675.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 4196 words.
Number of 4-grams hit = 4178  (99.57%)
Number of 3-grams hit = 15  (0.36%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle676.out
Perplexity = 2.83, Entropy = 1.50 bits
Computation based on 531 words.
Number of 4-grams hit = 528  (99.44%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle677.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 684 words.
Number of 4-grams hit = 673  (98.39%)
Number of 3-grams hit = 6  (0.88%)
Number of 2-grams hit = 4  (0.58%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle678.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 373 words.
Number of 4-grams hit = 369  (98.93%)
Number of 3-grams hit = 2  (0.54%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle679.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1075 words.
Number of 4-grams hit = 1062  (98.79%)
Number of 3-grams hit = 10  (0.93%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle680.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 356 words.
Number of 4-grams hit = 352  (98.88%)
Number of 3-grams hit = 2  (0.56%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle681.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 522 words.
Number of 4-grams hit = 513  (98.28%)
Number of 3-grams hit = 5  (0.96%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle682.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 4014 words.
Number of 4-grams hit = 3988  (99.35%)
Number of 3-grams hit = 23  (0.57%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle683.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 279 words.
Number of 4-grams hit = 274  (98.21%)
Number of 3-grams hit = 3  (1.08%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle684.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 4719 words.
Number of 4-grams hit = 4680  (99.17%)
Number of 3-grams hit = 34  (0.72%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle685.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 485 words.
Number of 4-grams hit = 480  (98.97%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle686.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 2419 words.
Number of 4-grams hit = 2398  (99.13%)
Number of 3-grams hit = 16  (0.66%)
Number of 2-grams hit = 4  (0.17%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle687.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 556 words.
Number of 4-grams hit = 550  (98.92%)
Number of 3-grams hit = 4  (0.72%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle688.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 332 words.
Number of 4-grams hit = 329  (99.10%)
Number of 3-grams hit = 1  (0.30%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle689.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 554 words.
Number of 4-grams hit = 551  (99.46%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle690.out
Perplexity = 3.54, Entropy = 1.83 bits
Computation based on 824 words.
Number of 4-grams hit = 816  (99.03%)
Number of 3-grams hit = 6  (0.73%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle691.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 138 words.
Number of 4-grams hit = 135  (97.83%)
Number of 3-grams hit = 1  (0.72%)
Number of 2-grams hit = 1  (0.72%)
Number of 1-grams hit = 1  (0.72%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle692.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 460 words.
Number of 4-grams hit = 455  (98.91%)
Number of 3-grams hit = 3  (0.65%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle693.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 1025 words.
Number of 4-grams hit = 1011  (98.63%)
Number of 3-grams hit = 11  (1.07%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle694.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 669 words.
Number of 4-grams hit = 664  (99.25%)
Number of 3-grams hit = 3  (0.45%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle695.out
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 924 words.
Number of 4-grams hit = 918  (99.35%)
Number of 3-grams hit = 4  (0.43%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle696.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 541 words.
Number of 4-grams hit = 538  (99.45%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle697.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 1021 words.
Number of 4-grams hit = 1010  (98.92%)
Number of 3-grams hit = 8  (0.78%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle698.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 630 words.
Number of 4-grams hit = 626  (99.37%)
Number of 3-grams hit = 2  (0.32%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle699.out
Perplexity = 4.07, Entropy = 2.02 bits
Computation based on 377 words.
Number of 4-grams hit = 365  (96.82%)
Number of 3-grams hit = 10  (2.65%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle700.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 3659 words.
Number of 4-grams hit = 3623  (99.02%)
Number of 3-grams hit = 31  (0.85%)
Number of 2-grams hit = 4  (0.11%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle701.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 878 words.
Number of 4-grams hit = 870  (99.09%)
Number of 3-grams hit = 6  (0.68%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle702.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 2030 words.
Number of 4-grams hit = 2020  (99.51%)
Number of 3-grams hit = 8  (0.39%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle703.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 470 words.
Number of 4-grams hit = 460  (97.87%)
Number of 3-grams hit = 6  (1.28%)
Number of 2-grams hit = 3  (0.64%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle704.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 341 words.
Number of 4-grams hit = 337  (98.83%)
Number of 3-grams hit = 2  (0.59%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle705.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1100 words.
Number of 4-grams hit = 1089  (99.00%)
Number of 3-grams hit = 6  (0.55%)
Number of 2-grams hit = 4  (0.36%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle706.out
Perplexity = 3.00, Entropy = 1.59 bits
Computation based on 509 words.
Number of 4-grams hit = 504  (99.02%)
Number of 3-grams hit = 3  (0.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle707.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 941 words.
Number of 4-grams hit = 931  (98.94%)
Number of 3-grams hit = 8  (0.85%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle708.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 562 words.
Number of 4-grams hit = 558  (99.29%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle709.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 602 words.
Number of 4-grams hit = 596  (99.00%)
Number of 3-grams hit = 3  (0.50%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle710.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 823 words.
Number of 4-grams hit = 815  (99.03%)
Number of 3-grams hit = 5  (0.61%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle711.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 1017 words.
Number of 4-grams hit = 1010  (99.31%)
Number of 3-grams hit = 5  (0.49%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle712.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 406 words.
Number of 4-grams hit = 396  (97.54%)
Number of 3-grams hit = 7  (1.72%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle713.out
Perplexity = 4.35, Entropy = 2.12 bits
Computation based on 971 words.
Number of 4-grams hit = 956  (98.46%)
Number of 3-grams hit = 10  (1.03%)
Number of 2-grams hit = 4  (0.41%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle714.out
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 404 words.
Number of 4-grams hit = 395  (97.77%)
Number of 3-grams hit = 6  (1.49%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle715.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 525 words.
Number of 4-grams hit = 522  (99.43%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle716.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 521 words.
Number of 4-grams hit = 512  (98.27%)
Number of 3-grams hit = 7  (1.34%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle717.out
Perplexity = 3.93, Entropy = 1.98 bits
Computation based on 403 words.
Number of 4-grams hit = 395  (98.01%)
Number of 3-grams hit = 6  (1.49%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle718.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 7638 words.
Number of 4-grams hit = 7584  (99.29%)
Number of 3-grams hit = 45  (0.59%)
Number of 2-grams hit = 7  (0.09%)
Number of 1-grams hit = 2  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle719.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1275 words.
Number of 4-grams hit = 1269  (99.53%)
Number of 3-grams hit = 4  (0.31%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle720.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 872 words.
Number of 4-grams hit = 864  (99.08%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle721.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 452 words.
Number of 4-grams hit = 443  (98.01%)
Number of 3-grams hit = 7  (1.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle722.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 1116 words.
Number of 4-grams hit = 1105  (99.01%)
Number of 3-grams hit = 9  (0.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle723.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 472 words.
Number of 4-grams hit = 465  (98.52%)
Number of 3-grams hit = 5  (1.06%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle724.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 500 words.
Number of 4-grams hit = 489  (97.80%)
Number of 3-grams hit = 8  (1.60%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle725.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 792 words.
Number of 4-grams hit = 783  (98.86%)
Number of 3-grams hit = 7  (0.88%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle726.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 573 words.
Number of 4-grams hit = 569  (99.30%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle727.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 519 words.
Number of 4-grams hit = 514  (99.04%)
Number of 3-grams hit = 3  (0.58%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle728.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 453 words.
Number of 4-grams hit = 447  (98.68%)
Number of 3-grams hit = 3  (0.66%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle729.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 468 words.
Number of 4-grams hit = 465  (99.36%)
Number of 3-grams hit = 1  (0.21%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle730.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 5476 words.
Number of 4-grams hit = 5435  (99.25%)
Number of 3-grams hit = 33  (0.60%)
Number of 2-grams hit = 7  (0.13%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle731.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1562 words.
Number of 4-grams hit = 1545  (98.91%)
Number of 3-grams hit = 14  (0.90%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle732.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 769 words.
Number of 4-grams hit = 754  (98.05%)
Number of 3-grams hit = 12  (1.56%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle733.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 609 words.
Number of 4-grams hit = 601  (98.69%)
Number of 3-grams hit = 5  (0.82%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle734.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 450 words.
Number of 4-grams hit = 447  (99.33%)
Number of 3-grams hit = 1  (0.22%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle735.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 1274 words.
Number of 4-grams hit = 1251  (98.19%)
Number of 3-grams hit = 14  (1.10%)
Number of 2-grams hit = 6  (0.47%)
Number of 1-grams hit = 3  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle736.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 733 words.
Number of 4-grams hit = 724  (98.77%)
Number of 3-grams hit = 5  (0.68%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle737.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 697 words.
Number of 4-grams hit = 689  (98.85%)
Number of 3-grams hit = 5  (0.72%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle738.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1069 words.
Number of 4-grams hit = 1059  (99.06%)
Number of 3-grams hit = 4  (0.37%)
Number of 2-grams hit = 5  (0.47%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle739.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 484 words.
Number of 4-grams hit = 480  (99.17%)
Number of 3-grams hit = 2  (0.41%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle740.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1194 words.
Number of 4-grams hit = 1188  (99.50%)
Number of 3-grams hit = 4  (0.34%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle741.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 824 words.
Number of 4-grams hit = 814  (98.79%)
Number of 3-grams hit = 7  (0.85%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle742.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 690 words.
Number of 4-grams hit = 681  (98.70%)
Number of 3-grams hit = 6  (0.87%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle743.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 2142 words.
Number of 4-grams hit = 2133  (99.58%)
Number of 3-grams hit = 7  (0.33%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle744.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 580 words.
Number of 4-grams hit = 574  (98.97%)
Number of 3-grams hit = 3  (0.52%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle745.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 740 words.
Number of 4-grams hit = 730  (98.65%)
Number of 3-grams hit = 8  (1.08%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle746.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1498 words.
Number of 4-grams hit = 1487  (99.27%)
Number of 3-grams hit = 9  (0.60%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle747.out
Perplexity = 2.99, Entropy = 1.58 bits
Computation based on 571 words.
Number of 4-grams hit = 566  (99.12%)
Number of 3-grams hit = 3  (0.53%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle748.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 544 words.
Number of 4-grams hit = 541  (99.45%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle749.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 590 words.
Number of 4-grams hit = 579  (98.14%)
Number of 3-grams hit = 9  (1.53%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle750.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 513 words.
Number of 4-grams hit = 507  (98.83%)
Number of 3-grams hit = 4  (0.78%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle751.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 2179 words.
Number of 4-grams hit = 2167  (99.45%)
Number of 3-grams hit = 10  (0.46%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle752.out
Perplexity = 4.21, Entropy = 2.07 bits
Computation based on 468 words.
Number of 4-grams hit = 461  (98.50%)
Number of 3-grams hit = 5  (1.07%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle753.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 572 words.
Number of 4-grams hit = 569  (99.48%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle754.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 2031 words.
Number of 4-grams hit = 2011  (99.02%)
Number of 3-grams hit = 17  (0.84%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle755.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 487 words.
Number of 4-grams hit = 482  (98.97%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle756.out
Perplexity = 4.15, Entropy = 2.05 bits
Computation based on 74 words.
Number of 4-grams hit = 70  (94.59%)
Number of 3-grams hit = 2  (2.70%)
Number of 2-grams hit = 1  (1.35%)
Number of 1-grams hit = 1  (1.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle757.out
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 557 words.
Number of 4-grams hit = 549  (98.56%)
Number of 3-grams hit = 6  (1.08%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle758.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 2743 words.
Number of 4-grams hit = 2719  (99.13%)
Number of 3-grams hit = 19  (0.69%)
Number of 2-grams hit = 4  (0.15%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle759.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 3515 words.
Number of 4-grams hit = 3487  (99.20%)
Number of 3-grams hit = 24  (0.68%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle760.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 618 words.
Number of 4-grams hit = 609  (98.54%)
Number of 3-grams hit = 5  (0.81%)
Number of 2-grams hit = 3  (0.49%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle761.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 684 words.
Number of 4-grams hit = 679  (99.27%)
Number of 3-grams hit = 3  (0.44%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle762.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 861 words.
Number of 4-grams hit = 851  (98.84%)
Number of 3-grams hit = 8  (0.93%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle763.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 782 words.
Number of 4-grams hit = 775  (99.10%)
Number of 3-grams hit = 4  (0.51%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle764.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 582 words.
Number of 4-grams hit = 575  (98.80%)
Number of 3-grams hit = 5  (0.86%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle765.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 888 words.
Number of 4-grams hit = 874  (98.42%)
Number of 3-grams hit = 9  (1.01%)
Number of 2-grams hit = 4  (0.45%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle766.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 293 words.
Number of 4-grams hit = 290  (98.98%)
Number of 3-grams hit = 1  (0.34%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle767.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 4625 words.
Number of 4-grams hit = 4593  (99.31%)
Number of 3-grams hit = 29  (0.63%)
Number of 2-grams hit = 2  (0.04%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle768.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 503 words.
Number of 4-grams hit = 498  (99.01%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle769.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 730 words.
Number of 4-grams hit = 722  (98.90%)
Number of 3-grams hit = 4  (0.55%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle770.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 658 words.
Number of 4-grams hit = 650  (98.78%)
Number of 3-grams hit = 6  (0.91%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle771.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 501 words.
Number of 4-grams hit = 495  (98.80%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 2  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle772.out
Perplexity = 3.17, Entropy = 1.67 bits
Computation based on 590 words.
Number of 4-grams hit = 586  (99.32%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle773.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 3261 words.
Number of 4-grams hit = 3233  (99.14%)
Number of 3-grams hit = 21  (0.64%)
Number of 2-grams hit = 6  (0.18%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle774.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 2158 words.
Number of 4-grams hit = 2140  (99.17%)
Number of 3-grams hit = 13  (0.60%)
Number of 2-grams hit = 4  (0.19%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle775.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 558 words.
Number of 4-grams hit = 555  (99.46%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle776.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 48 words.
Number of 4-grams hit = 45  (93.75%)
Number of 3-grams hit = 1  (2.08%)
Number of 2-grams hit = 1  (2.08%)
Number of 1-grams hit = 1  (2.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle777.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 10354 words.
Number of 4-grams hit = 10246  (98.96%)
Number of 3-grams hit = 91  (0.88%)
Number of 2-grams hit = 16  (0.15%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle778.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 545 words.
Number of 4-grams hit = 538  (98.72%)
Number of 3-grams hit = 5  (0.92%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle779.out
Perplexity = 4.34, Entropy = 2.12 bits
Computation based on 1014 words.
Number of 4-grams hit = 995  (98.13%)
Number of 3-grams hit = 15  (1.48%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle780.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 445 words.
Number of 4-grams hit = 438  (98.43%)
Number of 3-grams hit = 5  (1.12%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle781.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 786 words.
Number of 4-grams hit = 777  (98.85%)
Number of 3-grams hit = 7  (0.89%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle782.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 1485 words.
Number of 4-grams hit = 1473  (99.19%)
Number of 3-grams hit = 9  (0.61%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle783.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 549 words.
Number of 4-grams hit = 539  (98.18%)
Number of 3-grams hit = 8  (1.46%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle784.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1881 words.
Number of 4-grams hit = 1871  (99.47%)
Number of 3-grams hit = 7  (0.37%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle785.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 2868 words.
Number of 4-grams hit = 2845  (99.20%)
Number of 3-grams hit = 19  (0.66%)
Number of 2-grams hit = 3  (0.10%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle786.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 370 words.
Number of 4-grams hit = 366  (98.92%)
Number of 3-grams hit = 2  (0.54%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle787.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 512 words.
Number of 4-grams hit = 509  (99.41%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle788.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 1616 words.
Number of 4-grams hit = 1598  (98.89%)
Number of 3-grams hit = 15  (0.93%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle789.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 12475 words.
Number of 4-grams hit = 12397  (99.37%)
Number of 3-grams hit = 66  (0.53%)
Number of 2-grams hit = 11  (0.09%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle790.out
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 1019 words.
Number of 4-grams hit = 1010  (99.12%)
Number of 3-grams hit = 7  (0.69%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle791.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 749 words.
Number of 4-grams hit = 738  (98.53%)
Number of 3-grams hit = 9  (1.20%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle792.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 3144 words.
Number of 4-grams hit = 3125  (99.40%)
Number of 3-grams hit = 14  (0.45%)
Number of 2-grams hit = 4  (0.13%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle793.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 197 words.
Number of 4-grams hit = 192  (97.46%)
Number of 3-grams hit = 3  (1.52%)
Number of 2-grams hit = 1  (0.51%)
Number of 1-grams hit = 1  (0.51%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle794.out
Perplexity = 4.32, Entropy = 2.11 bits
Computation based on 548 words.
Number of 4-grams hit = 539  (98.36%)
Number of 3-grams hit = 7  (1.28%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle795.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 460 words.
Number of 4-grams hit = 454  (98.70%)
Number of 3-grams hit = 4  (0.87%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle796.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 1620 words.
Number of 4-grams hit = 1607  (99.20%)
Number of 3-grams hit = 10  (0.62%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle797.out
Perplexity = 3.17, Entropy = 1.66 bits
Computation based on 413 words.
Number of 4-grams hit = 408  (98.79%)
Number of 3-grams hit = 3  (0.73%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle798.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 503 words.
Number of 4-grams hit = 498  (99.01%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle799.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 816 words.
Number of 4-grams hit = 808  (99.02%)
Number of 3-grams hit = 6  (0.74%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle800.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 473 words.
Number of 4-grams hit = 468  (98.94%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle801.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 3418 words.
Number of 4-grams hit = 3401  (99.50%)
Number of 3-grams hit = 14  (0.41%)
Number of 2-grams hit = 2  (0.06%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle802.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 2237 words.
Number of 4-grams hit = 2227  (99.55%)
Number of 3-grams hit = 6  (0.27%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle803.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 4594 words.
Number of 4-grams hit = 4554  (99.13%)
Number of 3-grams hit = 30  (0.65%)
Number of 2-grams hit = 9  (0.20%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle804.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1564 words.
Number of 4-grams hit = 1545  (98.79%)
Number of 3-grams hit = 16  (1.02%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle805.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 993 words.
Number of 4-grams hit = 980  (98.69%)
Number of 3-grams hit = 8  (0.81%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 2  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle806.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 3603 words.
Number of 4-grams hit = 3565  (98.95%)
Number of 3-grams hit = 32  (0.89%)
Number of 2-grams hit = 5  (0.14%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle807.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 756 words.
Number of 4-grams hit = 750  (99.21%)
Number of 3-grams hit = 4  (0.53%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle808.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1830 words.
Number of 4-grams hit = 1816  (99.23%)
Number of 3-grams hit = 11  (0.60%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle809.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 2165 words.
Number of 4-grams hit = 2148  (99.21%)
Number of 3-grams hit = 13  (0.60%)
Number of 2-grams hit = 3  (0.14%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle810.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 937 words.
Number of 4-grams hit = 933  (99.57%)
Number of 3-grams hit = 2  (0.21%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle811.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 4883 words.
Number of 4-grams hit = 4849  (99.30%)
Number of 3-grams hit = 29  (0.59%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle812.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 3604 words.
Number of 4-grams hit = 3581  (99.36%)
Number of 3-grams hit = 17  (0.47%)
Number of 2-grams hit = 5  (0.14%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle813.out
Perplexity = 3.17, Entropy = 1.67 bits
Computation based on 1497 words.
Number of 4-grams hit = 1492  (99.67%)
Number of 3-grams hit = 3  (0.20%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle814.out
Perplexity = 3.03, Entropy = 1.60 bits
Computation based on 908 words.
Number of 4-grams hit = 902  (99.34%)
Number of 3-grams hit = 4  (0.44%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle815.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1390 words.
Number of 4-grams hit = 1380  (99.28%)
Number of 3-grams hit = 8  (0.58%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle816.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 1434 words.
Number of 4-grams hit = 1421  (99.09%)
Number of 3-grams hit = 9  (0.63%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle817.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 580 words.
Number of 4-grams hit = 569  (98.10%)
Number of 3-grams hit = 8  (1.38%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle818.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 604 words.
Number of 4-grams hit = 597  (98.84%)
Number of 3-grams hit = 5  (0.83%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle819.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 1947 words.
Number of 4-grams hit = 1937  (99.49%)
Number of 3-grams hit = 8  (0.41%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle820.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 880 words.
Number of 4-grams hit = 873  (99.20%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle821.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 1187 words.
Number of 4-grams hit = 1180  (99.41%)
Number of 3-grams hit = 5  (0.42%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle822.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 2654 words.
Number of 4-grams hit = 2631  (99.13%)
Number of 3-grams hit = 16  (0.60%)
Number of 2-grams hit = 5  (0.19%)
Number of 1-grams hit = 2  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle823.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1435 words.
Number of 4-grams hit = 1428  (99.51%)
Number of 3-grams hit = 5  (0.35%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle824.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 533 words.
Number of 4-grams hit = 526  (98.69%)
Number of 3-grams hit = 4  (0.75%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle825.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 639 words.
Number of 4-grams hit = 627  (98.12%)
Number of 3-grams hit = 10  (1.56%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle826.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 978 words.
Number of 4-grams hit = 973  (99.49%)
Number of 3-grams hit = 3  (0.31%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle827.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 734 words.
Number of 4-grams hit = 727  (99.05%)
Number of 3-grams hit = 4  (0.54%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle828.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 13188 words.
Number of 4-grams hit = 13081  (99.19%)
Number of 3-grams hit = 87  (0.66%)
Number of 2-grams hit = 17  (0.13%)
Number of 1-grams hit = 3  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle829.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 578 words.
Number of 4-grams hit = 572  (98.96%)
Number of 3-grams hit = 4  (0.69%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle830.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 2488 words.
Number of 4-grams hit = 2473  (99.40%)
Number of 3-grams hit = 12  (0.48%)
Number of 2-grams hit = 2  (0.08%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle831.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1106 words.
Number of 4-grams hit = 1098  (99.28%)
Number of 3-grams hit = 5  (0.45%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle832.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 460 words.
Number of 4-grams hit = 456  (99.13%)
Number of 3-grams hit = 2  (0.43%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle833.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 888 words.
Number of 4-grams hit = 880  (99.10%)
Number of 3-grams hit = 5  (0.56%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle834.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 963 words.
Number of 4-grams hit = 957  (99.38%)
Number of 3-grams hit = 4  (0.42%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle835.out
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 491 words.
Number of 4-grams hit = 486  (98.98%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle836.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 528 words.
Number of 4-grams hit = 522  (98.86%)
Number of 3-grams hit = 4  (0.76%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle837.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 900 words.
Number of 4-grams hit = 896  (99.56%)
Number of 3-grams hit = 2  (0.22%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle838.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 2446 words.
Number of 4-grams hit = 2435  (99.55%)
Number of 3-grams hit = 9  (0.37%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle839.out
Perplexity = 4.13, Entropy = 2.05 bits
Computation based on 496 words.
Number of 4-grams hit = 488  (98.39%)
Number of 3-grams hit = 6  (1.21%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle840.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 560 words.
Number of 4-grams hit = 555  (99.11%)
Number of 3-grams hit = 3  (0.54%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle841.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 1009 words.
Number of 4-grams hit = 999  (99.01%)
Number of 3-grams hit = 8  (0.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle842.out
Perplexity = 2.87, Entropy = 1.52 bits
Computation based on 503 words.
Number of 4-grams hit = 500  (99.40%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle843.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1419 words.
Number of 4-grams hit = 1400  (98.66%)
Number of 3-grams hit = 13  (0.92%)
Number of 2-grams hit = 5  (0.35%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle844.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 9009 words.
Number of 4-grams hit = 8928  (99.10%)
Number of 3-grams hit = 68  (0.75%)
Number of 2-grams hit = 12  (0.13%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle845.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 648 words.
Number of 4-grams hit = 643  (99.23%)
Number of 3-grams hit = 3  (0.46%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle846.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 609 words.
Number of 4-grams hit = 605  (99.34%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle847.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 1795 words.
Number of 4-grams hit = 1766  (98.38%)
Number of 3-grams hit = 24  (1.34%)
Number of 2-grams hit = 4  (0.22%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle848.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 389 words.
Number of 4-grams hit = 386  (99.23%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle849.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 1705 words.
Number of 4-grams hit = 1695  (99.41%)
Number of 3-grams hit = 7  (0.41%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle850.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 477 words.
Number of 4-grams hit = 471  (98.74%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle851.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 604 words.
Number of 4-grams hit = 600  (99.34%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle852.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 684 words.
Number of 4-grams hit = 681  (99.56%)
Number of 3-grams hit = 1  (0.15%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle853.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1621 words.
Number of 4-grams hit = 1602  (98.83%)
Number of 3-grams hit = 14  (0.86%)
Number of 2-grams hit = 4  (0.25%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle854.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 2917 words.
Number of 4-grams hit = 2900  (99.42%)
Number of 3-grams hit = 14  (0.48%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle855.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 465 words.
Number of 4-grams hit = 460  (98.92%)
Number of 3-grams hit = 3  (0.65%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle856.out
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 889 words.
Number of 4-grams hit = 885  (99.55%)
Number of 3-grams hit = 2  (0.22%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle857.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 379 words.
Number of 4-grams hit = 376  (99.21%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle858.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 255 words.
Number of 4-grams hit = 251  (98.43%)
Number of 3-grams hit = 2  (0.78%)
Number of 2-grams hit = 1  (0.39%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle859.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1192 words.
Number of 4-grams hit = 1181  (99.08%)
Number of 3-grams hit = 8  (0.67%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle860.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 503 words.
Number of 4-grams hit = 497  (98.81%)
Number of 3-grams hit = 4  (0.80%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle861.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 432 words.
Number of 4-grams hit = 429  (99.31%)
Number of 3-grams hit = 1  (0.23%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle862.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 487 words.
Number of 4-grams hit = 481  (98.77%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle863.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 712 words.
Number of 4-grams hit = 704  (98.88%)
Number of 3-grams hit = 5  (0.70%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle864.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 494 words.
Number of 4-grams hit = 485  (98.18%)
Number of 3-grams hit = 4  (0.81%)
Number of 2-grams hit = 4  (0.81%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle865.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 6948 words.
Number of 4-grams hit = 6914  (99.51%)
Number of 3-grams hit = 30  (0.43%)
Number of 2-grams hit = 3  (0.04%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle866.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 668 words.
Number of 4-grams hit = 662  (99.10%)
Number of 3-grams hit = 3  (0.45%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle867.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 377 words.
Number of 4-grams hit = 367  (97.35%)
Number of 3-grams hit = 8  (2.12%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle868.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1799 words.
Number of 4-grams hit = 1787  (99.33%)
Number of 3-grams hit = 8  (0.44%)
Number of 2-grams hit = 3  (0.17%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle869.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 3365 words.
Number of 4-grams hit = 3344  (99.38%)
Number of 3-grams hit = 16  (0.48%)
Number of 2-grams hit = 4  (0.12%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle870.out
Perplexity = 3.09, Entropy = 1.63 bits
Computation based on 831 words.
Number of 4-grams hit = 824  (99.16%)
Number of 3-grams hit = 4  (0.48%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle871.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 444 words.
Number of 4-grams hit = 437  (98.42%)
Number of 3-grams hit = 5  (1.13%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle872.out
Perplexity = 4.25, Entropy = 2.09 bits
Computation based on 316 words.
Number of 4-grams hit = 310  (98.10%)
Number of 3-grams hit = 4  (1.27%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle873.out
Perplexity = 3.26, Entropy = 1.71 bits
Computation based on 1015 words.
Number of 4-grams hit = 1004  (98.92%)
Number of 3-grams hit = 9  (0.89%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle874.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 460 words.
Number of 4-grams hit = 457  (99.35%)
Number of 3-grams hit = 1  (0.22%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle875.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 855 words.
Number of 4-grams hit = 845  (98.83%)
Number of 3-grams hit = 8  (0.94%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle876.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 10045 words.
Number of 4-grams hit = 9972  (99.27%)
Number of 3-grams hit = 59  (0.59%)
Number of 2-grams hit = 12  (0.12%)
Number of 1-grams hit = 2  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle877.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 502 words.
Number of 4-grams hit = 496  (98.80%)
Number of 3-grams hit = 4  (0.80%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle878.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 364 words.
Number of 4-grams hit = 361  (99.18%)
Number of 3-grams hit = 1  (0.27%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle879.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 591 words.
Number of 4-grams hit = 586  (99.15%)
Number of 3-grams hit = 3  (0.51%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle880.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 509 words.
Number of 4-grams hit = 505  (99.21%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle881.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 376 words.
Number of 4-grams hit = 371  (98.67%)
Number of 3-grams hit = 3  (0.80%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle882.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 638 words.
Number of 4-grams hit = 631  (98.90%)
Number of 3-grams hit = 5  (0.78%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle883.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 638 words.
Number of 4-grams hit = 634  (99.37%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle884.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 777 words.
Number of 4-grams hit = 766  (98.58%)
Number of 3-grams hit = 8  (1.03%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle885.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 534 words.
Number of 4-grams hit = 529  (99.06%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle886.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 508 words.
Number of 4-grams hit = 505  (99.41%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle887.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 517 words.
Number of 4-grams hit = 513  (99.23%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle888.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1311 words.
Number of 4-grams hit = 1305  (99.54%)
Number of 3-grams hit = 4  (0.31%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle889.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 4700 words.
Number of 4-grams hit = 4665  (99.26%)
Number of 3-grams hit = 29  (0.62%)
Number of 2-grams hit = 5  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle890.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1148 words.
Number of 4-grams hit = 1144  (99.65%)
Number of 3-grams hit = 2  (0.17%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle891.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 873 words.
Number of 4-grams hit = 868  (99.43%)
Number of 3-grams hit = 3  (0.34%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle892.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 3489 words.
Number of 4-grams hit = 3474  (99.57%)
Number of 3-grams hit = 12  (0.34%)
Number of 2-grams hit = 2  (0.06%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle893.out
Perplexity = 4.18, Entropy = 2.06 bits
Computation based on 699 words.
Number of 4-grams hit = 687  (98.28%)
Number of 3-grams hit = 10  (1.43%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle894.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 805 words.
Number of 4-grams hit = 793  (98.51%)
Number of 3-grams hit = 10  (1.24%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle895.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 188 words.
Number of 4-grams hit = 185  (98.40%)
Number of 3-grams hit = 1  (0.53%)
Number of 2-grams hit = 1  (0.53%)
Number of 1-grams hit = 1  (0.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle896.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 452 words.
Number of 4-grams hit = 447  (98.89%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle897.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 767 words.
Number of 4-grams hit = 754  (98.31%)
Number of 3-grams hit = 9  (1.17%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle898.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1746 words.
Number of 4-grams hit = 1736  (99.43%)
Number of 3-grams hit = 7  (0.40%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle899.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 219 words.
Number of 4-grams hit = 216  (98.63%)
Number of 3-grams hit = 1  (0.46%)
Number of 2-grams hit = 1  (0.46%)
Number of 1-grams hit = 1  (0.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle900.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 1135 words.
Number of 4-grams hit = 1126  (99.21%)
Number of 3-grams hit = 7  (0.62%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle901.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 428 words.
Number of 4-grams hit = 424  (99.07%)
Number of 3-grams hit = 2  (0.47%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle902.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 2155 words.
Number of 4-grams hit = 2130  (98.84%)
Number of 3-grams hit = 22  (1.02%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle903.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 1273 words.
Number of 4-grams hit = 1267  (99.53%)
Number of 3-grams hit = 4  (0.31%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle904.out
Perplexity = 2.69, Entropy = 1.43 bits
Computation based on 649 words.
Number of 4-grams hit = 642  (98.92%)
Number of 3-grams hit = 5  (0.77%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle905.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 388 words.
Number of 4-grams hit = 384  (98.97%)
Number of 3-grams hit = 2  (0.52%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle906.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 476 words.
Number of 4-grams hit = 470  (98.74%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle907.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 488 words.
Number of 4-grams hit = 481  (98.57%)
Number of 3-grams hit = 5  (1.02%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle908.out
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 552 words.
Number of 4-grams hit = 547  (99.09%)
Number of 3-grams hit = 3  (0.54%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle909.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 456 words.
Number of 4-grams hit = 452  (99.12%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle910.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 737 words.
Number of 4-grams hit = 726  (98.51%)
Number of 3-grams hit = 7  (0.95%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle911.out
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 688 words.
Number of 4-grams hit = 683  (99.27%)
Number of 3-grams hit = 3  (0.44%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle912.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 743 words.
Number of 4-grams hit = 731  (98.38%)
Number of 3-grams hit = 9  (1.21%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle913.out
Perplexity = 4.06, Entropy = 2.02 bits
Computation based on 743 words.
Number of 4-grams hit = 731  (98.38%)
Number of 3-grams hit = 9  (1.21%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle914.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 575 words.
Number of 4-grams hit = 569  (98.96%)
Number of 3-grams hit = 4  (0.70%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle915.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1309 words.
Number of 4-grams hit = 1294  (98.85%)
Number of 3-grams hit = 13  (0.99%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle916.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 1003 words.
Number of 4-grams hit = 997  (99.40%)
Number of 3-grams hit = 4  (0.40%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle917.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1180 words.
Number of 4-grams hit = 1172  (99.32%)
Number of 3-grams hit = 6  (0.51%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle918.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 535 words.
Number of 4-grams hit = 529  (98.88%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle919.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 3585 words.
Number of 4-grams hit = 3562  (99.36%)
Number of 3-grams hit = 18  (0.50%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 2  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle920.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1001 words.
Number of 4-grams hit = 992  (99.10%)
Number of 3-grams hit = 6  (0.60%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle921.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 844 words.
Number of 4-grams hit = 828  (98.10%)
Number of 3-grams hit = 12  (1.42%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle922.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 515 words.
Number of 4-grams hit = 510  (99.03%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle923.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 560 words.
Number of 4-grams hit = 552  (98.57%)
Number of 3-grams hit = 4  (0.71%)
Number of 2-grams hit = 3  (0.54%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle924.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 376 words.
Number of 4-grams hit = 371  (98.67%)
Number of 3-grams hit = 3  (0.80%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle925.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 4136 words.
Number of 4-grams hit = 4101  (99.15%)
Number of 3-grams hit = 28  (0.68%)
Number of 2-grams hit = 6  (0.15%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle926.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1421 words.
Number of 4-grams hit = 1411  (99.30%)
Number of 3-grams hit = 7  (0.49%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle927.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 440 words.
Number of 4-grams hit = 436  (99.09%)
Number of 3-grams hit = 2  (0.45%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle928.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 516 words.
Number of 4-grams hit = 509  (98.64%)
Number of 3-grams hit = 5  (0.97%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle929.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 538 words.
Number of 4-grams hit = 533  (99.07%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle930.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 465 words.
Number of 4-grams hit = 458  (98.49%)
Number of 3-grams hit = 4  (0.86%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle931.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 645 words.
Number of 4-grams hit = 634  (98.29%)
Number of 3-grams hit = 7  (1.09%)
Number of 2-grams hit = 3  (0.47%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle932.out
Perplexity = 2.71, Entropy = 1.44 bits
Computation based on 228 words.
Number of 4-grams hit = 224  (98.25%)
Number of 3-grams hit = 2  (0.88%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle933.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 453 words.
Number of 4-grams hit = 446  (98.45%)
Number of 3-grams hit = 5  (1.10%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle934.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 416 words.
Number of 4-grams hit = 409  (98.32%)
Number of 3-grams hit = 5  (1.20%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle935.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 1259 words.
Number of 4-grams hit = 1255  (99.68%)
Number of 3-grams hit = 2  (0.16%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle936.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 483 words.
Number of 4-grams hit = 478  (98.96%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle937.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 239 words.
Number of 4-grams hit = 236  (98.74%)
Number of 3-grams hit = 1  (0.42%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle938.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 1402 words.
Number of 4-grams hit = 1390  (99.14%)
Number of 3-grams hit = 9  (0.64%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle939.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 519 words.
Number of 4-grams hit = 514  (99.04%)
Number of 3-grams hit = 3  (0.58%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle940.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 2076 words.
Number of 4-grams hit = 2059  (99.18%)
Number of 3-grams hit = 14  (0.67%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle941.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1305 words.
Number of 4-grams hit = 1297  (99.39%)
Number of 3-grams hit = 6  (0.46%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle942.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 571 words.
Number of 4-grams hit = 562  (98.42%)
Number of 3-grams hit = 7  (1.23%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle943.out
Perplexity = 3.05, Entropy = 1.61 bits
Computation based on 1491 words.
Number of 4-grams hit = 1486  (99.66%)
Number of 3-grams hit = 3  (0.20%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle944.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 477 words.
Number of 4-grams hit = 472  (98.95%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle945.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 3747 words.
Number of 4-grams hit = 3717  (99.20%)
Number of 3-grams hit = 23  (0.61%)
Number of 2-grams hit = 6  (0.16%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle946.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 611 words.
Number of 4-grams hit = 600  (98.20%)
Number of 3-grams hit = 9  (1.47%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle947.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 459 words.
Number of 4-grams hit = 450  (98.04%)
Number of 3-grams hit = 5  (1.09%)
Number of 2-grams hit = 3  (0.65%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle948.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 627 words.
Number of 4-grams hit = 621  (99.04%)
Number of 3-grams hit = 3  (0.48%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle949.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1862 words.
Number of 4-grams hit = 1843  (98.98%)
Number of 3-grams hit = 14  (0.75%)
Number of 2-grams hit = 4  (0.21%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle950.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 543 words.
Number of 4-grams hit = 536  (98.71%)
Number of 3-grams hit = 5  (0.92%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle951.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 622 words.
Number of 4-grams hit = 614  (98.71%)
Number of 3-grams hit = 6  (0.96%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle952.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 1389 words.
Number of 4-grams hit = 1381  (99.42%)
Number of 3-grams hit = 5  (0.36%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle953.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 2294 words.
Number of 4-grams hit = 2275  (99.17%)
Number of 3-grams hit = 15  (0.65%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle954.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 800 words.
Number of 4-grams hit = 792  (99.00%)
Number of 3-grams hit = 4  (0.50%)
Number of 2-grams hit = 3  (0.38%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle955.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1574 words.
Number of 4-grams hit = 1557  (98.92%)
Number of 3-grams hit = 15  (0.95%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle956.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1107 words.
Number of 4-grams hit = 1099  (99.28%)
Number of 3-grams hit = 6  (0.54%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle957.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 323 words.
Number of 4-grams hit = 320  (99.07%)
Number of 3-grams hit = 1  (0.31%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle958.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1014 words.
Number of 4-grams hit = 1000  (98.62%)
Number of 3-grams hit = 11  (1.08%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle959.out
Perplexity = 4.32, Entropy = 2.11 bits
Computation based on 218 words.
Number of 4-grams hit = 213  (97.71%)
Number of 3-grams hit = 3  (1.38%)
Number of 2-grams hit = 1  (0.46%)
Number of 1-grams hit = 1  (0.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle960.out
Perplexity = 2.83, Entropy = 1.50 bits
Computation based on 652 words.
Number of 4-grams hit = 649  (99.54%)
Number of 3-grams hit = 1  (0.15%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle961.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 534 words.
Number of 4-grams hit = 524  (98.13%)
Number of 3-grams hit = 7  (1.31%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle962.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 2369 words.
Number of 4-grams hit = 2357  (99.49%)
Number of 3-grams hit = 8  (0.34%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle963.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 2187 words.
Number of 4-grams hit = 2165  (98.99%)
Number of 3-grams hit = 17  (0.78%)
Number of 2-grams hit = 4  (0.18%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle964.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 1090 words.
Number of 4-grams hit = 1080  (99.08%)
Number of 3-grams hit = 7  (0.64%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle965.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 701 words.
Number of 4-grams hit = 692  (98.72%)
Number of 3-grams hit = 7  (1.00%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle966.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 850 words.
Number of 4-grams hit = 841  (98.94%)
Number of 3-grams hit = 7  (0.82%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle967.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 426 words.
Number of 4-grams hit = 422  (99.06%)
Number of 3-grams hit = 2  (0.47%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle968.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1147 words.
Number of 4-grams hit = 1141  (99.48%)
Number of 3-grams hit = 4  (0.35%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle969.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 773 words.
Number of 4-grams hit = 763  (98.71%)
Number of 3-grams hit = 7  (0.91%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle970.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 798 words.
Number of 4-grams hit = 790  (99.00%)
Number of 3-grams hit = 5  (0.63%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle971.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 504 words.
Number of 4-grams hit = 500  (99.21%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle972.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 2041 words.
Number of 4-grams hit = 2018  (98.87%)
Number of 3-grams hit = 16  (0.78%)
Number of 2-grams hit = 6  (0.29%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle973.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 663 words.
Number of 4-grams hit = 659  (99.40%)
Number of 3-grams hit = 2  (0.30%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle974.out
Perplexity = 2.87, Entropy = 1.52 bits
Computation based on 1022 words.
Number of 4-grams hit = 1014  (99.22%)
Number of 3-grams hit = 6  (0.59%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle975.out
Perplexity = 3.26, Entropy = 1.71 bits
Computation based on 380 words.
Number of 4-grams hit = 373  (98.16%)
Number of 3-grams hit = 5  (1.32%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle976.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 385 words.
Number of 4-grams hit = 378  (98.18%)
Number of 3-grams hit = 5  (1.30%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle977.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 917 words.
Number of 4-grams hit = 902  (98.36%)
Number of 3-grams hit = 12  (1.31%)
Number of 2-grams hit = 2  (0.22%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle978.out
Perplexity = 3.93, Entropy = 1.98 bits
Computation based on 503 words.
Number of 4-grams hit = 499  (99.20%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle979.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1716 words.
Number of 4-grams hit = 1703  (99.24%)
Number of 3-grams hit = 11  (0.64%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle980.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 681 words.
Number of 4-grams hit = 673  (98.83%)
Number of 3-grams hit = 4  (0.59%)
Number of 2-grams hit = 3  (0.44%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle981.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 371 words.
Number of 4-grams hit = 364  (98.11%)
Number of 3-grams hit = 3  (0.81%)
Number of 2-grams hit = 3  (0.81%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle982.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 890 words.
Number of 4-grams hit = 881  (98.99%)
Number of 3-grams hit = 7  (0.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle983.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 520 words.
Number of 4-grams hit = 513  (98.65%)
Number of 3-grams hit = 5  (0.96%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle984.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 479 words.
Number of 4-grams hit = 474  (98.96%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle985.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 1351 words.
Number of 4-grams hit = 1341  (99.26%)
Number of 3-grams hit = 7  (0.52%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle986.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 297 words.
Number of 4-grams hit = 293  (98.65%)
Number of 3-grams hit = 2  (0.67%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle987.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1891 words.
Number of 4-grams hit = 1872  (99.00%)
Number of 3-grams hit = 15  (0.79%)
Number of 2-grams hit = 3  (0.16%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle988.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1353 words.
Number of 4-grams hit = 1343  (99.26%)
Number of 3-grams hit = 6  (0.44%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle989.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 2476 words.
Number of 4-grams hit = 2460  (99.35%)
Number of 3-grams hit = 13  (0.53%)
Number of 2-grams hit = 2  (0.08%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle990.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 710 words.
Number of 4-grams hit = 703  (99.01%)
Number of 3-grams hit = 4  (0.56%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle991.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 1688 words.
Number of 4-grams hit = 1668  (98.82%)
Number of 3-grams hit = 17  (1.01%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle992.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 578 words.
Number of 4-grams hit = 571  (98.79%)
Number of 3-grams hit = 5  (0.87%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle993.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 2666 words.
Number of 4-grams hit = 2643  (99.14%)
Number of 3-grams hit = 19  (0.71%)
Number of 2-grams hit = 3  (0.11%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle994.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 527 words.
Number of 4-grams hit = 522  (99.05%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle995.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 7968 words.
Number of 4-grams hit = 7916  (99.35%)
Number of 3-grams hit = 45  (0.56%)
Number of 2-grams hit = 6  (0.08%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle996.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 627 words.
Number of 4-grams hit = 621  (99.04%)
Number of 3-grams hit = 4  (0.64%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle997.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 5785 words.
Number of 4-grams hit = 5740  (99.22%)
Number of 3-grams hit = 35  (0.61%)
Number of 2-grams hit = 9  (0.16%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle998.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 572 words.
Number of 4-grams hit = 563  (98.43%)
Number of 3-grams hit = 7  (1.22%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle999.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 523 words.
Number of 4-grams hit = 516  (98.66%)
Number of 3-grams hit = 4  (0.76%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : 