evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle0.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 1366 words.
Number of 5-grams hit = 1337  (97.88%)
Number of 4-grams hit = 18  (1.32%)
Number of 3-grams hit = 9  (0.66%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle1.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1620 words.
Number of 5-grams hit = 1579  (97.47%)
Number of 4-grams hit = 30  (1.85%)
Number of 3-grams hit = 9  (0.56%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle2.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 591 words.
Number of 5-grams hit = 574  (97.12%)
Number of 4-grams hit = 10  (1.69%)
Number of 3-grams hit = 5  (0.85%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle3.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 661 words.
Number of 5-grams hit = 646  (97.73%)
Number of 4-grams hit = 8  (1.21%)
Number of 3-grams hit = 5  (0.76%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle4.out
Perplexity = 3.02, Entropy = 1.59 bits
Computation based on 425 words.
Number of 5-grams hit = 413  (97.18%)
Number of 4-grams hit = 7  (1.65%)
Number of 3-grams hit = 3  (0.71%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle5.out
Perplexity = 3.04, Entropy = 1.60 bits
Computation based on 957 words.
Number of 5-grams hit = 937  (97.91%)
Number of 4-grams hit = 12  (1.25%)
Number of 3-grams hit = 5  (0.52%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle6.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 317 words.
Number of 5-grams hit = 311  (98.11%)
Number of 4-grams hit = 3  (0.95%)
Number of 3-grams hit = 1  (0.32%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle7.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 665 words.
Number of 5-grams hit = 646  (97.14%)
Number of 4-grams hit = 13  (1.95%)
Number of 3-grams hit = 4  (0.60%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle8.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 528 words.
Number of 5-grams hit = 518  (98.11%)
Number of 4-grams hit = 6  (1.14%)
Number of 3-grams hit = 2  (0.38%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle9.out
Perplexity = 3.93, Entropy = 1.98 bits
Computation based on 345 words.
Number of 5-grams hit = 331  (95.94%)
Number of 4-grams hit = 8  (2.32%)
Number of 3-grams hit = 4  (1.16%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle10.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 327 words.
Number of 5-grams hit = 311  (95.11%)
Number of 4-grams hit = 10  (3.06%)
Number of 3-grams hit = 4  (1.22%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle11.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 325 words.
Number of 5-grams hit = 314  (96.62%)
Number of 4-grams hit = 6  (1.85%)
Number of 3-grams hit = 3  (0.92%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle12.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 326 words.
Number of 5-grams hit = 312  (95.71%)
Number of 4-grams hit = 8  (2.45%)
Number of 3-grams hit = 4  (1.23%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle13.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 488 words.
Number of 5-grams hit = 471  (96.52%)
Number of 4-grams hit = 11  (2.25%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle14.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 523 words.
Number of 5-grams hit = 516  (98.66%)
Number of 4-grams hit = 4  (0.76%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle15.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 543 words.
Number of 5-grams hit = 526  (96.87%)
Number of 4-grams hit = 10  (1.84%)
Number of 3-grams hit = 4  (0.74%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle16.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 400 words.
Number of 5-grams hit = 391  (97.75%)
Number of 4-grams hit = 6  (1.50%)
Number of 3-grams hit = 1  (0.25%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle17.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 722 words.
Number of 5-grams hit = 704  (97.51%)
Number of 4-grams hit = 12  (1.66%)
Number of 3-grams hit = 3  (0.42%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle18.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 445 words.
Number of 5-grams hit = 428  (96.18%)
Number of 4-grams hit = 12  (2.70%)
Number of 3-grams hit = 3  (0.67%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle19.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 560 words.
Number of 5-grams hit = 534  (95.36%)
Number of 4-grams hit = 15  (2.68%)
Number of 3-grams hit = 8  (1.43%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle20.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 386 words.
Number of 5-grams hit = 376  (97.41%)
Number of 4-grams hit = 5  (1.30%)
Number of 3-grams hit = 3  (0.78%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle21.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 500 words.
Number of 5-grams hit = 481  (96.20%)
Number of 4-grams hit = 12  (2.40%)
Number of 3-grams hit = 5  (1.00%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle22.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 387 words.
Number of 5-grams hit = 379  (97.93%)
Number of 4-grams hit = 5  (1.29%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle23.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 378 words.
Number of 5-grams hit = 367  (97.09%)
Number of 4-grams hit = 4  (1.06%)
Number of 3-grams hit = 5  (1.32%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle24.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 599 words.
Number of 5-grams hit = 588  (98.16%)
Number of 4-grams hit = 6  (1.00%)
Number of 3-grams hit = 3  (0.50%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle25.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 413 words.
Number of 5-grams hit = 394  (95.40%)
Number of 4-grams hit = 11  (2.66%)
Number of 3-grams hit = 6  (1.45%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle26.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1649 words.
Number of 5-grams hit = 1604  (97.27%)
Number of 4-grams hit = 34  (2.06%)
Number of 3-grams hit = 9  (0.55%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle27.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 424 words.
Number of 5-grams hit = 414  (97.64%)
Number of 4-grams hit = 6  (1.42%)
Number of 3-grams hit = 2  (0.47%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle28.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1511 words.
Number of 5-grams hit = 1476  (97.68%)
Number of 4-grams hit = 24  (1.59%)
Number of 3-grams hit = 7  (0.46%)
Number of 2-grams hit = 3  (0.20%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle29.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 346 words.
Number of 5-grams hit = 338  (97.69%)
Number of 4-grams hit = 5  (1.45%)
Number of 3-grams hit = 1  (0.29%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle30.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 523 words.
Number of 5-grams hit = 513  (98.09%)
Number of 4-grams hit = 5  (0.96%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle31.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 481 words.
Number of 5-grams hit = 468  (97.30%)
Number of 4-grams hit = 8  (1.66%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle32.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 516 words.
Number of 5-grams hit = 497  (96.32%)
Number of 4-grams hit = 13  (2.52%)
Number of 3-grams hit = 4  (0.78%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle33.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 476 words.
Number of 5-grams hit = 462  (97.06%)
Number of 4-grams hit = 9  (1.89%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle34.out
Perplexity = 2.99, Entropy = 1.58 bits
Computation based on 326 words.
Number of 5-grams hit = 318  (97.55%)
Number of 4-grams hit = 5  (1.53%)
Number of 3-grams hit = 1  (0.31%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle35.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 402 words.
Number of 5-grams hit = 386  (96.02%)
Number of 4-grams hit = 12  (2.99%)
Number of 3-grams hit = 2  (0.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle36.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 1088 words.
Number of 5-grams hit = 1060  (97.43%)
Number of 4-grams hit = 16  (1.47%)
Number of 3-grams hit = 10  (0.92%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle37.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 604 words.
Number of 5-grams hit = 585  (96.85%)
Number of 4-grams hit = 10  (1.66%)
Number of 3-grams hit = 6  (0.99%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle38.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 444 words.
Number of 5-grams hit = 433  (97.52%)
Number of 4-grams hit = 7  (1.58%)
Number of 3-grams hit = 2  (0.45%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle39.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 2871 words.
Number of 5-grams hit = 2784  (96.97%)
Number of 4-grams hit = 60  (2.09%)
Number of 3-grams hit = 24  (0.84%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle40.out
Perplexity = 4.25, Entropy = 2.09 bits
Computation based on 442 words.
Number of 5-grams hit = 419  (94.80%)
Number of 4-grams hit = 13  (2.94%)
Number of 3-grams hit = 7  (1.58%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle41.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1060 words.
Number of 5-grams hit = 1042  (98.30%)
Number of 4-grams hit = 12  (1.13%)
Number of 3-grams hit = 3  (0.28%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle42.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 591 words.
Number of 5-grams hit = 573  (96.95%)
Number of 4-grams hit = 9  (1.52%)
Number of 3-grams hit = 6  (1.02%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle43.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 490 words.
Number of 5-grams hit = 473  (96.53%)
Number of 4-grams hit = 9  (1.84%)
Number of 3-grams hit = 5  (1.02%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle44.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 549 words.
Number of 5-grams hit = 537  (97.81%)
Number of 4-grams hit = 9  (1.64%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle45.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 502 words.
Number of 5-grams hit = 484  (96.41%)
Number of 4-grams hit = 13  (2.59%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle46.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 436 words.
Number of 5-grams hit = 420  (96.33%)
Number of 4-grams hit = 11  (2.52%)
Number of 3-grams hit = 3  (0.69%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle47.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 2914 words.
Number of 5-grams hit = 2842  (97.53%)
Number of 4-grams hit = 51  (1.75%)
Number of 3-grams hit = 18  (0.62%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle48.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 400 words.
Number of 5-grams hit = 388  (97.00%)
Number of 4-grams hit = 5  (1.25%)
Number of 3-grams hit = 3  (0.75%)
Number of 2-grams hit = 3  (0.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle49.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 395 words.
Number of 5-grams hit = 386  (97.72%)
Number of 4-grams hit = 4  (1.01%)
Number of 3-grams hit = 3  (0.76%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle50.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 563 words.
Number of 5-grams hit = 549  (97.51%)
Number of 4-grams hit = 7  (1.24%)
Number of 3-grams hit = 4  (0.71%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle51.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 1184 words.
Number of 5-grams hit = 1169  (98.73%)
Number of 4-grams hit = 9  (0.76%)
Number of 3-grams hit = 4  (0.34%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle52.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 401 words.
Number of 5-grams hit = 387  (96.51%)
Number of 4-grams hit = 8  (2.00%)
Number of 3-grams hit = 3  (0.75%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle53.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 521 words.
Number of 5-grams hit = 501  (96.16%)
Number of 4-grams hit = 15  (2.88%)
Number of 3-grams hit = 3  (0.58%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle54.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1201 words.
Number of 5-grams hit = 1166  (97.09%)
Number of 4-grams hit = 22  (1.83%)
Number of 3-grams hit = 11  (0.92%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle55.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 578 words.
Number of 5-grams hit = 558  (96.54%)
Number of 4-grams hit = 14  (2.42%)
Number of 3-grams hit = 4  (0.69%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle56.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 988 words.
Number of 5-grams hit = 946  (95.75%)
Number of 4-grams hit = 27  (2.73%)
Number of 3-grams hit = 11  (1.11%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle57.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1691 words.
Number of 5-grams hit = 1657  (97.99%)
Number of 4-grams hit = 22  (1.30%)
Number of 3-grams hit = 9  (0.53%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle58.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 416 words.
Number of 5-grams hit = 405  (97.36%)
Number of 4-grams hit = 8  (1.92%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle59.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 231 words.
Number of 5-grams hit = 219  (94.81%)
Number of 4-grams hit = 6  (2.60%)
Number of 3-grams hit = 3  (1.30%)
Number of 2-grams hit = 2  (0.87%)
Number of 1-grams hit = 1  (0.43%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle60.out
Perplexity = 2.94, Entropy = 1.55 bits
Computation based on 354 words.
Number of 5-grams hit = 344  (97.18%)
Number of 4-grams hit = 4  (1.13%)
Number of 3-grams hit = 4  (1.13%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle61.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 530 words.
Number of 5-grams hit = 511  (96.42%)
Number of 4-grams hit = 10  (1.89%)
Number of 3-grams hit = 7  (1.32%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle62.out
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 1398 words.
Number of 5-grams hit = 1350  (96.57%)
Number of 4-grams hit = 34  (2.43%)
Number of 3-grams hit = 11  (0.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle63.out
Perplexity = 3.69, Entropy = 1.89 bits
Computation based on 1066 words.
Number of 5-grams hit = 1038  (97.37%)
Number of 4-grams hit = 21  (1.97%)
Number of 3-grams hit = 5  (0.47%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle64.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 4550 words.
Number of 5-grams hit = 4426  (97.27%)
Number of 4-grams hit = 89  (1.96%)
Number of 3-grams hit = 28  (0.62%)
Number of 2-grams hit = 6  (0.13%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle65.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 697 words.
Number of 5-grams hit = 672  (96.41%)
Number of 4-grams hit = 15  (2.15%)
Number of 3-grams hit = 7  (1.00%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle66.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 514 words.
Number of 5-grams hit = 501  (97.47%)
Number of 4-grams hit = 9  (1.75%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle67.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 484 words.
Number of 5-grams hit = 475  (98.14%)
Number of 4-grams hit = 6  (1.24%)
Number of 3-grams hit = 1  (0.21%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle68.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 1206 words.
Number of 5-grams hit = 1178  (97.68%)
Number of 4-grams hit = 21  (1.74%)
Number of 3-grams hit = 4  (0.33%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle69.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 407 words.
Number of 5-grams hit = 403  (99.02%)
Number of 4-grams hit = 1  (0.25%)
Number of 3-grams hit = 1  (0.25%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle70.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1071 words.
Number of 5-grams hit = 1036  (96.73%)
Number of 4-grams hit = 22  (2.05%)
Number of 3-grams hit = 9  (0.84%)
Number of 2-grams hit = 3  (0.28%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle71.out
Perplexity = 3.10, Entropy = 1.63 bits
Computation based on 591 words.
Number of 5-grams hit = 577  (97.63%)
Number of 4-grams hit = 8  (1.35%)
Number of 3-grams hit = 3  (0.51%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle72.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1423 words.
Number of 5-grams hit = 1380  (96.98%)
Number of 4-grams hit = 31  (2.18%)
Number of 3-grams hit = 9  (0.63%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle73.out
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 566 words.
Number of 5-grams hit = 548  (96.82%)
Number of 4-grams hit = 14  (2.47%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle74.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 901 words.
Number of 5-grams hit = 874  (97.00%)
Number of 4-grams hit = 16  (1.78%)
Number of 3-grams hit = 9  (1.00%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle75.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1288 words.
Number of 5-grams hit = 1255  (97.44%)
Number of 4-grams hit = 23  (1.79%)
Number of 3-grams hit = 8  (0.62%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle76.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1515 words.
Number of 5-grams hit = 1470  (97.03%)
Number of 4-grams hit = 32  (2.11%)
Number of 3-grams hit = 10  (0.66%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle77.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 322 words.
Number of 5-grams hit = 297  (92.24%)
Number of 4-grams hit = 16  (4.97%)
Number of 3-grams hit = 5  (1.55%)
Number of 2-grams hit = 3  (0.93%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle78.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 1223 words.
Number of 5-grams hit = 1183  (96.73%)
Number of 4-grams hit = 29  (2.37%)
Number of 3-grams hit = 6  (0.49%)
Number of 2-grams hit = 4  (0.33%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle79.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1163 words.
Number of 5-grams hit = 1128  (96.99%)
Number of 4-grams hit = 23  (1.98%)
Number of 3-grams hit = 8  (0.69%)
Number of 2-grams hit = 3  (0.26%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle80.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 412 words.
Number of 5-grams hit = 393  (95.39%)
Number of 4-grams hit = 12  (2.91%)
Number of 3-grams hit = 5  (1.21%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle81.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 4418 words.
Number of 5-grams hit = 4281  (96.90%)
Number of 4-grams hit = 97  (2.20%)
Number of 3-grams hit = 35  (0.79%)
Number of 2-grams hit = 4  (0.09%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle82.out
Perplexity = 3.54, Entropy = 1.83 bits
Computation based on 510 words.
Number of 5-grams hit = 500  (98.04%)
Number of 4-grams hit = 3  (0.59%)
Number of 3-grams hit = 5  (0.98%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle83.out
Perplexity = 4.06, Entropy = 2.02 bits
Computation based on 476 words.
Number of 5-grams hit = 456  (95.80%)
Number of 4-grams hit = 13  (2.73%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 2  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle84.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 431 words.
Number of 5-grams hit = 423  (98.14%)
Number of 4-grams hit = 5  (1.16%)
Number of 3-grams hit = 1  (0.23%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle85.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 1317 words.
Number of 5-grams hit = 1279  (97.11%)
Number of 4-grams hit = 29  (2.20%)
Number of 3-grams hit = 6  (0.46%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle86.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 309 words.
Number of 5-grams hit = 304  (98.38%)
Number of 4-grams hit = 2  (0.65%)
Number of 3-grams hit = 1  (0.32%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle87.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 676 words.
Number of 5-grams hit = 649  (96.01%)
Number of 4-grams hit = 14  (2.07%)
Number of 3-grams hit = 9  (1.33%)
Number of 2-grams hit = 3  (0.44%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle88.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 1583 words.
Number of 5-grams hit = 1547  (97.73%)
Number of 4-grams hit = 25  (1.58%)
Number of 3-grams hit = 7  (0.44%)
Number of 2-grams hit = 3  (0.19%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle89.out
Perplexity = 4.23, Entropy = 2.08 bits
Computation based on 414 words.
Number of 5-grams hit = 394  (95.17%)
Number of 4-grams hit = 12  (2.90%)
Number of 3-grams hit = 5  (1.21%)
Number of 2-grams hit = 2  (0.48%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle90.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 555 words.
Number of 5-grams hit = 537  (96.76%)
Number of 4-grams hit = 9  (1.62%)
Number of 3-grams hit = 7  (1.26%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle91.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 371 words.
Number of 5-grams hit = 356  (95.96%)
Number of 4-grams hit = 8  (2.16%)
Number of 3-grams hit = 5  (1.35%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle92.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1134 words.
Number of 5-grams hit = 1109  (97.80%)
Number of 4-grams hit = 19  (1.68%)
Number of 3-grams hit = 4  (0.35%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle93.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 745 words.
Number of 5-grams hit = 722  (96.91%)
Number of 4-grams hit = 14  (1.88%)
Number of 3-grams hit = 7  (0.94%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle94.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1395 words.
Number of 5-grams hit = 1354  (97.06%)
Number of 4-grams hit = 27  (1.94%)
Number of 3-grams hit = 10  (0.72%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle95.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 546 words.
Number of 5-grams hit = 532  (97.44%)
Number of 4-grams hit = 8  (1.47%)
Number of 3-grams hit = 4  (0.73%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle96.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 439 words.
Number of 5-grams hit = 422  (96.13%)
Number of 4-grams hit = 10  (2.28%)
Number of 3-grams hit = 4  (0.91%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle97.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 601 words.
Number of 5-grams hit = 586  (97.50%)
Number of 4-grams hit = 7  (1.16%)
Number of 3-grams hit = 5  (0.83%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle98.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 391 words.
Number of 5-grams hit = 379  (96.93%)
Number of 4-grams hit = 7  (1.79%)
Number of 3-grams hit = 2  (0.51%)
Number of 2-grams hit = 2  (0.51%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle99.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 497 words.
Number of 5-grams hit = 483  (97.18%)
Number of 4-grams hit = 9  (1.81%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle100.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 513 words.
Number of 5-grams hit = 496  (96.69%)
Number of 4-grams hit = 9  (1.75%)
Number of 3-grams hit = 6  (1.17%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle101.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 554 words.
Number of 5-grams hit = 540  (97.47%)
Number of 4-grams hit = 10  (1.81%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle102.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 448 words.
Number of 5-grams hit = 428  (95.54%)
Number of 4-grams hit = 12  (2.68%)
Number of 3-grams hit = 6  (1.34%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle103.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 536 words.
Number of 5-grams hit = 526  (98.13%)
Number of 4-grams hit = 7  (1.31%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle104.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 6983 words.
Number of 5-grams hit = 6762  (96.84%)
Number of 4-grams hit = 164  (2.35%)
Number of 3-grams hit = 47  (0.67%)
Number of 2-grams hit = 9  (0.13%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle105.out
Perplexity = 4.53, Entropy = 2.18 bits
Computation based on 485 words.
Number of 5-grams hit = 454  (93.61%)
Number of 4-grams hit = 22  (4.54%)
Number of 3-grams hit = 7  (1.44%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle106.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 875 words.
Number of 5-grams hit = 855  (97.71%)
Number of 4-grams hit = 15  (1.71%)
Number of 3-grams hit = 3  (0.34%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle107.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 529 words.
Number of 5-grams hit = 512  (96.79%)
Number of 4-grams hit = 10  (1.89%)
Number of 3-grams hit = 4  (0.76%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle108.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 568 words.
Number of 5-grams hit = 556  (97.89%)
Number of 4-grams hit = 6  (1.06%)
Number of 3-grams hit = 3  (0.53%)
Number of 2-grams hit = 2  (0.35%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle109.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1223 words.
Number of 5-grams hit = 1175  (96.08%)
Number of 4-grams hit = 29  (2.37%)
Number of 3-grams hit = 16  (1.31%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle110.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 981 words.
Number of 5-grams hit = 955  (97.35%)
Number of 4-grams hit = 19  (1.94%)
Number of 3-grams hit = 5  (0.51%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle111.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 1359 words.
Number of 5-grams hit = 1347  (99.12%)
Number of 4-grams hit = 8  (0.59%)
Number of 3-grams hit = 2  (0.15%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle112.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 355 words.
Number of 5-grams hit = 347  (97.75%)
Number of 4-grams hit = 4  (1.13%)
Number of 3-grams hit = 2  (0.56%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle113.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 6382 words.
Number of 5-grams hit = 6193  (97.04%)
Number of 4-grams hit = 124  (1.94%)
Number of 3-grams hit = 53  (0.83%)
Number of 2-grams hit = 11  (0.17%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle114.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 552 words.
Number of 5-grams hit = 544  (98.55%)
Number of 4-grams hit = 5  (0.91%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle115.out
Perplexity = 3.54, Entropy = 1.83 bits
Computation based on 698 words.
Number of 5-grams hit = 685  (98.14%)
Number of 4-grams hit = 9  (1.29%)
Number of 3-grams hit = 2  (0.29%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle116.out
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 1647 words.
Number of 5-grams hit = 1567  (95.14%)
Number of 4-grams hit = 54  (3.28%)
Number of 3-grams hit = 20  (1.21%)
Number of 2-grams hit = 5  (0.30%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle117.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 557 words.
Number of 5-grams hit = 545  (97.85%)
Number of 4-grams hit = 6  (1.08%)
Number of 3-grams hit = 3  (0.54%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle118.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 667 words.
Number of 5-grams hit = 647  (97.00%)
Number of 4-grams hit = 15  (2.25%)
Number of 3-grams hit = 3  (0.45%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle119.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 424 words.
Number of 5-grams hit = 414  (97.64%)
Number of 4-grams hit = 6  (1.42%)
Number of 3-grams hit = 2  (0.47%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle120.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 608 words.
Number of 5-grams hit = 576  (94.74%)
Number of 4-grams hit = 22  (3.62%)
Number of 3-grams hit = 5  (0.82%)
Number of 2-grams hit = 4  (0.66%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle121.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 7778 words.
Number of 5-grams hit = 7598  (97.69%)
Number of 4-grams hit = 131  (1.68%)
Number of 3-grams hit = 43  (0.55%)
Number of 2-grams hit = 5  (0.06%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle122.out
Perplexity = 4.19, Entropy = 2.07 bits
Computation based on 353 words.
Number of 5-grams hit = 336  (95.18%)
Number of 4-grams hit = 12  (3.40%)
Number of 3-grams hit = 3  (0.85%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle123.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 553 words.
Number of 5-grams hit = 539  (97.47%)
Number of 4-grams hit = 8  (1.45%)
Number of 3-grams hit = 4  (0.72%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle124.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 873 words.
Number of 5-grams hit = 857  (98.17%)
Number of 4-grams hit = 9  (1.03%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle125.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 525 words.
Number of 5-grams hit = 513  (97.71%)
Number of 4-grams hit = 7  (1.33%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle126.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 587 words.
Number of 5-grams hit = 568  (96.76%)
Number of 4-grams hit = 12  (2.04%)
Number of 3-grams hit = 5  (0.85%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle127.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1001 words.
Number of 5-grams hit = 957  (95.60%)
Number of 4-grams hit = 28  (2.80%)
Number of 3-grams hit = 13  (1.30%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle128.out
Perplexity = 4.21, Entropy = 2.07 bits
Computation based on 462 words.
Number of 5-grams hit = 440  (95.24%)
Number of 4-grams hit = 14  (3.03%)
Number of 3-grams hit = 6  (1.30%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle129.out
Perplexity = 4.07, Entropy = 2.03 bits
Computation based on 511 words.
Number of 5-grams hit = 496  (97.06%)
Number of 4-grams hit = 6  (1.17%)
Number of 3-grams hit = 7  (1.37%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle130.out
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 1659 words.
Number of 5-grams hit = 1628  (98.13%)
Number of 4-grams hit = 21  (1.27%)
Number of 3-grams hit = 8  (0.48%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle131.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 7398 words.
Number of 5-grams hit = 7240  (97.86%)
Number of 4-grams hit = 116  (1.57%)
Number of 3-grams hit = 37  (0.50%)
Number of 2-grams hit = 4  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle132.out
Perplexity = 3.28, Entropy = 1.72 bits
Computation based on 1222 words.
Number of 5-grams hit = 1203  (98.45%)
Number of 4-grams hit = 12  (0.98%)
Number of 3-grams hit = 5  (0.41%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle133.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 262 words.
Number of 5-grams hit = 250  (95.42%)
Number of 4-grams hit = 5  (1.91%)
Number of 3-grams hit = 5  (1.91%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle134.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 539 words.
Number of 5-grams hit = 519  (96.29%)
Number of 4-grams hit = 15  (2.78%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle135.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 521 words.
Number of 5-grams hit = 506  (97.12%)
Number of 4-grams hit = 11  (2.11%)
Number of 3-grams hit = 2  (0.38%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle136.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 567 words.
Number of 5-grams hit = 554  (97.71%)
Number of 4-grams hit = 9  (1.59%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle137.out
Perplexity = 4.07, Entropy = 2.03 bits
Computation based on 361 words.
Number of 5-grams hit = 339  (93.91%)
Number of 4-grams hit = 14  (3.88%)
Number of 3-grams hit = 5  (1.39%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle138.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 342 words.
Number of 5-grams hit = 325  (95.03%)
Number of 4-grams hit = 13  (3.80%)
Number of 3-grams hit = 2  (0.58%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle139.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 298 words.
Number of 5-grams hit = 292  (97.99%)
Number of 4-grams hit = 3  (1.01%)
Number of 3-grams hit = 1  (0.34%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle140.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 400 words.
Number of 5-grams hit = 379  (94.75%)
Number of 4-grams hit = 13  (3.25%)
Number of 3-grams hit = 6  (1.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle141.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 467 words.
Number of 5-grams hit = 453  (97.00%)
Number of 4-grams hit = 9  (1.93%)
Number of 3-grams hit = 2  (0.43%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle142.out
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 758 words.
Number of 5-grams hit = 721  (95.12%)
Number of 4-grams hit = 24  (3.17%)
Number of 3-grams hit = 11  (1.45%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle143.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 412 words.
Number of 5-grams hit = 404  (98.06%)
Number of 4-grams hit = 4  (0.97%)
Number of 3-grams hit = 2  (0.49%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle144.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 541 words.
Number of 5-grams hit = 531  (98.15%)
Number of 4-grams hit = 7  (1.29%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle145.out
Perplexity = 3.33, Entropy = 1.73 bits
Computation based on 2107 words.
Number of 5-grams hit = 2070  (98.24%)
Number of 4-grams hit = 25  (1.19%)
Number of 3-grams hit = 10  (0.47%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle146.out
Perplexity = 4.10, Entropy = 2.04 bits
Computation based on 262 words.
Number of 5-grams hit = 247  (94.27%)
Number of 4-grams hit = 7  (2.67%)
Number of 3-grams hit = 4  (1.53%)
Number of 2-grams hit = 3  (1.15%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle147.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 714 words.
Number of 5-grams hit = 691  (96.78%)
Number of 4-grams hit = 15  (2.10%)
Number of 3-grams hit = 5  (0.70%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle148.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 1886 words.
Number of 5-grams hit = 1837  (97.40%)
Number of 4-grams hit = 29  (1.54%)
Number of 3-grams hit = 18  (0.95%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle149.out
Perplexity = 3.17, Entropy = 1.67 bits
Computation based on 612 words.
Number of 5-grams hit = 599  (97.88%)
Number of 4-grams hit = 8  (1.31%)
Number of 3-grams hit = 3  (0.49%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle150.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 686 words.
Number of 5-grams hit = 655  (95.48%)
Number of 4-grams hit = 18  (2.62%)
Number of 3-grams hit = 10  (1.46%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle151.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 188 words.
Number of 5-grams hit = 174  (92.55%)
Number of 4-grams hit = 6  (3.19%)
Number of 3-grams hit = 5  (2.66%)
Number of 2-grams hit = 2  (1.06%)
Number of 1-grams hit = 1  (0.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle152.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 1984 words.
Number of 5-grams hit = 1932  (97.38%)
Number of 4-grams hit = 33  (1.66%)
Number of 3-grams hit = 16  (0.81%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle153.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 629 words.
Number of 5-grams hit = 606  (96.34%)
Number of 4-grams hit = 15  (2.38%)
Number of 3-grams hit = 5  (0.79%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle154.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1160 words.
Number of 5-grams hit = 1136  (97.93%)
Number of 4-grams hit = 18  (1.55%)
Number of 3-grams hit = 4  (0.34%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle155.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 695 words.
Number of 5-grams hit = 672  (96.69%)
Number of 4-grams hit = 13  (1.87%)
Number of 3-grams hit = 7  (1.01%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle156.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 1549 words.
Number of 5-grams hit = 1494  (96.45%)
Number of 4-grams hit = 37  (2.39%)
Number of 3-grams hit = 13  (0.84%)
Number of 2-grams hit = 4  (0.26%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle157.out
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 431 words.
Number of 5-grams hit = 404  (93.74%)
Number of 4-grams hit = 18  (4.18%)
Number of 3-grams hit = 6  (1.39%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle158.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 623 words.
Number of 5-grams hit = 600  (96.31%)
Number of 4-grams hit = 14  (2.25%)
Number of 3-grams hit = 6  (0.96%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle159.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1362 words.
Number of 5-grams hit = 1313  (96.40%)
Number of 4-grams hit = 30  (2.20%)
Number of 3-grams hit = 12  (0.88%)
Number of 2-grams hit = 6  (0.44%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle160.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 584 words.
Number of 5-grams hit = 560  (95.89%)
Number of 4-grams hit = 16  (2.74%)
Number of 3-grams hit = 6  (1.03%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle161.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 211 words.
Number of 5-grams hit = 202  (95.73%)
Number of 4-grams hit = 6  (2.84%)
Number of 3-grams hit = 1  (0.47%)
Number of 2-grams hit = 1  (0.47%)
Number of 1-grams hit = 1  (0.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle162.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 1245 words.
Number of 5-grams hit = 1210  (97.19%)
Number of 4-grams hit = 23  (1.85%)
Number of 3-grams hit = 9  (0.72%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle163.out
Perplexity = 2.87, Entropy = 1.52 bits
Computation based on 752 words.
Number of 5-grams hit = 740  (98.40%)
Number of 4-grams hit = 7  (0.93%)
Number of 3-grams hit = 3  (0.40%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle164.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1948 words.
Number of 5-grams hit = 1885  (96.77%)
Number of 4-grams hit = 45  (2.31%)
Number of 3-grams hit = 15  (0.77%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle165.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 593 words.
Number of 5-grams hit = 579  (97.64%)
Number of 4-grams hit = 9  (1.52%)
Number of 3-grams hit = 3  (0.51%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle166.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 555 words.
Number of 5-grams hit = 539  (97.12%)
Number of 4-grams hit = 7  (1.26%)
Number of 3-grams hit = 5  (0.90%)
Number of 2-grams hit = 3  (0.54%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle167.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1260 words.
Number of 5-grams hit = 1234  (97.94%)
Number of 4-grams hit = 15  (1.19%)
Number of 3-grams hit = 7  (0.56%)
Number of 2-grams hit = 3  (0.24%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle168.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 571 words.
Number of 5-grams hit = 560  (98.07%)
Number of 4-grams hit = 7  (1.23%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle169.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 551 words.
Number of 5-grams hit = 535  (97.10%)
Number of 4-grams hit = 12  (2.18%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle170.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1235 words.
Number of 5-grams hit = 1210  (97.98%)
Number of 4-grams hit = 20  (1.62%)
Number of 3-grams hit = 3  (0.24%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle171.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 656 words.
Number of 5-grams hit = 627  (95.58%)
Number of 4-grams hit = 17  (2.59%)
Number of 3-grams hit = 9  (1.37%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle172.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 734 words.
Number of 5-grams hit = 719  (97.96%)
Number of 4-grams hit = 7  (0.95%)
Number of 3-grams hit = 5  (0.68%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle173.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 1110 words.
Number of 5-grams hit = 1084  (97.66%)
Number of 4-grams hit = 18  (1.62%)
Number of 3-grams hit = 4  (0.36%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle174.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 493 words.
Number of 5-grams hit = 477  (96.75%)
Number of 4-grams hit = 11  (2.23%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle175.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 486 words.
Number of 5-grams hit = 456  (93.83%)
Number of 4-grams hit = 22  (4.53%)
Number of 3-grams hit = 5  (1.03%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle176.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 800 words.
Number of 5-grams hit = 771  (96.38%)
Number of 4-grams hit = 19  (2.38%)
Number of 3-grams hit = 7  (0.88%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle177.out
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 557 words.
Number of 5-grams hit = 529  (94.97%)
Number of 4-grams hit = 17  (3.05%)
Number of 3-grams hit = 9  (1.62%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle178.out
Perplexity = 4.11, Entropy = 2.04 bits
Computation based on 183 words.
Number of 5-grams hit = 171  (93.44%)
Number of 4-grams hit = 6  (3.28%)
Number of 3-grams hit = 3  (1.64%)
Number of 2-grams hit = 2  (1.09%)
Number of 1-grams hit = 1  (0.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle179.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 491 words.
Number of 5-grams hit = 480  (97.76%)
Number of 4-grams hit = 5  (1.02%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle180.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 473 words.
Number of 5-grams hit = 459  (97.04%)
Number of 4-grams hit = 8  (1.69%)
Number of 3-grams hit = 4  (0.85%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle181.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 955 words.
Number of 5-grams hit = 934  (97.80%)
Number of 4-grams hit = 15  (1.57%)
Number of 3-grams hit = 4  (0.42%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle182.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 407 words.
Number of 5-grams hit = 390  (95.82%)
Number of 4-grams hit = 10  (2.46%)
Number of 3-grams hit = 4  (0.98%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle183.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 504 words.
Number of 5-grams hit = 495  (98.21%)
Number of 4-grams hit = 3  (0.60%)
Number of 3-grams hit = 4  (0.79%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle184.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 5958 words.
Number of 5-grams hit = 5830  (97.85%)
Number of 4-grams hit = 96  (1.61%)
Number of 3-grams hit = 28  (0.47%)
Number of 2-grams hit = 3  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle185.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 1070 words.
Number of 5-grams hit = 1046  (97.76%)
Number of 4-grams hit = 17  (1.59%)
Number of 3-grams hit = 5  (0.47%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle186.out
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 529 words.
Number of 5-grams hit = 524  (99.05%)
Number of 4-grams hit = 2  (0.38%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle187.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 3137 words.
Number of 5-grams hit = 3034  (96.72%)
Number of 4-grams hit = 83  (2.65%)
Number of 3-grams hit = 18  (0.57%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle188.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 729 words.
Number of 5-grams hit = 708  (97.12%)
Number of 4-grams hit = 15  (2.06%)
Number of 3-grams hit = 4  (0.55%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle189.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 580 words.
Number of 5-grams hit = 563  (97.07%)
Number of 4-grams hit = 9  (1.55%)
Number of 3-grams hit = 5  (0.86%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle190.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 421 words.
Number of 5-grams hit = 414  (98.34%)
Number of 4-grams hit = 4  (0.95%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle191.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 5916 words.
Number of 5-grams hit = 5771  (97.55%)
Number of 4-grams hit = 111  (1.88%)
Number of 3-grams hit = 27  (0.46%)
Number of 2-grams hit = 4  (0.07%)
Number of 1-grams hit = 3  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle192.out
Perplexity = 2.68, Entropy = 1.42 bits
Computation based on 818 words.
Number of 5-grams hit = 810  (99.02%)
Number of 4-grams hit = 4  (0.49%)
Number of 3-grams hit = 2  (0.24%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle193.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 170 words.
Number of 5-grams hit = 162  (95.29%)
Number of 4-grams hit = 5  (2.94%)
Number of 3-grams hit = 1  (0.59%)
Number of 2-grams hit = 1  (0.59%)
Number of 1-grams hit = 1  (0.59%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle194.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 661 words.
Number of 5-grams hit = 642  (97.13%)
Number of 4-grams hit = 11  (1.66%)
Number of 3-grams hit = 6  (0.91%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle195.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 864 words.
Number of 5-grams hit = 840  (97.22%)
Number of 4-grams hit = 15  (1.74%)
Number of 3-grams hit = 7  (0.81%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle196.out
Perplexity = 3.26, Entropy = 1.71 bits
Computation based on 730 words.
Number of 5-grams hit = 709  (97.12%)
Number of 4-grams hit = 11  (1.51%)
Number of 3-grams hit = 8  (1.10%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle197.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 251 words.
Number of 5-grams hit = 245  (97.61%)
Number of 4-grams hit = 3  (1.20%)
Number of 3-grams hit = 1  (0.40%)
Number of 2-grams hit = 1  (0.40%)
Number of 1-grams hit = 1  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle198.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 768 words.
Number of 5-grams hit = 750  (97.66%)
Number of 4-grams hit = 9  (1.17%)
Number of 3-grams hit = 6  (0.78%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle199.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 4967 words.
Number of 5-grams hit = 4867  (97.99%)
Number of 4-grams hit = 78  (1.57%)
Number of 3-grams hit = 18  (0.36%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle200.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 702 words.
Number of 5-grams hit = 688  (98.01%)
Number of 4-grams hit = 7  (1.00%)
Number of 3-grams hit = 5  (0.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle201.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 507 words.
Number of 5-grams hit = 488  (96.25%)
Number of 4-grams hit = 12  (2.37%)
Number of 3-grams hit = 5  (0.99%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle202.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 1262 words.
Number of 5-grams hit = 1215  (96.28%)
Number of 4-grams hit = 33  (2.61%)
Number of 3-grams hit = 11  (0.87%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle203.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 5293 words.
Number of 5-grams hit = 5203  (98.30%)
Number of 4-grams hit = 69  (1.30%)
Number of 3-grams hit = 17  (0.32%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle204.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 5882 words.
Number of 5-grams hit = 5721  (97.26%)
Number of 4-grams hit = 118  (2.01%)
Number of 3-grams hit = 39  (0.66%)
Number of 2-grams hit = 3  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle205.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 603 words.
Number of 5-grams hit = 581  (96.35%)
Number of 4-grams hit = 13  (2.16%)
Number of 3-grams hit = 7  (1.16%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle206.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 636 words.
Number of 5-grams hit = 610  (95.91%)
Number of 4-grams hit = 18  (2.83%)
Number of 3-grams hit = 5  (0.79%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle207.out
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 779 words.
Number of 5-grams hit = 748  (96.02%)
Number of 4-grams hit = 18  (2.31%)
Number of 3-grams hit = 9  (1.16%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle208.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 951 words.
Number of 5-grams hit = 920  (96.74%)
Number of 4-grams hit = 18  (1.89%)
Number of 3-grams hit = 9  (0.95%)
Number of 2-grams hit = 3  (0.32%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle209.out
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 283 words.
Number of 5-grams hit = 270  (95.41%)
Number of 4-grams hit = 6  (2.12%)
Number of 3-grams hit = 5  (1.77%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle210.out
Perplexity = 3.17, Entropy = 1.66 bits
Computation based on 886 words.
Number of 5-grams hit = 871  (98.31%)
Number of 4-grams hit = 9  (1.02%)
Number of 3-grams hit = 4  (0.45%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle211.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 504 words.
Number of 5-grams hit = 488  (96.83%)
Number of 4-grams hit = 12  (2.38%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle212.out
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 449 words.
Number of 5-grams hit = 429  (95.55%)
Number of 4-grams hit = 12  (2.67%)
Number of 3-grams hit = 6  (1.34%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle213.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 695 words.
Number of 5-grams hit = 672  (96.69%)
Number of 4-grams hit = 12  (1.73%)
Number of 3-grams hit = 8  (1.15%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle214.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 509 words.
Number of 5-grams hit = 488  (95.87%)
Number of 4-grams hit = 13  (2.55%)
Number of 3-grams hit = 4  (0.79%)
Number of 2-grams hit = 3  (0.59%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle215.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 742 words.
Number of 5-grams hit = 712  (95.96%)
Number of 4-grams hit = 15  (2.02%)
Number of 3-grams hit = 11  (1.48%)
Number of 2-grams hit = 3  (0.40%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle216.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 525 words.
Number of 5-grams hit = 510  (97.14%)
Number of 4-grams hit = 9  (1.71%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle217.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 323 words.
Number of 5-grams hit = 313  (96.90%)
Number of 4-grams hit = 4  (1.24%)
Number of 3-grams hit = 3  (0.93%)
Number of 2-grams hit = 2  (0.62%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle218.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 792 words.
Number of 5-grams hit = 760  (95.96%)
Number of 4-grams hit = 22  (2.78%)
Number of 3-grams hit = 8  (1.01%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle219.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 439 words.
Number of 5-grams hit = 413  (94.08%)
Number of 4-grams hit = 12  (2.73%)
Number of 3-grams hit = 11  (2.51%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle220.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 4706 words.
Number of 5-grams hit = 4605  (97.85%)
Number of 4-grams hit = 71  (1.51%)
Number of 3-grams hit = 26  (0.55%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle221.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 1255 words.
Number of 5-grams hit = 1226  (97.69%)
Number of 4-grams hit = 22  (1.75%)
Number of 3-grams hit = 5  (0.40%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle222.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 4839 words.
Number of 5-grams hit = 4688  (96.88%)
Number of 4-grams hit = 99  (2.05%)
Number of 3-grams hit = 44  (0.91%)
Number of 2-grams hit = 7  (0.14%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle223.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 895 words.
Number of 5-grams hit = 883  (98.66%)
Number of 4-grams hit = 7  (0.78%)
Number of 3-grams hit = 3  (0.34%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle224.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 406 words.
Number of 5-grams hit = 396  (97.54%)
Number of 4-grams hit = 5  (1.23%)
Number of 3-grams hit = 3  (0.74%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle225.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 777 words.
Number of 5-grams hit = 752  (96.78%)
Number of 4-grams hit = 14  (1.80%)
Number of 3-grams hit = 7  (0.90%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle226.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 1314 words.
Number of 5-grams hit = 1285  (97.79%)
Number of 4-grams hit = 23  (1.75%)
Number of 3-grams hit = 4  (0.30%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle227.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1017 words.
Number of 5-grams hit = 988  (97.15%)
Number of 4-grams hit = 21  (2.06%)
Number of 3-grams hit = 5  (0.49%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle228.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 851 words.
Number of 5-grams hit = 831  (97.65%)
Number of 4-grams hit = 14  (1.65%)
Number of 3-grams hit = 4  (0.47%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle229.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 518 words.
Number of 5-grams hit = 501  (96.72%)
Number of 4-grams hit = 7  (1.35%)
Number of 3-grams hit = 7  (1.35%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle230.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 337 words.
Number of 5-grams hit = 328  (97.33%)
Number of 4-grams hit = 6  (1.78%)
Number of 3-grams hit = 1  (0.30%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle231.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 850 words.
Number of 5-grams hit = 833  (98.00%)
Number of 4-grams hit = 12  (1.41%)
Number of 3-grams hit = 2  (0.24%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle232.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 391 words.
Number of 5-grams hit = 374  (95.65%)
Number of 4-grams hit = 11  (2.81%)
Number of 3-grams hit = 4  (1.02%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle233.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 734 words.
Number of 5-grams hit = 717  (97.68%)
Number of 4-grams hit = 11  (1.50%)
Number of 3-grams hit = 4  (0.54%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle234.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 1446 words.
Number of 5-grams hit = 1407  (97.30%)
Number of 4-grams hit = 22  (1.52%)
Number of 3-grams hit = 12  (0.83%)
Number of 2-grams hit = 4  (0.28%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle235.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 379 words.
Number of 5-grams hit = 367  (96.83%)
Number of 4-grams hit = 6  (1.58%)
Number of 3-grams hit = 3  (0.79%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle236.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 801 words.
Number of 5-grams hit = 774  (96.63%)
Number of 4-grams hit = 21  (2.62%)
Number of 3-grams hit = 4  (0.50%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle237.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 423 words.
Number of 5-grams hit = 412  (97.40%)
Number of 4-grams hit = 4  (0.95%)
Number of 3-grams hit = 4  (0.95%)
Number of 2-grams hit = 2  (0.47%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle238.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 339 words.
Number of 5-grams hit = 328  (96.76%)
Number of 4-grams hit = 7  (2.06%)
Number of 3-grams hit = 2  (0.59%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle239.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 530 words.
Number of 5-grams hit = 515  (97.17%)
Number of 4-grams hit = 9  (1.70%)
Number of 3-grams hit = 4  (0.75%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle240.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 350 words.
Number of 5-grams hit = 343  (98.00%)
Number of 4-grams hit = 4  (1.14%)
Number of 3-grams hit = 1  (0.29%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle241.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 334 words.
Number of 5-grams hit = 320  (95.81%)
Number of 4-grams hit = 4  (1.20%)
Number of 3-grams hit = 6  (1.80%)
Number of 2-grams hit = 3  (0.90%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle242.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 319 words.
Number of 5-grams hit = 313  (98.12%)
Number of 4-grams hit = 3  (0.94%)
Number of 3-grams hit = 1  (0.31%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle243.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 813 words.
Number of 5-grams hit = 799  (98.28%)
Number of 4-grams hit = 7  (0.86%)
Number of 3-grams hit = 4  (0.49%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle244.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 457 words.
Number of 5-grams hit = 443  (96.94%)
Number of 4-grams hit = 10  (2.19%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle245.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 5533 words.
Number of 5-grams hit = 5400  (97.60%)
Number of 4-grams hit = 91  (1.64%)
Number of 3-grams hit = 35  (0.63%)
Number of 2-grams hit = 6  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle246.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 304 words.
Number of 5-grams hit = 288  (94.74%)
Number of 4-grams hit = 11  (3.62%)
Number of 3-grams hit = 3  (0.99%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle247.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 492 words.
Number of 5-grams hit = 478  (97.15%)
Number of 4-grams hit = 9  (1.83%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle248.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 388 words.
Number of 5-grams hit = 378  (97.42%)
Number of 4-grams hit = 7  (1.80%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle249.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 928 words.
Number of 5-grams hit = 897  (96.66%)
Number of 4-grams hit = 21  (2.26%)
Number of 3-grams hit = 8  (0.86%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle250.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 5273 words.
Number of 5-grams hit = 5148  (97.63%)
Number of 4-grams hit = 78  (1.48%)
Number of 3-grams hit = 40  (0.76%)
Number of 2-grams hit = 6  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle251.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 5439 words.
Number of 5-grams hit = 5302  (97.48%)
Number of 4-grams hit = 98  (1.80%)
Number of 3-grams hit = 33  (0.61%)
Number of 2-grams hit = 5  (0.09%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle252.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 579 words.
Number of 5-grams hit = 558  (96.37%)
Number of 4-grams hit = 12  (2.07%)
Number of 3-grams hit = 6  (1.04%)
Number of 2-grams hit = 2  (0.35%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle253.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 5348 words.
Number of 5-grams hit = 5213  (97.48%)
Number of 4-grams hit = 94  (1.76%)
Number of 3-grams hit = 32  (0.60%)
Number of 2-grams hit = 8  (0.15%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle254.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 836 words.
Number of 5-grams hit = 824  (98.56%)
Number of 4-grams hit = 6  (0.72%)
Number of 3-grams hit = 4  (0.48%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle255.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 526 words.
Number of 5-grams hit = 513  (97.53%)
Number of 4-grams hit = 7  (1.33%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle256.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 837 words.
Number of 5-grams hit = 785  (93.79%)
Number of 4-grams hit = 28  (3.35%)
Number of 3-grams hit = 18  (2.15%)
Number of 2-grams hit = 5  (0.60%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle257.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 6431 words.
Number of 5-grams hit = 6243  (97.08%)
Number of 4-grams hit = 124  (1.93%)
Number of 3-grams hit = 51  (0.79%)
Number of 2-grams hit = 10  (0.16%)
Number of 1-grams hit = 3  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle258.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 444 words.
Number of 5-grams hit = 425  (95.72%)
Number of 4-grams hit = 8  (1.80%)
Number of 3-grams hit = 9  (2.03%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle259.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 764 words.
Number of 5-grams hit = 750  (98.17%)
Number of 4-grams hit = 9  (1.18%)
Number of 3-grams hit = 3  (0.39%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle260.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 7634 words.
Number of 5-grams hit = 7492  (98.14%)
Number of 4-grams hit = 113  (1.48%)
Number of 3-grams hit = 24  (0.31%)
Number of 2-grams hit = 4  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle261.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 717 words.
Number of 5-grams hit = 683  (95.26%)
Number of 4-grams hit = 21  (2.93%)
Number of 3-grams hit = 10  (1.39%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle262.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 721 words.
Number of 5-grams hit = 704  (97.64%)
Number of 4-grams hit = 11  (1.53%)
Number of 3-grams hit = 3  (0.42%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle263.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 411 words.
Number of 5-grams hit = 389  (94.65%)
Number of 4-grams hit = 15  (3.65%)
Number of 3-grams hit = 5  (1.22%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle264.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 857 words.
Number of 5-grams hit = 851  (99.30%)
Number of 4-grams hit = 3  (0.35%)
Number of 3-grams hit = 1  (0.12%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle265.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 602 words.
Number of 5-grams hit = 590  (98.01%)
Number of 4-grams hit = 8  (1.33%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle266.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 468 words.
Number of 5-grams hit = 455  (97.22%)
Number of 4-grams hit = 7  (1.50%)
Number of 3-grams hit = 4  (0.85%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle267.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 6787 words.
Number of 5-grams hit = 6633  (97.73%)
Number of 4-grams hit = 104  (1.53%)
Number of 3-grams hit = 42  (0.62%)
Number of 2-grams hit = 7  (0.10%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle268.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 6349 words.
Number of 5-grams hit = 6167  (97.13%)
Number of 4-grams hit = 128  (2.02%)
Number of 3-grams hit = 46  (0.72%)
Number of 2-grams hit = 7  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle269.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 457 words.
Number of 5-grams hit = 446  (97.59%)
Number of 4-grams hit = 7  (1.53%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle270.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 404 words.
Number of 5-grams hit = 387  (95.79%)
Number of 4-grams hit = 9  (2.23%)
Number of 3-grams hit = 5  (1.24%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle271.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 7909 words.
Number of 5-grams hit = 7705  (97.42%)
Number of 4-grams hit = 151  (1.91%)
Number of 3-grams hit = 49  (0.62%)
Number of 2-grams hit = 3  (0.04%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle272.out
Perplexity = 3.88, Entropy = 1.95 bits
Computation based on 4304 words.
Number of 5-grams hit = 4174  (96.98%)
Number of 4-grams hit = 88  (2.04%)
Number of 3-grams hit = 39  (0.91%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle273.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 683 words.
Number of 5-grams hit = 665  (97.36%)
Number of 4-grams hit = 10  (1.46%)
Number of 3-grams hit = 5  (0.73%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle274.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 704 words.
Number of 5-grams hit = 679  (96.45%)
Number of 4-grams hit = 18  (2.56%)
Number of 3-grams hit = 5  (0.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle275.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 2091 words.
Number of 5-grams hit = 2032  (97.18%)
Number of 4-grams hit = 45  (2.15%)
Number of 3-grams hit = 12  (0.57%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle276.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 506 words.
Number of 5-grams hit = 499  (98.62%)
Number of 4-grams hit = 4  (0.79%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle277.out
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 835 words.
Number of 5-grams hit = 827  (99.04%)
Number of 4-grams hit = 5  (0.60%)
Number of 3-grams hit = 1  (0.12%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle278.out
Perplexity = 4.15, Entropy = 2.05 bits
Computation based on 856 words.
Number of 5-grams hit = 821  (95.91%)
Number of 4-grams hit = 24  (2.80%)
Number of 3-grams hit = 7  (0.82%)
Number of 2-grams hit = 3  (0.35%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle279.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 446 words.
Number of 5-grams hit = 434  (97.31%)
Number of 4-grams hit = 6  (1.35%)
Number of 3-grams hit = 4  (0.90%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle280.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1844 words.
Number of 5-grams hit = 1778  (96.42%)
Number of 4-grams hit = 48  (2.60%)
Number of 3-grams hit = 16  (0.87%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle281.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 610 words.
Number of 5-grams hit = 599  (98.20%)
Number of 4-grams hit = 7  (1.15%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle282.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 779 words.
Number of 5-grams hit = 762  (97.82%)
Number of 4-grams hit = 10  (1.28%)
Number of 3-grams hit = 5  (0.64%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle283.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1294 words.
Number of 5-grams hit = 1255  (96.99%)
Number of 4-grams hit = 27  (2.09%)
Number of 3-grams hit = 8  (0.62%)
Number of 2-grams hit = 3  (0.23%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle284.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 1757 words.
Number of 5-grams hit = 1692  (96.30%)
Number of 4-grams hit = 42  (2.39%)
Number of 3-grams hit = 18  (1.02%)
Number of 2-grams hit = 4  (0.23%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle285.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1159 words.
Number of 5-grams hit = 1131  (97.58%)
Number of 4-grams hit = 16  (1.38%)
Number of 3-grams hit = 8  (0.69%)
Number of 2-grams hit = 3  (0.26%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle286.out
Perplexity = 4.18, Entropy = 2.06 bits
Computation based on 485 words.
Number of 5-grams hit = 468  (96.49%)
Number of 4-grams hit = 11  (2.27%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle287.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1778 words.
Number of 5-grams hit = 1743  (98.03%)
Number of 4-grams hit = 22  (1.24%)
Number of 3-grams hit = 11  (0.62%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle288.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 635 words.
Number of 5-grams hit = 615  (96.85%)
Number of 4-grams hit = 11  (1.73%)
Number of 3-grams hit = 6  (0.94%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle289.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 774 words.
Number of 5-grams hit = 755  (97.55%)
Number of 4-grams hit = 10  (1.29%)
Number of 3-grams hit = 6  (0.78%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle290.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1396 words.
Number of 5-grams hit = 1351  (96.78%)
Number of 4-grams hit = 31  (2.22%)
Number of 3-grams hit = 10  (0.72%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle291.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 552 words.
Number of 5-grams hit = 536  (97.10%)
Number of 4-grams hit = 10  (1.81%)
Number of 3-grams hit = 4  (0.72%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle292.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 575 words.
Number of 5-grams hit = 562  (97.74%)
Number of 4-grams hit = 10  (1.74%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle293.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 593 words.
Number of 5-grams hit = 580  (97.81%)
Number of 4-grams hit = 9  (1.52%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle294.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 2240 words.
Number of 5-grams hit = 2198  (98.12%)
Number of 4-grams hit = 33  (1.47%)
Number of 3-grams hit = 7  (0.31%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle295.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 644 words.
Number of 5-grams hit = 630  (97.83%)
Number of 4-grams hit = 10  (1.55%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle296.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1670 words.
Number of 5-grams hit = 1614  (96.65%)
Number of 4-grams hit = 46  (2.75%)
Number of 3-grams hit = 8  (0.48%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle297.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 550 words.
Number of 5-grams hit = 527  (95.82%)
Number of 4-grams hit = 16  (2.91%)
Number of 3-grams hit = 4  (0.73%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle298.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 3737 words.
Number of 5-grams hit = 3633  (97.22%)
Number of 4-grams hit = 70  (1.87%)
Number of 3-grams hit = 30  (0.80%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle299.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 3001 words.
Number of 5-grams hit = 2935  (97.80%)
Number of 4-grams hit = 51  (1.70%)
Number of 3-grams hit = 13  (0.43%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle300.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 2733 words.
Number of 5-grams hit = 2651  (97.00%)
Number of 4-grams hit = 55  (2.01%)
Number of 3-grams hit = 22  (0.80%)
Number of 2-grams hit = 4  (0.15%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle301.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 2305 words.
Number of 5-grams hit = 2248  (97.53%)
Number of 4-grams hit = 40  (1.74%)
Number of 3-grams hit = 12  (0.52%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 2  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle302.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 698 words.
Number of 5-grams hit = 678  (97.13%)
Number of 4-grams hit = 11  (1.58%)
Number of 3-grams hit = 6  (0.86%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle303.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 941 words.
Number of 5-grams hit = 911  (96.81%)
Number of 4-grams hit = 21  (2.23%)
Number of 3-grams hit = 5  (0.53%)
Number of 2-grams hit = 3  (0.32%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle304.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 446 words.
Number of 5-grams hit = 423  (94.84%)
Number of 4-grams hit = 14  (3.14%)
Number of 3-grams hit = 6  (1.35%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle305.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 331 words.
Number of 5-grams hit = 326  (98.49%)
Number of 4-grams hit = 2  (0.60%)
Number of 3-grams hit = 1  (0.30%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle306.out
Perplexity = 2.79, Entropy = 1.48 bits
Computation based on 635 words.
Number of 5-grams hit = 622  (97.95%)
Number of 4-grams hit = 6  (0.94%)
Number of 3-grams hit = 5  (0.79%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle307.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 557 words.
Number of 5-grams hit = 538  (96.59%)
Number of 4-grams hit = 11  (1.97%)
Number of 3-grams hit = 6  (1.08%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle308.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 417 words.
Number of 5-grams hit = 410  (98.32%)
Number of 4-grams hit = 4  (0.96%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle309.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 265 words.
Number of 5-grams hit = 257  (96.98%)
Number of 4-grams hit = 4  (1.51%)
Number of 3-grams hit = 2  (0.75%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle310.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 522 words.
Number of 5-grams hit = 509  (97.51%)
Number of 4-grams hit = 8  (1.53%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle311.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 497 words.
Number of 5-grams hit = 490  (98.59%)
Number of 4-grams hit = 4  (0.80%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle312.out
Perplexity = 2.85, Entropy = 1.51 bits
Computation based on 651 words.
Number of 5-grams hit = 642  (98.62%)
Number of 4-grams hit = 5  (0.77%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle313.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 512 words.
Number of 5-grams hit = 498  (97.27%)
Number of 4-grams hit = 8  (1.56%)
Number of 3-grams hit = 3  (0.59%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle314.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 3615 words.
Number of 5-grams hit = 3524  (97.48%)
Number of 4-grams hit = 63  (1.74%)
Number of 3-grams hit = 24  (0.66%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle315.out
Perplexity = 3.19, Entropy = 1.68 bits
Computation based on 1186 words.
Number of 5-grams hit = 1166  (98.31%)
Number of 4-grams hit = 12  (1.01%)
Number of 3-grams hit = 6  (0.51%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle316.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 363 words.
Number of 5-grams hit = 348  (95.87%)
Number of 4-grams hit = 7  (1.93%)
Number of 3-grams hit = 5  (1.38%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle317.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 675 words.
Number of 5-grams hit = 658  (97.48%)
Number of 4-grams hit = 10  (1.48%)
Number of 3-grams hit = 5  (0.74%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle318.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 3867 words.
Number of 5-grams hit = 3763  (97.31%)
Number of 4-grams hit = 71  (1.84%)
Number of 3-grams hit = 28  (0.72%)
Number of 2-grams hit = 4  (0.10%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle319.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 641 words.
Number of 5-grams hit = 619  (96.57%)
Number of 4-grams hit = 11  (1.72%)
Number of 3-grams hit = 9  (1.40%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle320.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 405 words.
Number of 5-grams hit = 397  (98.02%)
Number of 4-grams hit = 5  (1.23%)
Number of 3-grams hit = 1  (0.25%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle321.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1172 words.
Number of 5-grams hit = 1130  (96.42%)
Number of 4-grams hit = 30  (2.56%)
Number of 3-grams hit = 9  (0.77%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle322.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1851 words.
Number of 5-grams hit = 1802  (97.35%)
Number of 4-grams hit = 29  (1.57%)
Number of 3-grams hit = 15  (0.81%)
Number of 2-grams hit = 4  (0.22%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle323.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 877 words.
Number of 5-grams hit = 853  (97.26%)
Number of 4-grams hit = 17  (1.94%)
Number of 3-grams hit = 4  (0.46%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle324.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 1808 words.
Number of 5-grams hit = 1776  (98.23%)
Number of 4-grams hit = 24  (1.33%)
Number of 3-grams hit = 6  (0.33%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle325.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 1207 words.
Number of 5-grams hit = 1160  (96.11%)
Number of 4-grams hit = 31  (2.57%)
Number of 3-grams hit = 12  (0.99%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle326.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 655 words.
Number of 5-grams hit = 640  (97.71%)
Number of 4-grams hit = 8  (1.22%)
Number of 3-grams hit = 5  (0.76%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle327.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1223 words.
Number of 5-grams hit = 1199  (98.04%)
Number of 4-grams hit = 20  (1.64%)
Number of 3-grams hit = 2  (0.16%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle328.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 277 words.
Number of 5-grams hit = 270  (97.47%)
Number of 4-grams hit = 4  (1.44%)
Number of 3-grams hit = 1  (0.36%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle329.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 445 words.
Number of 5-grams hit = 422  (94.83%)
Number of 4-grams hit = 13  (2.92%)
Number of 3-grams hit = 7  (1.57%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle330.out
Perplexity = 3.75, Entropy = 1.90 bits
Computation based on 1141 words.
Number of 5-grams hit = 1105  (96.84%)
Number of 4-grams hit = 20  (1.75%)
Number of 3-grams hit = 8  (0.70%)
Number of 2-grams hit = 6  (0.53%)
Number of 1-grams hit = 2  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle331.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 2113 words.
Number of 5-grams hit = 2072  (98.06%)
Number of 4-grams hit = 30  (1.42%)
Number of 3-grams hit = 8  (0.38%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle332.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 626 words.
Number of 5-grams hit = 605  (96.65%)
Number of 4-grams hit = 14  (2.24%)
Number of 3-grams hit = 5  (0.80%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle333.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1432 words.
Number of 5-grams hit = 1375  (96.02%)
Number of 4-grams hit = 37  (2.58%)
Number of 3-grams hit = 17  (1.19%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle334.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 1057 words.
Number of 5-grams hit = 1037  (98.11%)
Number of 4-grams hit = 15  (1.42%)
Number of 3-grams hit = 3  (0.28%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle335.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1850 words.
Number of 5-grams hit = 1808  (97.73%)
Number of 4-grams hit = 27  (1.46%)
Number of 3-grams hit = 12  (0.65%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle336.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 1239 words.
Number of 5-grams hit = 1196  (96.53%)
Number of 4-grams hit = 27  (2.18%)
Number of 3-grams hit = 12  (0.97%)
Number of 2-grams hit = 3  (0.24%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle337.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 438 words.
Number of 5-grams hit = 426  (97.26%)
Number of 4-grams hit = 8  (1.83%)
Number of 3-grams hit = 2  (0.46%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle338.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 3804 words.
Number of 5-grams hit = 3728  (98.00%)
Number of 4-grams hit = 46  (1.21%)
Number of 3-grams hit = 23  (0.60%)
Number of 2-grams hit = 6  (0.16%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle339.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 408 words.
Number of 5-grams hit = 385  (94.36%)
Number of 4-grams hit = 15  (3.68%)
Number of 3-grams hit = 5  (1.23%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle340.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1243 words.
Number of 5-grams hit = 1201  (96.62%)
Number of 4-grams hit = 31  (2.49%)
Number of 3-grams hit = 8  (0.64%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle341.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1498 words.
Number of 5-grams hit = 1450  (96.80%)
Number of 4-grams hit = 34  (2.27%)
Number of 3-grams hit = 10  (0.67%)
Number of 2-grams hit = 3  (0.20%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle342.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1604 words.
Number of 5-grams hit = 1542  (96.13%)
Number of 4-grams hit = 41  (2.56%)
Number of 3-grams hit = 17  (1.06%)
Number of 2-grams hit = 3  (0.19%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle343.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 522 words.
Number of 5-grams hit = 497  (95.21%)
Number of 4-grams hit = 13  (2.49%)
Number of 3-grams hit = 9  (1.72%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle344.out
Perplexity = 3.00, Entropy = 1.58 bits
Computation based on 620 words.
Number of 5-grams hit = 611  (98.55%)
Number of 4-grams hit = 4  (0.65%)
Number of 3-grams hit = 3  (0.48%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle345.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 3421 words.
Number of 5-grams hit = 3361  (98.25%)
Number of 4-grams hit = 40  (1.17%)
Number of 3-grams hit = 16  (0.47%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle346.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1725 words.
Number of 5-grams hit = 1679  (97.33%)
Number of 4-grams hit = 36  (2.09%)
Number of 3-grams hit = 8  (0.46%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle347.out
Perplexity = 2.97, Entropy = 1.57 bits
Computation based on 690 words.
Number of 5-grams hit = 671  (97.25%)
Number of 4-grams hit = 12  (1.74%)
Number of 3-grams hit = 5  (0.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle348.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 2288 words.
Number of 5-grams hit = 2231  (97.51%)
Number of 4-grams hit = 48  (2.10%)
Number of 3-grams hit = 7  (0.31%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle349.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 2145 words.
Number of 5-grams hit = 2080  (96.97%)
Number of 4-grams hit = 44  (2.05%)
Number of 3-grams hit = 19  (0.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle350.out
Perplexity = 4.18, Entropy = 2.06 bits
Computation based on 400 words.
Number of 5-grams hit = 379  (94.75%)
Number of 4-grams hit = 11  (2.75%)
Number of 3-grams hit = 7  (1.75%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle351.out
Perplexity = 2.92, Entropy = 1.55 bits
Computation based on 464 words.
Number of 5-grams hit = 455  (98.06%)
Number of 4-grams hit = 6  (1.29%)
Number of 3-grams hit = 1  (0.22%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle352.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 488 words.
Number of 5-grams hit = 458  (93.85%)
Number of 4-grams hit = 18  (3.69%)
Number of 3-grams hit = 10  (2.05%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle353.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 471 words.
Number of 5-grams hit = 454  (96.39%)
Number of 4-grams hit = 9  (1.91%)
Number of 3-grams hit = 6  (1.27%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle354.out
Perplexity = 3.14, Entropy = 1.65 bits
Computation based on 551 words.
Number of 5-grams hit = 540  (98.00%)
Number of 4-grams hit = 5  (0.91%)
Number of 3-grams hit = 3  (0.54%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle355.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 448 words.
Number of 5-grams hit = 432  (96.43%)
Number of 4-grams hit = 8  (1.79%)
Number of 3-grams hit = 5  (1.12%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle356.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 743 words.
Number of 5-grams hit = 725  (97.58%)
Number of 4-grams hit = 12  (1.62%)
Number of 3-grams hit = 4  (0.54%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle357.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 834 words.
Number of 5-grams hit = 811  (97.24%)
Number of 4-grams hit = 15  (1.80%)
Number of 3-grams hit = 4  (0.48%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle358.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1334 words.
Number of 5-grams hit = 1301  (97.53%)
Number of 4-grams hit = 21  (1.57%)
Number of 3-grams hit = 10  (0.75%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle359.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 1380 words.
Number of 5-grams hit = 1356  (98.26%)
Number of 4-grams hit = 18  (1.30%)
Number of 3-grams hit = 4  (0.29%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle360.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 760 words.
Number of 5-grams hit = 737  (96.97%)
Number of 4-grams hit = 14  (1.84%)
Number of 3-grams hit = 6  (0.79%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle361.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 1178 words.
Number of 5-grams hit = 1159  (98.39%)
Number of 4-grams hit = 11  (0.93%)
Number of 3-grams hit = 5  (0.42%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle362.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1543 words.
Number of 5-grams hit = 1486  (96.31%)
Number of 4-grams hit = 37  (2.40%)
Number of 3-grams hit = 17  (1.10%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle363.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1251 words.
Number of 5-grams hit = 1220  (97.52%)
Number of 4-grams hit = 23  (1.84%)
Number of 3-grams hit = 6  (0.48%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle364.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 345 words.
Number of 5-grams hit = 330  (95.65%)
Number of 4-grams hit = 9  (2.61%)
Number of 3-grams hit = 4  (1.16%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle365.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 430 words.
Number of 5-grams hit = 414  (96.28%)
Number of 4-grams hit = 7  (1.63%)
Number of 3-grams hit = 7  (1.63%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle366.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 390 words.
Number of 5-grams hit = 380  (97.44%)
Number of 4-grams hit = 5  (1.28%)
Number of 3-grams hit = 3  (0.77%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle367.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 1013 words.
Number of 5-grams hit = 991  (97.83%)
Number of 4-grams hit = 12  (1.18%)
Number of 3-grams hit = 7  (0.69%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle368.out
Perplexity = 4.59, Entropy = 2.20 bits
Computation based on 345 words.
Number of 5-grams hit = 322  (93.33%)
Number of 4-grams hit = 17  (4.93%)
Number of 3-grams hit = 4  (1.16%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle369.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 352 words.
Number of 5-grams hit = 339  (96.31%)
Number of 4-grams hit = 7  (1.99%)
Number of 3-grams hit = 4  (1.14%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle370.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1781 words.
Number of 5-grams hit = 1735  (97.42%)
Number of 4-grams hit = 35  (1.97%)
Number of 3-grams hit = 8  (0.45%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle371.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 280 words.
Number of 5-grams hit = 272  (97.14%)
Number of 4-grams hit = 4  (1.43%)
Number of 3-grams hit = 2  (0.71%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle372.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 456 words.
Number of 5-grams hit = 443  (97.15%)
Number of 4-grams hit = 7  (1.54%)
Number of 3-grams hit = 3  (0.66%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle373.out
Perplexity = 3.10, Entropy = 1.63 bits
Computation based on 546 words.
Number of 5-grams hit = 532  (97.44%)
Number of 4-grams hit = 10  (1.83%)
Number of 3-grams hit = 2  (0.37%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle374.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1668 words.
Number of 5-grams hit = 1647  (98.74%)
Number of 4-grams hit = 14  (0.84%)
Number of 3-grams hit = 5  (0.30%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle375.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 1770 words.
Number of 5-grams hit = 1733  (97.91%)
Number of 4-grams hit = 26  (1.47%)
Number of 3-grams hit = 9  (0.51%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle376.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 621 words.
Number of 5-grams hit = 595  (95.81%)
Number of 4-grams hit = 15  (2.42%)
Number of 3-grams hit = 8  (1.29%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle377.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 809 words.
Number of 5-grams hit = 791  (97.78%)
Number of 4-grams hit = 11  (1.36%)
Number of 3-grams hit = 5  (0.62%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle378.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 671 words.
Number of 5-grams hit = 656  (97.76%)
Number of 4-grams hit = 10  (1.49%)
Number of 3-grams hit = 3  (0.45%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle379.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 2784 words.
Number of 5-grams hit = 2698  (96.91%)
Number of 4-grams hit = 61  (2.19%)
Number of 3-grams hit = 18  (0.65%)
Number of 2-grams hit = 5  (0.18%)
Number of 1-grams hit = 2  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle380.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 377 words.
Number of 5-grams hit = 364  (96.55%)
Number of 4-grams hit = 9  (2.39%)
Number of 3-grams hit = 2  (0.53%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle381.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 700 words.
Number of 5-grams hit = 687  (98.14%)
Number of 4-grams hit = 9  (1.29%)
Number of 3-grams hit = 2  (0.29%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle382.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1415 words.
Number of 5-grams hit = 1369  (96.75%)
Number of 4-grams hit = 26  (1.84%)
Number of 3-grams hit = 13  (0.92%)
Number of 2-grams hit = 6  (0.42%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle383.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1120 words.
Number of 5-grams hit = 1092  (97.50%)
Number of 4-grams hit = 16  (1.43%)
Number of 3-grams hit = 10  (0.89%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle384.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1098 words.
Number of 5-grams hit = 1070  (97.45%)
Number of 4-grams hit = 17  (1.55%)
Number of 3-grams hit = 9  (0.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle385.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 2797 words.
Number of 5-grams hit = 2724  (97.39%)
Number of 4-grams hit = 53  (1.89%)
Number of 3-grams hit = 17  (0.61%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle386.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 442 words.
Number of 5-grams hit = 429  (97.06%)
Number of 4-grams hit = 6  (1.36%)
Number of 3-grams hit = 4  (0.90%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle387.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 1354 words.
Number of 5-grams hit = 1313  (96.97%)
Number of 4-grams hit = 29  (2.14%)
Number of 3-grams hit = 8  (0.59%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle388.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 4692 words.
Number of 5-grams hit = 4593  (97.89%)
Number of 4-grams hit = 75  (1.60%)
Number of 3-grams hit = 22  (0.47%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle389.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 1197 words.
Number of 5-grams hit = 1151  (96.16%)
Number of 4-grams hit = 29  (2.42%)
Number of 3-grams hit = 13  (1.09%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle390.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1281 words.
Number of 5-grams hit = 1255  (97.97%)
Number of 4-grams hit = 16  (1.25%)
Number of 3-grams hit = 8  (0.62%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle391.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 3885 words.
Number of 5-grams hit = 3784  (97.40%)
Number of 4-grams hit = 68  (1.75%)
Number of 3-grams hit = 26  (0.67%)
Number of 2-grams hit = 5  (0.13%)
Number of 1-grams hit = 2  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle392.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 256 words.
Number of 5-grams hit = 250  (97.66%)
Number of 4-grams hit = 2  (0.78%)
Number of 3-grams hit = 2  (0.78%)
Number of 2-grams hit = 1  (0.39%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle393.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 508 words.
Number of 5-grams hit = 496  (97.64%)
Number of 4-grams hit = 8  (1.57%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle394.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 417 words.
Number of 5-grams hit = 406  (97.36%)
Number of 4-grams hit = 6  (1.44%)
Number of 3-grams hit = 3  (0.72%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle395.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 332 words.
Number of 5-grams hit = 313  (94.28%)
Number of 4-grams hit = 11  (3.31%)
Number of 3-grams hit = 5  (1.51%)
Number of 2-grams hit = 2  (0.60%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle396.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 3378 words.
Number of 5-grams hit = 3286  (97.28%)
Number of 4-grams hit = 68  (2.01%)
Number of 3-grams hit = 20  (0.59%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle397.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 4002 words.
Number of 5-grams hit = 3898  (97.40%)
Number of 4-grams hit = 78  (1.95%)
Number of 3-grams hit = 23  (0.57%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle398.out
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 878 words.
Number of 5-grams hit = 857  (97.61%)
Number of 4-grams hit = 14  (1.59%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle399.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 3969 words.
Number of 5-grams hit = 3851  (97.03%)
Number of 4-grams hit = 77  (1.94%)
Number of 3-grams hit = 33  (0.83%)
Number of 2-grams hit = 6  (0.15%)
Number of 1-grams hit = 2  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle400.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 634 words.
Number of 5-grams hit = 608  (95.90%)
Number of 4-grams hit = 19  (3.00%)
Number of 3-grams hit = 5  (0.79%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle401.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 721 words.
Number of 5-grams hit = 697  (96.67%)
Number of 4-grams hit = 14  (1.94%)
Number of 3-grams hit = 5  (0.69%)
Number of 2-grams hit = 4  (0.55%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle402.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 3978 words.
Number of 5-grams hit = 3841  (96.56%)
Number of 4-grams hit = 85  (2.14%)
Number of 3-grams hit = 46  (1.16%)
Number of 2-grams hit = 5  (0.13%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle403.out
Perplexity = 2.96, Entropy = 1.56 bits
Computation based on 668 words.
Number of 5-grams hit = 657  (98.35%)
Number of 4-grams hit = 7  (1.05%)
Number of 3-grams hit = 2  (0.30%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle404.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 228 words.
Number of 5-grams hit = 221  (96.93%)
Number of 4-grams hit = 4  (1.75%)
Number of 3-grams hit = 1  (0.44%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle405.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 172 words.
Number of 5-grams hit = 167  (97.09%)
Number of 4-grams hit = 2  (1.16%)
Number of 3-grams hit = 1  (0.58%)
Number of 2-grams hit = 1  (0.58%)
Number of 1-grams hit = 1  (0.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle406.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1046 words.
Number of 5-grams hit = 1031  (98.57%)
Number of 4-grams hit = 11  (1.05%)
Number of 3-grams hit = 2  (0.19%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle407.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 755 words.
Number of 5-grams hit = 727  (96.29%)
Number of 4-grams hit = 19  (2.52%)
Number of 3-grams hit = 6  (0.79%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle408.out
Perplexity = 4.24, Entropy = 2.08 bits
Computation based on 469 words.
Number of 5-grams hit = 444  (94.67%)
Number of 4-grams hit = 15  (3.20%)
Number of 3-grams hit = 7  (1.49%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle409.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 595 words.
Number of 5-grams hit = 571  (95.97%)
Number of 4-grams hit = 18  (3.03%)
Number of 3-grams hit = 4  (0.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle410.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 261 words.
Number of 5-grams hit = 249  (95.40%)
Number of 4-grams hit = 6  (2.30%)
Number of 3-grams hit = 3  (1.15%)
Number of 2-grams hit = 2  (0.77%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle411.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 465 words.
Number of 5-grams hit = 456  (98.06%)
Number of 4-grams hit = 4  (0.86%)
Number of 3-grams hit = 3  (0.65%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle412.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 4945 words.
Number of 5-grams hit = 4786  (96.78%)
Number of 4-grams hit = 111  (2.24%)
Number of 3-grams hit = 39  (0.79%)
Number of 2-grams hit = 8  (0.16%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle413.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 4175 words.
Number of 5-grams hit = 4055  (97.13%)
Number of 4-grams hit = 82  (1.96%)
Number of 3-grams hit = 30  (0.72%)
Number of 2-grams hit = 7  (0.17%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle414.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 430 words.
Number of 5-grams hit = 416  (96.74%)
Number of 4-grams hit = 7  (1.63%)
Number of 3-grams hit = 5  (1.16%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle415.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 509 words.
Number of 5-grams hit = 499  (98.04%)
Number of 4-grams hit = 7  (1.38%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle416.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 635 words.
Number of 5-grams hit = 619  (97.48%)
Number of 4-grams hit = 8  (1.26%)
Number of 3-grams hit = 6  (0.94%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle417.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 947 words.
Number of 5-grams hit = 928  (97.99%)
Number of 4-grams hit = 12  (1.27%)
Number of 3-grams hit = 4  (0.42%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle418.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 863 words.
Number of 5-grams hit = 847  (98.15%)
Number of 4-grams hit = 12  (1.39%)
Number of 3-grams hit = 2  (0.23%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle419.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 581 words.
Number of 5-grams hit = 568  (97.76%)
Number of 4-grams hit = 7  (1.20%)
Number of 3-grams hit = 3  (0.52%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle420.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 667 words.
Number of 5-grams hit = 646  (96.85%)
Number of 4-grams hit = 14  (2.10%)
Number of 3-grams hit = 5  (0.75%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle421.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 893 words.
Number of 5-grams hit = 870  (97.42%)
Number of 4-grams hit = 16  (1.79%)
Number of 3-grams hit = 5  (0.56%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle422.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 602 words.
Number of 5-grams hit = 596  (99.00%)
Number of 4-grams hit = 2  (0.33%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle423.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 652 words.
Number of 5-grams hit = 643  (98.62%)
Number of 4-grams hit = 6  (0.92%)
Number of 3-grams hit = 1  (0.15%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle424.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 582 words.
Number of 5-grams hit = 575  (98.80%)
Number of 4-grams hit = 3  (0.52%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle425.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 385 words.
Number of 5-grams hit = 366  (95.06%)
Number of 4-grams hit = 14  (3.64%)
Number of 3-grams hit = 3  (0.78%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle426.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 537 words.
Number of 5-grams hit = 519  (96.65%)
Number of 4-grams hit = 13  (2.42%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle427.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 844 words.
Number of 5-grams hit = 801  (94.91%)
Number of 4-grams hit = 31  (3.67%)
Number of 3-grams hit = 10  (1.18%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle428.out
Perplexity = 2.89, Entropy = 1.53 bits
Computation based on 680 words.
Number of 5-grams hit = 671  (98.68%)
Number of 4-grams hit = 5  (0.74%)
Number of 3-grams hit = 2  (0.29%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle429.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 812 words.
Number of 5-grams hit = 793  (97.66%)
Number of 4-grams hit = 13  (1.60%)
Number of 3-grams hit = 3  (0.37%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle430.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 699 words.
Number of 5-grams hit = 686  (98.14%)
Number of 4-grams hit = 10  (1.43%)
Number of 3-grams hit = 1  (0.14%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle431.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 612 words.
Number of 5-grams hit = 592  (96.73%)
Number of 4-grams hit = 10  (1.63%)
Number of 3-grams hit = 5  (0.82%)
Number of 2-grams hit = 4  (0.65%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle432.out
Perplexity = 3.19, Entropy = 1.68 bits
Computation based on 388 words.
Number of 5-grams hit = 375  (96.65%)
Number of 4-grams hit = 6  (1.55%)
Number of 3-grams hit = 4  (1.03%)
Number of 2-grams hit = 2  (0.52%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle433.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 1055 words.
Number of 5-grams hit = 1024  (97.06%)
Number of 4-grams hit = 21  (1.99%)
Number of 3-grams hit = 6  (0.57%)
Number of 2-grams hit = 3  (0.28%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle434.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 414 words.
Number of 5-grams hit = 405  (97.83%)
Number of 4-grams hit = 6  (1.45%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle435.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 797 words.
Number of 5-grams hit = 779  (97.74%)
Number of 4-grams hit = 14  (1.76%)
Number of 3-grams hit = 2  (0.25%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle436.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 585 words.
Number of 5-grams hit = 569  (97.26%)
Number of 4-grams hit = 8  (1.37%)
Number of 3-grams hit = 6  (1.03%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle437.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 1077 words.
Number of 5-grams hit = 1048  (97.31%)
Number of 4-grams hit = 20  (1.86%)
Number of 3-grams hit = 6  (0.56%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle438.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 318 words.
Number of 5-grams hit = 303  (95.28%)
Number of 4-grams hit = 8  (2.52%)
Number of 3-grams hit = 4  (1.26%)
Number of 2-grams hit = 2  (0.63%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle439.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 569 words.
Number of 5-grams hit = 539  (94.73%)
Number of 4-grams hit = 18  (3.16%)
Number of 3-grams hit = 10  (1.76%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle440.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 753 words.
Number of 5-grams hit = 726  (96.41%)
Number of 4-grams hit = 20  (2.66%)
Number of 3-grams hit = 5  (0.66%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle441.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 439 words.
Number of 5-grams hit = 419  (95.44%)
Number of 4-grams hit = 11  (2.51%)
Number of 3-grams hit = 6  (1.37%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle442.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 11076 words.
Number of 5-grams hit = 10829  (97.77%)
Number of 4-grams hit = 177  (1.60%)
Number of 3-grams hit = 62  (0.56%)
Number of 2-grams hit = 7  (0.06%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle443.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 9530 words.
Number of 5-grams hit = 9337  (97.97%)
Number of 4-grams hit = 140  (1.47%)
Number of 3-grams hit = 47  (0.49%)
Number of 2-grams hit = 5  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle444.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 703 words.
Number of 5-grams hit = 686  (97.58%)
Number of 4-grams hit = 12  (1.71%)
Number of 3-grams hit = 3  (0.43%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle445.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 556 words.
Number of 5-grams hit = 542  (97.48%)
Number of 4-grams hit = 9  (1.62%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle446.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 969 words.
Number of 5-grams hit = 951  (98.14%)
Number of 4-grams hit = 12  (1.24%)
Number of 3-grams hit = 4  (0.41%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle447.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 787 words.
Number of 5-grams hit = 768  (97.59%)
Number of 4-grams hit = 13  (1.65%)
Number of 3-grams hit = 4  (0.51%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle448.out
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 346 words.
Number of 5-grams hit = 332  (95.95%)
Number of 4-grams hit = 7  (2.02%)
Number of 3-grams hit = 4  (1.16%)
Number of 2-grams hit = 2  (0.58%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle449.out
Perplexity = 2.81, Entropy = 1.49 bits
Computation based on 520 words.
Number of 5-grams hit = 507  (97.50%)
Number of 4-grams hit = 9  (1.73%)
Number of 3-grams hit = 2  (0.38%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle450.out
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 386 words.
Number of 5-grams hit = 377  (97.67%)
Number of 4-grams hit = 5  (1.30%)
Number of 3-grams hit = 2  (0.52%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle451.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 615 words.
Number of 5-grams hit = 606  (98.54%)
Number of 4-grams hit = 4  (0.65%)
Number of 3-grams hit = 3  (0.49%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle452.out
Perplexity = 2.96, Entropy = 1.57 bits
Computation based on 262 words.
Number of 5-grams hit = 249  (95.04%)
Number of 4-grams hit = 9  (3.44%)
Number of 3-grams hit = 2  (0.76%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle453.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 679 words.
Number of 5-grams hit = 663  (97.64%)
Number of 4-grams hit = 10  (1.47%)
Number of 3-grams hit = 4  (0.59%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle454.out
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 598 words.
Number of 5-grams hit = 575  (96.15%)
Number of 4-grams hit = 8  (1.34%)
Number of 3-grams hit = 9  (1.51%)
Number of 2-grams hit = 5  (0.84%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle455.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 405 words.
Number of 5-grams hit = 393  (97.04%)
Number of 4-grams hit = 7  (1.73%)
Number of 3-grams hit = 3  (0.74%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle456.out
Perplexity = 2.94, Entropy = 1.56 bits
Computation based on 719 words.
Number of 5-grams hit = 712  (99.03%)
Number of 4-grams hit = 4  (0.56%)
Number of 3-grams hit = 1  (0.14%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle457.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 366 words.
Number of 5-grams hit = 348  (95.08%)
Number of 4-grams hit = 11  (3.01%)
Number of 3-grams hit = 4  (1.09%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle458.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 299 words.
Number of 5-grams hit = 291  (97.32%)
Number of 4-grams hit = 3  (1.00%)
Number of 3-grams hit = 2  (0.67%)
Number of 2-grams hit = 2  (0.67%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle459.out
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 752 words.
Number of 5-grams hit = 732  (97.34%)
Number of 4-grams hit = 12  (1.60%)
Number of 3-grams hit = 6  (0.80%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle460.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 354 words.
Number of 5-grams hit = 334  (94.35%)
Number of 4-grams hit = 10  (2.82%)
Number of 3-grams hit = 8  (2.26%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle461.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 440 words.
Number of 5-grams hit = 431  (97.95%)
Number of 4-grams hit = 6  (1.36%)
Number of 3-grams hit = 1  (0.23%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle462.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1537 words.
Number of 5-grams hit = 1475  (95.97%)
Number of 4-grams hit = 37  (2.41%)
Number of 3-grams hit = 22  (1.43%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle463.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 610 words.
Number of 5-grams hit = 600  (98.36%)
Number of 4-grams hit = 5  (0.82%)
Number of 3-grams hit = 3  (0.49%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle464.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 466 words.
Number of 5-grams hit = 451  (96.78%)
Number of 4-grams hit = 8  (1.72%)
Number of 3-grams hit = 5  (1.07%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle465.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1935 words.
Number of 5-grams hit = 1891  (97.73%)
Number of 4-grams hit = 29  (1.50%)
Number of 3-grams hit = 12  (0.62%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle466.out
Perplexity = 4.24, Entropy = 2.08 bits
Computation based on 225 words.
Number of 5-grams hit = 212  (94.22%)
Number of 4-grams hit = 6  (2.67%)
Number of 3-grams hit = 4  (1.78%)
Number of 2-grams hit = 2  (0.89%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle467.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1127 words.
Number of 5-grams hit = 1096  (97.25%)
Number of 4-grams hit = 22  (1.95%)
Number of 3-grams hit = 7  (0.62%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle468.out
Perplexity = 2.89, Entropy = 1.53 bits
Computation based on 843 words.
Number of 5-grams hit = 818  (97.03%)
Number of 4-grams hit = 13  (1.54%)
Number of 3-grams hit = 6  (0.71%)
Number of 2-grams hit = 5  (0.59%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle469.out
Perplexity = 2.84, Entropy = 1.51 bits
Computation based on 381 words.
Number of 5-grams hit = 373  (97.90%)
Number of 4-grams hit = 5  (1.31%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle470.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 259 words.
Number of 5-grams hit = 246  (94.98%)
Number of 4-grams hit = 7  (2.70%)
Number of 3-grams hit = 2  (0.77%)
Number of 2-grams hit = 3  (1.16%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle471.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 2170 words.
Number of 5-grams hit = 2114  (97.42%)
Number of 4-grams hit = 35  (1.61%)
Number of 3-grams hit = 14  (0.65%)
Number of 2-grams hit = 6  (0.28%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle472.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 1013 words.
Number of 5-grams hit = 985  (97.24%)
Number of 4-grams hit = 15  (1.48%)
Number of 3-grams hit = 10  (0.99%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle473.out
Perplexity = 3.14, Entropy = 1.65 bits
Computation based on 247 words.
Number of 5-grams hit = 242  (97.98%)
Number of 4-grams hit = 2  (0.81%)
Number of 3-grams hit = 1  (0.40%)
Number of 2-grams hit = 1  (0.40%)
Number of 1-grams hit = 1  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle474.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 838 words.
Number of 5-grams hit = 813  (97.02%)
Number of 4-grams hit = 15  (1.79%)
Number of 3-grams hit = 6  (0.72%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle475.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 588 words.
Number of 5-grams hit = 567  (96.43%)
Number of 4-grams hit = 15  (2.55%)
Number of 3-grams hit = 4  (0.68%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle476.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 626 words.
Number of 5-grams hit = 606  (96.81%)
Number of 4-grams hit = 13  (2.08%)
Number of 3-grams hit = 4  (0.64%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle477.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 474 words.
Number of 5-grams hit = 461  (97.26%)
Number of 4-grams hit = 7  (1.48%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle478.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 547 words.
Number of 5-grams hit = 531  (97.07%)
Number of 4-grams hit = 9  (1.65%)
Number of 3-grams hit = 5  (0.91%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle479.out
Perplexity = 4.38, Entropy = 2.13 bits
Computation based on 458 words.
Number of 5-grams hit = 428  (93.45%)
Number of 4-grams hit = 23  (5.02%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 3  (0.66%)
Number of 1-grams hit = 2  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle480.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 749 words.
Number of 5-grams hit = 717  (95.73%)
Number of 4-grams hit = 22  (2.94%)
Number of 3-grams hit = 7  (0.93%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle481.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 379 words.
Number of 5-grams hit = 363  (95.78%)
Number of 4-grams hit = 9  (2.37%)
Number of 3-grams hit = 5  (1.32%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle482.out
Perplexity = 3.04, Entropy = 1.60 bits
Computation based on 542 words.
Number of 5-grams hit = 527  (97.23%)
Number of 4-grams hit = 9  (1.66%)
Number of 3-grams hit = 2  (0.37%)
Number of 2-grams hit = 3  (0.55%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle483.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1863 words.
Number of 5-grams hit = 1828  (98.12%)
Number of 4-grams hit = 25  (1.34%)
Number of 3-grams hit = 8  (0.43%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle484.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1569 words.
Number of 5-grams hit = 1531  (97.58%)
Number of 4-grams hit = 28  (1.78%)
Number of 3-grams hit = 7  (0.45%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle485.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 402 words.
Number of 5-grams hit = 382  (95.02%)
Number of 4-grams hit = 9  (2.24%)
Number of 3-grams hit = 9  (2.24%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle486.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 369 words.
Number of 5-grams hit = 346  (93.77%)
Number of 4-grams hit = 12  (3.25%)
Number of 3-grams hit = 8  (2.17%)
Number of 2-grams hit = 2  (0.54%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle487.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 462 words.
Number of 5-grams hit = 439  (95.02%)
Number of 4-grams hit = 16  (3.46%)
Number of 3-grams hit = 5  (1.08%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle488.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 770 words.
Number of 5-grams hit = 756  (98.18%)
Number of 4-grams hit = 11  (1.43%)
Number of 3-grams hit = 1  (0.13%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle489.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 577 words.
Number of 5-grams hit = 569  (98.61%)
Number of 4-grams hit = 5  (0.87%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle490.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 533 words.
Number of 5-grams hit = 516  (96.81%)
Number of 4-grams hit = 12  (2.25%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle491.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1520 words.
Number of 5-grams hit = 1474  (96.97%)
Number of 4-grams hit = 29  (1.91%)
Number of 3-grams hit = 12  (0.79%)
Number of 2-grams hit = 4  (0.26%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle492.out
Perplexity = 4.07, Entropy = 2.03 bits
Computation based on 473 words.
Number of 5-grams hit = 456  (96.41%)
Number of 4-grams hit = 11  (2.33%)
Number of 3-grams hit = 4  (0.85%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle493.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 827 words.
Number of 5-grams hit = 802  (96.98%)
Number of 4-grams hit = 18  (2.18%)
Number of 3-grams hit = 5  (0.60%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle494.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 540 words.
Number of 5-grams hit = 524  (97.04%)
Number of 4-grams hit = 12  (2.22%)
Number of 3-grams hit = 2  (0.37%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle495.out
Perplexity = 3.93, Entropy = 1.98 bits
Computation based on 454 words.
Number of 5-grams hit = 434  (95.59%)
Number of 4-grams hit = 11  (2.42%)
Number of 3-grams hit = 6  (1.32%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle496.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 608 words.
Number of 5-grams hit = 596  (98.03%)
Number of 4-grams hit = 7  (1.15%)
Number of 3-grams hit = 3  (0.49%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle497.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 711 words.
Number of 5-grams hit = 697  (98.03%)
Number of 4-grams hit = 9  (1.27%)
Number of 3-grams hit = 3  (0.42%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle498.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1824 words.
Number of 5-grams hit = 1779  (97.53%)
Number of 4-grams hit = 28  (1.54%)
Number of 3-grams hit = 11  (0.60%)
Number of 2-grams hit = 5  (0.27%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle499.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 542 words.
Number of 5-grams hit = 525  (96.86%)
Number of 4-grams hit = 10  (1.85%)
Number of 3-grams hit = 5  (0.92%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle500.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 486 words.
Number of 5-grams hit = 475  (97.74%)
Number of 4-grams hit = 6  (1.23%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle501.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 448 words.
Number of 5-grams hit = 428  (95.54%)
Number of 4-grams hit = 12  (2.68%)
Number of 3-grams hit = 5  (1.12%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle502.out
Perplexity = 2.98, Entropy = 1.58 bits
Computation based on 535 words.
Number of 5-grams hit = 527  (98.50%)
Number of 4-grams hit = 5  (0.93%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle503.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 1109 words.
Number of 5-grams hit = 1065  (96.03%)
Number of 4-grams hit = 31  (2.80%)
Number of 3-grams hit = 10  (0.90%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle504.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 658 words.
Number of 5-grams hit = 639  (97.11%)
Number of 4-grams hit = 13  (1.98%)
Number of 3-grams hit = 4  (0.61%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle505.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 768 words.
Number of 5-grams hit = 742  (96.61%)
Number of 4-grams hit = 16  (2.08%)
Number of 3-grams hit = 8  (1.04%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle506.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 594 words.
Number of 5-grams hit = 574  (96.63%)
Number of 4-grams hit = 13  (2.19%)
Number of 3-grams hit = 5  (0.84%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle507.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 1702 words.
Number of 5-grams hit = 1645  (96.65%)
Number of 4-grams hit = 41  (2.41%)
Number of 3-grams hit = 13  (0.76%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle508.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 700 words.
Number of 5-grams hit = 689  (98.43%)
Number of 4-grams hit = 7  (1.00%)
Number of 3-grams hit = 2  (0.29%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle509.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 383 words.
Number of 5-grams hit = 369  (96.34%)
Number of 4-grams hit = 6  (1.57%)
Number of 3-grams hit = 4  (1.04%)
Number of 2-grams hit = 3  (0.78%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle510.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1032 words.
Number of 5-grams hit = 1007  (97.58%)
Number of 4-grams hit = 14  (1.36%)
Number of 3-grams hit = 7  (0.68%)
Number of 2-grams hit = 3  (0.29%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle511.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 677 words.
Number of 5-grams hit = 660  (97.49%)
Number of 4-grams hit = 11  (1.62%)
Number of 3-grams hit = 4  (0.59%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle512.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 2057 words.
Number of 5-grams hit = 2013  (97.86%)
Number of 4-grams hit = 33  (1.60%)
Number of 3-grams hit = 8  (0.39%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle513.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 448 words.
Number of 5-grams hit = 435  (97.10%)
Number of 4-grams hit = 7  (1.56%)
Number of 3-grams hit = 4  (0.89%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle514.out
Perplexity = 2.98, Entropy = 1.58 bits
Computation based on 422 words.
Number of 5-grams hit = 413  (97.87%)
Number of 4-grams hit = 6  (1.42%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle515.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1070 words.
Number of 5-grams hit = 1046  (97.76%)
Number of 4-grams hit = 18  (1.68%)
Number of 3-grams hit = 3  (0.28%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle516.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 440 words.
Number of 5-grams hit = 431  (97.95%)
Number of 4-grams hit = 5  (1.14%)
Number of 3-grams hit = 2  (0.45%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle517.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1145 words.
Number of 5-grams hit = 1111  (97.03%)
Number of 4-grams hit = 24  (2.10%)
Number of 3-grams hit = 8  (0.70%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle518.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 639 words.
Number of 5-grams hit = 619  (96.87%)
Number of 4-grams hit = 15  (2.35%)
Number of 3-grams hit = 3  (0.47%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle519.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 592 words.
Number of 5-grams hit = 571  (96.45%)
Number of 4-grams hit = 11  (1.86%)
Number of 3-grams hit = 8  (1.35%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle520.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 663 words.
Number of 5-grams hit = 647  (97.59%)
Number of 4-grams hit = 12  (1.81%)
Number of 3-grams hit = 2  (0.30%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle521.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 548 words.
Number of 5-grams hit = 530  (96.72%)
Number of 4-grams hit = 10  (1.82%)
Number of 3-grams hit = 6  (1.09%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle522.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1741 words.
Number of 5-grams hit = 1699  (97.59%)
Number of 4-grams hit = 33  (1.90%)
Number of 3-grams hit = 7  (0.40%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle523.out
Perplexity = 4.35, Entropy = 2.12 bits
Computation based on 354 words.
Number of 5-grams hit = 335  (94.63%)
Number of 4-grams hit = 8  (2.26%)
Number of 3-grams hit = 6  (1.69%)
Number of 2-grams hit = 4  (1.13%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle524.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1488 words.
Number of 5-grams hit = 1423  (95.63%)
Number of 4-grams hit = 39  (2.62%)
Number of 3-grams hit = 21  (1.41%)
Number of 2-grams hit = 4  (0.27%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle525.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 2121 words.
Number of 5-grams hit = 2063  (97.27%)
Number of 4-grams hit = 39  (1.84%)
Number of 3-grams hit = 16  (0.75%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle526.out
Perplexity = 3.33, Entropy = 1.73 bits
Computation based on 2067 words.
Number of 5-grams hit = 2028  (98.11%)
Number of 4-grams hit = 29  (1.40%)
Number of 3-grams hit = 7  (0.34%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle527.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 793 words.
Number of 5-grams hit = 774  (97.60%)
Number of 4-grams hit = 14  (1.77%)
Number of 3-grams hit = 3  (0.38%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle528.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 788 words.
Number of 5-grams hit = 765  (97.08%)
Number of 4-grams hit = 14  (1.78%)
Number of 3-grams hit = 7  (0.89%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle529.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 957 words.
Number of 5-grams hit = 940  (98.22%)
Number of 4-grams hit = 9  (0.94%)
Number of 3-grams hit = 4  (0.42%)
Number of 2-grams hit = 3  (0.31%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle530.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 451 words.
Number of 5-grams hit = 440  (97.56%)
Number of 4-grams hit = 6  (1.33%)
Number of 3-grams hit = 3  (0.67%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle531.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 1046 words.
Number of 5-grams hit = 1020  (97.51%)
Number of 4-grams hit = 16  (1.53%)
Number of 3-grams hit = 5  (0.48%)
Number of 2-grams hit = 4  (0.38%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle532.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1019 words.
Number of 5-grams hit = 994  (97.55%)
Number of 4-grams hit = 17  (1.67%)
Number of 3-grams hit = 6  (0.59%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle533.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1210 words.
Number of 5-grams hit = 1175  (97.11%)
Number of 4-grams hit = 25  (2.07%)
Number of 3-grams hit = 8  (0.66%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle534.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 549 words.
Number of 5-grams hit = 532  (96.90%)
Number of 4-grams hit = 13  (2.37%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle535.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 375 words.
Number of 5-grams hit = 366  (97.60%)
Number of 4-grams hit = 5  (1.33%)
Number of 3-grams hit = 2  (0.53%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle536.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1602 words.
Number of 5-grams hit = 1583  (98.81%)
Number of 4-grams hit = 11  (0.69%)
Number of 3-grams hit = 6  (0.37%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle537.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1098 words.
Number of 5-grams hit = 1065  (96.99%)
Number of 4-grams hit = 25  (2.28%)
Number of 3-grams hit = 5  (0.46%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle538.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 3722 words.
Number of 5-grams hit = 3627  (97.45%)
Number of 4-grams hit = 72  (1.93%)
Number of 3-grams hit = 19  (0.51%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle539.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 581 words.
Number of 5-grams hit = 571  (98.28%)
Number of 4-grams hit = 6  (1.03%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle540.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 2359 words.
Number of 5-grams hit = 2300  (97.50%)
Number of 4-grams hit = 38  (1.61%)
Number of 3-grams hit = 19  (0.81%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle541.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 544 words.
Number of 5-grams hit = 525  (96.51%)
Number of 4-grams hit = 14  (2.57%)
Number of 3-grams hit = 3  (0.55%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle542.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 1758 words.
Number of 5-grams hit = 1720  (97.84%)
Number of 4-grams hit = 32  (1.82%)
Number of 3-grams hit = 4  (0.23%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle543.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 592 words.
Number of 5-grams hit = 587  (99.16%)
Number of 4-grams hit = 2  (0.34%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle544.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 1399 words.
Number of 5-grams hit = 1352  (96.64%)
Number of 4-grams hit = 32  (2.29%)
Number of 3-grams hit = 10  (0.71%)
Number of 2-grams hit = 4  (0.29%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle545.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 802 words.
Number of 5-grams hit = 776  (96.76%)
Number of 4-grams hit = 17  (2.12%)
Number of 3-grams hit = 6  (0.75%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle546.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 753 words.
Number of 5-grams hit = 733  (97.34%)
Number of 4-grams hit = 13  (1.73%)
Number of 3-grams hit = 5  (0.66%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle547.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 953 words.
Number of 5-grams hit = 928  (97.38%)
Number of 4-grams hit = 13  (1.36%)
Number of 3-grams hit = 8  (0.84%)
Number of 2-grams hit = 3  (0.31%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle548.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 671 words.
Number of 5-grams hit = 648  (96.57%)
Number of 4-grams hit = 10  (1.49%)
Number of 3-grams hit = 8  (1.19%)
Number of 2-grams hit = 3  (0.45%)
Number of 1-grams hit = 2  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle549.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 387 words.
Number of 5-grams hit = 370  (95.61%)
Number of 4-grams hit = 10  (2.58%)
Number of 3-grams hit = 5  (1.29%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle550.out
Perplexity = 4.41, Entropy = 2.14 bits
Computation based on 204 words.
Number of 5-grams hit = 188  (92.16%)
Number of 4-grams hit = 10  (4.90%)
Number of 3-grams hit = 4  (1.96%)
Number of 2-grams hit = 1  (0.49%)
Number of 1-grams hit = 1  (0.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle551.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 996 words.
Number of 5-grams hit = 959  (96.29%)
Number of 4-grams hit = 26  (2.61%)
Number of 3-grams hit = 8  (0.80%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle552.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 1143 words.
Number of 5-grams hit = 1122  (98.16%)
Number of 4-grams hit = 13  (1.14%)
Number of 3-grams hit = 6  (0.52%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle553.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1347 words.
Number of 5-grams hit = 1304  (96.81%)
Number of 4-grams hit = 30  (2.23%)
Number of 3-grams hit = 11  (0.82%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle554.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 4358 words.
Number of 5-grams hit = 4223  (96.90%)
Number of 4-grams hit = 91  (2.09%)
Number of 3-grams hit = 35  (0.80%)
Number of 2-grams hit = 8  (0.18%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle555.out
Perplexity = 3.96, Entropy = 1.98 bits
Computation based on 544 words.
Number of 5-grams hit = 521  (95.77%)
Number of 4-grams hit = 12  (2.21%)
Number of 3-grams hit = 7  (1.29%)
Number of 2-grams hit = 3  (0.55%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle556.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 605 words.
Number of 5-grams hit = 579  (95.70%)
Number of 4-grams hit = 18  (2.98%)
Number of 3-grams hit = 5  (0.83%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle557.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 418 words.
Number of 5-grams hit = 407  (97.37%)
Number of 4-grams hit = 5  (1.20%)
Number of 3-grams hit = 3  (0.72%)
Number of 2-grams hit = 2  (0.48%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle558.out
Perplexity = 3.88, Entropy = 1.95 bits
Computation based on 1850 words.
Number of 5-grams hit = 1795  (97.03%)
Number of 4-grams hit = 42  (2.27%)
Number of 3-grams hit = 9  (0.49%)
Number of 2-grams hit = 3  (0.16%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle559.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 694 words.
Number of 5-grams hit = 675  (97.26%)
Number of 4-grams hit = 9  (1.30%)
Number of 3-grams hit = 6  (0.86%)
Number of 2-grams hit = 3  (0.43%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle560.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 595 words.
Number of 5-grams hit = 577  (96.97%)
Number of 4-grams hit = 12  (2.02%)
Number of 3-grams hit = 4  (0.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle561.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 362 words.
Number of 5-grams hit = 351  (96.96%)
Number of 4-grams hit = 6  (1.66%)
Number of 3-grams hit = 3  (0.83%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle562.out
Perplexity = 4.20, Entropy = 2.07 bits
Computation based on 331 words.
Number of 5-grams hit = 320  (96.68%)
Number of 4-grams hit = 6  (1.81%)
Number of 3-grams hit = 3  (0.91%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle563.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 524 words.
Number of 5-grams hit = 509  (97.14%)
Number of 4-grams hit = 7  (1.34%)
Number of 3-grams hit = 6  (1.15%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle564.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1451 words.
Number of 5-grams hit = 1413  (97.38%)
Number of 4-grams hit = 29  (2.00%)
Number of 3-grams hit = 6  (0.41%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle565.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 1392 words.
Number of 5-grams hit = 1366  (98.13%)
Number of 4-grams hit = 16  (1.15%)
Number of 3-grams hit = 8  (0.57%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle566.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 4543 words.
Number of 5-grams hit = 4408  (97.03%)
Number of 4-grams hit = 103  (2.27%)
Number of 3-grams hit = 25  (0.55%)
Number of 2-grams hit = 5  (0.11%)
Number of 1-grams hit = 2  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle567.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 596 words.
Number of 5-grams hit = 568  (95.30%)
Number of 4-grams hit = 20  (3.36%)
Number of 3-grams hit = 6  (1.01%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle568.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 355 words.
Number of 5-grams hit = 348  (98.03%)
Number of 4-grams hit = 3  (0.85%)
Number of 3-grams hit = 2  (0.56%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle569.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 4706 words.
Number of 5-grams hit = 4600  (97.75%)
Number of 4-grams hit = 74  (1.57%)
Number of 3-grams hit = 27  (0.57%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle570.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 897 words.
Number of 5-grams hit = 870  (96.99%)
Number of 4-grams hit = 20  (2.23%)
Number of 3-grams hit = 5  (0.56%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle571.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 576 words.
Number of 5-grams hit = 568  (98.61%)
Number of 4-grams hit = 5  (0.87%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle572.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 619 words.
Number of 5-grams hit = 595  (96.12%)
Number of 4-grams hit = 17  (2.75%)
Number of 3-grams hit = 5  (0.81%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle573.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 279 words.
Number of 5-grams hit = 262  (93.91%)
Number of 4-grams hit = 11  (3.94%)
Number of 3-grams hit = 2  (0.72%)
Number of 2-grams hit = 3  (1.08%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle574.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1179 words.
Number of 5-grams hit = 1131  (95.93%)
Number of 4-grams hit = 32  (2.71%)
Number of 3-grams hit = 12  (1.02%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle575.out
Perplexity = 2.89, Entropy = 1.53 bits
Computation based on 643 words.
Number of 5-grams hit = 632  (98.29%)
Number of 4-grams hit = 7  (1.09%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle576.out
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 489 words.
Number of 5-grams hit = 478  (97.75%)
Number of 4-grams hit = 6  (1.23%)
Number of 3-grams hit = 2  (0.41%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle577.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 405 words.
Number of 5-grams hit = 394  (97.28%)
Number of 4-grams hit = 6  (1.48%)
Number of 3-grams hit = 3  (0.74%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle578.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 447 words.
Number of 5-grams hit = 432  (96.64%)
Number of 4-grams hit = 9  (2.01%)
Number of 3-grams hit = 3  (0.67%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle579.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 479 words.
Number of 5-grams hit = 460  (96.03%)
Number of 4-grams hit = 13  (2.71%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle580.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 585 words.
Number of 5-grams hit = 573  (97.95%)
Number of 4-grams hit = 8  (1.37%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle581.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 584 words.
Number of 5-grams hit = 567  (97.09%)
Number of 4-grams hit = 12  (2.05%)
Number of 3-grams hit = 3  (0.51%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle582.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 547 words.
Number of 5-grams hit = 535  (97.81%)
Number of 4-grams hit = 7  (1.28%)
Number of 3-grams hit = 3  (0.55%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle583.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 722 words.
Number of 5-grams hit = 701  (97.09%)
Number of 4-grams hit = 17  (2.35%)
Number of 3-grams hit = 2  (0.28%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle584.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 477 words.
Number of 5-grams hit = 458  (96.02%)
Number of 4-grams hit = 12  (2.52%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle585.out
Perplexity = 3.00, Entropy = 1.59 bits
Computation based on 1036 words.
Number of 5-grams hit = 1019  (98.36%)
Number of 4-grams hit = 11  (1.06%)
Number of 3-grams hit = 4  (0.39%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle586.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 782 words.
Number of 5-grams hit = 762  (97.44%)
Number of 4-grams hit = 12  (1.53%)
Number of 3-grams hit = 5  (0.64%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle587.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 576 words.
Number of 5-grams hit = 565  (98.09%)
Number of 4-grams hit = 6  (1.04%)
Number of 3-grams hit = 3  (0.52%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle588.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1045 words.
Number of 5-grams hit = 1011  (96.75%)
Number of 4-grams hit = 25  (2.39%)
Number of 3-grams hit = 7  (0.67%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle589.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 538 words.
Number of 5-grams hit = 522  (97.03%)
Number of 4-grams hit = 12  (2.23%)
Number of 3-grams hit = 2  (0.37%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle590.out
Perplexity = 3.26, Entropy = 1.71 bits
Computation based on 1468 words.
Number of 5-grams hit = 1438  (97.96%)
Number of 4-grams hit = 20  (1.36%)
Number of 3-grams hit = 8  (0.54%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle591.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 749 words.
Number of 5-grams hit = 727  (97.06%)
Number of 4-grams hit = 18  (2.40%)
Number of 3-grams hit = 2  (0.27%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle592.out
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 367 words.
Number of 5-grams hit = 358  (97.55%)
Number of 4-grams hit = 5  (1.36%)
Number of 3-grams hit = 2  (0.54%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle593.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 409 words.
Number of 5-grams hit = 397  (97.07%)
Number of 4-grams hit = 5  (1.22%)
Number of 3-grams hit = 4  (0.98%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle594.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 653 words.
Number of 5-grams hit = 644  (98.62%)
Number of 4-grams hit = 5  (0.77%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle595.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 398 words.
Number of 5-grams hit = 384  (96.48%)
Number of 4-grams hit = 8  (2.01%)
Number of 3-grams hit = 3  (0.75%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle596.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1041 words.
Number of 5-grams hit = 1002  (96.25%)
Number of 4-grams hit = 26  (2.50%)
Number of 3-grams hit = 9  (0.86%)
Number of 2-grams hit = 3  (0.29%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle597.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1109 words.
Number of 5-grams hit = 1085  (97.84%)
Number of 4-grams hit = 18  (1.62%)
Number of 3-grams hit = 4  (0.36%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle598.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 327 words.
Number of 5-grams hit = 311  (95.11%)
Number of 4-grams hit = 8  (2.45%)
Number of 3-grams hit = 5  (1.53%)
Number of 2-grams hit = 2  (0.61%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle599.out
Perplexity = 3.54, Entropy = 1.83 bits
Computation based on 879 words.
Number of 5-grams hit = 856  (97.38%)
Number of 4-grams hit = 16  (1.82%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle600.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 5209 words.
Number of 5-grams hit = 5088  (97.68%)
Number of 4-grams hit = 87  (1.67%)
Number of 3-grams hit = 29  (0.56%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle601.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1181 words.
Number of 5-grams hit = 1149  (97.29%)
Number of 4-grams hit = 20  (1.69%)
Number of 3-grams hit = 8  (0.68%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle602.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 652 words.
Number of 5-grams hit = 632  (96.93%)
Number of 4-grams hit = 13  (1.99%)
Number of 3-grams hit = 5  (0.77%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle603.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 94 words.
Number of 5-grams hit = 90  (95.74%)
Number of 4-grams hit = 1  (1.06%)
Number of 3-grams hit = 1  (1.06%)
Number of 2-grams hit = 1  (1.06%)
Number of 1-grams hit = 1  (1.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle604.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 612 words.
Number of 5-grams hit = 590  (96.41%)
Number of 4-grams hit = 15  (2.45%)
Number of 3-grams hit = 5  (0.82%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle605.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 3406 words.
Number of 5-grams hit = 3347  (98.27%)
Number of 4-grams hit = 39  (1.15%)
Number of 3-grams hit = 15  (0.44%)
Number of 2-grams hit = 4  (0.12%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle606.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 978 words.
Number of 5-grams hit = 952  (97.34%)
Number of 4-grams hit = 19  (1.94%)
Number of 3-grams hit = 5  (0.51%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle607.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1157 words.
Number of 5-grams hit = 1131  (97.75%)
Number of 4-grams hit = 20  (1.73%)
Number of 3-grams hit = 4  (0.35%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle608.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 674 words.
Number of 5-grams hit = 654  (97.03%)
Number of 4-grams hit = 13  (1.93%)
Number of 3-grams hit = 4  (0.59%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle609.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 4849 words.
Number of 5-grams hit = 4736  (97.67%)
Number of 4-grams hit = 80  (1.65%)
Number of 3-grams hit = 29  (0.60%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle610.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 6480 words.
Number of 5-grams hit = 6305  (97.30%)
Number of 4-grams hit = 130  (2.01%)
Number of 3-grams hit = 40  (0.62%)
Number of 2-grams hit = 4  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle611.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 856 words.
Number of 5-grams hit = 825  (96.38%)
Number of 4-grams hit = 17  (1.99%)
Number of 3-grams hit = 11  (1.29%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle612.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1757 words.
Number of 5-grams hit = 1707  (97.15%)
Number of 4-grams hit = 37  (2.11%)
Number of 3-grams hit = 9  (0.51%)
Number of 2-grams hit = 3  (0.17%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle613.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 4584 words.
Number of 5-grams hit = 4484  (97.82%)
Number of 4-grams hit = 79  (1.72%)
Number of 3-grams hit = 17  (0.37%)
Number of 2-grams hit = 3  (0.07%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle614.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1629 words.
Number of 5-grams hit = 1586  (97.36%)
Number of 4-grams hit = 28  (1.72%)
Number of 3-grams hit = 11  (0.68%)
Number of 2-grams hit = 3  (0.18%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle615.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 494 words.
Number of 5-grams hit = 473  (95.75%)
Number of 4-grams hit = 12  (2.43%)
Number of 3-grams hit = 6  (1.21%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle616.out
Perplexity = 2.85, Entropy = 1.51 bits
Computation based on 827 words.
Number of 5-grams hit = 803  (97.10%)
Number of 4-grams hit = 16  (1.93%)
Number of 3-grams hit = 6  (0.73%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle617.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 1418 words.
Number of 5-grams hit = 1399  (98.66%)
Number of 4-grams hit = 12  (0.85%)
Number of 3-grams hit = 5  (0.35%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle618.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 4055 words.
Number of 5-grams hit = 3948  (97.36%)
Number of 4-grams hit = 68  (1.68%)
Number of 3-grams hit = 34  (0.84%)
Number of 2-grams hit = 4  (0.10%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle619.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 853 words.
Number of 5-grams hit = 819  (96.01%)
Number of 4-grams hit = 21  (2.46%)
Number of 3-grams hit = 7  (0.82%)
Number of 2-grams hit = 5  (0.59%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle620.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 5770 words.
Number of 5-grams hit = 5634  (97.64%)
Number of 4-grams hit = 93  (1.61%)
Number of 3-grams hit = 35  (0.61%)
Number of 2-grams hit = 7  (0.12%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle621.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 347 words.
Number of 5-grams hit = 334  (96.25%)
Number of 4-grams hit = 7  (2.02%)
Number of 3-grams hit = 3  (0.86%)
Number of 2-grams hit = 2  (0.58%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle622.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 503 words.
Number of 5-grams hit = 491  (97.61%)
Number of 4-grams hit = 6  (1.19%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 2  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle623.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1430 words.
Number of 5-grams hit = 1401  (97.97%)
Number of 4-grams hit = 21  (1.47%)
Number of 3-grams hit = 6  (0.42%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle624.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 5610 words.
Number of 5-grams hit = 5477  (97.63%)
Number of 4-grams hit = 95  (1.69%)
Number of 3-grams hit = 29  (0.52%)
Number of 2-grams hit = 7  (0.12%)
Number of 1-grams hit = 2  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle625.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1191 words.
Number of 5-grams hit = 1149  (96.47%)
Number of 4-grams hit = 24  (2.02%)
Number of 3-grams hit = 15  (1.26%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle626.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 483 words.
Number of 5-grams hit = 477  (98.76%)
Number of 4-grams hit = 2  (0.41%)
Number of 3-grams hit = 2  (0.41%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle627.out
Perplexity = 2.97, Entropy = 1.57 bits
Computation based on 159 words.
Number of 5-grams hit = 155  (97.48%)
Number of 4-grams hit = 1  (0.63%)
Number of 3-grams hit = 1  (0.63%)
Number of 2-grams hit = 1  (0.63%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle628.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1142 words.
Number of 5-grams hit = 1099  (96.23%)
Number of 4-grams hit = 26  (2.28%)
Number of 3-grams hit = 15  (1.31%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle629.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1114 words.
Number of 5-grams hit = 1079  (96.86%)
Number of 4-grams hit = 24  (2.15%)
Number of 3-grams hit = 8  (0.72%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle630.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 349 words.
Number of 5-grams hit = 343  (98.28%)
Number of 4-grams hit = 3  (0.86%)
Number of 3-grams hit = 1  (0.29%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle631.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 931 words.
Number of 5-grams hit = 904  (97.10%)
Number of 4-grams hit = 19  (2.04%)
Number of 3-grams hit = 6  (0.64%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle632.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 439 words.
Number of 5-grams hit = 413  (94.08%)
Number of 4-grams hit = 15  (3.42%)
Number of 3-grams hit = 8  (1.82%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle633.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 579 words.
Number of 5-grams hit = 555  (95.85%)
Number of 4-grams hit = 17  (2.94%)
Number of 3-grams hit = 5  (0.86%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle634.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 603 words.
Number of 5-grams hit = 581  (96.35%)
Number of 4-grams hit = 17  (2.82%)
Number of 3-grams hit = 3  (0.50%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle635.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 169 words.
Number of 5-grams hit = 165  (97.63%)
Number of 4-grams hit = 1  (0.59%)
Number of 3-grams hit = 1  (0.59%)
Number of 2-grams hit = 1  (0.59%)
Number of 1-grams hit = 1  (0.59%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle636.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 1435 words.
Number of 5-grams hit = 1407  (98.05%)
Number of 4-grams hit = 17  (1.18%)
Number of 3-grams hit = 8  (0.56%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle637.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 1118 words.
Number of 5-grams hit = 1075  (96.15%)
Number of 4-grams hit = 27  (2.42%)
Number of 3-grams hit = 10  (0.89%)
Number of 2-grams hit = 5  (0.45%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle638.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 586 words.
Number of 5-grams hit = 575  (98.12%)
Number of 4-grams hit = 5  (0.85%)
Number of 3-grams hit = 4  (0.68%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle639.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 561 words.
Number of 5-grams hit = 547  (97.50%)
Number of 4-grams hit = 8  (1.43%)
Number of 3-grams hit = 4  (0.71%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle640.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 227 words.
Number of 5-grams hit = 220  (96.92%)
Number of 4-grams hit = 4  (1.76%)
Number of 3-grams hit = 1  (0.44%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle641.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 888 words.
Number of 5-grams hit = 879  (98.99%)
Number of 4-grams hit = 5  (0.56%)
Number of 3-grams hit = 2  (0.23%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle642.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 477 words.
Number of 5-grams hit = 471  (98.74%)
Number of 4-grams hit = 3  (0.63%)
Number of 3-grams hit = 1  (0.21%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle643.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 522 words.
Number of 5-grams hit = 505  (96.74%)
Number of 4-grams hit = 11  (2.11%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle644.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1207 words.
Number of 5-grams hit = 1177  (97.51%)
Number of 4-grams hit = 21  (1.74%)
Number of 3-grams hit = 6  (0.50%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle645.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 475 words.
Number of 5-grams hit = 468  (98.53%)
Number of 4-grams hit = 4  (0.84%)
Number of 3-grams hit = 1  (0.21%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle646.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 615 words.
Number of 5-grams hit = 600  (97.56%)
Number of 4-grams hit = 11  (1.79%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle647.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 653 words.
Number of 5-grams hit = 629  (96.32%)
Number of 4-grams hit = 16  (2.45%)
Number of 3-grams hit = 6  (0.92%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle648.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 511 words.
Number of 5-grams hit = 498  (97.46%)
Number of 4-grams hit = 8  (1.57%)
Number of 3-grams hit = 3  (0.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle649.out
Perplexity = 3.11, Entropy = 1.63 bits
Computation based on 577 words.
Number of 5-grams hit = 562  (97.40%)
Number of 4-grams hit = 11  (1.91%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle650.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 579 words.
Number of 5-grams hit = 568  (98.10%)
Number of 4-grams hit = 7  (1.21%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle651.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 503 words.
Number of 5-grams hit = 486  (96.62%)
Number of 4-grams hit = 8  (1.59%)
Number of 3-grams hit = 6  (1.19%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle652.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 956 words.
Number of 5-grams hit = 916  (95.82%)
Number of 4-grams hit = 19  (1.99%)
Number of 3-grams hit = 16  (1.67%)
Number of 2-grams hit = 4  (0.42%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle653.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 848 words.
Number of 5-grams hit = 833  (98.23%)
Number of 4-grams hit = 10  (1.18%)
Number of 3-grams hit = 3  (0.35%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle654.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 827 words.
Number of 5-grams hit = 798  (96.49%)
Number of 4-grams hit = 18  (2.18%)
Number of 3-grams hit = 8  (0.97%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle655.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 547 words.
Number of 5-grams hit = 529  (96.71%)
Number of 4-grams hit = 13  (2.38%)
Number of 3-grams hit = 3  (0.55%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle656.out
Perplexity = 4.31, Entropy = 2.11 bits
Computation based on 501 words.
Number of 5-grams hit = 479  (95.61%)
Number of 4-grams hit = 17  (3.39%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle657.out
Perplexity = 3.01, Entropy = 1.59 bits
Computation based on 412 words.
Number of 5-grams hit = 406  (98.54%)
Number of 4-grams hit = 3  (0.73%)
Number of 3-grams hit = 1  (0.24%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle658.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1097 words.
Number of 5-grams hit = 1062  (96.81%)
Number of 4-grams hit = 24  (2.19%)
Number of 3-grams hit = 7  (0.64%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle659.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1082 words.
Number of 5-grams hit = 1048  (96.86%)
Number of 4-grams hit = 23  (2.13%)
Number of 3-grams hit = 9  (0.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle660.out
Perplexity = 2.31, Entropy = 1.21 bits
Computation based on 151 words.
Number of 5-grams hit = 147  (97.35%)
Number of 4-grams hit = 1  (0.66%)
Number of 3-grams hit = 1  (0.66%)
Number of 2-grams hit = 1  (0.66%)
Number of 1-grams hit = 1  (0.66%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle661.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1330 words.
Number of 5-grams hit = 1297  (97.52%)
Number of 4-grams hit = 22  (1.65%)
Number of 3-grams hit = 7  (0.53%)
Number of 2-grams hit = 3  (0.23%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle662.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 131 words.
Number of 5-grams hit = 124  (94.66%)
Number of 4-grams hit = 3  (2.29%)
Number of 3-grams hit = 2  (1.53%)
Number of 2-grams hit = 1  (0.76%)
Number of 1-grams hit = 1  (0.76%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle663.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 1445 words.
Number of 5-grams hit = 1411  (97.65%)
Number of 4-grams hit = 22  (1.52%)
Number of 3-grams hit = 7  (0.48%)
Number of 2-grams hit = 4  (0.28%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle664.out
Perplexity = 4.25, Entropy = 2.09 bits
Computation based on 770 words.
Number of 5-grams hit = 730  (94.81%)
Number of 4-grams hit = 24  (3.12%)
Number of 3-grams hit = 12  (1.56%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle665.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 830 words.
Number of 5-grams hit = 801  (96.51%)
Number of 4-grams hit = 18  (2.17%)
Number of 3-grams hit = 8  (0.96%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle666.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 413 words.
Number of 5-grams hit = 401  (97.09%)
Number of 4-grams hit = 6  (1.45%)
Number of 3-grams hit = 4  (0.97%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle667.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1455 words.
Number of 5-grams hit = 1408  (96.77%)
Number of 4-grams hit = 30  (2.06%)
Number of 3-grams hit = 13  (0.89%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle668.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 504 words.
Number of 5-grams hit = 488  (96.83%)
Number of 4-grams hit = 10  (1.98%)
Number of 3-grams hit = 4  (0.79%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle669.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 524 words.
Number of 5-grams hit = 514  (98.09%)
Number of 4-grams hit = 6  (1.15%)
Number of 3-grams hit = 2  (0.38%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle670.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 481 words.
Number of 5-grams hit = 466  (96.88%)
Number of 4-grams hit = 9  (1.87%)
Number of 3-grams hit = 4  (0.83%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle671.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 693 words.
Number of 5-grams hit = 664  (95.82%)
Number of 4-grams hit = 21  (3.03%)
Number of 3-grams hit = 5  (0.72%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle672.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 316 words.
Number of 5-grams hit = 308  (97.47%)
Number of 4-grams hit = 5  (1.58%)
Number of 3-grams hit = 1  (0.32%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle673.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 336 words.
Number of 5-grams hit = 325  (96.73%)
Number of 4-grams hit = 7  (2.08%)
Number of 3-grams hit = 2  (0.60%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle674.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 159 words.
Number of 5-grams hit = 153  (96.23%)
Number of 4-grams hit = 3  (1.89%)
Number of 3-grams hit = 1  (0.63%)
Number of 2-grams hit = 1  (0.63%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle675.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 4196 words.
Number of 5-grams hit = 4116  (98.09%)
Number of 4-grams hit = 62  (1.48%)
Number of 3-grams hit = 15  (0.36%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle676.out
Perplexity = 2.81, Entropy = 1.49 bits
Computation based on 531 words.
Number of 5-grams hit = 520  (97.93%)
Number of 4-grams hit = 8  (1.51%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle677.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 684 words.
Number of 5-grams hit = 664  (97.08%)
Number of 4-grams hit = 9  (1.32%)
Number of 3-grams hit = 6  (0.88%)
Number of 2-grams hit = 4  (0.58%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle678.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 373 words.
Number of 5-grams hit = 364  (97.59%)
Number of 4-grams hit = 5  (1.34%)
Number of 3-grams hit = 2  (0.54%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle679.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 1075 words.
Number of 5-grams hit = 1041  (96.84%)
Number of 4-grams hit = 21  (1.95%)
Number of 3-grams hit = 10  (0.93%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle680.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 356 words.
Number of 5-grams hit = 347  (97.47%)
Number of 4-grams hit = 5  (1.40%)
Number of 3-grams hit = 2  (0.56%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle681.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 522 words.
Number of 5-grams hit = 503  (96.36%)
Number of 4-grams hit = 10  (1.92%)
Number of 3-grams hit = 5  (0.96%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle682.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 4014 words.
Number of 5-grams hit = 3905  (97.28%)
Number of 4-grams hit = 83  (2.07%)
Number of 3-grams hit = 23  (0.57%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle683.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 279 words.
Number of 5-grams hit = 269  (96.42%)
Number of 4-grams hit = 5  (1.79%)
Number of 3-grams hit = 3  (1.08%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle684.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 4719 words.
Number of 5-grams hit = 4597  (97.41%)
Number of 4-grams hit = 83  (1.76%)
Number of 3-grams hit = 34  (0.72%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle685.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 485 words.
Number of 5-grams hit = 471  (97.11%)
Number of 4-grams hit = 9  (1.86%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle686.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 2419 words.
Number of 5-grams hit = 2362  (97.64%)
Number of 4-grams hit = 36  (1.49%)
Number of 3-grams hit = 16  (0.66%)
Number of 2-grams hit = 4  (0.17%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle687.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 556 words.
Number of 5-grams hit = 537  (96.58%)
Number of 4-grams hit = 13  (2.34%)
Number of 3-grams hit = 4  (0.72%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle688.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 332 words.
Number of 5-grams hit = 323  (97.29%)
Number of 4-grams hit = 6  (1.81%)
Number of 3-grams hit = 1  (0.30%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle689.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 554 words.
Number of 5-grams hit = 547  (98.74%)
Number of 4-grams hit = 4  (0.72%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle690.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 824 words.
Number of 5-grams hit = 802  (97.33%)
Number of 4-grams hit = 14  (1.70%)
Number of 3-grams hit = 6  (0.73%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle691.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 138 words.
Number of 5-grams hit = 133  (96.38%)
Number of 4-grams hit = 2  (1.45%)
Number of 3-grams hit = 1  (0.72%)
Number of 2-grams hit = 1  (0.72%)
Number of 1-grams hit = 1  (0.72%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle692.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 460 words.
Number of 5-grams hit = 447  (97.17%)
Number of 4-grams hit = 8  (1.74%)
Number of 3-grams hit = 3  (0.65%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle693.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 1025 words.
Number of 5-grams hit = 986  (96.20%)
Number of 4-grams hit = 25  (2.44%)
Number of 3-grams hit = 11  (1.07%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle694.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 669 words.
Number of 5-grams hit = 654  (97.76%)
Number of 4-grams hit = 10  (1.49%)
Number of 3-grams hit = 3  (0.45%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle695.out
Perplexity = 3.04, Entropy = 1.61 bits
Computation based on 924 words.
Number of 5-grams hit = 904  (97.84%)
Number of 4-grams hit = 14  (1.52%)
Number of 3-grams hit = 4  (0.43%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle696.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 541 words.
Number of 5-grams hit = 532  (98.34%)
Number of 4-grams hit = 6  (1.11%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle697.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1021 words.
Number of 5-grams hit = 990  (96.96%)
Number of 4-grams hit = 20  (1.96%)
Number of 3-grams hit = 8  (0.78%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle698.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 630 words.
Number of 5-grams hit = 618  (98.10%)
Number of 4-grams hit = 8  (1.27%)
Number of 3-grams hit = 2  (0.32%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle699.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 377 words.
Number of 5-grams hit = 355  (94.16%)
Number of 4-grams hit = 10  (2.65%)
Number of 3-grams hit = 10  (2.65%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle700.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 3659 words.
Number of 5-grams hit = 3546  (96.91%)
Number of 4-grams hit = 77  (2.10%)
Number of 3-grams hit = 31  (0.85%)
Number of 2-grams hit = 4  (0.11%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle701.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 878 words.
Number of 5-grams hit = 846  (96.36%)
Number of 4-grams hit = 24  (2.73%)
Number of 3-grams hit = 6  (0.68%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle702.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 2030 words.
Number of 5-grams hit = 1989  (97.98%)
Number of 4-grams hit = 31  (1.53%)
Number of 3-grams hit = 8  (0.39%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle703.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 470 words.
Number of 5-grams hit = 446  (94.89%)
Number of 4-grams hit = 14  (2.98%)
Number of 3-grams hit = 6  (1.28%)
Number of 2-grams hit = 3  (0.64%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle704.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 341 words.
Number of 5-grams hit = 322  (94.43%)
Number of 4-grams hit = 15  (4.40%)
Number of 3-grams hit = 2  (0.59%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle705.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1100 words.
Number of 5-grams hit = 1070  (97.27%)
Number of 4-grams hit = 19  (1.73%)
Number of 3-grams hit = 6  (0.55%)
Number of 2-grams hit = 4  (0.36%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle706.out
Perplexity = 2.96, Entropy = 1.57 bits
Computation based on 509 words.
Number of 5-grams hit = 495  (97.25%)
Number of 4-grams hit = 9  (1.77%)
Number of 3-grams hit = 3  (0.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle707.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 941 words.
Number of 5-grams hit = 914  (97.13%)
Number of 4-grams hit = 17  (1.81%)
Number of 3-grams hit = 8  (0.85%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle708.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 562 words.
Number of 5-grams hit = 550  (97.86%)
Number of 4-grams hit = 8  (1.42%)
Number of 3-grams hit = 2  (0.36%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle709.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 602 words.
Number of 5-grams hit = 581  (96.51%)
Number of 4-grams hit = 15  (2.49%)
Number of 3-grams hit = 3  (0.50%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle710.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 823 words.
Number of 5-grams hit = 806  (97.93%)
Number of 4-grams hit = 9  (1.09%)
Number of 3-grams hit = 5  (0.61%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle711.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1017 words.
Number of 5-grams hit = 984  (96.76%)
Number of 4-grams hit = 26  (2.56%)
Number of 3-grams hit = 5  (0.49%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle712.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 406 words.
Number of 5-grams hit = 387  (95.32%)
Number of 4-grams hit = 9  (2.22%)
Number of 3-grams hit = 7  (1.72%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle713.out
Perplexity = 4.34, Entropy = 2.12 bits
Computation based on 971 words.
Number of 5-grams hit = 927  (95.47%)
Number of 4-grams hit = 29  (2.99%)
Number of 3-grams hit = 10  (1.03%)
Number of 2-grams hit = 4  (0.41%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle714.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 404 words.
Number of 5-grams hit = 386  (95.54%)
Number of 4-grams hit = 9  (2.23%)
Number of 3-grams hit = 6  (1.49%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle715.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 525 words.
Number of 5-grams hit = 514  (97.90%)
Number of 4-grams hit = 8  (1.52%)
Number of 3-grams hit = 1  (0.19%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle716.out
Perplexity = 3.06, Entropy = 1.61 bits
Computation based on 521 words.
Number of 5-grams hit = 507  (97.31%)
Number of 4-grams hit = 5  (0.96%)
Number of 3-grams hit = 7  (1.34%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle717.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 403 words.
Number of 5-grams hit = 387  (96.03%)
Number of 4-grams hit = 8  (1.99%)
Number of 3-grams hit = 6  (1.49%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle718.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 7638 words.
Number of 5-grams hit = 7463  (97.71%)
Number of 4-grams hit = 121  (1.58%)
Number of 3-grams hit = 45  (0.59%)
Number of 2-grams hit = 7  (0.09%)
Number of 1-grams hit = 2  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle719.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1275 words.
Number of 5-grams hit = 1239  (97.18%)
Number of 4-grams hit = 30  (2.35%)
Number of 3-grams hit = 4  (0.31%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle720.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 872 words.
Number of 5-grams hit = 843  (96.67%)
Number of 4-grams hit = 21  (2.41%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle721.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 452 words.
Number of 5-grams hit = 436  (96.46%)
Number of 4-grams hit = 7  (1.55%)
Number of 3-grams hit = 7  (1.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle722.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 1116 words.
Number of 5-grams hit = 1079  (96.68%)
Number of 4-grams hit = 26  (2.33%)
Number of 3-grams hit = 9  (0.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle723.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 472 words.
Number of 5-grams hit = 457  (96.82%)
Number of 4-grams hit = 8  (1.69%)
Number of 3-grams hit = 5  (1.06%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle724.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 500 words.
Number of 5-grams hit = 476  (95.20%)
Number of 4-grams hit = 13  (2.60%)
Number of 3-grams hit = 8  (1.60%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle725.out
Perplexity = 3.28, Entropy = 1.72 bits
Computation based on 792 words.
Number of 5-grams hit = 770  (97.22%)
Number of 4-grams hit = 13  (1.64%)
Number of 3-grams hit = 7  (0.88%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle726.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 573 words.
Number of 5-grams hit = 560  (97.73%)
Number of 4-grams hit = 9  (1.57%)
Number of 3-grams hit = 2  (0.35%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle727.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 519 words.
Number of 5-grams hit = 502  (96.72%)
Number of 4-grams hit = 12  (2.31%)
Number of 3-grams hit = 3  (0.58%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle728.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 453 words.
Number of 5-grams hit = 444  (98.01%)
Number of 4-grams hit = 3  (0.66%)
Number of 3-grams hit = 3  (0.66%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle729.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 468 words.
Number of 5-grams hit = 462  (98.72%)
Number of 4-grams hit = 3  (0.64%)
Number of 3-grams hit = 1  (0.21%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle730.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 5476 words.
Number of 5-grams hit = 5323  (97.21%)
Number of 4-grams hit = 112  (2.05%)
Number of 3-grams hit = 33  (0.60%)
Number of 2-grams hit = 7  (0.13%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle731.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1562 words.
Number of 5-grams hit = 1514  (96.93%)
Number of 4-grams hit = 31  (1.98%)
Number of 3-grams hit = 14  (0.90%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle732.out
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 769 words.
Number of 5-grams hit = 727  (94.54%)
Number of 4-grams hit = 27  (3.51%)
Number of 3-grams hit = 12  (1.56%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle733.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 609 words.
Number of 5-grams hit = 592  (97.21%)
Number of 4-grams hit = 9  (1.48%)
Number of 3-grams hit = 5  (0.82%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle734.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 450 words.
Number of 5-grams hit = 440  (97.78%)
Number of 4-grams hit = 7  (1.56%)
Number of 3-grams hit = 1  (0.22%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle735.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1274 words.
Number of 5-grams hit = 1222  (95.92%)
Number of 4-grams hit = 29  (2.28%)
Number of 3-grams hit = 14  (1.10%)
Number of 2-grams hit = 6  (0.47%)
Number of 1-grams hit = 3  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle736.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 733 words.
Number of 5-grams hit = 708  (96.59%)
Number of 4-grams hit = 16  (2.18%)
Number of 3-grams hit = 5  (0.68%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle737.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 697 words.
Number of 5-grams hit = 679  (97.42%)
Number of 4-grams hit = 10  (1.43%)
Number of 3-grams hit = 5  (0.72%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle738.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1069 words.
Number of 5-grams hit = 1043  (97.57%)
Number of 4-grams hit = 16  (1.50%)
Number of 3-grams hit = 4  (0.37%)
Number of 2-grams hit = 5  (0.47%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle739.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 484 words.
Number of 5-grams hit = 469  (96.90%)
Number of 4-grams hit = 11  (2.27%)
Number of 3-grams hit = 2  (0.41%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle740.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 1194 words.
Number of 5-grams hit = 1168  (97.82%)
Number of 4-grams hit = 20  (1.68%)
Number of 3-grams hit = 4  (0.34%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle741.out
Perplexity = 3.14, Entropy = 1.65 bits
Computation based on 824 words.
Number of 5-grams hit = 796  (96.60%)
Number of 4-grams hit = 18  (2.18%)
Number of 3-grams hit = 7  (0.85%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle742.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 690 words.
Number of 5-grams hit = 663  (96.09%)
Number of 4-grams hit = 18  (2.61%)
Number of 3-grams hit = 6  (0.87%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle743.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 2142 words.
Number of 5-grams hit = 2108  (98.41%)
Number of 4-grams hit = 25  (1.17%)
Number of 3-grams hit = 7  (0.33%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle744.out
Perplexity = 3.10, Entropy = 1.63 bits
Computation based on 580 words.
Number of 5-grams hit = 568  (97.93%)
Number of 4-grams hit = 6  (1.03%)
Number of 3-grams hit = 3  (0.52%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle745.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 740 words.
Number of 5-grams hit = 716  (96.76%)
Number of 4-grams hit = 14  (1.89%)
Number of 3-grams hit = 8  (1.08%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle746.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1498 words.
Number of 5-grams hit = 1455  (97.13%)
Number of 4-grams hit = 32  (2.14%)
Number of 3-grams hit = 9  (0.60%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle747.out
Perplexity = 2.94, Entropy = 1.56 bits
Computation based on 571 words.
Number of 5-grams hit = 555  (97.20%)
Number of 4-grams hit = 11  (1.93%)
Number of 3-grams hit = 3  (0.53%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle748.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 544 words.
Number of 5-grams hit = 536  (98.53%)
Number of 4-grams hit = 5  (0.92%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle749.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 590 words.
Number of 5-grams hit = 570  (96.61%)
Number of 4-grams hit = 9  (1.53%)
Number of 3-grams hit = 9  (1.53%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle750.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 513 words.
Number of 5-grams hit = 499  (97.27%)
Number of 4-grams hit = 8  (1.56%)
Number of 3-grams hit = 4  (0.78%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle751.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 2179 words.
Number of 5-grams hit = 2141  (98.26%)
Number of 4-grams hit = 26  (1.19%)
Number of 3-grams hit = 10  (0.46%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle752.out
Perplexity = 4.19, Entropy = 2.07 bits
Computation based on 468 words.
Number of 5-grams hit = 450  (96.15%)
Number of 4-grams hit = 11  (2.35%)
Number of 3-grams hit = 5  (1.07%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle753.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 572 words.
Number of 5-grams hit = 559  (97.73%)
Number of 4-grams hit = 10  (1.75%)
Number of 3-grams hit = 1  (0.17%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle754.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 2031 words.
Number of 5-grams hit = 1978  (97.39%)
Number of 4-grams hit = 33  (1.62%)
Number of 3-grams hit = 17  (0.84%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle755.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 487 words.
Number of 5-grams hit = 472  (96.92%)
Number of 4-grams hit = 10  (2.05%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle756.out
Perplexity = 4.26, Entropy = 2.09 bits
Computation based on 74 words.
Number of 5-grams hit = 68  (91.89%)
Number of 4-grams hit = 2  (2.70%)
Number of 3-grams hit = 2  (2.70%)
Number of 2-grams hit = 1  (1.35%)
Number of 1-grams hit = 1  (1.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle757.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 557 words.
Number of 5-grams hit = 532  (95.51%)
Number of 4-grams hit = 17  (3.05%)
Number of 3-grams hit = 6  (1.08%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle758.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 2743 words.
Number of 5-grams hit = 2673  (97.45%)
Number of 4-grams hit = 46  (1.68%)
Number of 3-grams hit = 19  (0.69%)
Number of 2-grams hit = 4  (0.15%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle759.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 3515 words.
Number of 5-grams hit = 3413  (97.10%)
Number of 4-grams hit = 74  (2.11%)
Number of 3-grams hit = 24  (0.68%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle760.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 618 words.
Number of 5-grams hit = 600  (97.09%)
Number of 4-grams hit = 9  (1.46%)
Number of 3-grams hit = 5  (0.81%)
Number of 2-grams hit = 3  (0.49%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle761.out
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 684 words.
Number of 5-grams hit = 664  (97.08%)
Number of 4-grams hit = 15  (2.19%)
Number of 3-grams hit = 3  (0.44%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle762.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 861 words.
Number of 5-grams hit = 832  (96.63%)
Number of 4-grams hit = 19  (2.21%)
Number of 3-grams hit = 8  (0.93%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle763.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 782 words.
Number of 5-grams hit = 763  (97.57%)
Number of 4-grams hit = 12  (1.53%)
Number of 3-grams hit = 4  (0.51%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle764.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 582 words.
Number of 5-grams hit = 564  (96.91%)
Number of 4-grams hit = 11  (1.89%)
Number of 3-grams hit = 5  (0.86%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle765.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 888 words.
Number of 5-grams hit = 853  (96.06%)
Number of 4-grams hit = 21  (2.36%)
Number of 3-grams hit = 9  (1.01%)
Number of 2-grams hit = 4  (0.45%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle766.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 293 words.
Number of 5-grams hit = 287  (97.95%)
Number of 4-grams hit = 3  (1.02%)
Number of 3-grams hit = 1  (0.34%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle767.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 4625 words.
Number of 5-grams hit = 4518  (97.69%)
Number of 4-grams hit = 75  (1.62%)
Number of 3-grams hit = 29  (0.63%)
Number of 2-grams hit = 2  (0.04%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle768.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 503 words.
Number of 5-grams hit = 484  (96.22%)
Number of 4-grams hit = 14  (2.78%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle769.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 730 words.
Number of 5-grams hit = 712  (97.53%)
Number of 4-grams hit = 10  (1.37%)
Number of 3-grams hit = 4  (0.55%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle770.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 658 words.
Number of 5-grams hit = 634  (96.35%)
Number of 4-grams hit = 16  (2.43%)
Number of 3-grams hit = 6  (0.91%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle771.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 501 words.
Number of 5-grams hit = 487  (97.21%)
Number of 4-grams hit = 8  (1.60%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 2  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle772.out
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 590 words.
Number of 5-grams hit = 576  (97.63%)
Number of 4-grams hit = 10  (1.69%)
Number of 3-grams hit = 2  (0.34%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle773.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 3261 words.
Number of 5-grams hit = 3172  (97.27%)
Number of 4-grams hit = 61  (1.87%)
Number of 3-grams hit = 21  (0.64%)
Number of 2-grams hit = 6  (0.18%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle774.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 2158 words.
Number of 5-grams hit = 2103  (97.45%)
Number of 4-grams hit = 37  (1.71%)
Number of 3-grams hit = 13  (0.60%)
Number of 2-grams hit = 4  (0.19%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle775.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 558 words.
Number of 5-grams hit = 545  (97.67%)
Number of 4-grams hit = 10  (1.79%)
Number of 3-grams hit = 1  (0.18%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle776.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 48 words.
Number of 5-grams hit = 44  (91.67%)
Number of 4-grams hit = 1  (2.08%)
Number of 3-grams hit = 1  (2.08%)
Number of 2-grams hit = 1  (2.08%)
Number of 1-grams hit = 1  (2.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle777.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 10354 words.
Number of 5-grams hit = 10060  (97.16%)
Number of 4-grams hit = 186  (1.80%)
Number of 3-grams hit = 91  (0.88%)
Number of 2-grams hit = 16  (0.15%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle778.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 545 words.
Number of 5-grams hit = 526  (96.51%)
Number of 4-grams hit = 12  (2.20%)
Number of 3-grams hit = 5  (0.92%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle779.out
Perplexity = 4.22, Entropy = 2.08 bits
Computation based on 1014 words.
Number of 5-grams hit = 953  (93.98%)
Number of 4-grams hit = 42  (4.14%)
Number of 3-grams hit = 15  (1.48%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle780.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 445 words.
Number of 5-grams hit = 432  (97.08%)
Number of 4-grams hit = 6  (1.35%)
Number of 3-grams hit = 5  (1.12%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle781.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 786 words.
Number of 5-grams hit = 755  (96.06%)
Number of 4-grams hit = 22  (2.80%)
Number of 3-grams hit = 7  (0.89%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle782.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 1485 words.
Number of 5-grams hit = 1449  (97.58%)
Number of 4-grams hit = 24  (1.62%)
Number of 3-grams hit = 9  (0.61%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle783.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 549 words.
Number of 5-grams hit = 530  (96.54%)
Number of 4-grams hit = 9  (1.64%)
Number of 3-grams hit = 8  (1.46%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle784.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 1881 words.
Number of 5-grams hit = 1845  (98.09%)
Number of 4-grams hit = 26  (1.38%)
Number of 3-grams hit = 7  (0.37%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle785.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 2868 words.
Number of 5-grams hit = 2792  (97.35%)
Number of 4-grams hit = 53  (1.85%)
Number of 3-grams hit = 19  (0.66%)
Number of 2-grams hit = 3  (0.10%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle786.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 370 words.
Number of 5-grams hit = 360  (97.30%)
Number of 4-grams hit = 6  (1.62%)
Number of 3-grams hit = 2  (0.54%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle787.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 512 words.
Number of 5-grams hit = 500  (97.66%)
Number of 4-grams hit = 9  (1.76%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle788.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1616 words.
Number of 5-grams hit = 1574  (97.40%)
Number of 4-grams hit = 24  (1.49%)
Number of 3-grams hit = 15  (0.93%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle789.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 12475 words.
Number of 5-grams hit = 12179  (97.63%)
Number of 4-grams hit = 218  (1.75%)
Number of 3-grams hit = 66  (0.53%)
Number of 2-grams hit = 11  (0.09%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle790.out
Perplexity = 3.02, Entropy = 1.60 bits
Computation based on 1019 words.
Number of 5-grams hit = 994  (97.55%)
Number of 4-grams hit = 16  (1.57%)
Number of 3-grams hit = 7  (0.69%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle791.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 749 words.
Number of 5-grams hit = 722  (96.40%)
Number of 4-grams hit = 16  (2.14%)
Number of 3-grams hit = 9  (1.20%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle792.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 3144 words.
Number of 5-grams hit = 3075  (97.81%)
Number of 4-grams hit = 50  (1.59%)
Number of 3-grams hit = 14  (0.45%)
Number of 2-grams hit = 4  (0.13%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle793.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 197 words.
Number of 5-grams hit = 186  (94.42%)
Number of 4-grams hit = 6  (3.05%)
Number of 3-grams hit = 3  (1.52%)
Number of 2-grams hit = 1  (0.51%)
Number of 1-grams hit = 1  (0.51%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle794.out
Perplexity = 4.21, Entropy = 2.07 bits
Computation based on 548 words.
Number of 5-grams hit = 526  (95.99%)
Number of 4-grams hit = 13  (2.37%)
Number of 3-grams hit = 7  (1.28%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle795.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 460 words.
Number of 5-grams hit = 439  (95.43%)
Number of 4-grams hit = 15  (3.26%)
Number of 3-grams hit = 4  (0.87%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle796.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 1620 words.
Number of 5-grams hit = 1585  (97.84%)
Number of 4-grams hit = 22  (1.36%)
Number of 3-grams hit = 10  (0.62%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle797.out
Perplexity = 3.09, Entropy = 1.63 bits
Computation based on 413 words.
Number of 5-grams hit = 401  (97.09%)
Number of 4-grams hit = 7  (1.69%)
Number of 3-grams hit = 3  (0.73%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle798.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 503 words.
Number of 5-grams hit = 492  (97.81%)
Number of 4-grams hit = 6  (1.19%)
Number of 3-grams hit = 3  (0.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle799.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 816 words.
Number of 5-grams hit = 790  (96.81%)
Number of 4-grams hit = 18  (2.21%)
Number of 3-grams hit = 6  (0.74%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle800.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 473 words.
Number of 5-grams hit = 462  (97.67%)
Number of 4-grams hit = 6  (1.27%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle801.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 3418 words.
Number of 5-grams hit = 3356  (98.19%)
Number of 4-grams hit = 45  (1.32%)
Number of 3-grams hit = 14  (0.41%)
Number of 2-grams hit = 2  (0.06%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle802.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 2237 words.
Number of 5-grams hit = 2202  (98.44%)
Number of 4-grams hit = 25  (1.12%)
Number of 3-grams hit = 6  (0.27%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle803.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 4594 words.
Number of 5-grams hit = 4461  (97.10%)
Number of 4-grams hit = 93  (2.02%)
Number of 3-grams hit = 30  (0.65%)
Number of 2-grams hit = 9  (0.20%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle804.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 1564 words.
Number of 5-grams hit = 1512  (96.68%)
Number of 4-grams hit = 33  (2.11%)
Number of 3-grams hit = 16  (1.02%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle805.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 993 words.
Number of 5-grams hit = 966  (97.28%)
Number of 4-grams hit = 14  (1.41%)
Number of 3-grams hit = 8  (0.81%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 2  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle806.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 3603 words.
Number of 5-grams hit = 3497  (97.06%)
Number of 4-grams hit = 68  (1.89%)
Number of 3-grams hit = 32  (0.89%)
Number of 2-grams hit = 5  (0.14%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle807.out
Perplexity = 3.10, Entropy = 1.63 bits
Computation based on 756 words.
Number of 5-grams hit = 746  (98.68%)
Number of 4-grams hit = 4  (0.53%)
Number of 3-grams hit = 4  (0.53%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle808.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1830 words.
Number of 5-grams hit = 1779  (97.21%)
Number of 4-grams hit = 37  (2.02%)
Number of 3-grams hit = 11  (0.60%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle809.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 2165 words.
Number of 5-grams hit = 2099  (96.95%)
Number of 4-grams hit = 49  (2.26%)
Number of 3-grams hit = 13  (0.60%)
Number of 2-grams hit = 3  (0.14%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle810.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 937 words.
Number of 5-grams hit = 924  (98.61%)
Number of 4-grams hit = 9  (0.96%)
Number of 3-grams hit = 2  (0.21%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle811.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 4883 words.
Number of 5-grams hit = 4777  (97.83%)
Number of 4-grams hit = 72  (1.47%)
Number of 3-grams hit = 29  (0.59%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle812.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 3604 words.
Number of 5-grams hit = 3518  (97.61%)
Number of 4-grams hit = 63  (1.75%)
Number of 3-grams hit = 17  (0.47%)
Number of 2-grams hit = 5  (0.14%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle813.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 1497 words.
Number of 5-grams hit = 1482  (99.00%)
Number of 4-grams hit = 10  (0.67%)
Number of 3-grams hit = 3  (0.20%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle814.out
Perplexity = 3.00, Entropy = 1.58 bits
Computation based on 908 words.
Number of 5-grams hit = 893  (98.35%)
Number of 4-grams hit = 9  (0.99%)
Number of 3-grams hit = 4  (0.44%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle815.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1390 words.
Number of 5-grams hit = 1347  (96.91%)
Number of 4-grams hit = 33  (2.37%)
Number of 3-grams hit = 8  (0.58%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle816.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1434 words.
Number of 5-grams hit = 1388  (96.79%)
Number of 4-grams hit = 33  (2.30%)
Number of 3-grams hit = 9  (0.63%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle817.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 580 words.
Number of 5-grams hit = 552  (95.17%)
Number of 4-grams hit = 17  (2.93%)
Number of 3-grams hit = 8  (1.38%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle818.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 604 words.
Number of 5-grams hit = 582  (96.36%)
Number of 4-grams hit = 15  (2.48%)
Number of 3-grams hit = 5  (0.83%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle819.out
Perplexity = 3.14, Entropy = 1.65 bits
Computation based on 1947 words.
Number of 5-grams hit = 1901  (97.64%)
Number of 4-grams hit = 36  (1.85%)
Number of 3-grams hit = 8  (0.41%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle820.out
Perplexity = 3.09, Entropy = 1.63 bits
Computation based on 880 words.
Number of 5-grams hit = 859  (97.61%)
Number of 4-grams hit = 14  (1.59%)
Number of 3-grams hit = 5  (0.57%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle821.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 1187 words.
Number of 5-grams hit = 1161  (97.81%)
Number of 4-grams hit = 19  (1.60%)
Number of 3-grams hit = 5  (0.42%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle822.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 2654 words.
Number of 5-grams hit = 2597  (97.85%)
Number of 4-grams hit = 34  (1.28%)
Number of 3-grams hit = 16  (0.60%)
Number of 2-grams hit = 5  (0.19%)
Number of 1-grams hit = 2  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle823.out
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 1435 words.
Number of 5-grams hit = 1403  (97.77%)
Number of 4-grams hit = 25  (1.74%)
Number of 3-grams hit = 5  (0.35%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle824.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 533 words.
Number of 5-grams hit = 519  (97.37%)
Number of 4-grams hit = 7  (1.31%)
Number of 3-grams hit = 4  (0.75%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle825.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 639 words.
Number of 5-grams hit = 613  (95.93%)
Number of 4-grams hit = 14  (2.19%)
Number of 3-grams hit = 10  (1.56%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle826.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 978 words.
Number of 5-grams hit = 964  (98.57%)
Number of 4-grams hit = 9  (0.92%)
Number of 3-grams hit = 3  (0.31%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle827.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 734 words.
Number of 5-grams hit = 713  (97.14%)
Number of 4-grams hit = 14  (1.91%)
Number of 3-grams hit = 4  (0.54%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle828.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 13188 words.
Number of 5-grams hit = 12819  (97.20%)
Number of 4-grams hit = 262  (1.99%)
Number of 3-grams hit = 87  (0.66%)
Number of 2-grams hit = 17  (0.13%)
Number of 1-grams hit = 3  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle829.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 578 words.
Number of 5-grams hit = 563  (97.40%)
Number of 4-grams hit = 9  (1.56%)
Number of 3-grams hit = 4  (0.69%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle830.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 2488 words.
Number of 5-grams hit = 2431  (97.71%)
Number of 4-grams hit = 42  (1.69%)
Number of 3-grams hit = 12  (0.48%)
Number of 2-grams hit = 2  (0.08%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle831.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1106 words.
Number of 5-grams hit = 1062  (96.02%)
Number of 4-grams hit = 36  (3.25%)
Number of 3-grams hit = 5  (0.45%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle832.out
Perplexity = 3.45, Entropy = 1.78 bits
Computation based on 460 words.
Number of 5-grams hit = 454  (98.70%)
Number of 4-grams hit = 2  (0.43%)
Number of 3-grams hit = 2  (0.43%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle833.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 888 words.
Number of 5-grams hit = 866  (97.52%)
Number of 4-grams hit = 14  (1.58%)
Number of 3-grams hit = 5  (0.56%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle834.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 963 words.
Number of 5-grams hit = 948  (98.44%)
Number of 4-grams hit = 9  (0.93%)
Number of 3-grams hit = 4  (0.42%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle835.out
Perplexity = 3.01, Entropy = 1.59 bits
Computation based on 491 words.
Number of 5-grams hit = 472  (96.13%)
Number of 4-grams hit = 14  (2.85%)
Number of 3-grams hit = 3  (0.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle836.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 528 words.
Number of 5-grams hit = 513  (97.16%)
Number of 4-grams hit = 9  (1.70%)
Number of 3-grams hit = 4  (0.76%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle837.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 900 words.
Number of 5-grams hit = 885  (98.33%)
Number of 4-grams hit = 11  (1.22%)
Number of 3-grams hit = 2  (0.22%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle838.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 2446 words.
Number of 5-grams hit = 2385  (97.51%)
Number of 4-grams hit = 50  (2.04%)
Number of 3-grams hit = 9  (0.37%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle839.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 496 words.
Number of 5-grams hit = 469  (94.56%)
Number of 4-grams hit = 19  (3.83%)
Number of 3-grams hit = 6  (1.21%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle840.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 560 words.
Number of 5-grams hit = 547  (97.68%)
Number of 4-grams hit = 8  (1.43%)
Number of 3-grams hit = 3  (0.54%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle841.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 1009 words.
Number of 5-grams hit = 977  (96.83%)
Number of 4-grams hit = 22  (2.18%)
Number of 3-grams hit = 8  (0.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle842.out
Perplexity = 2.86, Entropy = 1.51 bits
Computation based on 503 words.
Number of 5-grams hit = 496  (98.61%)
Number of 4-grams hit = 4  (0.80%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle843.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1419 words.
Number of 5-grams hit = 1366  (96.26%)
Number of 4-grams hit = 34  (2.40%)
Number of 3-grams hit = 13  (0.92%)
Number of 2-grams hit = 5  (0.35%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle844.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 9009 words.
Number of 5-grams hit = 8734  (96.95%)
Number of 4-grams hit = 194  (2.15%)
Number of 3-grams hit = 68  (0.75%)
Number of 2-grams hit = 12  (0.13%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle845.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 648 words.
Number of 5-grams hit = 631  (97.38%)
Number of 4-grams hit = 12  (1.85%)
Number of 3-grams hit = 3  (0.46%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle846.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 609 words.
Number of 5-grams hit = 597  (98.03%)
Number of 4-grams hit = 8  (1.31%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle847.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1795 words.
Number of 5-grams hit = 1720  (95.82%)
Number of 4-grams hit = 46  (2.56%)
Number of 3-grams hit = 24  (1.34%)
Number of 2-grams hit = 4  (0.22%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle848.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 389 words.
Number of 5-grams hit = 385  (98.97%)
Number of 4-grams hit = 1  (0.26%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle849.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1705 words.
Number of 5-grams hit = 1678  (98.42%)
Number of 4-grams hit = 17  (1.00%)
Number of 3-grams hit = 7  (0.41%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle850.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 477 words.
Number of 5-grams hit = 462  (96.86%)
Number of 4-grams hit = 9  (1.89%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle851.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 604 words.
Number of 5-grams hit = 582  (96.36%)
Number of 4-grams hit = 18  (2.98%)
Number of 3-grams hit = 2  (0.33%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle852.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 684 words.
Number of 5-grams hit = 675  (98.68%)
Number of 4-grams hit = 6  (0.88%)
Number of 3-grams hit = 1  (0.15%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle853.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1621 words.
Number of 5-grams hit = 1571  (96.92%)
Number of 4-grams hit = 31  (1.91%)
Number of 3-grams hit = 14  (0.86%)
Number of 2-grams hit = 4  (0.25%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle854.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 2917 words.
Number of 5-grams hit = 2853  (97.81%)
Number of 4-grams hit = 47  (1.61%)
Number of 3-grams hit = 14  (0.48%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle855.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 465 words.
Number of 5-grams hit = 449  (96.56%)
Number of 4-grams hit = 11  (2.37%)
Number of 3-grams hit = 3  (0.65%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle856.out
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 889 words.
Number of 5-grams hit = 880  (98.99%)
Number of 4-grams hit = 5  (0.56%)
Number of 3-grams hit = 2  (0.22%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle857.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 379 words.
Number of 5-grams hit = 373  (98.42%)
Number of 4-grams hit = 3  (0.79%)
Number of 3-grams hit = 1  (0.26%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle858.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 255 words.
Number of 5-grams hit = 248  (97.25%)
Number of 4-grams hit = 3  (1.18%)
Number of 3-grams hit = 2  (0.78%)
Number of 2-grams hit = 1  (0.39%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle859.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1192 words.
Number of 5-grams hit = 1162  (97.48%)
Number of 4-grams hit = 19  (1.59%)
Number of 3-grams hit = 8  (0.67%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle860.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 503 words.
Number of 5-grams hit = 489  (97.22%)
Number of 4-grams hit = 8  (1.59%)
Number of 3-grams hit = 4  (0.80%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle861.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 432 words.
Number of 5-grams hit = 428  (99.07%)
Number of 4-grams hit = 1  (0.23%)
Number of 3-grams hit = 1  (0.23%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle862.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 487 words.
Number of 5-grams hit = 472  (96.92%)
Number of 4-grams hit = 9  (1.85%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle863.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 712 words.
Number of 5-grams hit = 694  (97.47%)
Number of 4-grams hit = 10  (1.40%)
Number of 3-grams hit = 5  (0.70%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle864.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 494 words.
Number of 5-grams hit = 475  (96.15%)
Number of 4-grams hit = 10  (2.02%)
Number of 3-grams hit = 4  (0.81%)
Number of 2-grams hit = 4  (0.81%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle865.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 6948 words.
Number of 5-grams hit = 6810  (98.01%)
Number of 4-grams hit = 104  (1.50%)
Number of 3-grams hit = 30  (0.43%)
Number of 2-grams hit = 3  (0.04%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle866.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 668 words.
Number of 5-grams hit = 651  (97.46%)
Number of 4-grams hit = 11  (1.65%)
Number of 3-grams hit = 3  (0.45%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle867.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 377 words.
Number of 5-grams hit = 355  (94.16%)
Number of 4-grams hit = 12  (3.18%)
Number of 3-grams hit = 8  (2.12%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle868.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1799 words.
Number of 5-grams hit = 1755  (97.55%)
Number of 4-grams hit = 32  (1.78%)
Number of 3-grams hit = 8  (0.44%)
Number of 2-grams hit = 3  (0.17%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle869.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 3365 words.
Number of 5-grams hit = 3279  (97.44%)
Number of 4-grams hit = 65  (1.93%)
Number of 3-grams hit = 16  (0.48%)
Number of 2-grams hit = 4  (0.12%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle870.out
Perplexity = 3.01, Entropy = 1.59 bits
Computation based on 831 words.
Number of 5-grams hit = 817  (98.32%)
Number of 4-grams hit = 7  (0.84%)
Number of 3-grams hit = 4  (0.48%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle871.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 444 words.
Number of 5-grams hit = 433  (97.52%)
Number of 4-grams hit = 4  (0.90%)
Number of 3-grams hit = 5  (1.13%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle872.out
Perplexity = 4.31, Entropy = 2.11 bits
Computation based on 316 words.
Number of 5-grams hit = 302  (95.57%)
Number of 4-grams hit = 8  (2.53%)
Number of 3-grams hit = 4  (1.27%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle873.out
Perplexity = 3.17, Entropy = 1.67 bits
Computation based on 1015 words.
Number of 5-grams hit = 986  (97.14%)
Number of 4-grams hit = 18  (1.77%)
Number of 3-grams hit = 9  (0.89%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle874.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 460 words.
Number of 5-grams hit = 447  (97.17%)
Number of 4-grams hit = 10  (2.17%)
Number of 3-grams hit = 1  (0.22%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle875.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 855 words.
Number of 5-grams hit = 826  (96.61%)
Number of 4-grams hit = 19  (2.22%)
Number of 3-grams hit = 8  (0.94%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle876.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 10045 words.
Number of 5-grams hit = 9792  (97.48%)
Number of 4-grams hit = 180  (1.79%)
Number of 3-grams hit = 59  (0.59%)
Number of 2-grams hit = 12  (0.12%)
Number of 1-grams hit = 2  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle877.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 502 words.
Number of 5-grams hit = 488  (97.21%)
Number of 4-grams hit = 8  (1.59%)
Number of 3-grams hit = 4  (0.80%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle878.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 364 words.
Number of 5-grams hit = 356  (97.80%)
Number of 4-grams hit = 5  (1.37%)
Number of 3-grams hit = 1  (0.27%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle879.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 591 words.
Number of 5-grams hit = 573  (96.95%)
Number of 4-grams hit = 13  (2.20%)
Number of 3-grams hit = 3  (0.51%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle880.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 509 words.
Number of 5-grams hit = 493  (96.86%)
Number of 4-grams hit = 12  (2.36%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle881.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 376 words.
Number of 5-grams hit = 363  (96.54%)
Number of 4-grams hit = 8  (2.13%)
Number of 3-grams hit = 3  (0.80%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle882.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 638 words.
Number of 5-grams hit = 619  (97.02%)
Number of 4-grams hit = 12  (1.88%)
Number of 3-grams hit = 5  (0.78%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle883.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 638 words.
Number of 5-grams hit = 624  (97.81%)
Number of 4-grams hit = 10  (1.57%)
Number of 3-grams hit = 2  (0.31%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle884.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 777 words.
Number of 5-grams hit = 756  (97.30%)
Number of 4-grams hit = 10  (1.29%)
Number of 3-grams hit = 8  (1.03%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle885.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 534 words.
Number of 5-grams hit = 523  (97.94%)
Number of 4-grams hit = 6  (1.12%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle886.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 508 words.
Number of 5-grams hit = 491  (96.65%)
Number of 4-grams hit = 14  (2.76%)
Number of 3-grams hit = 1  (0.20%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle887.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 517 words.
Number of 5-grams hit = 502  (97.10%)
Number of 4-grams hit = 11  (2.13%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle888.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1311 words.
Number of 5-grams hit = 1285  (98.02%)
Number of 4-grams hit = 20  (1.53%)
Number of 3-grams hit = 4  (0.31%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle889.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 4700 words.
Number of 5-grams hit = 4594  (97.74%)
Number of 4-grams hit = 71  (1.51%)
Number of 3-grams hit = 29  (0.62%)
Number of 2-grams hit = 5  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle890.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 1148 words.
Number of 5-grams hit = 1134  (98.78%)
Number of 4-grams hit = 10  (0.87%)
Number of 3-grams hit = 2  (0.17%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle891.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 873 words.
Number of 5-grams hit = 852  (97.59%)
Number of 4-grams hit = 16  (1.83%)
Number of 3-grams hit = 3  (0.34%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle892.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 3489 words.
Number of 5-grams hit = 3436  (98.48%)
Number of 4-grams hit = 38  (1.09%)
Number of 3-grams hit = 12  (0.34%)
Number of 2-grams hit = 2  (0.06%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle893.out
Perplexity = 4.11, Entropy = 2.04 bits
Computation based on 699 words.
Number of 5-grams hit = 668  (95.57%)
Number of 4-grams hit = 19  (2.72%)
Number of 3-grams hit = 10  (1.43%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle894.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 805 words.
Number of 5-grams hit = 777  (96.52%)
Number of 4-grams hit = 16  (1.99%)
Number of 3-grams hit = 10  (1.24%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle895.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 188 words.
Number of 5-grams hit = 183  (97.34%)
Number of 4-grams hit = 2  (1.06%)
Number of 3-grams hit = 1  (0.53%)
Number of 2-grams hit = 1  (0.53%)
Number of 1-grams hit = 1  (0.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle896.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 452 words.
Number of 5-grams hit = 442  (97.79%)
Number of 4-grams hit = 5  (1.11%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle897.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 767 words.
Number of 5-grams hit = 735  (95.83%)
Number of 4-grams hit = 19  (2.48%)
Number of 3-grams hit = 9  (1.17%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle898.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1746 words.
Number of 5-grams hit = 1706  (97.71%)
Number of 4-grams hit = 30  (1.72%)
Number of 3-grams hit = 7  (0.40%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle899.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 219 words.
Number of 5-grams hit = 214  (97.72%)
Number of 4-grams hit = 2  (0.91%)
Number of 3-grams hit = 1  (0.46%)
Number of 2-grams hit = 1  (0.46%)
Number of 1-grams hit = 1  (0.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle900.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 1135 words.
Number of 5-grams hit = 1109  (97.71%)
Number of 4-grams hit = 17  (1.50%)
Number of 3-grams hit = 7  (0.62%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle901.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 428 words.
Number of 5-grams hit = 418  (97.66%)
Number of 4-grams hit = 6  (1.40%)
Number of 3-grams hit = 2  (0.47%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle902.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 2155 words.
Number of 5-grams hit = 2075  (96.29%)
Number of 4-grams hit = 55  (2.55%)
Number of 3-grams hit = 22  (1.02%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle903.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1273 words.
Number of 5-grams hit = 1251  (98.27%)
Number of 4-grams hit = 16  (1.26%)
Number of 3-grams hit = 4  (0.31%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle904.out
Perplexity = 2.60, Entropy = 1.38 bits
Computation based on 649 words.
Number of 5-grams hit = 634  (97.69%)
Number of 4-grams hit = 8  (1.23%)
Number of 3-grams hit = 5  (0.77%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle905.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 388 words.
Number of 5-grams hit = 367  (94.59%)
Number of 4-grams hit = 17  (4.38%)
Number of 3-grams hit = 2  (0.52%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle906.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 476 words.
Number of 5-grams hit = 465  (97.69%)
Number of 4-grams hit = 5  (1.05%)
Number of 3-grams hit = 4  (0.84%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle907.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 488 words.
Number of 5-grams hit = 473  (96.93%)
Number of 4-grams hit = 8  (1.64%)
Number of 3-grams hit = 5  (1.02%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle908.out
Perplexity = 2.99, Entropy = 1.58 bits
Computation based on 552 words.
Number of 5-grams hit = 543  (98.37%)
Number of 4-grams hit = 4  (0.72%)
Number of 3-grams hit = 3  (0.54%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle909.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 456 words.
Number of 5-grams hit = 444  (97.37%)
Number of 4-grams hit = 8  (1.75%)
Number of 3-grams hit = 2  (0.44%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle910.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 737 words.
Number of 5-grams hit = 706  (95.79%)
Number of 4-grams hit = 20  (2.71%)
Number of 3-grams hit = 7  (0.95%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle911.out
Perplexity = 3.02, Entropy = 1.59 bits
Computation based on 688 words.
Number of 5-grams hit = 674  (97.97%)
Number of 4-grams hit = 9  (1.31%)
Number of 3-grams hit = 3  (0.44%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle912.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 743 words.
Number of 5-grams hit = 709  (95.42%)
Number of 4-grams hit = 22  (2.96%)
Number of 3-grams hit = 9  (1.21%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle913.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 743 words.
Number of 5-grams hit = 711  (95.69%)
Number of 4-grams hit = 20  (2.69%)
Number of 3-grams hit = 9  (1.21%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle914.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 575 words.
Number of 5-grams hit = 558  (97.04%)
Number of 4-grams hit = 11  (1.91%)
Number of 3-grams hit = 4  (0.70%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle915.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 1309 words.
Number of 5-grams hit = 1275  (97.40%)
Number of 4-grams hit = 19  (1.45%)
Number of 3-grams hit = 13  (0.99%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle916.out
Perplexity = 3.45, Entropy = 1.78 bits
Computation based on 1003 words.
Number of 5-grams hit = 989  (98.60%)
Number of 4-grams hit = 8  (0.80%)
Number of 3-grams hit = 4  (0.40%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle917.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1180 words.
Number of 5-grams hit = 1152  (97.63%)
Number of 4-grams hit = 20  (1.69%)
Number of 3-grams hit = 6  (0.51%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle918.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 535 words.
Number of 5-grams hit = 522  (97.57%)
Number of 4-grams hit = 7  (1.31%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle919.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 3585 words.
Number of 5-grams hit = 3500  (97.63%)
Number of 4-grams hit = 62  (1.73%)
Number of 3-grams hit = 18  (0.50%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 2  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle920.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1001 words.
Number of 5-grams hit = 975  (97.40%)
Number of 4-grams hit = 17  (1.70%)
Number of 3-grams hit = 6  (0.60%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle921.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 844 words.
Number of 5-grams hit = 811  (96.09%)
Number of 4-grams hit = 17  (2.01%)
Number of 3-grams hit = 12  (1.42%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle922.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 515 words.
Number of 5-grams hit = 501  (97.28%)
Number of 4-grams hit = 9  (1.75%)
Number of 3-grams hit = 2  (0.39%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle923.out
Perplexity = 3.03, Entropy = 1.60 bits
Computation based on 560 words.
Number of 5-grams hit = 541  (96.61%)
Number of 4-grams hit = 11  (1.96%)
Number of 3-grams hit = 4  (0.71%)
Number of 2-grams hit = 3  (0.54%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle924.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 376 words.
Number of 5-grams hit = 366  (97.34%)
Number of 4-grams hit = 5  (1.33%)
Number of 3-grams hit = 3  (0.80%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle925.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 4136 words.
Number of 5-grams hit = 4025  (97.32%)
Number of 4-grams hit = 76  (1.84%)
Number of 3-grams hit = 28  (0.68%)
Number of 2-grams hit = 6  (0.15%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle926.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 1421 words.
Number of 5-grams hit = 1388  (97.68%)
Number of 4-grams hit = 23  (1.62%)
Number of 3-grams hit = 7  (0.49%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle927.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 440 words.
Number of 5-grams hit = 424  (96.36%)
Number of 4-grams hit = 12  (2.73%)
Number of 3-grams hit = 2  (0.45%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle928.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 516 words.
Number of 5-grams hit = 498  (96.51%)
Number of 4-grams hit = 11  (2.13%)
Number of 3-grams hit = 5  (0.97%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle929.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 538 words.
Number of 5-grams hit = 523  (97.21%)
Number of 4-grams hit = 10  (1.86%)
Number of 3-grams hit = 3  (0.56%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle930.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 465 words.
Number of 5-grams hit = 454  (97.63%)
Number of 4-grams hit = 4  (0.86%)
Number of 3-grams hit = 4  (0.86%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle931.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 645 words.
Number of 5-grams hit = 622  (96.43%)
Number of 4-grams hit = 12  (1.86%)
Number of 3-grams hit = 7  (1.09%)
Number of 2-grams hit = 3  (0.47%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle932.out
Perplexity = 2.61, Entropy = 1.39 bits
Computation based on 228 words.
Number of 5-grams hit = 222  (97.37%)
Number of 4-grams hit = 2  (0.88%)
Number of 3-grams hit = 2  (0.88%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle933.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 453 words.
Number of 5-grams hit = 438  (96.69%)
Number of 4-grams hit = 8  (1.77%)
Number of 3-grams hit = 5  (1.10%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle934.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 416 words.
Number of 5-grams hit = 397  (95.43%)
Number of 4-grams hit = 12  (2.88%)
Number of 3-grams hit = 5  (1.20%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle935.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 1259 words.
Number of 5-grams hit = 1241  (98.57%)
Number of 4-grams hit = 14  (1.11%)
Number of 3-grams hit = 2  (0.16%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle936.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 483 words.
Number of 5-grams hit = 472  (97.72%)
Number of 4-grams hit = 6  (1.24%)
Number of 3-grams hit = 3  (0.62%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle937.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 239 words.
Number of 5-grams hit = 231  (96.65%)
Number of 4-grams hit = 5  (2.09%)
Number of 3-grams hit = 1  (0.42%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle938.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 1402 words.
Number of 5-grams hit = 1361  (97.08%)
Number of 4-grams hit = 29  (2.07%)
Number of 3-grams hit = 9  (0.64%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle939.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 519 words.
Number of 5-grams hit = 510  (98.27%)
Number of 4-grams hit = 4  (0.77%)
Number of 3-grams hit = 3  (0.58%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle940.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 2076 words.
Number of 5-grams hit = 2030  (97.78%)
Number of 4-grams hit = 29  (1.40%)
Number of 3-grams hit = 14  (0.67%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle941.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1305 words.
Number of 5-grams hit = 1271  (97.39%)
Number of 4-grams hit = 26  (1.99%)
Number of 3-grams hit = 6  (0.46%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle942.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 571 words.
Number of 5-grams hit = 552  (96.67%)
Number of 4-grams hit = 10  (1.75%)
Number of 3-grams hit = 7  (1.23%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle943.out
Perplexity = 2.99, Entropy = 1.58 bits
Computation based on 1491 words.
Number of 5-grams hit = 1478  (99.13%)
Number of 4-grams hit = 8  (0.54%)
Number of 3-grams hit = 3  (0.20%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle944.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 477 words.
Number of 5-grams hit = 462  (96.86%)
Number of 4-grams hit = 10  (2.10%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle945.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 3747 words.
Number of 5-grams hit = 3656  (97.57%)
Number of 4-grams hit = 61  (1.63%)
Number of 3-grams hit = 23  (0.61%)
Number of 2-grams hit = 6  (0.16%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle946.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 611 words.
Number of 5-grams hit = 584  (95.58%)
Number of 4-grams hit = 16  (2.62%)
Number of 3-grams hit = 9  (1.47%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle947.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 459 words.
Number of 5-grams hit = 437  (95.21%)
Number of 4-grams hit = 13  (2.83%)
Number of 3-grams hit = 5  (1.09%)
Number of 2-grams hit = 3  (0.65%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle948.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 627 words.
Number of 5-grams hit = 605  (96.49%)
Number of 4-grams hit = 16  (2.55%)
Number of 3-grams hit = 3  (0.48%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle949.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 1862 words.
Number of 5-grams hit = 1806  (96.99%)
Number of 4-grams hit = 37  (1.99%)
Number of 3-grams hit = 14  (0.75%)
Number of 2-grams hit = 4  (0.21%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle950.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 543 words.
Number of 5-grams hit = 521  (95.95%)
Number of 4-grams hit = 15  (2.76%)
Number of 3-grams hit = 5  (0.92%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle951.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 622 words.
Number of 5-grams hit = 594  (95.50%)
Number of 4-grams hit = 20  (3.22%)
Number of 3-grams hit = 6  (0.96%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle952.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 1389 words.
Number of 5-grams hit = 1366  (98.34%)
Number of 4-grams hit = 15  (1.08%)
Number of 3-grams hit = 5  (0.36%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle953.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 2294 words.
Number of 5-grams hit = 2238  (97.56%)
Number of 4-grams hit = 37  (1.61%)
Number of 3-grams hit = 15  (0.65%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle954.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 800 words.
Number of 5-grams hit = 783  (97.88%)
Number of 4-grams hit = 9  (1.12%)
Number of 3-grams hit = 4  (0.50%)
Number of 2-grams hit = 3  (0.38%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle955.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1574 words.
Number of 5-grams hit = 1532  (97.33%)
Number of 4-grams hit = 25  (1.59%)
Number of 3-grams hit = 15  (0.95%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle956.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1107 words.
Number of 5-grams hit = 1070  (96.66%)
Number of 4-grams hit = 29  (2.62%)
Number of 3-grams hit = 6  (0.54%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle957.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 323 words.
Number of 5-grams hit = 319  (98.76%)
Number of 4-grams hit = 1  (0.31%)
Number of 3-grams hit = 1  (0.31%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle958.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1014 words.
Number of 5-grams hit = 987  (97.34%)
Number of 4-grams hit = 13  (1.28%)
Number of 3-grams hit = 11  (1.08%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle959.out
Perplexity = 4.35, Entropy = 2.12 bits
Computation based on 218 words.
Number of 5-grams hit = 204  (93.58%)
Number of 4-grams hit = 9  (4.13%)
Number of 3-grams hit = 3  (1.38%)
Number of 2-grams hit = 1  (0.46%)
Number of 1-grams hit = 1  (0.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle960.out
Perplexity = 2.79, Entropy = 1.48 bits
Computation based on 652 words.
Number of 5-grams hit = 648  (99.39%)
Number of 4-grams hit = 1  (0.15%)
Number of 3-grams hit = 1  (0.15%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle961.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 534 words.
Number of 5-grams hit = 511  (95.69%)
Number of 4-grams hit = 13  (2.43%)
Number of 3-grams hit = 7  (1.31%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle962.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 2369 words.
Number of 5-grams hit = 2331  (98.40%)
Number of 4-grams hit = 26  (1.10%)
Number of 3-grams hit = 8  (0.34%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle963.out
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 2187 words.
Number of 5-grams hit = 2122  (97.03%)
Number of 4-grams hit = 43  (1.97%)
Number of 3-grams hit = 17  (0.78%)
Number of 2-grams hit = 4  (0.18%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle964.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 1090 words.
Number of 5-grams hit = 1049  (96.24%)
Number of 4-grams hit = 31  (2.84%)
Number of 3-grams hit = 7  (0.64%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle965.out
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 701 words.
Number of 5-grams hit = 675  (96.29%)
Number of 4-grams hit = 17  (2.43%)
Number of 3-grams hit = 7  (1.00%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle966.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 850 words.
Number of 5-grams hit = 814  (95.76%)
Number of 4-grams hit = 27  (3.18%)
Number of 3-grams hit = 7  (0.82%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle967.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 426 words.
Number of 5-grams hit = 415  (97.42%)
Number of 4-grams hit = 7  (1.64%)
Number of 3-grams hit = 2  (0.47%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle968.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1147 words.
Number of 5-grams hit = 1130  (98.52%)
Number of 4-grams hit = 11  (0.96%)
Number of 3-grams hit = 4  (0.35%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle969.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 773 words.
Number of 5-grams hit = 748  (96.77%)
Number of 4-grams hit = 15  (1.94%)
Number of 3-grams hit = 7  (0.91%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle970.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 798 words.
Number of 5-grams hit = 774  (96.99%)
Number of 4-grams hit = 16  (2.01%)
Number of 3-grams hit = 5  (0.63%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle971.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 504 words.
Number of 5-grams hit = 495  (98.21%)
Number of 4-grams hit = 5  (0.99%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle972.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 2041 words.
Number of 5-grams hit = 1966  (96.33%)
Number of 4-grams hit = 52  (2.55%)
Number of 3-grams hit = 16  (0.78%)
Number of 2-grams hit = 6  (0.29%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle973.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 663 words.
Number of 5-grams hit = 655  (98.79%)
Number of 4-grams hit = 4  (0.60%)
Number of 3-grams hit = 2  (0.30%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle974.out
Perplexity = 2.80, Entropy = 1.48 bits
Computation based on 1022 words.
Number of 5-grams hit = 1000  (97.85%)
Number of 4-grams hit = 14  (1.37%)
Number of 3-grams hit = 6  (0.59%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle975.out
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 380 words.
Number of 5-grams hit = 364  (95.79%)
Number of 4-grams hit = 9  (2.37%)
Number of 3-grams hit = 5  (1.32%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle976.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 385 words.
Number of 5-grams hit = 370  (96.10%)
Number of 4-grams hit = 8  (2.08%)
Number of 3-grams hit = 5  (1.30%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle977.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 917 words.
Number of 5-grams hit = 879  (95.86%)
Number of 4-grams hit = 23  (2.51%)
Number of 3-grams hit = 12  (1.31%)
Number of 2-grams hit = 2  (0.22%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle978.out
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 503 words.
Number of 5-grams hit = 487  (96.82%)
Number of 4-grams hit = 12  (2.39%)
Number of 3-grams hit = 2  (0.40%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle979.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1716 words.
Number of 5-grams hit = 1677  (97.73%)
Number of 4-grams hit = 26  (1.52%)
Number of 3-grams hit = 11  (0.64%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle980.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 681 words.
Number of 5-grams hit = 663  (97.36%)
Number of 4-grams hit = 10  (1.47%)
Number of 3-grams hit = 4  (0.59%)
Number of 2-grams hit = 3  (0.44%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle981.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 371 words.
Number of 5-grams hit = 356  (95.96%)
Number of 4-grams hit = 8  (2.16%)
Number of 3-grams hit = 3  (0.81%)
Number of 2-grams hit = 3  (0.81%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle982.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 890 words.
Number of 5-grams hit = 861  (96.74%)
Number of 4-grams hit = 20  (2.25%)
Number of 3-grams hit = 7  (0.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle983.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 520 words.
Number of 5-grams hit = 498  (95.77%)
Number of 4-grams hit = 15  (2.88%)
Number of 3-grams hit = 5  (0.96%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle984.out
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 479 words.
Number of 5-grams hit = 467  (97.49%)
Number of 4-grams hit = 7  (1.46%)
Number of 3-grams hit = 3  (0.63%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle985.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 1351 words.
Number of 5-grams hit = 1328  (98.30%)
Number of 4-grams hit = 13  (0.96%)
Number of 3-grams hit = 7  (0.52%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle986.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 297 words.
Number of 5-grams hit = 285  (95.96%)
Number of 4-grams hit = 8  (2.69%)
Number of 3-grams hit = 2  (0.67%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle987.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1891 words.
Number of 5-grams hit = 1835  (97.04%)
Number of 4-grams hit = 37  (1.96%)
Number of 3-grams hit = 15  (0.79%)
Number of 2-grams hit = 3  (0.16%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle988.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 1353 words.
Number of 5-grams hit = 1321  (97.63%)
Number of 4-grams hit = 22  (1.63%)
Number of 3-grams hit = 6  (0.44%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle989.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 2476 words.
Number of 5-grams hit = 2428  (98.06%)
Number of 4-grams hit = 32  (1.29%)
Number of 3-grams hit = 13  (0.53%)
Number of 2-grams hit = 2  (0.08%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle990.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 710 words.
Number of 5-grams hit = 693  (97.61%)
Number of 4-grams hit = 10  (1.41%)
Number of 3-grams hit = 4  (0.56%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle991.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1688 words.
Number of 5-grams hit = 1638  (97.04%)
Number of 4-grams hit = 30  (1.78%)
Number of 3-grams hit = 17  (1.01%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle992.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 578 words.
Number of 5-grams hit = 555  (96.02%)
Number of 4-grams hit = 16  (2.77%)
Number of 3-grams hit = 5  (0.87%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle993.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 2666 words.
Number of 5-grams hit = 2592  (97.22%)
Number of 4-grams hit = 51  (1.91%)
Number of 3-grams hit = 19  (0.71%)
Number of 2-grams hit = 3  (0.11%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle994.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 527 words.
Number of 5-grams hit = 508  (96.39%)
Number of 4-grams hit = 14  (2.66%)
Number of 3-grams hit = 3  (0.57%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle995.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 7968 words.
Number of 5-grams hit = 7770  (97.52%)
Number of 4-grams hit = 146  (1.83%)
Number of 3-grams hit = 45  (0.56%)
Number of 2-grams hit = 6  (0.08%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle996.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 627 words.
Number of 5-grams hit = 610  (97.29%)
Number of 4-grams hit = 11  (1.75%)
Number of 3-grams hit = 4  (0.64%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle997.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 5785 words.
Number of 5-grams hit = 5649  (97.65%)
Number of 4-grams hit = 91  (1.57%)
Number of 3-grams hit = 35  (0.61%)
Number of 2-grams hit = 9  (0.16%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle998.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 572 words.
Number of 5-grams hit = 547  (95.63%)
Number of 4-grams hit = 16  (2.80%)
Number of 3-grams hit = 7  (1.22%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle999.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 523 words.
Number of 5-grams hit = 508  (97.13%)
Number of 4-grams hit = 8  (1.53%)
Number of 3-grams hit = 4  (0.76%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : 