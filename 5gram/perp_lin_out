evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article0.text
Perplexity = 141.81, Entropy = 7.15 bits
Computation based on 1255 words.
Number of 5-grams hit = 263  (20.96%)
Number of 4-grams hit = 324  (25.82%)
Number of 3-grams hit = 401  (31.95%)
Number of 2-grams hit = 220  (17.53%)
Number of 1-grams hit = 47  (3.75%)
7 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article1.text
Perplexity = 101.57, Entropy = 6.67 bits
Computation based on 1499 words.
Number of 5-grams hit = 481  (32.09%)
Number of 4-grams hit = 356  (23.75%)
Number of 3-grams hit = 367  (24.48%)
Number of 2-grams hit = 249  (16.61%)
Number of 1-grams hit = 46  (3.07%)
6 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article2.text
Perplexity = 151.77, Entropy = 7.25 bits
Computation based on 540 words.
Number of 5-grams hit = 112  (20.74%)
Number of 4-grams hit = 145  (26.85%)
Number of 3-grams hit = 163  (30.19%)
Number of 2-grams hit = 104  (19.26%)
Number of 1-grams hit = 16  (2.96%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article3.text
Perplexity = 63.80, Entropy = 6.00 bits
Computation based on 620 words.
Number of 5-grams hit = 221  (35.65%)
Number of 4-grams hit = 135  (21.77%)
Number of 3-grams hit = 155  (25.00%)
Number of 2-grams hit = 98  (15.81%)
Number of 1-grams hit = 11  (1.77%)
5 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article4.text
Perplexity = 237.79, Entropy = 7.89 bits
Computation based on 395 words.
Number of 5-grams hit = 55  (13.92%)
Number of 4-grams hit = 90  (22.78%)
Number of 3-grams hit = 134  (33.92%)
Number of 2-grams hit = 91  (23.04%)
Number of 1-grams hit = 25  (6.33%)
3 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article5.text
Perplexity = 178.80, Entropy = 7.48 bits
Computation based on 888 words.
Number of 5-grams hit = 189  (21.28%)
Number of 4-grams hit = 226  (25.45%)
Number of 3-grams hit = 266  (29.95%)
Number of 2-grams hit = 161  (18.13%)
Number of 1-grams hit = 46  (5.18%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article6.text
Perplexity = 405.23, Entropy = 8.66 bits
Computation based on 281 words.
Number of 5-grams hit = 40  (14.23%)
Number of 4-grams hit = 45  (16.01%)
Number of 3-grams hit = 91  (32.38%)
Number of 2-grams hit = 75  (26.69%)
Number of 1-grams hit = 30  (10.68%)
6 OOVs (2.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article7.text
Perplexity = 21.16, Entropy = 4.40 bits
Computation based on 605 words.
Number of 5-grams hit = 446  (73.72%)
Number of 4-grams hit = 44  (7.27%)
Number of 3-grams hit = 48  (7.93%)
Number of 2-grams hit = 53  (8.76%)
Number of 1-grams hit = 14  (2.31%)
6 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article8.text
Perplexity = 233.42, Entropy = 7.87 bits
Computation based on 496 words.
Number of 5-grams hit = 110  (22.18%)
Number of 4-grams hit = 90  (18.15%)
Number of 3-grams hit = 133  (26.81%)
Number of 2-grams hit = 126  (25.40%)
Number of 1-grams hit = 37  (7.46%)
8 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article9.text
Perplexity = 154.12, Entropy = 7.27 bits
Computation based on 314 words.
Number of 5-grams hit = 64  (20.38%)
Number of 4-grams hit = 82  (26.11%)
Number of 3-grams hit = 101  (32.17%)
Number of 2-grams hit = 54  (17.20%)
Number of 1-grams hit = 13  (4.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article10.text
Perplexity = 179.41, Entropy = 7.49 bits
Computation based on 304 words.
Number of 5-grams hit = 60  (19.74%)
Number of 4-grams hit = 75  (24.67%)
Number of 3-grams hit = 90  (29.61%)
Number of 2-grams hit = 61  (20.07%)
Number of 1-grams hit = 18  (5.92%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article11.text
Perplexity = 171.41, Entropy = 7.42 bits
Computation based on 302 words.
Number of 5-grams hit = 56  (18.54%)
Number of 4-grams hit = 65  (21.52%)
Number of 3-grams hit = 94  (31.13%)
Number of 2-grams hit = 70  (23.18%)
Number of 1-grams hit = 17  (5.63%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article12.text
Perplexity = 108.76, Entropy = 6.76 bits
Computation based on 300 words.
Number of 5-grams hit = 85  (28.33%)
Number of 4-grams hit = 77  (25.67%)
Number of 3-grams hit = 79  (26.33%)
Number of 2-grams hit = 51  (17.00%)
Number of 1-grams hit = 8  (2.67%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article13.text
Perplexity = 185.60, Entropy = 7.54 bits
Computation based on 453 words.
Number of 5-grams hit = 96  (21.19%)
Number of 4-grams hit = 101  (22.30%)
Number of 3-grams hit = 134  (29.58%)
Number of 2-grams hit = 92  (20.31%)
Number of 1-grams hit = 30  (6.62%)
11 OOVs (2.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article14.text
Perplexity = 150.08, Entropy = 7.23 bits
Computation based on 468 words.
Number of 5-grams hit = 132  (28.21%)
Number of 4-grams hit = 97  (20.73%)
Number of 3-grams hit = 120  (25.64%)
Number of 2-grams hit = 88  (18.80%)
Number of 1-grams hit = 31  (6.62%)
7 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article15.text
Perplexity = 164.24, Entropy = 7.36 bits
Computation based on 493 words.
Number of 5-grams hit = 89  (18.05%)
Number of 4-grams hit = 131  (26.57%)
Number of 3-grams hit = 159  (32.25%)
Number of 2-grams hit = 99  (20.08%)
Number of 1-grams hit = 15  (3.04%)
4 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article16.text
Perplexity = 275.88, Entropy = 8.11 bits
Computation based on 365 words.
Number of 5-grams hit = 68  (18.63%)
Number of 4-grams hit = 82  (22.47%)
Number of 3-grams hit = 102  (27.95%)
Number of 2-grams hit = 91  (24.93%)
Number of 1-grams hit = 22  (6.03%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article17.text
Perplexity = 215.75, Entropy = 7.75 bits
Computation based on 673 words.
Number of 5-grams hit = 132  (19.61%)
Number of 4-grams hit = 164  (24.37%)
Number of 3-grams hit = 192  (28.53%)
Number of 2-grams hit = 140  (20.80%)
Number of 1-grams hit = 45  (6.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article18.text
Perplexity = 157.11, Entropy = 7.30 bits
Computation based on 406 words.
Number of 5-grams hit = 87  (21.43%)
Number of 4-grams hit = 102  (25.12%)
Number of 3-grams hit = 128  (31.53%)
Number of 2-grams hit = 75  (18.47%)
Number of 1-grams hit = 14  (3.45%)
6 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article19.text
Perplexity = 127.78, Entropy = 7.00 bits
Computation based on 525 words.
Number of 5-grams hit = 128  (24.38%)
Number of 4-grams hit = 118  (22.48%)
Number of 3-grams hit = 141  (26.86%)
Number of 2-grams hit = 113  (21.52%)
Number of 1-grams hit = 25  (4.76%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article20.text
Perplexity = 186.54, Entropy = 7.54 bits
Computation based on 348 words.
Number of 5-grams hit = 81  (23.28%)
Number of 4-grams hit = 82  (23.56%)
Number of 3-grams hit = 95  (27.30%)
Number of 2-grams hit = 67  (19.25%)
Number of 1-grams hit = 23  (6.61%)
4 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article21.text
Perplexity = 189.96, Entropy = 7.57 bits
Computation based on 461 words.
Number of 5-grams hit = 98  (21.26%)
Number of 4-grams hit = 84  (18.22%)
Number of 3-grams hit = 138  (29.93%)
Number of 2-grams hit = 106  (22.99%)
Number of 1-grams hit = 35  (7.59%)
4 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article22.text
Perplexity = 177.28, Entropy = 7.47 bits
Computation based on 360 words.
Number of 5-grams hit = 77  (21.39%)
Number of 4-grams hit = 74  (20.56%)
Number of 3-grams hit = 109  (30.28%)
Number of 2-grams hit = 80  (22.22%)
Number of 1-grams hit = 20  (5.56%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article23.text
Perplexity = 212.63, Entropy = 7.73 bits
Computation based on 345 words.
Number of 5-grams hit = 60  (17.39%)
Number of 4-grams hit = 72  (20.87%)
Number of 3-grams hit = 108  (31.30%)
Number of 2-grams hit = 85  (24.64%)
Number of 1-grams hit = 20  (5.80%)
12 OOVs (3.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article24.text
Perplexity = 175.49, Entropy = 7.46 bits
Computation based on 561 words.
Number of 5-grams hit = 126  (22.46%)
Number of 4-grams hit = 137  (24.42%)
Number of 3-grams hit = 167  (29.77%)
Number of 2-grams hit = 100  (17.83%)
Number of 1-grams hit = 31  (5.53%)
5 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article25.text
Perplexity = 228.87, Entropy = 7.84 bits
Computation based on 373 words.
Number of 5-grams hit = 70  (18.77%)
Number of 4-grams hit = 75  (20.11%)
Number of 3-grams hit = 127  (34.05%)
Number of 2-grams hit = 76  (20.38%)
Number of 1-grams hit = 25  (6.70%)
7 OOVs (1.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article26.text
Perplexity = 199.39, Entropy = 7.64 bits
Computation based on 1519 words.
Number of 5-grams hit = 281  (18.50%)
Number of 4-grams hit = 342  (22.51%)
Number of 3-grams hit = 448  (29.49%)
Number of 2-grams hit = 366  (24.09%)
Number of 1-grams hit = 82  (5.40%)
13 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article27.text
Perplexity = 283.60, Entropy = 8.15 bits
Computation based on 389 words.
Number of 5-grams hit = 80  (20.57%)
Number of 4-grams hit = 76  (19.54%)
Number of 3-grams hit = 103  (26.48%)
Number of 2-grams hit = 95  (24.42%)
Number of 1-grams hit = 35  (9.00%)
4 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article28.text
Perplexity = 177.84, Entropy = 7.47 bits
Computation based on 1388 words.
Number of 5-grams hit = 312  (22.48%)
Number of 4-grams hit = 355  (25.58%)
Number of 3-grams hit = 394  (28.39%)
Number of 2-grams hit = 256  (18.44%)
Number of 1-grams hit = 71  (5.12%)
11 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article29.text
Perplexity = 239.95, Entropy = 7.91 bits
Computation based on 317 words.
Number of 5-grams hit = 54  (17.03%)
Number of 4-grams hit = 70  (22.08%)
Number of 3-grams hit = 106  (33.44%)
Number of 2-grams hit = 70  (22.08%)
Number of 1-grams hit = 17  (5.36%)
2 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article30.text
Perplexity = 162.52, Entropy = 7.34 bits
Computation based on 475 words.
Number of 5-grams hit = 126  (26.53%)
Number of 4-grams hit = 113  (23.79%)
Number of 3-grams hit = 131  (27.58%)
Number of 2-grams hit = 86  (18.11%)
Number of 1-grams hit = 19  (4.00%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article31.text
Perplexity = 212.26, Entropy = 7.73 bits
Computation based on 439 words.
Number of 5-grams hit = 84  (19.13%)
Number of 4-grams hit = 101  (23.01%)
Number of 3-grams hit = 143  (32.57%)
Number of 2-grams hit = 86  (19.59%)
Number of 1-grams hit = 25  (5.69%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article32.text
Perplexity = 159.30, Entropy = 7.32 bits
Computation based on 475 words.
Number of 5-grams hit = 114  (24.00%)
Number of 4-grams hit = 102  (21.47%)
Number of 3-grams hit = 132  (27.79%)
Number of 2-grams hit = 94  (19.79%)
Number of 1-grams hit = 33  (6.95%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article33.text
Perplexity = 239.09, Entropy = 7.90 bits
Computation based on 441 words.
Number of 5-grams hit = 82  (18.59%)
Number of 4-grams hit = 92  (20.86%)
Number of 3-grams hit = 140  (31.75%)
Number of 2-grams hit = 96  (21.77%)
Number of 1-grams hit = 31  (7.03%)
3 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article34.text
Perplexity = 172.09, Entropy = 7.43 bits
Computation based on 304 words.
Number of 5-grams hit = 50  (16.45%)
Number of 4-grams hit = 86  (28.29%)
Number of 3-grams hit = 104  (34.21%)
Number of 2-grams hit = 54  (17.76%)
Number of 1-grams hit = 10  (3.29%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article35.text
Perplexity = 139.98, Entropy = 7.13 bits
Computation based on 378 words.
Number of 5-grams hit = 80  (21.16%)
Number of 4-grams hit = 90  (23.81%)
Number of 3-grams hit = 117  (30.95%)
Number of 2-grams hit = 77  (20.37%)
Number of 1-grams hit = 14  (3.70%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article36.text
Perplexity = 141.69, Entropy = 7.15 bits
Computation based on 1000 words.
Number of 5-grams hit = 258  (25.80%)
Number of 4-grams hit = 254  (25.40%)
Number of 3-grams hit = 262  (26.20%)
Number of 2-grams hit = 186  (18.60%)
Number of 1-grams hit = 40  (4.00%)
11 OOVs (1.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article37.text
Perplexity = 146.49, Entropy = 7.19 bits
Computation based on 559 words.
Number of 5-grams hit = 140  (25.04%)
Number of 4-grams hit = 112  (20.04%)
Number of 3-grams hit = 159  (28.44%)
Number of 2-grams hit = 121  (21.65%)
Number of 1-grams hit = 27  (4.83%)
10 OOVs (1.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article38.text
Perplexity = 183.46, Entropy = 7.52 bits
Computation based on 408 words.
Number of 5-grams hit = 89  (21.81%)
Number of 4-grams hit = 83  (20.34%)
Number of 3-grams hit = 110  (26.96%)
Number of 2-grams hit = 105  (25.74%)
Number of 1-grams hit = 21  (5.15%)
4 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article39.text
Perplexity = 121.17, Entropy = 6.92 bits
Computation based on 2623 words.
Number of 5-grams hit = 741  (28.25%)
Number of 4-grams hit = 617  (23.52%)
Number of 3-grams hit = 737  (28.10%)
Number of 2-grams hit = 442  (16.85%)
Number of 1-grams hit = 86  (3.28%)
37 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article40.text
Perplexity = 61.91, Entropy = 5.95 bits
Computation based on 405 words.
Number of 5-grams hit = 158  (39.01%)
Number of 4-grams hit = 95  (23.46%)
Number of 3-grams hit = 84  (20.74%)
Number of 2-grams hit = 61  (15.06%)
Number of 1-grams hit = 7  (1.73%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article41.text
Perplexity = 139.26, Entropy = 7.12 bits
Computation based on 969 words.
Number of 5-grams hit = 256  (26.42%)
Number of 4-grams hit = 237  (24.46%)
Number of 3-grams hit = 283  (29.21%)
Number of 2-grams hit = 160  (16.51%)
Number of 1-grams hit = 33  (3.41%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article42.text
Perplexity = 180.89, Entropy = 7.50 bits
Computation based on 547 words.
Number of 5-grams hit = 109  (19.93%)
Number of 4-grams hit = 136  (24.86%)
Number of 3-grams hit = 176  (32.18%)
Number of 2-grams hit = 104  (19.01%)
Number of 1-grams hit = 22  (4.02%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article43.text
Perplexity = 121.88, Entropy = 6.93 bits
Computation based on 449 words.
Number of 5-grams hit = 130  (28.95%)
Number of 4-grams hit = 100  (22.27%)
Number of 3-grams hit = 123  (27.39%)
Number of 2-grams hit = 75  (16.70%)
Number of 1-grams hit = 21  (4.68%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article44.text
Perplexity = 141.80, Entropy = 7.15 bits
Computation based on 492 words.
Number of 5-grams hit = 114  (23.17%)
Number of 4-grams hit = 126  (25.61%)
Number of 3-grams hit = 136  (27.64%)
Number of 2-grams hit = 91  (18.50%)
Number of 1-grams hit = 25  (5.08%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article45.text
Perplexity = 229.41, Entropy = 7.84 bits
Computation based on 464 words.
Number of 5-grams hit = 85  (18.32%)
Number of 4-grams hit = 111  (23.92%)
Number of 3-grams hit = 133  (28.66%)
Number of 2-grams hit = 111  (23.92%)
Number of 1-grams hit = 24  (5.17%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article46.text
Perplexity = 212.16, Entropy = 7.73 bits
Computation based on 401 words.
Number of 5-grams hit = 85  (21.20%)
Number of 4-grams hit = 78  (19.45%)
Number of 3-grams hit = 105  (26.18%)
Number of 2-grams hit = 107  (26.68%)
Number of 1-grams hit = 26  (6.48%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article47.text
Perplexity = 157.23, Entropy = 7.30 bits
Computation based on 2678 words.
Number of 5-grams hit = 569  (21.25%)
Number of 4-grams hit = 693  (25.88%)
Number of 3-grams hit = 822  (30.69%)
Number of 2-grams hit = 493  (18.41%)
Number of 1-grams hit = 101  (3.77%)
6 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article48.text
Perplexity = 267.77, Entropy = 8.06 bits
Computation based on 366 words.
Number of 5-grams hit = 45  (12.30%)
Number of 4-grams hit = 79  (21.58%)
Number of 3-grams hit = 128  (34.97%)
Number of 2-grams hit = 97  (26.50%)
Number of 1-grams hit = 17  (4.64%)
7 OOVs (1.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article49.text
Perplexity = 137.86, Entropy = 7.11 bits
Computation based on 364 words.
Number of 5-grams hit = 85  (23.35%)
Number of 4-grams hit = 94  (25.82%)
Number of 3-grams hit = 107  (29.40%)
Number of 2-grams hit = 68  (18.68%)
Number of 1-grams hit = 10  (2.75%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article50.text
Perplexity = 115.18, Entropy = 6.85 bits
Computation based on 522 words.
Number of 5-grams hit = 148  (28.35%)
Number of 4-grams hit = 126  (24.14%)
Number of 3-grams hit = 137  (26.25%)
Number of 2-grams hit = 89  (17.05%)
Number of 1-grams hit = 22  (4.21%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article51.text
Perplexity = 235.95, Entropy = 7.88 bits
Computation based on 1102 words.
Number of 5-grams hit = 202  (18.33%)
Number of 4-grams hit = 237  (21.51%)
Number of 3-grams hit = 306  (27.77%)
Number of 2-grams hit = 288  (26.13%)
Number of 1-grams hit = 69  (6.26%)
19 OOVs (1.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article52.text
Perplexity = 120.26, Entropy = 6.91 bits
Computation based on 360 words.
Number of 5-grams hit = 76  (21.11%)
Number of 4-grams hit = 99  (27.50%)
Number of 3-grams hit = 119  (33.06%)
Number of 2-grams hit = 59  (16.39%)
Number of 1-grams hit = 7  (1.94%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article53.text
Perplexity = 189.30, Entropy = 7.56 bits
Computation based on 472 words.
Number of 5-grams hit = 120  (25.42%)
Number of 4-grams hit = 93  (19.70%)
Number of 3-grams hit = 124  (26.27%)
Number of 2-grams hit = 101  (21.40%)
Number of 1-grams hit = 34  (7.20%)
9 OOVs (1.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article54.text
Perplexity = 202.26, Entropy = 7.66 bits
Computation based on 1063 words.
Number of 5-grams hit = 224  (21.07%)
Number of 4-grams hit = 230  (21.64%)
Number of 3-grams hit = 295  (27.75%)
Number of 2-grams hit = 237  (22.30%)
Number of 1-grams hit = 77  (7.24%)
21 OOVs (1.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article55.text
Perplexity = 123.60, Entropy = 6.95 bits
Computation based on 533 words.
Number of 5-grams hit = 159  (29.83%)
Number of 4-grams hit = 144  (27.02%)
Number of 3-grams hit = 146  (27.39%)
Number of 2-grams hit = 61  (11.44%)
Number of 1-grams hit = 23  (4.32%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article56.text
Perplexity = 186.58, Entropy = 7.54 bits
Computation based on 898 words.
Number of 5-grams hit = 228  (25.39%)
Number of 4-grams hit = 184  (20.49%)
Number of 3-grams hit = 249  (27.73%)
Number of 2-grams hit = 179  (19.93%)
Number of 1-grams hit = 58  (6.46%)
7 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article57.text
Perplexity = 100.75, Entropy = 6.65 bits
Computation based on 1537 words.
Number of 5-grams hit = 469  (30.51%)
Number of 4-grams hit = 391  (25.44%)
Number of 3-grams hit = 415  (27.00%)
Number of 2-grams hit = 223  (14.51%)
Number of 1-grams hit = 39  (2.54%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article58.text
Perplexity = 169.05, Entropy = 7.40 bits
Computation based on 383 words.
Number of 5-grams hit = 99  (25.85%)
Number of 4-grams hit = 88  (22.98%)
Number of 3-grams hit = 110  (28.72%)
Number of 2-grams hit = 63  (16.45%)
Number of 1-grams hit = 23  (6.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article59.text
Perplexity = 99.71, Entropy = 6.64 bits
Computation based on 208 words.
Number of 5-grams hit = 59  (28.37%)
Number of 4-grams hit = 57  (27.40%)
Number of 3-grams hit = 58  (27.88%)
Number of 2-grams hit = 28  (13.46%)
Number of 1-grams hit = 6  (2.88%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article60.text
Perplexity = 129.18, Entropy = 7.01 bits
Computation based on 321 words.
Number of 5-grams hit = 70  (21.81%)
Number of 4-grams hit = 69  (21.50%)
Number of 3-grams hit = 105  (32.71%)
Number of 2-grams hit = 65  (20.25%)
Number of 1-grams hit = 12  (3.74%)
13 OOVs (3.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article61.text
Perplexity = 177.01, Entropy = 7.47 bits
Computation based on 489 words.
Number of 5-grams hit = 83  (16.97%)
Number of 4-grams hit = 139  (28.43%)
Number of 3-grams hit = 143  (29.24%)
Number of 2-grams hit = 104  (21.27%)
Number of 1-grams hit = 20  (4.09%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article62.text
Perplexity = 146.35, Entropy = 7.19 bits
Computation based on 1282 words.
Number of 5-grams hit = 351  (27.38%)
Number of 4-grams hit = 250  (19.50%)
Number of 3-grams hit = 322  (25.12%)
Number of 2-grams hit = 287  (22.39%)
Number of 1-grams hit = 72  (5.62%)
11 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article63.text
Perplexity = 166.27, Entropy = 7.38 bits
Computation based on 965 words.
Number of 5-grams hit = 215  (22.28%)
Number of 4-grams hit = 233  (24.15%)
Number of 3-grams hit = 265  (27.46%)
Number of 2-grams hit = 201  (20.83%)
Number of 1-grams hit = 51  (5.28%)
25 OOVs (2.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article64.text
Perplexity = 125.19, Entropy = 6.97 bits
Computation based on 4162 words.
Number of 5-grams hit = 1187  (28.52%)
Number of 4-grams hit = 906  (21.77%)
Number of 3-grams hit = 1091  (26.21%)
Number of 2-grams hit = 800  (19.22%)
Number of 1-grams hit = 178  (4.28%)
54 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article65.text
Perplexity = 151.33, Entropy = 7.24 bits
Computation based on 628 words.
Number of 5-grams hit = 182  (28.98%)
Number of 4-grams hit = 140  (22.29%)
Number of 3-grams hit = 162  (25.80%)
Number of 2-grams hit = 111  (17.68%)
Number of 1-grams hit = 33  (5.25%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article66.text
Perplexity = 105.30, Entropy = 6.72 bits
Computation based on 472 words.
Number of 5-grams hit = 144  (30.51%)
Number of 4-grams hit = 112  (23.73%)
Number of 3-grams hit = 124  (26.27%)
Number of 2-grams hit = 80  (16.95%)
Number of 1-grams hit = 12  (2.54%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article67.text
Perplexity = 192.93, Entropy = 7.59 bits
Computation based on 449 words.
Number of 5-grams hit = 91  (20.27%)
Number of 4-grams hit = 83  (18.49%)
Number of 3-grams hit = 131  (29.18%)
Number of 2-grams hit = 117  (26.06%)
Number of 1-grams hit = 27  (6.01%)
5 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article68.text
Perplexity = 133.34, Entropy = 7.06 bits
Computation based on 1109 words.
Number of 5-grams hit = 304  (27.41%)
Number of 4-grams hit = 258  (23.26%)
Number of 3-grams hit = 308  (27.77%)
Number of 2-grams hit = 198  (17.85%)
Number of 1-grams hit = 41  (3.70%)
11 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article69.text
Perplexity = 26.15, Entropy = 4.71 bits
Computation based on 380 words.
Number of 5-grams hit = 241  (63.42%)
Number of 4-grams hit = 34  (8.95%)
Number of 3-grams hit = 38  (10.00%)
Number of 2-grams hit = 53  (13.95%)
Number of 1-grams hit = 14  (3.68%)
3 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article70.text
Perplexity = 97.88, Entropy = 6.61 bits
Computation based on 959 words.
Number of 5-grams hit = 286  (29.82%)
Number of 4-grams hit = 241  (25.13%)
Number of 3-grams hit = 251  (26.17%)
Number of 2-grams hit = 165  (17.21%)
Number of 1-grams hit = 16  (1.67%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article71.text
Perplexity = 168.17, Entropy = 7.39 bits
Computation based on 550 words.
Number of 5-grams hit = 106  (19.27%)
Number of 4-grams hit = 136  (24.73%)
Number of 3-grams hit = 193  (35.09%)
Number of 2-grams hit = 97  (17.64%)
Number of 1-grams hit = 18  (3.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article72.text
Perplexity = 188.91, Entropy = 7.56 bits
Computation based on 1302 words.
Number of 5-grams hit = 257  (19.74%)
Number of 4-grams hit = 336  (25.81%)
Number of 3-grams hit = 402  (30.88%)
Number of 2-grams hit = 245  (18.82%)
Number of 1-grams hit = 62  (4.76%)
6 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article73.text
Perplexity = 119.26, Entropy = 6.90 bits
Computation based on 509 words.
Number of 5-grams hit = 161  (31.63%)
Number of 4-grams hit = 115  (22.59%)
Number of 3-grams hit = 137  (26.92%)
Number of 2-grams hit = 79  (15.52%)
Number of 1-grams hit = 17  (3.34%)
4 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article74.text
Perplexity = 200.09, Entropy = 7.64 bits
Computation based on 818 words.
Number of 5-grams hit = 148  (18.09%)
Number of 4-grams hit = 192  (23.47%)
Number of 3-grams hit = 271  (33.13%)
Number of 2-grams hit = 167  (20.42%)
Number of 1-grams hit = 40  (4.89%)
5 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article75.text
Perplexity = 91.10, Entropy = 6.51 bits
Computation based on 1190 words.
Number of 5-grams hit = 450  (37.82%)
Number of 4-grams hit = 219  (18.40%)
Number of 3-grams hit = 265  (22.27%)
Number of 2-grams hit = 209  (17.56%)
Number of 1-grams hit = 47  (3.95%)
4 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article76.text
Perplexity = 126.88, Entropy = 6.99 bits
Computation based on 1377 words.
Number of 5-grams hit = 338  (24.55%)
Number of 4-grams hit = 362  (26.29%)
Number of 3-grams hit = 420  (30.50%)
Number of 2-grams hit = 217  (15.76%)
Number of 1-grams hit = 40  (2.90%)
5 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article77.text
Perplexity = 213.78, Entropy = 7.74 bits
Computation based on 288 words.
Number of 5-grams hit = 60  (20.83%)
Number of 4-grams hit = 75  (26.04%)
Number of 3-grams hit = 82  (28.47%)
Number of 2-grams hit = 50  (17.36%)
Number of 1-grams hit = 21  (7.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article78.text
Perplexity = 150.82, Entropy = 7.24 bits
Computation based on 1118 words.
Number of 5-grams hit = 249  (22.27%)
Number of 4-grams hit = 296  (26.48%)
Number of 3-grams hit = 337  (30.14%)
Number of 2-grams hit = 193  (17.26%)
Number of 1-grams hit = 43  (3.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article79.text
Perplexity = 223.99, Entropy = 7.81 bits
Computation based on 1065 words.
Number of 5-grams hit = 200  (18.78%)
Number of 4-grams hit = 238  (22.35%)
Number of 3-grams hit = 342  (32.11%)
Number of 2-grams hit = 212  (19.91%)
Number of 1-grams hit = 73  (6.85%)
8 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article80.text
Perplexity = 150.45, Entropy = 7.23 bits
Computation based on 354 words.
Number of 5-grams hit = 93  (26.27%)
Number of 4-grams hit = 69  (19.49%)
Number of 3-grams hit = 91  (25.71%)
Number of 2-grams hit = 83  (23.45%)
Number of 1-grams hit = 18  (5.08%)
16 OOVs (4.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article81.text
Perplexity = 158.26, Entropy = 7.31 bits
Computation based on 4029 words.
Number of 5-grams hit = 913  (22.66%)
Number of 4-grams hit = 1044  (25.91%)
Number of 3-grams hit = 1220  (30.28%)
Number of 2-grams hit = 702  (17.42%)
Number of 1-grams hit = 150  (3.72%)
21 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article82.text
Perplexity = 135.77, Entropy = 7.09 bits
Computation based on 467 words.
Number of 5-grams hit = 104  (22.27%)
Number of 4-grams hit = 114  (24.41%)
Number of 3-grams hit = 138  (29.55%)
Number of 2-grams hit = 97  (20.77%)
Number of 1-grams hit = 14  (3.00%)
4 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article83.text
Perplexity = 144.13, Entropy = 7.17 bits
Computation based on 431 words.
Number of 5-grams hit = 101  (23.43%)
Number of 4-grams hit = 128  (29.70%)
Number of 3-grams hit = 120  (27.84%)
Number of 2-grams hit = 66  (15.31%)
Number of 1-grams hit = 16  (3.71%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article84.text
Perplexity = 202.37, Entropy = 7.66 bits
Computation based on 399 words.
Number of 5-grams hit = 83  (20.80%)
Number of 4-grams hit = 108  (27.07%)
Number of 3-grams hit = 109  (27.32%)
Number of 2-grams hit = 82  (20.55%)
Number of 1-grams hit = 17  (4.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article85.text
Perplexity = 177.60, Entropy = 7.47 bits
Computation based on 1214 words.
Number of 5-grams hit = 250  (20.59%)
Number of 4-grams hit = 317  (26.11%)
Number of 3-grams hit = 363  (29.90%)
Number of 2-grams hit = 229  (18.86%)
Number of 1-grams hit = 55  (4.53%)
5 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article86.text
Perplexity = 178.26, Entropy = 7.48 bits
Computation based on 286 words.
Number of 5-grams hit = 62  (21.68%)
Number of 4-grams hit = 62  (21.68%)
Number of 3-grams hit = 87  (30.42%)
Number of 2-grams hit = 58  (20.28%)
Number of 1-grams hit = 17  (5.94%)
3 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article87.text
Perplexity = 229.45, Entropy = 7.84 bits
Computation based on 622 words.
Number of 5-grams hit = 134  (21.54%)
Number of 4-grams hit = 98  (15.76%)
Number of 3-grams hit = 179  (28.78%)
Number of 2-grams hit = 165  (26.53%)
Number of 1-grams hit = 46  (7.40%)
10 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article88.text
Perplexity = 120.45, Entropy = 6.91 bits
Computation based on 1476 words.
Number of 5-grams hit = 414  (28.05%)
Number of 4-grams hit = 360  (24.39%)
Number of 3-grams hit = 399  (27.03%)
Number of 2-grams hit = 246  (16.67%)
Number of 1-grams hit = 57  (3.86%)
23 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article89.text
Perplexity = 168.57, Entropy = 7.40 bits
Computation based on 390 words.
Number of 5-grams hit = 74  (18.97%)
Number of 4-grams hit = 91  (23.33%)
Number of 3-grams hit = 121  (31.03%)
Number of 2-grams hit = 84  (21.54%)
Number of 1-grams hit = 20  (5.13%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article90.text
Perplexity = 157.88, Entropy = 7.30 bits
Computation based on 506 words.
Number of 5-grams hit = 134  (26.48%)
Number of 4-grams hit = 113  (22.33%)
Number of 3-grams hit = 130  (25.69%)
Number of 2-grams hit = 104  (20.55%)
Number of 1-grams hit = 25  (4.94%)
14 OOVs (2.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article91.text
Perplexity = 162.72, Entropy = 7.35 bits
Computation based on 342 words.
Number of 5-grams hit = 74  (21.64%)
Number of 4-grams hit = 89  (26.02%)
Number of 3-grams hit = 106  (30.99%)
Number of 2-grams hit = 60  (17.54%)
Number of 1-grams hit = 13  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article92.text
Perplexity = 176.60, Entropy = 7.46 bits
Computation based on 1024 words.
Number of 5-grams hit = 238  (23.24%)
Number of 4-grams hit = 246  (24.02%)
Number of 3-grams hit = 324  (31.64%)
Number of 2-grams hit = 173  (16.89%)
Number of 1-grams hit = 43  (4.20%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article93.text
Perplexity = 195.72, Entropy = 7.61 bits
Computation based on 682 words.
Number of 5-grams hit = 128  (18.77%)
Number of 4-grams hit = 182  (26.69%)
Number of 3-grams hit = 206  (30.21%)
Number of 2-grams hit = 129  (18.91%)
Number of 1-grams hit = 37  (5.43%)
4 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article94.text
Perplexity = 148.00, Entropy = 7.21 bits
Computation based on 1286 words.
Number of 5-grams hit = 314  (24.42%)
Number of 4-grams hit = 323  (25.12%)
Number of 3-grams hit = 380  (29.55%)
Number of 2-grams hit = 228  (17.73%)
Number of 1-grams hit = 41  (3.19%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article95.text
Perplexity = 144.40, Entropy = 7.17 bits
Computation based on 499 words.
Number of 5-grams hit = 117  (23.45%)
Number of 4-grams hit = 126  (25.25%)
Number of 3-grams hit = 150  (30.06%)
Number of 2-grams hit = 88  (17.64%)
Number of 1-grams hit = 18  (3.61%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article96.text
Perplexity = 148.59, Entropy = 7.22 bits
Computation based on 397 words.
Number of 5-grams hit = 94  (23.68%)
Number of 4-grams hit = 101  (25.44%)
Number of 3-grams hit = 110  (27.71%)
Number of 2-grams hit = 75  (18.89%)
Number of 1-grams hit = 17  (4.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article97.text
Perplexity = 115.75, Entropy = 6.85 bits
Computation based on 544 words.
Number of 5-grams hit = 157  (28.86%)
Number of 4-grams hit = 136  (25.00%)
Number of 3-grams hit = 144  (26.47%)
Number of 2-grams hit = 90  (16.54%)
Number of 1-grams hit = 17  (3.12%)
9 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article98.text
Perplexity = 148.89, Entropy = 7.22 bits
Computation based on 363 words.
Number of 5-grams hit = 71  (19.56%)
Number of 4-grams hit = 96  (26.45%)
Number of 3-grams hit = 109  (30.03%)
Number of 2-grams hit = 74  (20.39%)
Number of 1-grams hit = 13  (3.58%)
3 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article99.text
Perplexity = 245.68, Entropy = 7.94 bits
Computation based on 431 words.
Number of 5-grams hit = 91  (21.11%)
Number of 4-grams hit = 90  (20.88%)
Number of 3-grams hit = 123  (28.54%)
Number of 2-grams hit = 101  (23.43%)
Number of 1-grams hit = 26  (6.03%)
25 OOVs (5.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article100.text
Perplexity = 186.42, Entropy = 7.54 bits
Computation based on 474 words.
Number of 5-grams hit = 104  (21.94%)
Number of 4-grams hit = 116  (24.47%)
Number of 3-grams hit = 143  (30.17%)
Number of 2-grams hit = 92  (19.41%)
Number of 1-grams hit = 19  (4.01%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article101.text
Perplexity = 144.36, Entropy = 7.17 bits
Computation based on 486 words.
Number of 5-grams hit = 115  (23.66%)
Number of 4-grams hit = 118  (24.28%)
Number of 3-grams hit = 132  (27.16%)
Number of 2-grams hit = 97  (19.96%)
Number of 1-grams hit = 24  (4.94%)
21 OOVs (4.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article102.text
Perplexity = 148.92, Entropy = 7.22 bits
Computation based on 412 words.
Number of 5-grams hit = 86  (20.87%)
Number of 4-grams hit = 99  (24.03%)
Number of 3-grams hit = 117  (28.40%)
Number of 2-grams hit = 88  (21.36%)
Number of 1-grams hit = 22  (5.34%)
3 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article103.text
Perplexity = 284.85, Entropy = 8.15 bits
Computation based on 476 words.
Number of 5-grams hit = 90  (18.91%)
Number of 4-grams hit = 102  (21.43%)
Number of 3-grams hit = 135  (28.36%)
Number of 2-grams hit = 109  (22.90%)
Number of 1-grams hit = 40  (8.40%)
22 OOVs (4.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article104.text
Perplexity = 104.90, Entropy = 6.71 bits
Computation based on 6340 words.
Number of 5-grams hit = 2122  (33.47%)
Number of 4-grams hit = 1509  (23.80%)
Number of 3-grams hit = 1515  (23.90%)
Number of 2-grams hit = 962  (15.17%)
Number of 1-grams hit = 232  (3.66%)
19 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article105.text
Perplexity = 168.62, Entropy = 7.40 bits
Computation based on 446 words.
Number of 5-grams hit = 102  (22.87%)
Number of 4-grams hit = 103  (23.09%)
Number of 3-grams hit = 131  (29.37%)
Number of 2-grams hit = 81  (18.16%)
Number of 1-grams hit = 29  (6.50%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article106.text
Perplexity = 212.42, Entropy = 7.73 bits
Computation based on 794 words.
Number of 5-grams hit = 166  (20.91%)
Number of 4-grams hit = 158  (19.90%)
Number of 3-grams hit = 215  (27.08%)
Number of 2-grams hit = 204  (25.69%)
Number of 1-grams hit = 51  (6.42%)
8 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article107.text
Perplexity = 399.42, Entropy = 8.64 bits
Computation based on 474 words.
Number of 5-grams hit = 89  (18.78%)
Number of 4-grams hit = 76  (16.03%)
Number of 3-grams hit = 123  (25.95%)
Number of 2-grams hit = 131  (27.64%)
Number of 1-grams hit = 55  (11.60%)
11 OOVs (2.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article108.text
Perplexity = 114.11, Entropy = 6.83 bits
Computation based on 528 words.
Number of 5-grams hit = 153  (28.98%)
Number of 4-grams hit = 108  (20.45%)
Number of 3-grams hit = 154  (29.17%)
Number of 2-grams hit = 100  (18.94%)
Number of 1-grams hit = 13  (2.46%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article109.text
Perplexity = 153.43, Entropy = 7.26 bits
Computation based on 1123 words.
Number of 5-grams hit = 246  (21.91%)
Number of 4-grams hit = 274  (24.40%)
Number of 3-grams hit = 341  (30.37%)
Number of 2-grams hit = 213  (18.97%)
Number of 1-grams hit = 49  (4.36%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article110.text
Perplexity = 133.22, Entropy = 7.06 bits
Computation based on 908 words.
Number of 5-grams hit = 252  (27.75%)
Number of 4-grams hit = 216  (23.79%)
Number of 3-grams hit = 221  (24.34%)
Number of 2-grams hit = 188  (20.70%)
Number of 1-grams hit = 31  (3.41%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article111.text
Perplexity = 145.71, Entropy = 7.19 bits
Computation based on 1223 words.
Number of 5-grams hit = 350  (28.62%)
Number of 4-grams hit = 256  (20.93%)
Number of 3-grams hit = 323  (26.41%)
Number of 2-grams hit = 238  (19.46%)
Number of 1-grams hit = 56  (4.58%)
10 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article112.text
Perplexity = 188.75, Entropy = 7.56 bits
Computation based on 328 words.
Number of 5-grams hit = 69  (21.04%)
Number of 4-grams hit = 88  (26.83%)
Number of 3-grams hit = 91  (27.74%)
Number of 2-grams hit = 64  (19.51%)
Number of 1-grams hit = 16  (4.88%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article113.text
Perplexity = 87.54, Entropy = 6.45 bits
Computation based on 5818 words.
Number of 5-grams hit = 2050  (35.24%)
Number of 4-grams hit = 1433  (24.63%)
Number of 3-grams hit = 1430  (24.58%)
Number of 2-grams hit = 780  (13.41%)
Number of 1-grams hit = 125  (2.15%)
13 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article114.text
Perplexity = 165.26, Entropy = 7.37 bits
Computation based on 499 words.
Number of 5-grams hit = 96  (19.24%)
Number of 4-grams hit = 137  (27.45%)
Number of 3-grams hit = 150  (30.06%)
Number of 2-grams hit = 100  (20.04%)
Number of 1-grams hit = 16  (3.21%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article115.text
Perplexity = 167.70, Entropy = 7.39 bits
Computation based on 641 words.
Number of 5-grams hit = 158  (24.65%)
Number of 4-grams hit = 140  (21.84%)
Number of 3-grams hit = 195  (30.42%)
Number of 2-grams hit = 119  (18.56%)
Number of 1-grams hit = 29  (4.52%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article116.text
Perplexity = 120.30, Entropy = 6.91 bits
Computation based on 1526 words.
Number of 5-grams hit = 443  (29.03%)
Number of 4-grams hit = 336  (22.02%)
Number of 3-grams hit = 430  (28.18%)
Number of 2-grams hit = 273  (17.89%)
Number of 1-grams hit = 44  (2.88%)
13 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article117.text
Perplexity = 180.83, Entropy = 7.50 bits
Computation based on 517 words.
Number of 5-grams hit = 124  (23.98%)
Number of 4-grams hit = 113  (21.86%)
Number of 3-grams hit = 150  (29.01%)
Number of 2-grams hit = 106  (20.50%)
Number of 1-grams hit = 24  (4.64%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article118.text
Perplexity = 145.04, Entropy = 7.18 bits
Computation based on 610 words.
Number of 5-grams hit = 137  (22.46%)
Number of 4-grams hit = 165  (27.05%)
Number of 3-grams hit = 180  (29.51%)
Number of 2-grams hit = 103  (16.89%)
Number of 1-grams hit = 25  (4.10%)
5 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article119.text
Perplexity = 154.82, Entropy = 7.27 bits
Computation based on 390 words.
Number of 5-grams hit = 64  (16.41%)
Number of 4-grams hit = 104  (26.67%)
Number of 3-grams hit = 136  (34.87%)
Number of 2-grams hit = 71  (18.21%)
Number of 1-grams hit = 15  (3.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article120.text
Perplexity = 168.19, Entropy = 7.39 bits
Computation based on 557 words.
Number of 5-grams hit = 129  (23.16%)
Number of 4-grams hit = 134  (24.06%)
Number of 3-grams hit = 157  (28.19%)
Number of 2-grams hit = 109  (19.57%)
Number of 1-grams hit = 28  (5.03%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article121.text
Perplexity = 158.21, Entropy = 7.31 bits
Computation based on 7137 words.
Number of 5-grams hit = 1664  (23.32%)
Number of 4-grams hit = 1782  (24.97%)
Number of 3-grams hit = 2167  (30.36%)
Number of 2-grams hit = 1246  (17.46%)
Number of 1-grams hit = 278  (3.90%)
24 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article122.text
Perplexity = 87.07, Entropy = 6.44 bits
Computation based on 325 words.
Number of 5-grams hit = 104  (32.00%)
Number of 4-grams hit = 85  (26.15%)
Number of 3-grams hit = 91  (28.00%)
Number of 2-grams hit = 39  (12.00%)
Number of 1-grams hit = 6  (1.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article123.text
Perplexity = 179.79, Entropy = 7.49 bits
Computation based on 524 words.
Number of 5-grams hit = 102  (19.47%)
Number of 4-grams hit = 105  (20.04%)
Number of 3-grams hit = 172  (32.82%)
Number of 2-grams hit = 126  (24.05%)
Number of 1-grams hit = 19  (3.63%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article124.text
Perplexity = 139.51, Entropy = 7.12 bits
Computation based on 798 words.
Number of 5-grams hit = 195  (24.44%)
Number of 4-grams hit = 215  (26.94%)
Number of 3-grams hit = 237  (29.70%)
Number of 2-grams hit = 121  (15.16%)
Number of 1-grams hit = 30  (3.76%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article125.text
Perplexity = 194.78, Entropy = 7.61 bits
Computation based on 484 words.
Number of 5-grams hit = 97  (20.04%)
Number of 4-grams hit = 126  (26.03%)
Number of 3-grams hit = 146  (30.17%)
Number of 2-grams hit = 88  (18.18%)
Number of 1-grams hit = 27  (5.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article126.text
Perplexity = 134.68, Entropy = 7.07 bits
Computation based on 540 words.
Number of 5-grams hit = 136  (25.19%)
Number of 4-grams hit = 135  (25.00%)
Number of 3-grams hit = 168  (31.11%)
Number of 2-grams hit = 78  (14.44%)
Number of 1-grams hit = 23  (4.26%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article127.text
Perplexity = 166.75, Entropy = 7.38 bits
Computation based on 923 words.
Number of 5-grams hit = 196  (21.24%)
Number of 4-grams hit = 237  (25.68%)
Number of 3-grams hit = 274  (29.69%)
Number of 2-grams hit = 176  (19.07%)
Number of 1-grams hit = 40  (4.33%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article128.text
Perplexity = 328.68, Entropy = 8.36 bits
Computation based on 427 words.
Number of 5-grams hit = 80  (18.74%)
Number of 4-grams hit = 73  (17.10%)
Number of 3-grams hit = 112  (26.23%)
Number of 2-grams hit = 125  (29.27%)
Number of 1-grams hit = 37  (8.67%)
4 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article129.text
Perplexity = 214.98, Entropy = 7.75 bits
Computation based on 469 words.
Number of 5-grams hit = 108  (23.03%)
Number of 4-grams hit = 96  (20.47%)
Number of 3-grams hit = 126  (26.87%)
Number of 2-grams hit = 108  (23.03%)
Number of 1-grams hit = 31  (6.61%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article130.text
Perplexity = 161.75, Entropy = 7.34 bits
Computation based on 1527 words.
Number of 5-grams hit = 367  (24.03%)
Number of 4-grams hit = 369  (24.17%)
Number of 3-grams hit = 448  (29.34%)
Number of 2-grams hit = 268  (17.55%)
Number of 1-grams hit = 75  (4.91%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article131.text
Perplexity = 173.88, Entropy = 7.44 bits
Computation based on 6797 words.
Number of 5-grams hit = 1491  (21.94%)
Number of 4-grams hit = 1683  (24.76%)
Number of 3-grams hit = 2074  (30.51%)
Number of 2-grams hit = 1266  (18.63%)
Number of 1-grams hit = 283  (4.16%)
19 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article132.text
Perplexity = 163.99, Entropy = 7.36 bits
Computation based on 1115 words.
Number of 5-grams hit = 269  (24.13%)
Number of 4-grams hit = 286  (25.65%)
Number of 3-grams hit = 307  (27.53%)
Number of 2-grams hit = 209  (18.74%)
Number of 1-grams hit = 44  (3.95%)
8 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article133.text
Perplexity = 107.36, Entropy = 6.75 bits
Computation based on 243 words.
Number of 5-grams hit = 57  (23.46%)
Number of 4-grams hit = 78  (32.10%)
Number of 3-grams hit = 74  (30.45%)
Number of 2-grams hit = 26  (10.70%)
Number of 1-grams hit = 8  (3.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article134.text
Perplexity = 175.99, Entropy = 7.46 bits
Computation based on 500 words.
Number of 5-grams hit = 89  (17.80%)
Number of 4-grams hit = 147  (29.40%)
Number of 3-grams hit = 160  (32.00%)
Number of 2-grams hit = 83  (16.60%)
Number of 1-grams hit = 21  (4.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article135.text
Perplexity = 314.51, Entropy = 8.30 bits
Computation based on 482 words.
Number of 5-grams hit = 72  (14.94%)
Number of 4-grams hit = 84  (17.43%)
Number of 3-grams hit = 157  (32.57%)
Number of 2-grams hit = 130  (26.97%)
Number of 1-grams hit = 39  (8.09%)
9 OOVs (1.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article136.text
Perplexity = 138.61, Entropy = 7.11 bits
Computation based on 526 words.
Number of 5-grams hit = 119  (22.62%)
Number of 4-grams hit = 135  (25.67%)
Number of 3-grams hit = 159  (30.23%)
Number of 2-grams hit = 93  (17.68%)
Number of 1-grams hit = 20  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article137.text
Perplexity = 124.38, Entropy = 6.96 bits
Computation based on 329 words.
Number of 5-grams hit = 74  (22.49%)
Number of 4-grams hit = 89  (27.05%)
Number of 3-grams hit = 106  (32.22%)
Number of 2-grams hit = 52  (15.81%)
Number of 1-grams hit = 8  (2.43%)
4 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article138.text
Perplexity = 131.54, Entropy = 7.04 bits
Computation based on 307 words.
Number of 5-grams hit = 82  (26.71%)
Number of 4-grams hit = 70  (22.80%)
Number of 3-grams hit = 92  (29.97%)
Number of 2-grams hit = 57  (18.57%)
Number of 1-grams hit = 6  (1.95%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article139.text
Perplexity = 124.38, Entropy = 6.96 bits
Computation based on 271 words.
Number of 5-grams hit = 65  (23.99%)
Number of 4-grams hit = 76  (28.04%)
Number of 3-grams hit = 91  (33.58%)
Number of 2-grams hit = 31  (11.44%)
Number of 1-grams hit = 8  (2.95%)
4 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article140.text
Perplexity = 208.50, Entropy = 7.70 bits
Computation based on 371 words.
Number of 5-grams hit = 69  (18.60%)
Number of 4-grams hit = 81  (21.83%)
Number of 3-grams hit = 101  (27.22%)
Number of 2-grams hit = 97  (26.15%)
Number of 1-grams hit = 23  (6.20%)
3 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article141.text
Perplexity = 199.74, Entropy = 7.64 bits
Computation based on 434 words.
Number of 5-grams hit = 73  (16.82%)
Number of 4-grams hit = 110  (25.35%)
Number of 3-grams hit = 139  (32.03%)
Number of 2-grams hit = 97  (22.35%)
Number of 1-grams hit = 15  (3.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article142.text
Perplexity = 184.18, Entropy = 7.53 bits
Computation based on 695 words.
Number of 5-grams hit = 134  (19.28%)
Number of 4-grams hit = 163  (23.45%)
Number of 3-grams hit = 235  (33.81%)
Number of 2-grams hit = 127  (18.27%)
Number of 1-grams hit = 36  (5.18%)
5 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article143.text
Perplexity = 149.58, Entropy = 7.22 bits
Computation based on 377 words.
Number of 5-grams hit = 100  (26.53%)
Number of 4-grams hit = 76  (20.16%)
Number of 3-grams hit = 98  (25.99%)
Number of 2-grams hit = 85  (22.55%)
Number of 1-grams hit = 18  (4.77%)
4 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article144.text
Perplexity = 190.65, Entropy = 7.57 bits
Computation based on 506 words.
Number of 5-grams hit = 100  (19.76%)
Number of 4-grams hit = 98  (19.37%)
Number of 3-grams hit = 143  (28.26%)
Number of 2-grams hit = 134  (26.48%)
Number of 1-grams hit = 31  (6.13%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article145.text
Perplexity = 189.91, Entropy = 7.57 bits
Computation based on 1941 words.
Number of 5-grams hit = 396  (20.40%)
Number of 4-grams hit = 475  (24.47%)
Number of 3-grams hit = 615  (31.68%)
Number of 2-grams hit = 366  (18.86%)
Number of 1-grams hit = 89  (4.59%)
3 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article146.text
Perplexity = 159.86, Entropy = 7.32 bits
Computation based on 238 words.
Number of 5-grams hit = 56  (23.53%)
Number of 4-grams hit = 59  (24.79%)
Number of 3-grams hit = 67  (28.15%)
Number of 2-grams hit = 45  (18.91%)
Number of 1-grams hit = 11  (4.62%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article147.text
Perplexity = 210.22, Entropy = 7.72 bits
Computation based on 657 words.
Number of 5-grams hit = 107  (16.29%)
Number of 4-grams hit = 167  (25.42%)
Number of 3-grams hit = 231  (35.16%)
Number of 2-grams hit = 118  (17.96%)
Number of 1-grams hit = 34  (5.18%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article148.text
Perplexity = 217.19, Entropy = 7.76 bits
Computation based on 1733 words.
Number of 5-grams hit = 362  (20.89%)
Number of 4-grams hit = 416  (24.00%)
Number of 3-grams hit = 481  (27.76%)
Number of 2-grams hit = 374  (21.58%)
Number of 1-grams hit = 100  (5.77%)
6 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article149.text
Perplexity = 151.10, Entropy = 7.24 bits
Computation based on 557 words.
Number of 5-grams hit = 139  (24.96%)
Number of 4-grams hit = 121  (21.72%)
Number of 3-grams hit = 153  (27.47%)
Number of 2-grams hit = 114  (20.47%)
Number of 1-grams hit = 30  (5.39%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article150.text
Perplexity = 145.81, Entropy = 7.19 bits
Computation based on 638 words.
Number of 5-grams hit = 137  (21.47%)
Number of 4-grams hit = 160  (25.08%)
Number of 3-grams hit = 201  (31.50%)
Number of 2-grams hit = 111  (17.40%)
Number of 1-grams hit = 29  (4.55%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article151.text
Perplexity = 144.85, Entropy = 7.18 bits
Computation based on 173 words.
Number of 5-grams hit = 34  (19.65%)
Number of 4-grams hit = 53  (30.64%)
Number of 3-grams hit = 53  (30.64%)
Number of 2-grams hit = 29  (16.76%)
Number of 1-grams hit = 4  (2.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article152.text
Perplexity = 149.65, Entropy = 7.23 bits
Computation based on 1817 words.
Number of 5-grams hit = 418  (23.00%)
Number of 4-grams hit = 467  (25.70%)
Number of 3-grams hit = 529  (29.11%)
Number of 2-grams hit = 334  (18.38%)
Number of 1-grams hit = 69  (3.80%)
2 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article153.text
Perplexity = 166.54, Entropy = 7.38 bits
Computation based on 588 words.
Number of 5-grams hit = 114  (19.39%)
Number of 4-grams hit = 151  (25.68%)
Number of 3-grams hit = 188  (31.97%)
Number of 2-grams hit = 104  (17.69%)
Number of 1-grams hit = 31  (5.27%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article154.text
Perplexity = 153.44, Entropy = 7.26 bits
Computation based on 1059 words.
Number of 5-grams hit = 246  (23.23%)
Number of 4-grams hit = 271  (25.59%)
Number of 3-grams hit = 312  (29.46%)
Number of 2-grams hit = 194  (18.32%)
Number of 1-grams hit = 36  (3.40%)
5 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article155.text
Perplexity = 168.82, Entropy = 7.40 bits
Computation based on 639 words.
Number of 5-grams hit = 133  (20.81%)
Number of 4-grams hit = 158  (24.73%)
Number of 3-grams hit = 211  (33.02%)
Number of 2-grams hit = 111  (17.37%)
Number of 1-grams hit = 26  (4.07%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article156.text
Perplexity = 52.76, Entropy = 5.72 bits
Computation based on 1373 words.
Number of 5-grams hit = 624  (45.45%)
Number of 4-grams hit = 294  (21.41%)
Number of 3-grams hit = 296  (21.56%)
Number of 2-grams hit = 139  (10.12%)
Number of 1-grams hit = 20  (1.46%)
5 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article157.text
Perplexity = 227.78, Entropy = 7.83 bits
Computation based on 404 words.
Number of 5-grams hit = 60  (14.85%)
Number of 4-grams hit = 92  (22.77%)
Number of 3-grams hit = 130  (32.18%)
Number of 2-grams hit = 99  (24.50%)
Number of 1-grams hit = 23  (5.69%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article158.text
Perplexity = 132.98, Entropy = 7.06 bits
Computation based on 574 words.
Number of 5-grams hit = 166  (28.92%)
Number of 4-grams hit = 124  (21.60%)
Number of 3-grams hit = 140  (24.39%)
Number of 2-grams hit = 121  (21.08%)
Number of 1-grams hit = 23  (4.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article159.text
Perplexity = 66.14, Entropy = 6.05 bits
Computation based on 1205 words.
Number of 5-grams hit = 553  (45.89%)
Number of 4-grams hit = 229  (19.00%)
Number of 3-grams hit = 250  (20.75%)
Number of 2-grams hit = 142  (11.78%)
Number of 1-grams hit = 31  (2.57%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article160.text
Perplexity = 186.04, Entropy = 7.54 bits
Computation based on 534 words.
Number of 5-grams hit = 122  (22.85%)
Number of 4-grams hit = 106  (19.85%)
Number of 3-grams hit = 135  (25.28%)
Number of 2-grams hit = 137  (25.66%)
Number of 1-grams hit = 34  (6.37%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article161.text
Perplexity = 353.38, Entropy = 8.47 bits
Computation based on 189 words.
Number of 5-grams hit = 20  (10.58%)
Number of 4-grams hit = 36  (19.05%)
Number of 3-grams hit = 62  (32.80%)
Number of 2-grams hit = 57  (30.16%)
Number of 1-grams hit = 14  (7.41%)
10 OOVs (5.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article162.text
Perplexity = 162.19, Entropy = 7.34 bits
Computation based on 1148 words.
Number of 5-grams hit = 228  (19.86%)
Number of 4-grams hit = 281  (24.48%)
Number of 3-grams hit = 379  (33.01%)
Number of 2-grams hit = 221  (19.25%)
Number of 1-grams hit = 39  (3.40%)
6 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article163.text
Perplexity = 184.38, Entropy = 7.53 bits
Computation based on 701 words.
Number of 5-grams hit = 126  (17.97%)
Number of 4-grams hit = 190  (27.10%)
Number of 3-grams hit = 212  (30.24%)
Number of 2-grams hit = 133  (18.97%)
Number of 1-grams hit = 40  (5.71%)
4 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article164.text
Perplexity = 111.39, Entropy = 6.80 bits
Computation based on 1767 words.
Number of 5-grams hit = 556  (31.47%)
Number of 4-grams hit = 398  (22.52%)
Number of 3-grams hit = 446  (25.24%)
Number of 2-grams hit = 305  (17.26%)
Number of 1-grams hit = 62  (3.51%)
10 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article165.text
Perplexity = 245.30, Entropy = 7.94 bits
Computation based on 546 words.
Number of 5-grams hit = 91  (16.67%)
Number of 4-grams hit = 102  (18.68%)
Number of 3-grams hit = 173  (31.68%)
Number of 2-grams hit = 149  (27.29%)
Number of 1-grams hit = 31  (5.68%)
11 OOVs (1.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article166.text
Perplexity = 145.18, Entropy = 7.18 bits
Computation based on 517 words.
Number of 5-grams hit = 112  (21.66%)
Number of 4-grams hit = 142  (27.47%)
Number of 3-grams hit = 161  (31.14%)
Number of 2-grams hit = 80  (15.47%)
Number of 1-grams hit = 22  (4.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article167.text
Perplexity = 153.23, Entropy = 7.26 bits
Computation based on 1170 words.
Number of 5-grams hit = 253  (21.62%)
Number of 4-grams hit = 296  (25.30%)
Number of 3-grams hit = 369  (31.54%)
Number of 2-grams hit = 207  (17.69%)
Number of 1-grams hit = 45  (3.85%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article168.text
Perplexity = 176.71, Entropy = 7.47 bits
Computation based on 524 words.
Number of 5-grams hit = 116  (22.14%)
Number of 4-grams hit = 146  (27.86%)
Number of 3-grams hit = 150  (28.63%)
Number of 2-grams hit = 83  (15.84%)
Number of 1-grams hit = 29  (5.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article169.text
Perplexity = 183.78, Entropy = 7.52 bits
Computation based on 499 words.
Number of 5-grams hit = 94  (18.84%)
Number of 4-grams hit = 129  (25.85%)
Number of 3-grams hit = 162  (32.46%)
Number of 2-grams hit = 92  (18.44%)
Number of 1-grams hit = 22  (4.41%)
5 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article170.text
Perplexity = 211.10, Entropy = 7.72 bits
Computation based on 1128 words.
Number of 5-grams hit = 227  (20.12%)
Number of 4-grams hit = 244  (21.63%)
Number of 3-grams hit = 301  (26.68%)
Number of 2-grams hit = 285  (25.27%)
Number of 1-grams hit = 71  (6.29%)
22 OOVs (1.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article171.text
Perplexity = 320.60, Entropy = 8.32 bits
Computation based on 605 words.
Number of 5-grams hit = 102  (16.86%)
Number of 4-grams hit = 113  (18.68%)
Number of 3-grams hit = 168  (27.77%)
Number of 2-grams hit = 166  (27.44%)
Number of 1-grams hit = 56  (9.26%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article172.text
Perplexity = 177.31, Entropy = 7.47 bits
Computation based on 681 words.
Number of 5-grams hit = 126  (18.50%)
Number of 4-grams hit = 167  (24.52%)
Number of 3-grams hit = 220  (32.31%)
Number of 2-grams hit = 140  (20.56%)
Number of 1-grams hit = 28  (4.11%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article173.text
Perplexity = 187.33, Entropy = 7.55 bits
Computation based on 1009 words.
Number of 5-grams hit = 203  (20.12%)
Number of 4-grams hit = 214  (21.21%)
Number of 3-grams hit = 291  (28.84%)
Number of 2-grams hit = 234  (23.19%)
Number of 1-grams hit = 67  (6.64%)
12 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article174.text
Perplexity = 170.37, Entropy = 7.41 bits
Computation based on 446 words.
Number of 5-grams hit = 98  (21.97%)
Number of 4-grams hit = 118  (26.46%)
Number of 3-grams hit = 130  (29.15%)
Number of 2-grams hit = 82  (18.39%)
Number of 1-grams hit = 18  (4.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article175.text
Perplexity = 155.74, Entropy = 7.28 bits
Computation based on 458 words.
Number of 5-grams hit = 73  (15.94%)
Number of 4-grams hit = 106  (23.14%)
Number of 3-grams hit = 176  (38.43%)
Number of 2-grams hit = 84  (18.34%)
Number of 1-grams hit = 19  (4.15%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article176.text
Perplexity = 189.91, Entropy = 7.57 bits
Computation based on 743 words.
Number of 5-grams hit = 126  (16.96%)
Number of 4-grams hit = 180  (24.23%)
Number of 3-grams hit = 236  (31.76%)
Number of 2-grams hit = 174  (23.42%)
Number of 1-grams hit = 27  (3.63%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article177.text
Perplexity = 169.66, Entropy = 7.41 bits
Computation based on 511 words.
Number of 5-grams hit = 121  (23.68%)
Number of 4-grams hit = 96  (18.79%)
Number of 3-grams hit = 153  (29.94%)
Number of 2-grams hit = 115  (22.50%)
Number of 1-grams hit = 26  (5.09%)
7 OOVs (1.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article178.text
Perplexity = 178.65, Entropy = 7.48 bits
Computation based on 168 words.
Number of 5-grams hit = 28  (16.67%)
Number of 4-grams hit = 47  (27.98%)
Number of 3-grams hit = 52  (30.95%)
Number of 2-grams hit = 29  (17.26%)
Number of 1-grams hit = 12  (7.14%)
1 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article179.text
Perplexity = 200.57, Entropy = 7.65 bits
Computation based on 449 words.
Number of 5-grams hit = 109  (24.28%)
Number of 4-grams hit = 94  (20.94%)
Number of 3-grams hit = 123  (27.39%)
Number of 2-grams hit = 93  (20.71%)
Number of 1-grams hit = 30  (6.68%)
5 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article180.text
Perplexity = 163.88, Entropy = 7.36 bits
Computation based on 427 words.
Number of 5-grams hit = 92  (21.55%)
Number of 4-grams hit = 109  (25.53%)
Number of 3-grams hit = 128  (29.98%)
Number of 2-grams hit = 77  (18.03%)
Number of 1-grams hit = 21  (4.92%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article181.text
Perplexity = 80.75, Entropy = 6.34 bits
Computation based on 849 words.
Number of 5-grams hit = 329  (38.75%)
Number of 4-grams hit = 206  (24.26%)
Number of 3-grams hit = 179  (21.08%)
Number of 2-grams hit = 108  (12.72%)
Number of 1-grams hit = 27  (3.18%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article182.text
Perplexity = 89.78, Entropy = 6.49 bits
Computation based on 366 words.
Number of 5-grams hit = 137  (37.43%)
Number of 4-grams hit = 73  (19.95%)
Number of 3-grams hit = 86  (23.50%)
Number of 2-grams hit = 58  (15.85%)
Number of 1-grams hit = 12  (3.28%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article183.text
Perplexity = 139.19, Entropy = 7.12 bits
Computation based on 458 words.
Number of 5-grams hit = 134  (29.26%)
Number of 4-grams hit = 124  (27.07%)
Number of 3-grams hit = 105  (22.93%)
Number of 2-grams hit = 73  (15.94%)
Number of 1-grams hit = 22  (4.80%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article184.text
Perplexity = 170.47, Entropy = 7.41 bits
Computation based on 5472 words.
Number of 5-grams hit = 1208  (22.08%)
Number of 4-grams hit = 1340  (24.49%)
Number of 3-grams hit = 1633  (29.84%)
Number of 2-grams hit = 1030  (18.82%)
Number of 1-grams hit = 261  (4.77%)
20 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article185.text
Perplexity = 162.53, Entropy = 7.34 bits
Computation based on 975 words.
Number of 5-grams hit = 215  (22.05%)
Number of 4-grams hit = 255  (26.15%)
Number of 3-grams hit = 278  (28.51%)
Number of 2-grams hit = 189  (19.38%)
Number of 1-grams hit = 38  (3.90%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article186.text
Perplexity = 99.66, Entropy = 6.64 bits
Computation based on 487 words.
Number of 5-grams hit = 172  (35.32%)
Number of 4-grams hit = 98  (20.12%)
Number of 3-grams hit = 118  (24.23%)
Number of 2-grams hit = 81  (16.63%)
Number of 1-grams hit = 18  (3.70%)
3 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article187.text
Perplexity = 181.47, Entropy = 7.50 bits
Computation based on 6010 words.
Number of 5-grams hit = 1264  (21.03%)
Number of 4-grams hit = 1475  (24.54%)
Number of 3-grams hit = 1871  (31.13%)
Number of 2-grams hit = 1129  (18.79%)
Number of 1-grams hit = 271  (4.51%)
17 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article188.text
Perplexity = 156.72, Entropy = 7.29 bits
Computation based on 684 words.
Number of 5-grams hit = 148  (21.64%)
Number of 4-grams hit = 152  (22.22%)
Number of 3-grams hit = 208  (30.41%)
Number of 2-grams hit = 155  (22.66%)
Number of 1-grams hit = 21  (3.07%)
7 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article189.text
Perplexity = 224.94, Entropy = 7.81 bits
Computation based on 521 words.
Number of 5-grams hit = 106  (20.35%)
Number of 4-grams hit = 103  (19.77%)
Number of 3-grams hit = 155  (29.75%)
Number of 2-grams hit = 125  (23.99%)
Number of 1-grams hit = 32  (6.14%)
15 OOVs (2.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article190.text
Perplexity = 225.70, Entropy = 7.82 bits
Computation based on 390 words.
Number of 5-grams hit = 90  (23.08%)
Number of 4-grams hit = 72  (18.46%)
Number of 3-grams hit = 109  (27.95%)
Number of 2-grams hit = 98  (25.13%)
Number of 1-grams hit = 21  (5.38%)
6 OOVs (1.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article191.text
Perplexity = 174.28, Entropy = 7.45 bits
Computation based on 5442 words.
Number of 5-grams hit = 1253  (23.02%)
Number of 4-grams hit = 1366  (25.10%)
Number of 3-grams hit = 1604  (29.47%)
Number of 2-grams hit = 972  (17.86%)
Number of 1-grams hit = 247  (4.54%)
26 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article192.text
Perplexity = 132.76, Entropy = 7.05 bits
Computation based on 767 words.
Number of 5-grams hit = 164  (21.38%)
Number of 4-grams hit = 190  (24.77%)
Number of 3-grams hit = 238  (31.03%)
Number of 2-grams hit = 154  (20.08%)
Number of 1-grams hit = 21  (2.74%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article193.text
Perplexity = 98.90, Entropy = 6.63 bits
Computation based on 161 words.
Number of 5-grams hit = 40  (24.84%)
Number of 4-grams hit = 36  (22.36%)
Number of 3-grams hit = 48  (29.81%)
Number of 2-grams hit = 30  (18.63%)
Number of 1-grams hit = 7  (4.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article194.text
Perplexity = 169.89, Entropy = 7.41 bits
Computation based on 604 words.
Number of 5-grams hit = 146  (24.17%)
Number of 4-grams hit = 155  (25.66%)
Number of 3-grams hit = 171  (28.31%)
Number of 2-grams hit = 104  (17.22%)
Number of 1-grams hit = 28  (4.64%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article195.text
Perplexity = 164.15, Entropy = 7.36 bits
Computation based on 792 words.
Number of 5-grams hit = 197  (24.87%)
Number of 4-grams hit = 206  (26.01%)
Number of 3-grams hit = 214  (27.02%)
Number of 2-grams hit = 140  (17.68%)
Number of 1-grams hit = 35  (4.42%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article196.text
Perplexity = 149.21, Entropy = 7.22 bits
Computation based on 685 words.
Number of 5-grams hit = 145  (21.17%)
Number of 4-grams hit = 166  (24.23%)
Number of 3-grams hit = 221  (32.26%)
Number of 2-grams hit = 133  (19.42%)
Number of 1-grams hit = 20  (2.92%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article197.text
Perplexity = 163.05, Entropy = 7.35 bits
Computation based on 223 words.
Number of 5-grams hit = 50  (22.42%)
Number of 4-grams hit = 58  (26.01%)
Number of 3-grams hit = 66  (29.60%)
Number of 2-grams hit = 36  (16.14%)
Number of 1-grams hit = 13  (5.83%)
3 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article198.text
Perplexity = 126.32, Entropy = 6.98 bits
Computation based on 718 words.
Number of 5-grams hit = 199  (27.72%)
Number of 4-grams hit = 140  (19.50%)
Number of 3-grams hit = 199  (27.72%)
Number of 2-grams hit = 143  (19.92%)
Number of 1-grams hit = 37  (5.15%)
3 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article199.text
Perplexity = 106.97, Entropy = 6.74 bits
Computation based on 4477 words.
Number of 5-grams hit = 1334  (29.80%)
Number of 4-grams hit = 1072  (23.94%)
Number of 3-grams hit = 1199  (26.78%)
Number of 2-grams hit = 728  (16.26%)
Number of 1-grams hit = 144  (3.22%)
16 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article200.text
Perplexity = 157.50, Entropy = 7.30 bits
Computation based on 638 words.
Number of 5-grams hit = 150  (23.51%)
Number of 4-grams hit = 153  (23.98%)
Number of 3-grams hit = 193  (30.25%)
Number of 2-grams hit = 114  (17.87%)
Number of 1-grams hit = 28  (4.39%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article201.text
Perplexity = 259.69, Entropy = 8.02 bits
Computation based on 463 words.
Number of 5-grams hit = 87  (18.79%)
Number of 4-grams hit = 103  (22.25%)
Number of 3-grams hit = 130  (28.08%)
Number of 2-grams hit = 114  (24.62%)
Number of 1-grams hit = 29  (6.26%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article202.text
Perplexity = 139.72, Entropy = 7.13 bits
Computation based on 1170 words.
Number of 5-grams hit = 316  (27.01%)
Number of 4-grams hit = 256  (21.88%)
Number of 3-grams hit = 315  (26.92%)
Number of 2-grams hit = 241  (20.60%)
Number of 1-grams hit = 42  (3.59%)
5 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article203.text
Perplexity = 98.35, Entropy = 6.62 bits
Computation based on 4808 words.
Number of 5-grams hit = 1684  (35.02%)
Number of 4-grams hit = 1060  (22.05%)
Number of 3-grams hit = 1177  (24.48%)
Number of 2-grams hit = 782  (16.26%)
Number of 1-grams hit = 105  (2.18%)
22 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article204.text
Perplexity = 107.13, Entropy = 6.74 bits
Computation based on 5390 words.
Number of 5-grams hit = 1723  (31.97%)
Number of 4-grams hit = 1297  (24.06%)
Number of 3-grams hit = 1370  (25.42%)
Number of 2-grams hit = 851  (15.79%)
Number of 1-grams hit = 149  (2.76%)
26 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article205.text
Perplexity = 150.50, Entropy = 7.23 bits
Computation based on 553 words.
Number of 5-grams hit = 112  (20.25%)
Number of 4-grams hit = 157  (28.39%)
Number of 3-grams hit = 171  (30.92%)
Number of 2-grams hit = 98  (17.72%)
Number of 1-grams hit = 15  (2.71%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article206.text
Perplexity = 169.57, Entropy = 7.41 bits
Computation based on 585 words.
Number of 5-grams hit = 120  (20.51%)
Number of 4-grams hit = 137  (23.42%)
Number of 3-grams hit = 165  (28.21%)
Number of 2-grams hit = 140  (23.93%)
Number of 1-grams hit = 23  (3.93%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article207.text
Perplexity = 181.13, Entropy = 7.50 bits
Computation based on 722 words.
Number of 5-grams hit = 135  (18.70%)
Number of 4-grams hit = 187  (25.90%)
Number of 3-grams hit = 221  (30.61%)
Number of 2-grams hit = 144  (19.94%)
Number of 1-grams hit = 35  (4.85%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article208.text
Perplexity = 68.92, Entropy = 6.11 bits
Computation based on 886 words.
Number of 5-grams hit = 378  (42.66%)
Number of 4-grams hit = 146  (16.48%)
Number of 3-grams hit = 185  (20.88%)
Number of 2-grams hit = 149  (16.82%)
Number of 1-grams hit = 28  (3.16%)
8 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article209.text
Perplexity = 54.24, Entropy = 5.76 bits
Computation based on 269 words.
Number of 5-grams hit = 124  (46.10%)
Number of 4-grams hit = 58  (21.56%)
Number of 3-grams hit = 53  (19.70%)
Number of 2-grams hit = 26  (9.67%)
Number of 1-grams hit = 8  (2.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article210.text
Perplexity = 188.14, Entropy = 7.56 bits
Computation based on 811 words.
Number of 5-grams hit = 185  (22.81%)
Number of 4-grams hit = 168  (20.72%)
Number of 3-grams hit = 230  (28.36%)
Number of 2-grams hit = 189  (23.30%)
Number of 1-grams hit = 39  (4.81%)
26 OOVs (3.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article211.text
Perplexity = 205.14, Entropy = 7.68 bits
Computation based on 462 words.
Number of 5-grams hit = 100  (21.65%)
Number of 4-grams hit = 97  (21.00%)
Number of 3-grams hit = 124  (26.84%)
Number of 2-grams hit = 107  (23.16%)
Number of 1-grams hit = 34  (7.36%)
11 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article212.text
Perplexity = 155.18, Entropy = 7.28 bits
Computation based on 416 words.
Number of 5-grams hit = 92  (22.12%)
Number of 4-grams hit = 113  (27.16%)
Number of 3-grams hit = 124  (29.81%)
Number of 2-grams hit = 74  (17.79%)
Number of 1-grams hit = 13  (3.12%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article213.text
Perplexity = 157.56, Entropy = 7.30 bits
Computation based on 630 words.
Number of 5-grams hit = 160  (25.40%)
Number of 4-grams hit = 155  (24.60%)
Number of 3-grams hit = 172  (27.30%)
Number of 2-grams hit = 118  (18.73%)
Number of 1-grams hit = 25  (3.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article214.text
Perplexity = 177.32, Entropy = 7.47 bits
Computation based on 444 words.
Number of 5-grams hit = 91  (20.50%)
Number of 4-grams hit = 94  (21.17%)
Number of 3-grams hit = 130  (29.28%)
Number of 2-grams hit = 106  (23.87%)
Number of 1-grams hit = 23  (5.18%)
20 OOVs (4.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article215.text
Perplexity = 429.24, Entropy = 8.75 bits
Computation based on 685 words.
Number of 5-grams hit = 103  (15.04%)
Number of 4-grams hit = 125  (18.25%)
Number of 3-grams hit = 193  (28.18%)
Number of 2-grams hit = 199  (29.05%)
Number of 1-grams hit = 65  (9.49%)
11 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article216.text
Perplexity = 321.67, Entropy = 8.33 bits
Computation based on 481 words.
Number of 5-grams hit = 75  (15.59%)
Number of 4-grams hit = 88  (18.30%)
Number of 3-grams hit = 144  (29.94%)
Number of 2-grams hit = 138  (28.69%)
Number of 1-grams hit = 36  (7.48%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article217.text
Perplexity = 149.01, Entropy = 7.22 bits
Computation based on 302 words.
Number of 5-grams hit = 48  (15.89%)
Number of 4-grams hit = 98  (32.45%)
Number of 3-grams hit = 105  (34.77%)
Number of 2-grams hit = 41  (13.58%)
Number of 1-grams hit = 10  (3.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article218.text
Perplexity = 87.77, Entropy = 6.46 bits
Computation based on 718 words.
Number of 5-grams hit = 208  (28.97%)
Number of 4-grams hit = 199  (27.72%)
Number of 3-grams hit = 192  (26.74%)
Number of 2-grams hit = 107  (14.90%)
Number of 1-grams hit = 12  (1.67%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article219.text
Perplexity = 183.25, Entropy = 7.52 bits
Computation based on 406 words.
Number of 5-grams hit = 80  (19.70%)
Number of 4-grams hit = 83  (20.44%)
Number of 3-grams hit = 115  (28.33%)
Number of 2-grams hit = 106  (26.11%)
Number of 1-grams hit = 22  (5.42%)
5 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article220.text
Perplexity = 77.62, Entropy = 6.28 bits
Computation based on 4293 words.
Number of 5-grams hit = 1700  (39.60%)
Number of 4-grams hit = 959  (22.34%)
Number of 3-grams hit = 991  (23.08%)
Number of 2-grams hit = 562  (13.09%)
Number of 1-grams hit = 81  (1.89%)
27 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article221.text
Perplexity = 145.20, Entropy = 7.18 bits
Computation based on 1174 words.
Number of 5-grams hit = 277  (23.59%)
Number of 4-grams hit = 267  (22.74%)
Number of 3-grams hit = 340  (28.96%)
Number of 2-grams hit = 253  (21.55%)
Number of 1-grams hit = 37  (3.15%)
8 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article222.text
Perplexity = 181.75, Entropy = 7.51 bits
Computation based on 4454 words.
Number of 5-grams hit = 913  (20.50%)
Number of 4-grams hit = 1088  (24.43%)
Number of 3-grams hit = 1388  (31.16%)
Number of 2-grams hit = 844  (18.95%)
Number of 1-grams hit = 221  (4.96%)
17 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article223.text
Perplexity = 127.26, Entropy = 6.99 bits
Computation based on 825 words.
Number of 5-grams hit = 225  (27.27%)
Number of 4-grams hit = 191  (23.15%)
Number of 3-grams hit = 220  (26.67%)
Number of 2-grams hit = 142  (17.21%)
Number of 1-grams hit = 47  (5.70%)
9 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article224.text
Perplexity = 122.14, Entropy = 6.93 bits
Computation based on 380 words.
Number of 5-grams hit = 99  (26.05%)
Number of 4-grams hit = 81  (21.32%)
Number of 3-grams hit = 109  (28.68%)
Number of 2-grams hit = 75  (19.74%)
Number of 1-grams hit = 16  (4.21%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article225.text
Perplexity = 280.83, Entropy = 8.13 bits
Computation based on 724 words.
Number of 5-grams hit = 122  (16.85%)
Number of 4-grams hit = 157  (21.69%)
Number of 3-grams hit = 208  (28.73%)
Number of 2-grams hit = 179  (24.72%)
Number of 1-grams hit = 58  (8.01%)
6 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article226.text
Perplexity = 157.68, Entropy = 7.30 bits
Computation based on 1224 words.
Number of 5-grams hit = 306  (25.00%)
Number of 4-grams hit = 268  (21.90%)
Number of 3-grams hit = 314  (25.65%)
Number of 2-grams hit = 286  (23.37%)
Number of 1-grams hit = 50  (4.08%)
12 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article227.text
Perplexity = 173.41, Entropy = 7.44 bits
Computation based on 936 words.
Number of 5-grams hit = 195  (20.83%)
Number of 4-grams hit = 220  (23.50%)
Number of 3-grams hit = 298  (31.84%)
Number of 2-grams hit = 184  (19.66%)
Number of 1-grams hit = 39  (4.17%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article228.text
Perplexity = 228.67, Entropy = 7.84 bits
Computation based on 783 words.
Number of 5-grams hit = 140  (17.88%)
Number of 4-grams hit = 186  (23.75%)
Number of 3-grams hit = 245  (31.29%)
Number of 2-grams hit = 177  (22.61%)
Number of 1-grams hit = 35  (4.47%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article229.text
Perplexity = 146.41, Entropy = 7.19 bits
Computation based on 471 words.
Number of 5-grams hit = 109  (23.14%)
Number of 4-grams hit = 106  (22.51%)
Number of 3-grams hit = 141  (29.94%)
Number of 2-grams hit = 100  (21.23%)
Number of 1-grams hit = 15  (3.18%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article230.text
Perplexity = 111.59, Entropy = 6.80 bits
Computation based on 301 words.
Number of 5-grams hit = 90  (29.90%)
Number of 4-grams hit = 78  (25.91%)
Number of 3-grams hit = 81  (26.91%)
Number of 2-grams hit = 42  (13.95%)
Number of 1-grams hit = 10  (3.32%)
5 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article231.text
Perplexity = 115.29, Entropy = 6.85 bits
Computation based on 801 words.
Number of 5-grams hit = 206  (25.72%)
Number of 4-grams hit = 193  (24.09%)
Number of 3-grams hit = 217  (27.09%)
Number of 2-grams hit = 156  (19.48%)
Number of 1-grams hit = 29  (3.62%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article232.text
Perplexity = 156.28, Entropy = 7.29 bits
Computation based on 363 words.
Number of 5-grams hit = 100  (27.55%)
Number of 4-grams hit = 70  (19.28%)
Number of 3-grams hit = 95  (26.17%)
Number of 2-grams hit = 83  (22.87%)
Number of 1-grams hit = 15  (4.13%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article233.text
Perplexity = 136.38, Entropy = 7.09 bits
Computation based on 668 words.
Number of 5-grams hit = 163  (24.40%)
Number of 4-grams hit = 163  (24.40%)
Number of 3-grams hit = 216  (32.34%)
Number of 2-grams hit = 108  (16.17%)
Number of 1-grams hit = 18  (2.69%)
5 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article234.text
Perplexity = 164.55, Entropy = 7.36 bits
Computation based on 1331 words.
Number of 5-grams hit = 263  (19.76%)
Number of 4-grams hit = 366  (27.50%)
Number of 3-grams hit = 413  (31.03%)
Number of 2-grams hit = 230  (17.28%)
Number of 1-grams hit = 59  (4.43%)
7 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article235.text
Perplexity = 179.69, Entropy = 7.49 bits
Computation based on 349 words.
Number of 5-grams hit = 66  (18.91%)
Number of 4-grams hit = 93  (26.65%)
Number of 3-grams hit = 99  (28.37%)
Number of 2-grams hit = 76  (21.78%)
Number of 1-grams hit = 15  (4.30%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article236.text
Perplexity = 287.59, Entropy = 8.17 bits
Computation based on 735 words.
Number of 5-grams hit = 131  (17.82%)
Number of 4-grams hit = 148  (20.14%)
Number of 3-grams hit = 203  (27.62%)
Number of 2-grams hit = 183  (24.90%)
Number of 1-grams hit = 70  (9.52%)
18 OOVs (2.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article237.text
Perplexity = 178.83, Entropy = 7.48 bits
Computation based on 389 words.
Number of 5-grams hit = 78  (20.05%)
Number of 4-grams hit = 87  (22.37%)
Number of 3-grams hit = 120  (30.85%)
Number of 2-grams hit = 83  (21.34%)
Number of 1-grams hit = 21  (5.40%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article238.text
Perplexity = 393.25, Entropy = 8.62 bits
Computation based on 300 words.
Number of 5-grams hit = 49  (16.33%)
Number of 4-grams hit = 53  (17.67%)
Number of 3-grams hit = 77  (25.67%)
Number of 2-grams hit = 88  (29.33%)
Number of 1-grams hit = 33  (11.00%)
13 OOVs (4.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article239.text
Perplexity = 188.75, Entropy = 7.56 bits
Computation based on 471 words.
Number of 5-grams hit = 109  (23.14%)
Number of 4-grams hit = 112  (23.78%)
Number of 3-grams hit = 142  (30.15%)
Number of 2-grams hit = 91  (19.32%)
Number of 1-grams hit = 17  (3.61%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article240.text
Perplexity = 201.04, Entropy = 7.65 bits
Computation based on 312 words.
Number of 5-grams hit = 74  (23.72%)
Number of 4-grams hit = 61  (19.55%)
Number of 3-grams hit = 77  (24.68%)
Number of 2-grams hit = 68  (21.79%)
Number of 1-grams hit = 32  (10.26%)
4 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article241.text
Perplexity = 304.07, Entropy = 8.25 bits
Computation based on 301 words.
Number of 5-grams hit = 58  (19.27%)
Number of 4-grams hit = 62  (20.60%)
Number of 3-grams hit = 78  (25.91%)
Number of 2-grams hit = 80  (26.58%)
Number of 1-grams hit = 23  (7.64%)
6 OOVs (1.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article242.text
Perplexity = 146.05, Entropy = 7.19 bits
Computation based on 291 words.
Number of 5-grams hit = 74  (25.43%)
Number of 4-grams hit = 83  (28.52%)
Number of 3-grams hit = 79  (27.15%)
Number of 2-grams hit = 44  (15.12%)
Number of 1-grams hit = 11  (3.78%)
3 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article243.text
Perplexity = 201.24, Entropy = 7.65 bits
Computation based on 743 words.
Number of 5-grams hit = 130  (17.50%)
Number of 4-grams hit = 192  (25.84%)
Number of 3-grams hit = 238  (32.03%)
Number of 2-grams hit = 150  (20.19%)
Number of 1-grams hit = 33  (4.44%)
6 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article244.text
Perplexity = 188.59, Entropy = 7.56 bits
Computation based on 424 words.
Number of 5-grams hit = 94  (22.17%)
Number of 4-grams hit = 105  (24.76%)
Number of 3-grams hit = 119  (28.07%)
Number of 2-grams hit = 81  (19.10%)
Number of 1-grams hit = 25  (5.90%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article245.text
Perplexity = 101.77, Entropy = 6.67 bits
Computation based on 5029 words.
Number of 5-grams hit = 1576  (31.34%)
Number of 4-grams hit = 1146  (22.79%)
Number of 3-grams hit = 1315  (26.15%)
Number of 2-grams hit = 836  (16.62%)
Number of 1-grams hit = 156  (3.10%)
19 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article246.text
Perplexity = 169.88, Entropy = 7.41 bits
Computation based on 275 words.
Number of 5-grams hit = 64  (23.27%)
Number of 4-grams hit = 60  (21.82%)
Number of 3-grams hit = 82  (29.82%)
Number of 2-grams hit = 54  (19.64%)
Number of 1-grams hit = 15  (5.45%)
2 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article247.text
Perplexity = 132.77, Entropy = 7.05 bits
Computation based on 449 words.
Number of 5-grams hit = 108  (24.05%)
Number of 4-grams hit = 124  (27.62%)
Number of 3-grams hit = 132  (29.40%)
Number of 2-grams hit = 67  (14.92%)
Number of 1-grams hit = 18  (4.01%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article248.text
Perplexity = 153.75, Entropy = 7.26 bits
Computation based on 355 words.
Number of 5-grams hit = 78  (21.97%)
Number of 4-grams hit = 97  (27.32%)
Number of 3-grams hit = 103  (29.01%)
Number of 2-grams hit = 66  (18.59%)
Number of 1-grams hit = 11  (3.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article249.text
Perplexity = 92.64, Entropy = 6.53 bits
Computation based on 858 words.
Number of 5-grams hit = 259  (30.19%)
Number of 4-grams hit = 210  (24.48%)
Number of 3-grams hit = 236  (27.51%)
Number of 2-grams hit = 133  (15.50%)
Number of 1-grams hit = 20  (2.33%)
5 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article250.text
Perplexity = 105.30, Entropy = 6.72 bits
Computation based on 4834 words.
Number of 5-grams hit = 1500  (31.03%)
Number of 4-grams hit = 1124  (23.25%)
Number of 3-grams hit = 1267  (26.21%)
Number of 2-grams hit = 812  (16.80%)
Number of 1-grams hit = 131  (2.71%)
30 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article251.text
Perplexity = 158.36, Entropy = 7.31 bits
Computation based on 4971 words.
Number of 5-grams hit = 1117  (22.47%)
Number of 4-grams hit = 1287  (25.89%)
Number of 3-grams hit = 1490  (29.97%)
Number of 2-grams hit = 890  (17.90%)
Number of 1-grams hit = 187  (3.76%)
15 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article252.text
Perplexity = 156.93, Entropy = 7.29 bits
Computation based on 525 words.
Number of 5-grams hit = 120  (22.86%)
Number of 4-grams hit = 129  (24.57%)
Number of 3-grams hit = 161  (30.67%)
Number of 2-grams hit = 95  (18.10%)
Number of 1-grams hit = 20  (3.81%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article253.text
Perplexity = 170.39, Entropy = 7.41 bits
Computation based on 4945 words.
Number of 5-grams hit = 1062  (21.48%)
Number of 4-grams hit = 1202  (24.31%)
Number of 3-grams hit = 1539  (31.12%)
Number of 2-grams hit = 926  (18.73%)
Number of 1-grams hit = 216  (4.37%)
13 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article254.text
Perplexity = 316.14, Entropy = 8.30 bits
Computation based on 774 words.
Number of 5-grams hit = 118  (15.25%)
Number of 4-grams hit = 154  (19.90%)
Number of 3-grams hit = 224  (28.94%)
Number of 2-grams hit = 220  (28.42%)
Number of 1-grams hit = 58  (7.49%)
9 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article255.text
Perplexity = 170.23, Entropy = 7.41 bits
Computation based on 486 words.
Number of 5-grams hit = 101  (20.78%)
Number of 4-grams hit = 110  (22.63%)
Number of 3-grams hit = 145  (29.84%)
Number of 2-grams hit = 106  (21.81%)
Number of 1-grams hit = 24  (4.94%)
7 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article256.text
Perplexity = 191.55, Entropy = 7.58 bits
Computation based on 783 words.
Number of 5-grams hit = 134  (17.11%)
Number of 4-grams hit = 196  (25.03%)
Number of 3-grams hit = 252  (32.18%)
Number of 2-grams hit = 167  (21.33%)
Number of 1-grams hit = 34  (4.34%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article257.text
Perplexity = 75.02, Entropy = 6.23 bits
Computation based on 5750 words.
Number of 5-grams hit = 2203  (38.31%)
Number of 4-grams hit = 1403  (24.40%)
Number of 3-grams hit = 1318  (22.92%)
Number of 2-grams hit = 731  (12.71%)
Number of 1-grams hit = 95  (1.65%)
31 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article258.text
Perplexity = 195.26, Entropy = 7.61 bits
Computation based on 409 words.
Number of 5-grams hit = 74  (18.09%)
Number of 4-grams hit = 108  (26.41%)
Number of 3-grams hit = 132  (32.27%)
Number of 2-grams hit = 74  (18.09%)
Number of 1-grams hit = 21  (5.13%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article259.text
Perplexity = 213.99, Entropy = 7.74 bits
Computation based on 685 words.
Number of 5-grams hit = 130  (18.98%)
Number of 4-grams hit = 149  (21.75%)
Number of 3-grams hit = 190  (27.74%)
Number of 2-grams hit = 170  (24.82%)
Number of 1-grams hit = 46  (6.72%)
16 OOVs (2.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article260.text
Perplexity = 157.42, Entropy = 7.30 bits
Computation based on 6973 words.
Number of 5-grams hit = 1591  (22.82%)
Number of 4-grams hit = 1753  (25.14%)
Number of 3-grams hit = 2125  (30.47%)
Number of 2-grams hit = 1231  (17.65%)
Number of 1-grams hit = 273  (3.92%)
22 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article261.text
Perplexity = 115.38, Entropy = 6.85 bits
Computation based on 645 words.
Number of 5-grams hit = 166  (25.74%)
Number of 4-grams hit = 177  (27.44%)
Number of 3-grams hit = 190  (29.46%)
Number of 2-grams hit = 91  (14.11%)
Number of 1-grams hit = 21  (3.26%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article262.text
Perplexity = 131.71, Entropy = 7.04 bits
Computation based on 661 words.
Number of 5-grams hit = 150  (22.69%)
Number of 4-grams hit = 176  (26.63%)
Number of 3-grams hit = 215  (32.53%)
Number of 2-grams hit = 103  (15.58%)
Number of 1-grams hit = 17  (2.57%)
3 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article263.text
Perplexity = 151.24, Entropy = 7.24 bits
Computation based on 375 words.
Number of 5-grams hit = 91  (24.27%)
Number of 4-grams hit = 95  (25.33%)
Number of 3-grams hit = 106  (28.27%)
Number of 2-grams hit = 62  (16.53%)
Number of 1-grams hit = 21  (5.60%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article264.text
Perplexity = 92.33, Entropy = 6.53 bits
Computation based on 798 words.
Number of 5-grams hit = 240  (30.08%)
Number of 4-grams hit = 187  (23.43%)
Number of 3-grams hit = 206  (25.81%)
Number of 2-grams hit = 138  (17.29%)
Number of 1-grams hit = 27  (3.38%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article265.text
Perplexity = 100.83, Entropy = 6.66 bits
Computation based on 569 words.
Number of 5-grams hit = 153  (26.89%)
Number of 4-grams hit = 137  (24.08%)
Number of 3-grams hit = 153  (26.89%)
Number of 2-grams hit = 111  (19.51%)
Number of 1-grams hit = 15  (2.64%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article266.text
Perplexity = 178.31, Entropy = 7.48 bits
Computation based on 435 words.
Number of 5-grams hit = 99  (22.76%)
Number of 4-grams hit = 117  (26.90%)
Number of 3-grams hit = 120  (27.59%)
Number of 2-grams hit = 78  (17.93%)
Number of 1-grams hit = 21  (4.83%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article267.text
Perplexity = 166.85, Entropy = 7.38 bits
Computation based on 6221 words.
Number of 5-grams hit = 1371  (22.04%)
Number of 4-grams hit = 1589  (25.54%)
Number of 3-grams hit = 1874  (30.12%)
Number of 2-grams hit = 1122  (18.04%)
Number of 1-grams hit = 265  (4.26%)
31 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article268.text
Perplexity = 175.64, Entropy = 7.46 bits
Computation based on 5821 words.
Number of 5-grams hit = 1266  (21.75%)
Number of 4-grams hit = 1475  (25.34%)
Number of 3-grams hit = 1728  (29.69%)
Number of 2-grams hit = 1080  (18.55%)
Number of 1-grams hit = 272  (4.67%)
19 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article269.text
Perplexity = 208.78, Entropy = 7.71 bits
Computation based on 418 words.
Number of 5-grams hit = 87  (20.81%)
Number of 4-grams hit = 100  (23.92%)
Number of 3-grams hit = 128  (30.62%)
Number of 2-grams hit = 81  (19.38%)
Number of 1-grams hit = 22  (5.26%)
4 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article270.text
Perplexity = 180.27, Entropy = 7.49 bits
Computation based on 375 words.
Number of 5-grams hit = 75  (20.00%)
Number of 4-grams hit = 86  (22.93%)
Number of 3-grams hit = 102  (27.20%)
Number of 2-grams hit = 87  (23.20%)
Number of 1-grams hit = 25  (6.67%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article271.text
Perplexity = 122.34, Entropy = 6.93 bits
Computation based on 7295 words.
Number of 5-grams hit = 2104  (28.84%)
Number of 4-grams hit = 1722  (23.61%)
Number of 3-grams hit = 1912  (26.21%)
Number of 2-grams hit = 1309  (17.94%)
Number of 1-grams hit = 248  (3.40%)
52 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article272.text
Perplexity = 90.18, Entropy = 6.49 bits
Computation based on 5195 words.
Number of 5-grams hit = 1740  (33.49%)
Number of 4-grams hit = 1261  (24.27%)
Number of 3-grams hit = 1300  (25.02%)
Number of 2-grams hit = 760  (14.63%)
Number of 1-grams hit = 134  (2.58%)
23 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article273.text
Perplexity = 142.00, Entropy = 7.15 bits
Computation based on 638 words.
Number of 5-grams hit = 141  (22.10%)
Number of 4-grams hit = 176  (27.59%)
Number of 3-grams hit = 193  (30.25%)
Number of 2-grams hit = 107  (16.77%)
Number of 1-grams hit = 21  (3.29%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article274.text
Perplexity = 167.34, Entropy = 7.39 bits
Computation based on 651 words.
Number of 5-grams hit = 126  (19.35%)
Number of 4-grams hit = 181  (27.80%)
Number of 3-grams hit = 194  (29.80%)
Number of 2-grams hit = 119  (18.28%)
Number of 1-grams hit = 31  (4.76%)
6 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article275.text
Perplexity = 188.48, Entropy = 7.56 bits
Computation based on 1923 words.
Number of 5-grams hit = 374  (19.45%)
Number of 4-grams hit = 457  (23.76%)
Number of 3-grams hit = 601  (31.25%)
Number of 2-grams hit = 407  (21.16%)
Number of 1-grams hit = 84  (4.37%)
4 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article276.text
Perplexity = 254.93, Entropy = 7.99 bits
Computation based on 455 words.
Number of 5-grams hit = 86  (18.90%)
Number of 4-grams hit = 90  (19.78%)
Number of 3-grams hit = 132  (29.01%)
Number of 2-grams hit = 114  (25.05%)
Number of 1-grams hit = 33  (7.25%)
16 OOVs (3.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article277.text
Perplexity = 169.83, Entropy = 7.41 bits
Computation based on 768 words.
Number of 5-grams hit = 164  (21.35%)
Number of 4-grams hit = 192  (25.00%)
Number of 3-grams hit = 233  (30.34%)
Number of 2-grams hit = 146  (19.01%)
Number of 1-grams hit = 33  (4.30%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article278.text
Perplexity = 126.82, Entropy = 6.99 bits
Computation based on 806 words.
Number of 5-grams hit = 192  (23.82%)
Number of 4-grams hit = 180  (22.33%)
Number of 3-grams hit = 241  (29.90%)
Number of 2-grams hit = 164  (20.35%)
Number of 1-grams hit = 29  (3.60%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article279.text
Perplexity = 141.16, Entropy = 7.14 bits
Computation based on 401 words.
Number of 5-grams hit = 105  (26.18%)
Number of 4-grams hit = 105  (26.18%)
Number of 3-grams hit = 115  (28.68%)
Number of 2-grams hit = 58  (14.46%)
Number of 1-grams hit = 18  (4.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article280.text
Perplexity = 168.83, Entropy = 7.40 bits
Computation based on 1708 words.
Number of 5-grams hit = 369  (21.60%)
Number of 4-grams hit = 461  (26.99%)
Number of 3-grams hit = 516  (30.21%)
Number of 2-grams hit = 291  (17.04%)
Number of 1-grams hit = 71  (4.16%)
2 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article281.text
Perplexity = 160.20, Entropy = 7.32 bits
Computation based on 546 words.
Number of 5-grams hit = 122  (22.34%)
Number of 4-grams hit = 129  (23.63%)
Number of 3-grams hit = 149  (27.29%)
Number of 2-grams hit = 129  (23.63%)
Number of 1-grams hit = 17  (3.11%)
20 OOVs (3.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article282.text
Perplexity = 133.19, Entropy = 7.06 bits
Computation based on 708 words.
Number of 5-grams hit = 199  (28.11%)
Number of 4-grams hit = 175  (24.72%)
Number of 3-grams hit = 199  (28.11%)
Number of 2-grams hit = 105  (14.83%)
Number of 1-grams hit = 30  (4.24%)
5 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article283.text
Perplexity = 148.01, Entropy = 7.21 bits
Computation based on 1182 words.
Number of 5-grams hit = 302  (25.55%)
Number of 4-grams hit = 280  (23.69%)
Number of 3-grams hit = 360  (30.46%)
Number of 2-grams hit = 193  (16.33%)
Number of 1-grams hit = 47  (3.98%)
4 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article284.text
Perplexity = 87.50, Entropy = 6.45 bits
Computation based on 1579 words.
Number of 5-grams hit = 528  (33.44%)
Number of 4-grams hit = 414  (26.22%)
Number of 3-grams hit = 382  (24.19%)
Number of 2-grams hit = 211  (13.36%)
Number of 1-grams hit = 44  (2.79%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article285.text
Perplexity = 119.13, Entropy = 6.90 bits
Computation based on 1094 words.
Number of 5-grams hit = 286  (26.14%)
Number of 4-grams hit = 277  (25.32%)
Number of 3-grams hit = 304  (27.79%)
Number of 2-grams hit = 194  (17.73%)
Number of 1-grams hit = 33  (3.02%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article286.text
Perplexity = 216.44, Entropy = 7.76 bits
Computation based on 442 words.
Number of 5-grams hit = 85  (19.23%)
Number of 4-grams hit = 109  (24.66%)
Number of 3-grams hit = 128  (28.96%)
Number of 2-grams hit = 96  (21.72%)
Number of 1-grams hit = 24  (5.43%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article287.text
Perplexity = 87.75, Entropy = 6.46 bits
Computation based on 1652 words.
Number of 5-grams hit = 563  (34.08%)
Number of 4-grams hit = 334  (20.22%)
Number of 3-grams hit = 425  (25.73%)
Number of 2-grams hit = 276  (16.71%)
Number of 1-grams hit = 54  (3.27%)
6 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article288.text
Perplexity = 162.93, Entropy = 7.35 bits
Computation based on 582 words.
Number of 5-grams hit = 107  (18.38%)
Number of 4-grams hit = 156  (26.80%)
Number of 3-grams hit = 180  (30.93%)
Number of 2-grams hit = 116  (19.93%)
Number of 1-grams hit = 23  (3.95%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article289.text
Perplexity = 83.71, Entropy = 6.39 bits
Computation based on 704 words.
Number of 5-grams hit = 244  (34.66%)
Number of 4-grams hit = 178  (25.28%)
Number of 3-grams hit = 172  (24.43%)
Number of 2-grams hit = 99  (14.06%)
Number of 1-grams hit = 11  (1.56%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article290.text
Perplexity = 118.47, Entropy = 6.89 bits
Computation based on 1280 words.
Number of 5-grams hit = 356  (27.81%)
Number of 4-grams hit = 313  (24.45%)
Number of 3-grams hit = 344  (26.88%)
Number of 2-grams hit = 230  (17.97%)
Number of 1-grams hit = 37  (2.89%)
13 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article291.text
Perplexity = 208.19, Entropy = 7.70 bits
Computation based on 507 words.
Number of 5-grams hit = 93  (18.34%)
Number of 4-grams hit = 104  (20.51%)
Number of 3-grams hit = 157  (30.97%)
Number of 2-grams hit = 121  (23.87%)
Number of 1-grams hit = 32  (6.31%)
9 OOVs (1.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article292.text
Perplexity = 117.20, Entropy = 6.87 bits
Computation based on 533 words.
Number of 5-grams hit = 147  (27.58%)
Number of 4-grams hit = 134  (25.14%)
Number of 3-grams hit = 147  (27.58%)
Number of 2-grams hit = 89  (16.70%)
Number of 1-grams hit = 16  (3.00%)
5 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article293.text
Perplexity = 230.38, Entropy = 7.85 bits
Computation based on 552 words.
Number of 5-grams hit = 124  (22.46%)
Number of 4-grams hit = 125  (22.64%)
Number of 3-grams hit = 152  (27.54%)
Number of 2-grams hit = 116  (21.01%)
Number of 1-grams hit = 35  (6.34%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article294.text
Perplexity = 288.95, Entropy = 8.17 bits
Computation based on 2100 words.
Number of 5-grams hit = 349  (16.62%)
Number of 4-grams hit = 429  (20.43%)
Number of 3-grams hit = 611  (29.10%)
Number of 2-grams hit = 565  (26.90%)
Number of 1-grams hit = 146  (6.95%)
20 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article295.text
Perplexity = 204.28, Entropy = 7.67 bits
Computation based on 583 words.
Number of 5-grams hit = 108  (18.52%)
Number of 4-grams hit = 125  (21.44%)
Number of 3-grams hit = 167  (28.64%)
Number of 2-grams hit = 152  (26.07%)
Number of 1-grams hit = 31  (5.32%)
10 OOVs (1.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article296.text
Perplexity = 142.05, Entropy = 7.15 bits
Computation based on 1523 words.
Number of 5-grams hit = 344  (22.59%)
Number of 4-grams hit = 387  (25.41%)
Number of 3-grams hit = 474  (31.12%)
Number of 2-grams hit = 271  (17.79%)
Number of 1-grams hit = 47  (3.09%)
7 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article297.text
Perplexity = 102.07, Entropy = 6.67 bits
Computation based on 492 words.
Number of 5-grams hit = 141  (28.66%)
Number of 4-grams hit = 134  (27.24%)
Number of 3-grams hit = 143  (29.07%)
Number of 2-grams hit = 66  (13.41%)
Number of 1-grams hit = 8  (1.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article298.text
Perplexity = 163.35, Entropy = 7.35 bits
Computation based on 3442 words.
Number of 5-grams hit = 789  (22.92%)
Number of 4-grams hit = 894  (25.97%)
Number of 3-grams hit = 1025  (29.78%)
Number of 2-grams hit = 597  (17.34%)
Number of 1-grams hit = 137  (3.98%)
12 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article299.text
Perplexity = 194.98, Entropy = 7.61 bits
Computation based on 2703 words.
Number of 5-grams hit = 566  (20.94%)
Number of 4-grams hit = 637  (23.57%)
Number of 3-grams hit = 780  (28.86%)
Number of 2-grams hit = 570  (21.09%)
Number of 1-grams hit = 150  (5.55%)
39 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article300.text
Perplexity = 93.73, Entropy = 6.55 bits
Computation based on 2528 words.
Number of 5-grams hit = 813  (32.16%)
Number of 4-grams hit = 598  (23.66%)
Number of 3-grams hit = 664  (26.27%)
Number of 2-grams hit = 396  (15.66%)
Number of 1-grams hit = 57  (2.25%)
11 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article301.text
Perplexity = 181.72, Entropy = 7.51 bits
Computation based on 2136 words.
Number of 5-grams hit = 394  (18.45%)
Number of 4-grams hit = 531  (24.86%)
Number of 3-grams hit = 703  (32.91%)
Number of 2-grams hit = 424  (19.85%)
Number of 1-grams hit = 84  (3.93%)
10 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article302.text
Perplexity = 121.84, Entropy = 6.93 bits
Computation based on 622 words.
Number of 5-grams hit = 180  (28.94%)
Number of 4-grams hit = 156  (25.08%)
Number of 3-grams hit = 168  (27.01%)
Number of 2-grams hit = 91  (14.63%)
Number of 1-grams hit = 27  (4.34%)
4 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article303.text
Perplexity = 59.28, Entropy = 5.89 bits
Computation based on 852 words.
Number of 5-grams hit = 427  (50.12%)
Number of 4-grams hit = 109  (12.79%)
Number of 3-grams hit = 144  (16.90%)
Number of 2-grams hit = 143  (16.78%)
Number of 1-grams hit = 29  (3.40%)
18 OOVs (2.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article304.text
Perplexity = 257.64, Entropy = 8.01 bits
Computation based on 392 words.
Number of 5-grams hit = 67  (17.09%)
Number of 4-grams hit = 73  (18.62%)
Number of 3-grams hit = 110  (28.06%)
Number of 2-grams hit = 111  (28.32%)
Number of 1-grams hit = 31  (7.91%)
12 OOVs (2.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article305.text
Perplexity = 170.04, Entropy = 7.41 bits
Computation based on 303 words.
Number of 5-grams hit = 64  (21.12%)
Number of 4-grams hit = 70  (23.10%)
Number of 3-grams hit = 95  (31.35%)
Number of 2-grams hit = 65  (21.45%)
Number of 1-grams hit = 9  (2.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article306.text
Perplexity = 156.37, Entropy = 7.29 bits
Computation based on 589 words.
Number of 5-grams hit = 138  (23.43%)
Number of 4-grams hit = 143  (24.28%)
Number of 3-grams hit = 188  (31.92%)
Number of 2-grams hit = 90  (15.28%)
Number of 1-grams hit = 30  (5.09%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article307.text
Perplexity = 147.12, Entropy = 7.20 bits
Computation based on 511 words.
Number of 5-grams hit = 123  (24.07%)
Number of 4-grams hit = 130  (25.44%)
Number of 3-grams hit = 138  (27.01%)
Number of 2-grams hit = 100  (19.57%)
Number of 1-grams hit = 20  (3.91%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article308.text
Perplexity = 144.31, Entropy = 7.17 bits
Computation based on 392 words.
Number of 5-grams hit = 89  (22.70%)
Number of 4-grams hit = 96  (24.49%)
Number of 3-grams hit = 112  (28.57%)
Number of 2-grams hit = 82  (20.92%)
Number of 1-grams hit = 13  (3.32%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article309.text
Perplexity = 158.23, Entropy = 7.31 bits
Computation based on 241 words.
Number of 5-grams hit = 51  (21.16%)
Number of 4-grams hit = 67  (27.80%)
Number of 3-grams hit = 80  (33.20%)
Number of 2-grams hit = 34  (14.11%)
Number of 1-grams hit = 9  (3.73%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article310.text
Perplexity = 159.74, Entropy = 7.32 bits
Computation based on 480 words.
Number of 5-grams hit = 109  (22.71%)
Number of 4-grams hit = 112  (23.33%)
Number of 3-grams hit = 132  (27.50%)
Number of 2-grams hit = 105  (21.88%)
Number of 1-grams hit = 22  (4.58%)
4 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article311.text
Perplexity = 127.07, Entropy = 6.99 bits
Computation based on 465 words.
Number of 5-grams hit = 127  (27.31%)
Number of 4-grams hit = 98  (21.08%)
Number of 3-grams hit = 127  (27.31%)
Number of 2-grams hit = 92  (19.78%)
Number of 1-grams hit = 21  (4.52%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article312.text
Perplexity = 228.72, Entropy = 7.84 bits
Computation based on 594 words.
Number of 5-grams hit = 120  (20.20%)
Number of 4-grams hit = 140  (23.57%)
Number of 3-grams hit = 178  (29.97%)
Number of 2-grams hit = 122  (20.54%)
Number of 1-grams hit = 34  (5.72%)
4 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article313.text
Perplexity = 212.54, Entropy = 7.73 bits
Computation based on 466 words.
Number of 5-grams hit = 103  (22.10%)
Number of 4-grams hit = 98  (21.03%)
Number of 3-grams hit = 126  (27.04%)
Number of 2-grams hit = 114  (24.46%)
Number of 1-grams hit = 25  (5.36%)
11 OOVs (2.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article314.text
Perplexity = 104.37, Entropy = 6.71 bits
Computation based on 3281 words.
Number of 5-grams hit = 1063  (32.40%)
Number of 4-grams hit = 764  (23.29%)
Number of 3-grams hit = 808  (24.63%)
Number of 2-grams hit = 555  (16.92%)
Number of 1-grams hit = 91  (2.77%)
15 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article315.text
Perplexity = 167.97, Entropy = 7.39 bits
Computation based on 1078 words.
Number of 5-grams hit = 280  (25.97%)
Number of 4-grams hit = 219  (20.32%)
Number of 3-grams hit = 294  (27.27%)
Number of 2-grams hit = 228  (21.15%)
Number of 1-grams hit = 57  (5.29%)
14 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article316.text
Perplexity = 171.21, Entropy = 7.42 bits
Computation based on 329 words.
Number of 5-grams hit = 66  (20.06%)
Number of 4-grams hit = 75  (22.80%)
Number of 3-grams hit = 112  (34.04%)
Number of 2-grams hit = 62  (18.84%)
Number of 1-grams hit = 14  (4.26%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article317.text
Perplexity = 171.63, Entropy = 7.42 bits
Computation based on 623 words.
Number of 5-grams hit = 132  (21.19%)
Number of 4-grams hit = 141  (22.63%)
Number of 3-grams hit = 204  (32.74%)
Number of 2-grams hit = 119  (19.10%)
Number of 1-grams hit = 27  (4.33%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article318.text
Perplexity = 165.16, Entropy = 7.37 bits
Computation based on 3565 words.
Number of 5-grams hit = 845  (23.70%)
Number of 4-grams hit = 867  (24.32%)
Number of 3-grams hit = 1062  (29.79%)
Number of 2-grams hit = 651  (18.26%)
Number of 1-grams hit = 140  (3.93%)
9 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article319.text
Perplexity = 376.71, Entropy = 8.56 bits
Computation based on 589 words.
Number of 5-grams hit = 94  (15.96%)
Number of 4-grams hit = 116  (19.69%)
Number of 3-grams hit = 162  (27.50%)
Number of 2-grams hit = 167  (28.35%)
Number of 1-grams hit = 50  (8.49%)
10 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article320.text
Perplexity = 192.25, Entropy = 7.59 bits
Computation based on 376 words.
Number of 5-grams hit = 77  (20.48%)
Number of 4-grams hit = 72  (19.15%)
Number of 3-grams hit = 110  (29.26%)
Number of 2-grams hit = 101  (26.86%)
Number of 1-grams hit = 16  (4.26%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article321.text
Perplexity = 190.50, Entropy = 7.57 bits
Computation based on 1074 words.
Number of 5-grams hit = 231  (21.51%)
Number of 4-grams hit = 268  (24.95%)
Number of 3-grams hit = 333  (31.01%)
Number of 2-grams hit = 189  (17.60%)
Number of 1-grams hit = 53  (4.93%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article322.text
Perplexity = 125.78, Entropy = 6.97 bits
Computation based on 1666 words.
Number of 5-grams hit = 536  (32.17%)
Number of 4-grams hit = 374  (22.45%)
Number of 3-grams hit = 404  (24.25%)
Number of 2-grams hit = 292  (17.53%)
Number of 1-grams hit = 60  (3.60%)
15 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article323.text
Perplexity = 341.90, Entropy = 8.42 bits
Computation based on 808 words.
Number of 5-grams hit = 114  (14.11%)
Number of 4-grams hit = 168  (20.79%)
Number of 3-grams hit = 231  (28.59%)
Number of 2-grams hit = 235  (29.08%)
Number of 1-grams hit = 60  (7.43%)
12 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article324.text
Perplexity = 143.35, Entropy = 7.16 bits
Computation based on 1642 words.
Number of 5-grams hit = 472  (28.75%)
Number of 4-grams hit = 371  (22.59%)
Number of 3-grams hit = 435  (26.49%)
Number of 2-grams hit = 293  (17.84%)
Number of 1-grams hit = 71  (4.32%)
15 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article325.text
Perplexity = 259.01, Entropy = 8.02 bits
Computation based on 1108 words.
Number of 5-grams hit = 186  (16.79%)
Number of 4-grams hit = 201  (18.14%)
Number of 3-grams hit = 350  (31.59%)
Number of 2-grams hit = 302  (27.26%)
Number of 1-grams hit = 69  (6.23%)
18 OOVs (1.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article326.text
Perplexity = 193.45, Entropy = 7.60 bits
Computation based on 606 words.
Number of 5-grams hit = 141  (23.27%)
Number of 4-grams hit = 145  (23.93%)
Number of 3-grams hit = 174  (28.71%)
Number of 2-grams hit = 112  (18.48%)
Number of 1-grams hit = 34  (5.61%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article327.text
Perplexity = 98.26, Entropy = 6.62 bits
Computation based on 1125 words.
Number of 5-grams hit = 344  (30.58%)
Number of 4-grams hit = 241  (21.42%)
Number of 3-grams hit = 302  (26.84%)
Number of 2-grams hit = 207  (18.40%)
Number of 1-grams hit = 31  (2.76%)
5 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article328.text
Perplexity = 115.03, Entropy = 6.85 bits
Computation based on 250 words.
Number of 5-grams hit = 67  (26.80%)
Number of 4-grams hit = 70  (28.00%)
Number of 3-grams hit = 69  (27.60%)
Number of 2-grams hit = 39  (15.60%)
Number of 1-grams hit = 5  (2.00%)
1 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article329.text
Perplexity = 92.77, Entropy = 6.54 bits
Computation based on 413 words.
Number of 5-grams hit = 132  (31.96%)
Number of 4-grams hit = 86  (20.82%)
Number of 3-grams hit = 117  (28.33%)
Number of 2-grams hit = 70  (16.95%)
Number of 1-grams hit = 8  (1.94%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article330.text
Perplexity = 107.20, Entropy = 6.74 bits
Computation based on 1051 words.
Number of 5-grams hit = 334  (31.78%)
Number of 4-grams hit = 219  (20.84%)
Number of 3-grams hit = 281  (26.74%)
Number of 2-grams hit = 184  (17.51%)
Number of 1-grams hit = 33  (3.14%)
8 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article331.text
Perplexity = 152.77, Entropy = 7.26 bits
Computation based on 1948 words.
Number of 5-grams hit = 446  (22.90%)
Number of 4-grams hit = 478  (24.54%)
Number of 3-grams hit = 622  (31.93%)
Number of 2-grams hit = 330  (16.94%)
Number of 1-grams hit = 72  (3.70%)
10 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article332.text
Perplexity = 143.06, Entropy = 7.16 bits
Computation based on 575 words.
Number of 5-grams hit = 145  (25.22%)
Number of 4-grams hit = 150  (26.09%)
Number of 3-grams hit = 174  (30.26%)
Number of 2-grams hit = 85  (14.78%)
Number of 1-grams hit = 21  (3.65%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article333.text
Perplexity = 203.95, Entropy = 7.67 bits
Computation based on 2146 words.
Number of 5-grams hit = 405  (18.87%)
Number of 4-grams hit = 543  (25.30%)
Number of 3-grams hit = 644  (30.01%)
Number of 2-grams hit = 439  (20.46%)
Number of 1-grams hit = 115  (5.36%)
7 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article334.text
Perplexity = 176.25, Entropy = 7.46 bits
Computation based on 961 words.
Number of 5-grams hit = 194  (20.19%)
Number of 4-grams hit = 236  (24.56%)
Number of 3-grams hit = 303  (31.53%)
Number of 2-grams hit = 185  (19.25%)
Number of 1-grams hit = 43  (4.47%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article335.text
Perplexity = 80.60, Entropy = 6.33 bits
Computation based on 1753 words.
Number of 5-grams hit = 618  (35.25%)
Number of 4-grams hit = 391  (22.30%)
Number of 3-grams hit = 440  (25.10%)
Number of 2-grams hit = 256  (14.60%)
Number of 1-grams hit = 48  (2.74%)
8 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article336.text
Perplexity = 161.90, Entropy = 7.34 bits
Computation based on 1140 words.
Number of 5-grams hit = 244  (21.40%)
Number of 4-grams hit = 297  (26.05%)
Number of 3-grams hit = 352  (30.88%)
Number of 2-grams hit = 210  (18.42%)
Number of 1-grams hit = 37  (3.25%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article337.text
Perplexity = 112.43, Entropy = 6.81 bits
Computation based on 399 words.
Number of 5-grams hit = 129  (32.33%)
Number of 4-grams hit = 92  (23.06%)
Number of 3-grams hit = 96  (24.06%)
Number of 2-grams hit = 69  (17.29%)
Number of 1-grams hit = 13  (3.26%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article338.text
Perplexity = 233.44, Entropy = 7.87 bits
Computation based on 3481 words.
Number of 5-grams hit = 666  (19.13%)
Number of 4-grams hit = 741  (21.29%)
Number of 3-grams hit = 1034  (29.70%)
Number of 2-grams hit = 827  (23.76%)
Number of 1-grams hit = 213  (6.12%)
91 OOVs (2.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article339.text
Perplexity = 174.76, Entropy = 7.45 bits
Computation based on 373 words.
Number of 5-grams hit = 83  (22.25%)
Number of 4-grams hit = 84  (22.52%)
Number of 3-grams hit = 117  (31.37%)
Number of 2-grams hit = 78  (20.91%)
Number of 1-grams hit = 11  (2.95%)
3 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article340.text
Perplexity = 170.88, Entropy = 7.42 bits
Computation based on 1153 words.
Number of 5-grams hit = 224  (19.43%)
Number of 4-grams hit = 284  (24.63%)
Number of 3-grams hit = 378  (32.78%)
Number of 2-grams hit = 221  (19.17%)
Number of 1-grams hit = 46  (3.99%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article341.text
Perplexity = 146.81, Entropy = 7.20 bits
Computation based on 1370 words.
Number of 5-grams hit = 311  (22.70%)
Number of 4-grams hit = 362  (26.42%)
Number of 3-grams hit = 405  (29.56%)
Number of 2-grams hit = 253  (18.47%)
Number of 1-grams hit = 39  (2.85%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article342.text
Perplexity = 98.08, Entropy = 6.62 bits
Computation based on 1442 words.
Number of 5-grams hit = 527  (36.55%)
Number of 4-grams hit = 328  (22.75%)
Number of 3-grams hit = 317  (21.98%)
Number of 2-grams hit = 221  (15.33%)
Number of 1-grams hit = 49  (3.40%)
9 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article343.text
Perplexity = 106.89, Entropy = 6.74 bits
Computation based on 477 words.
Number of 5-grams hit = 143  (29.98%)
Number of 4-grams hit = 108  (22.64%)
Number of 3-grams hit = 124  (26.00%)
Number of 2-grams hit = 93  (19.50%)
Number of 1-grams hit = 9  (1.89%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article344.text
Perplexity = 135.95, Entropy = 7.09 bits
Computation based on 581 words.
Number of 5-grams hit = 112  (19.28%)
Number of 4-grams hit = 160  (27.54%)
Number of 3-grams hit = 185  (31.84%)
Number of 2-grams hit = 107  (18.42%)
Number of 1-grams hit = 17  (2.93%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article345.text
Perplexity = 167.82, Entropy = 7.39 bits
Computation based on 3153 words.
Number of 5-grams hit = 656  (20.81%)
Number of 4-grams hit = 794  (25.18%)
Number of 3-grams hit = 979  (31.05%)
Number of 2-grams hit = 588  (18.65%)
Number of 1-grams hit = 136  (4.31%)
11 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article346.text
Perplexity = 151.31, Entropy = 7.24 bits
Computation based on 1581 words.
Number of 5-grams hit = 387  (24.48%)
Number of 4-grams hit = 383  (24.23%)
Number of 3-grams hit = 475  (30.04%)
Number of 2-grams hit = 280  (17.71%)
Number of 1-grams hit = 56  (3.54%)
4 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article347.text
Perplexity = 187.35, Entropy = 7.55 bits
Computation based on 647 words.
Number of 5-grams hit = 125  (19.32%)
Number of 4-grams hit = 156  (24.11%)
Number of 3-grams hit = 215  (33.23%)
Number of 2-grams hit = 126  (19.47%)
Number of 1-grams hit = 25  (3.86%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article348.text
Perplexity = 168.74, Entropy = 7.40 bits
Computation based on 2087 words.
Number of 5-grams hit = 473  (22.66%)
Number of 4-grams hit = 544  (26.07%)
Number of 3-grams hit = 614  (29.42%)
Number of 2-grams hit = 356  (17.06%)
Number of 1-grams hit = 100  (4.79%)
11 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article349.text
Perplexity = 111.09, Entropy = 6.80 bits
Computation based on 1922 words.
Number of 5-grams hit = 613  (31.89%)
Number of 4-grams hit = 444  (23.10%)
Number of 3-grams hit = 493  (25.65%)
Number of 2-grams hit = 321  (16.70%)
Number of 1-grams hit = 51  (2.65%)
19 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article350.text
Perplexity = 169.73, Entropy = 7.41 bits
Computation based on 357 words.
Number of 5-grams hit = 80  (22.41%)
Number of 4-grams hit = 81  (22.69%)
Number of 3-grams hit = 102  (28.57%)
Number of 2-grams hit = 74  (20.73%)
Number of 1-grams hit = 20  (5.60%)
4 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article351.text
Perplexity = 138.52, Entropy = 7.11 bits
Computation based on 432 words.
Number of 5-grams hit = 85  (19.68%)
Number of 4-grams hit = 110  (25.46%)
Number of 3-grams hit = 137  (31.71%)
Number of 2-grams hit = 84  (19.44%)
Number of 1-grams hit = 16  (3.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article352.text
Perplexity = 202.64, Entropy = 7.66 bits
Computation based on 446 words.
Number of 5-grams hit = 106  (23.77%)
Number of 4-grams hit = 92  (20.63%)
Number of 3-grams hit = 115  (25.78%)
Number of 2-grams hit = 98  (21.97%)
Number of 1-grams hit = 35  (7.85%)
5 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article353.text
Perplexity = 168.18, Entropy = 7.39 bits
Computation based on 425 words.
Number of 5-grams hit = 115  (27.06%)
Number of 4-grams hit = 93  (21.88%)
Number of 3-grams hit = 117  (27.53%)
Number of 2-grams hit = 83  (19.53%)
Number of 1-grams hit = 17  (4.00%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article354.text
Perplexity = 187.90, Entropy = 7.55 bits
Computation based on 508 words.
Number of 5-grams hit = 103  (20.28%)
Number of 4-grams hit = 128  (25.20%)
Number of 3-grams hit = 154  (30.31%)
Number of 2-grams hit = 90  (17.72%)
Number of 1-grams hit = 33  (6.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article355.text
Perplexity = 214.29, Entropy = 7.74 bits
Computation based on 407 words.
Number of 5-grams hit = 85  (20.88%)
Number of 4-grams hit = 87  (21.38%)
Number of 3-grams hit = 105  (25.80%)
Number of 2-grams hit = 105  (25.80%)
Number of 1-grams hit = 25  (6.14%)
6 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article356.text
Perplexity = 217.86, Entropy = 7.77 bits
Computation based on 691 words.
Number of 5-grams hit = 108  (15.63%)
Number of 4-grams hit = 147  (21.27%)
Number of 3-grams hit = 232  (33.57%)
Number of 2-grams hit = 169  (24.46%)
Number of 1-grams hit = 35  (5.07%)
18 OOVs (2.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article357.text
Perplexity = 86.17, Entropy = 6.43 bits
Computation based on 747 words.
Number of 5-grams hit = 271  (36.28%)
Number of 4-grams hit = 169  (22.62%)
Number of 3-grams hit = 179  (23.96%)
Number of 2-grams hit = 112  (14.99%)
Number of 1-grams hit = 16  (2.14%)
8 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article358.text
Perplexity = 243.53, Entropy = 7.93 bits
Computation based on 1164 words.
Number of 5-grams hit = 257  (22.08%)
Number of 4-grams hit = 240  (20.62%)
Number of 3-grams hit = 326  (28.01%)
Number of 2-grams hit = 266  (22.85%)
Number of 1-grams hit = 75  (6.44%)
33 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article359.text
Perplexity = 34.84, Entropy = 5.12 bits
Computation based on 1254 words.
Number of 5-grams hit = 729  (58.13%)
Number of 4-grams hit = 164  (13.08%)
Number of 3-grams hit = 201  (16.03%)
Number of 2-grams hit = 135  (10.77%)
Number of 1-grams hit = 25  (1.99%)
16 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article360.text
Perplexity = 161.82, Entropy = 7.34 bits
Computation based on 710 words.
Number of 5-grams hit = 154  (21.69%)
Number of 4-grams hit = 175  (24.65%)
Number of 3-grams hit = 214  (30.14%)
Number of 2-grams hit = 143  (20.14%)
Number of 1-grams hit = 24  (3.38%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article361.text
Perplexity = 183.16, Entropy = 7.52 bits
Computation based on 1080 words.
Number of 5-grams hit = 236  (21.85%)
Number of 4-grams hit = 277  (25.65%)
Number of 3-grams hit = 309  (28.61%)
Number of 2-grams hit = 198  (18.33%)
Number of 1-grams hit = 60  (5.56%)
5 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article362.text
Perplexity = 136.24, Entropy = 7.09 bits
Computation based on 1417 words.
Number of 5-grams hit = 349  (24.63%)
Number of 4-grams hit = 386  (27.24%)
Number of 3-grams hit = 432  (30.49%)
Number of 2-grams hit = 204  (14.40%)
Number of 1-grams hit = 46  (3.25%)
6 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article363.text
Perplexity = 167.13, Entropy = 7.38 bits
Computation based on 1157 words.
Number of 5-grams hit = 261  (22.56%)
Number of 4-grams hit = 277  (23.94%)
Number of 3-grams hit = 359  (31.03%)
Number of 2-grams hit = 203  (17.55%)
Number of 1-grams hit = 57  (4.93%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article364.text
Perplexity = 115.98, Entropy = 6.86 bits
Computation based on 326 words.
Number of 5-grams hit = 78  (23.93%)
Number of 4-grams hit = 83  (25.46%)
Number of 3-grams hit = 86  (26.38%)
Number of 2-grams hit = 68  (20.86%)
Number of 1-grams hit = 11  (3.37%)
3 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article365.text
Perplexity = 148.35, Entropy = 7.21 bits
Computation based on 405 words.
Number of 5-grams hit = 82  (20.25%)
Number of 4-grams hit = 101  (24.94%)
Number of 3-grams hit = 135  (33.33%)
Number of 2-grams hit = 76  (18.77%)
Number of 1-grams hit = 11  (2.72%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article366.text
Perplexity = 268.71, Entropy = 8.07 bits
Computation based on 345 words.
Number of 5-grams hit = 50  (14.49%)
Number of 4-grams hit = 78  (22.61%)
Number of 3-grams hit = 107  (31.01%)
Number of 2-grams hit = 82  (23.77%)
Number of 1-grams hit = 28  (8.12%)
6 OOVs (1.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article367.text
Perplexity = 78.64, Entropy = 6.30 bits
Computation based on 906 words.
Number of 5-grams hit = 336  (37.09%)
Number of 4-grams hit = 212  (23.40%)
Number of 3-grams hit = 203  (22.41%)
Number of 2-grams hit = 125  (13.80%)
Number of 1-grams hit = 30  (3.31%)
5 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article368.text
Perplexity = 224.49, Entropy = 7.81 bits
Computation based on 320 words.
Number of 5-grams hit = 53  (16.56%)
Number of 4-grams hit = 72  (22.50%)
Number of 3-grams hit = 93  (29.06%)
Number of 2-grams hit = 81  (25.31%)
Number of 1-grams hit = 21  (6.56%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article369.text
Perplexity = 122.82, Entropy = 6.94 bits
Computation based on 316 words.
Number of 5-grams hit = 82  (25.95%)
Number of 4-grams hit = 83  (26.27%)
Number of 3-grams hit = 93  (29.43%)
Number of 2-grams hit = 46  (14.56%)
Number of 1-grams hit = 12  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article370.text
Perplexity = 163.52, Entropy = 7.35 bits
Computation based on 1644 words.
Number of 5-grams hit = 336  (20.44%)
Number of 4-grams hit = 432  (26.28%)
Number of 3-grams hit = 528  (32.12%)
Number of 2-grams hit = 283  (17.21%)
Number of 1-grams hit = 65  (3.95%)
3 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article371.text
Perplexity = 166.75, Entropy = 7.38 bits
Computation based on 262 words.
Number of 5-grams hit = 63  (24.05%)
Number of 4-grams hit = 69  (26.34%)
Number of 3-grams hit = 71  (27.10%)
Number of 2-grams hit = 41  (15.65%)
Number of 1-grams hit = 18  (6.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article372.text
Perplexity = 158.53, Entropy = 7.31 bits
Computation based on 405 words.
Number of 5-grams hit = 123  (30.37%)
Number of 4-grams hit = 86  (21.23%)
Number of 3-grams hit = 104  (25.68%)
Number of 2-grams hit = 74  (18.27%)
Number of 1-grams hit = 18  (4.44%)
16 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article373.text
Perplexity = 138.67, Entropy = 7.12 bits
Computation based on 505 words.
Number of 5-grams hit = 110  (21.78%)
Number of 4-grams hit = 145  (28.71%)
Number of 3-grams hit = 149  (29.50%)
Number of 2-grams hit = 89  (17.62%)
Number of 1-grams hit = 12  (2.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article374.text
Perplexity = 149.21, Entropy = 7.22 bits
Computation based on 1521 words.
Number of 5-grams hit = 395  (25.97%)
Number of 4-grams hit = 365  (24.00%)
Number of 3-grams hit = 427  (28.07%)
Number of 2-grams hit = 277  (18.21%)
Number of 1-grams hit = 57  (3.75%)
2 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article375.text
Perplexity = 78.71, Entropy = 6.30 bits
Computation based on 1608 words.
Number of 5-grams hit = 567  (35.26%)
Number of 4-grams hit = 422  (26.24%)
Number of 3-grams hit = 380  (23.63%)
Number of 2-grams hit = 207  (12.87%)
Number of 1-grams hit = 32  (1.99%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article376.text
Perplexity = 178.17, Entropy = 7.48 bits
Computation based on 560 words.
Number of 5-grams hit = 125  (22.32%)
Number of 4-grams hit = 146  (26.07%)
Number of 3-grams hit = 157  (28.04%)
Number of 2-grams hit = 106  (18.93%)
Number of 1-grams hit = 26  (4.64%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article377.text
Perplexity = 177.07, Entropy = 7.47 bits
Computation based on 738 words.
Number of 5-grams hit = 170  (23.04%)
Number of 4-grams hit = 180  (24.39%)
Number of 3-grams hit = 214  (29.00%)
Number of 2-grams hit = 134  (18.16%)
Number of 1-grams hit = 40  (5.42%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article378.text
Perplexity = 276.93, Entropy = 8.11 bits
Computation based on 609 words.
Number of 5-grams hit = 87  (14.29%)
Number of 4-grams hit = 120  (19.70%)
Number of 3-grams hit = 195  (32.02%)
Number of 2-grams hit = 167  (27.42%)
Number of 1-grams hit = 40  (6.57%)
13 OOVs (2.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article379.text
Perplexity = 146.24, Entropy = 7.19 bits
Computation based on 2558 words.
Number of 5-grams hit = 532  (20.80%)
Number of 4-grams hit = 705  (27.56%)
Number of 3-grams hit = 808  (31.59%)
Number of 2-grams hit = 427  (16.69%)
Number of 1-grams hit = 86  (3.36%)
10 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article380.text
Perplexity = 213.45, Entropy = 7.74 bits
Computation based on 346 words.
Number of 5-grams hit = 70  (20.23%)
Number of 4-grams hit = 86  (24.86%)
Number of 3-grams hit = 96  (27.75%)
Number of 2-grams hit = 77  (22.25%)
Number of 1-grams hit = 17  (4.91%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article381.text
Perplexity = 19.80, Entropy = 4.31 bits
Computation based on 622 words.
Number of 5-grams hit = 466  (74.92%)
Number of 4-grams hit = 60  (9.65%)
Number of 3-grams hit = 50  (8.04%)
Number of 2-grams hit = 35  (5.63%)
Number of 1-grams hit = 11  (1.77%)
6 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article382.text
Perplexity = 136.89, Entropy = 7.10 bits
Computation based on 1302 words.
Number of 5-grams hit = 316  (24.27%)
Number of 4-grams hit = 341  (26.19%)
Number of 3-grams hit = 385  (29.57%)
Number of 2-grams hit = 214  (16.44%)
Number of 1-grams hit = 46  (3.53%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article383.text
Perplexity = 186.31, Entropy = 7.54 bits
Computation based on 1031 words.
Number of 5-grams hit = 237  (22.99%)
Number of 4-grams hit = 251  (24.35%)
Number of 3-grams hit = 303  (29.39%)
Number of 2-grams hit = 186  (18.04%)
Number of 1-grams hit = 54  (5.24%)
8 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article384.text
Perplexity = 176.02, Entropy = 7.46 bits
Computation based on 1005 words.
Number of 5-grams hit = 207  (20.60%)
Number of 4-grams hit = 240  (23.88%)
Number of 3-grams hit = 310  (30.85%)
Number of 2-grams hit = 202  (20.10%)
Number of 1-grams hit = 46  (4.58%)
3 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article385.text
Perplexity = 17.44, Entropy = 4.12 bits
Computation based on 2592 words.
Number of 5-grams hit = 1929  (74.42%)
Number of 4-grams hit = 230  (8.87%)
Number of 3-grams hit = 270  (10.42%)
Number of 2-grams hit = 144  (5.56%)
Number of 1-grams hit = 19  (0.73%)
14 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article386.text
Perplexity = 127.09, Entropy = 6.99 bits
Computation based on 405 words.
Number of 5-grams hit = 119  (29.38%)
Number of 4-grams hit = 85  (20.99%)
Number of 3-grams hit = 102  (25.19%)
Number of 2-grams hit = 85  (20.99%)
Number of 1-grams hit = 14  (3.46%)
12 OOVs (2.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article387.text
Perplexity = 141.37, Entropy = 7.14 bits
Computation based on 1197 words.
Number of 5-grams hit = 324  (27.07%)
Number of 4-grams hit = 265  (22.14%)
Number of 3-grams hit = 328  (27.40%)
Number of 2-grams hit = 233  (19.47%)
Number of 1-grams hit = 47  (3.93%)
18 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article388.text
Perplexity = 154.42, Entropy = 7.27 bits
Computation based on 4328 words.
Number of 5-grams hit = 991  (22.90%)
Number of 4-grams hit = 1090  (25.18%)
Number of 3-grams hit = 1311  (30.29%)
Number of 2-grams hit = 741  (17.12%)
Number of 1-grams hit = 195  (4.51%)
11 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article389.text
Perplexity = 169.61, Entropy = 7.41 bits
Computation based on 1099 words.
Number of 5-grams hit = 282  (25.66%)
Number of 4-grams hit = 235  (21.38%)
Number of 3-grams hit = 279  (25.39%)
Number of 2-grams hit = 246  (22.38%)
Number of 1-grams hit = 57  (5.19%)
22 OOVs (1.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article390.text
Perplexity = 154.05, Entropy = 7.27 bits
Computation based on 1177 words.
Number of 5-grams hit = 259  (22.01%)
Number of 4-grams hit = 293  (24.89%)
Number of 3-grams hit = 329  (27.95%)
Number of 2-grams hit = 253  (21.50%)
Number of 1-grams hit = 43  (3.65%)
23 OOVs (1.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article391.text
Perplexity = 118.05, Entropy = 6.88 bits
Computation based on 3505 words.
Number of 5-grams hit = 1056  (30.13%)
Number of 4-grams hit = 774  (22.08%)
Number of 3-grams hit = 919  (26.22%)
Number of 2-grams hit = 635  (18.12%)
Number of 1-grams hit = 121  (3.45%)
36 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article392.text
Perplexity = 129.77, Entropy = 7.02 bits
Computation based on 231 words.
Number of 5-grams hit = 51  (22.08%)
Number of 4-grams hit = 68  (29.44%)
Number of 3-grams hit = 71  (30.74%)
Number of 2-grams hit = 36  (15.58%)
Number of 1-grams hit = 5  (2.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article393.text
Perplexity = 244.81, Entropy = 7.94 bits
Computation based on 481 words.
Number of 5-grams hit = 99  (20.58%)
Number of 4-grams hit = 89  (18.50%)
Number of 3-grams hit = 131  (27.23%)
Number of 2-grams hit = 122  (25.36%)
Number of 1-grams hit = 40  (8.32%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article394.text
Perplexity = 222.31, Entropy = 7.80 bits
Computation based on 390 words.
Number of 5-grams hit = 75  (19.23%)
Number of 4-grams hit = 86  (22.05%)
Number of 3-grams hit = 114  (29.23%)
Number of 2-grams hit = 86  (22.05%)
Number of 1-grams hit = 29  (7.44%)
5 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article395.text
Perplexity = 114.19, Entropy = 6.84 bits
Computation based on 304 words.
Number of 5-grams hit = 70  (23.03%)
Number of 4-grams hit = 82  (26.97%)
Number of 3-grams hit = 99  (32.57%)
Number of 2-grams hit = 47  (15.46%)
Number of 1-grams hit = 6  (1.97%)
3 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article396.text
Perplexity = 69.65, Entropy = 6.12 bits
Computation based on 3049 words.
Number of 5-grams hit = 1324  (43.42%)
Number of 4-grams hit = 643  (21.09%)
Number of 3-grams hit = 654  (21.45%)
Number of 2-grams hit = 370  (12.14%)
Number of 1-grams hit = 58  (1.90%)
3 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article397.text
Perplexity = 178.16, Entropy = 7.48 bits
Computation based on 3657 words.
Number of 5-grams hit = 820  (22.42%)
Number of 4-grams hit = 887  (24.25%)
Number of 3-grams hit = 1090  (29.81%)
Number of 2-grams hit = 679  (18.57%)
Number of 1-grams hit = 181  (4.95%)
16 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article398.text
Perplexity = 185.85, Entropy = 7.54 bits
Computation based on 815 words.
Number of 5-grams hit = 166  (20.37%)
Number of 4-grams hit = 207  (25.40%)
Number of 3-grams hit = 251  (30.80%)
Number of 2-grams hit = 159  (19.51%)
Number of 1-grams hit = 32  (3.93%)
5 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article399.text
Perplexity = 170.77, Entropy = 7.42 bits
Computation based on 3657 words.
Number of 5-grams hit = 786  (21.49%)
Number of 4-grams hit = 922  (25.21%)
Number of 3-grams hit = 1156  (31.61%)
Number of 2-grams hit = 647  (17.69%)
Number of 1-grams hit = 146  (3.99%)
15 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article400.text
Perplexity = 166.45, Entropy = 7.38 bits
Computation based on 582 words.
Number of 5-grams hit = 113  (19.42%)
Number of 4-grams hit = 153  (26.29%)
Number of 3-grams hit = 176  (30.24%)
Number of 2-grams hit = 121  (20.79%)
Number of 1-grams hit = 19  (3.26%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article401.text
Perplexity = 209.90, Entropy = 7.71 bits
Computation based on 652 words.
Number of 5-grams hit = 146  (22.39%)
Number of 4-grams hit = 126  (19.33%)
Number of 3-grams hit = 174  (26.69%)
Number of 2-grams hit = 168  (25.77%)
Number of 1-grams hit = 38  (5.83%)
11 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article402.text
Perplexity = 116.93, Entropy = 6.87 bits
Computation based on 3662 words.
Number of 5-grams hit = 1112  (30.37%)
Number of 4-grams hit = 813  (22.20%)
Number of 3-grams hit = 924  (25.23%)
Number of 2-grams hit = 677  (18.49%)
Number of 1-grams hit = 136  (3.71%)
8 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article403.text
Perplexity = 156.04, Entropy = 7.29 bits
Computation based on 621 words.
Number of 5-grams hit = 142  (22.87%)
Number of 4-grams hit = 150  (24.15%)
Number of 3-grams hit = 176  (28.34%)
Number of 2-grams hit = 129  (20.77%)
Number of 1-grams hit = 24  (3.86%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article404.text
Perplexity = 234.19, Entropy = 7.87 bits
Computation based on 213 words.
Number of 5-grams hit = 34  (15.96%)
Number of 4-grams hit = 52  (24.41%)
Number of 3-grams hit = 66  (30.99%)
Number of 2-grams hit = 46  (21.60%)
Number of 1-grams hit = 15  (7.04%)
1 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article405.text
Perplexity = 233.62, Entropy = 7.87 bits
Computation based on 159 words.
Number of 5-grams hit = 31  (19.50%)
Number of 4-grams hit = 28  (17.61%)
Number of 3-grams hit = 50  (31.45%)
Number of 2-grams hit = 38  (23.90%)
Number of 1-grams hit = 12  (7.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article406.text
Perplexity = 163.93, Entropy = 7.36 bits
Computation based on 982 words.
Number of 5-grams hit = 216  (22.00%)
Number of 4-grams hit = 218  (22.20%)
Number of 3-grams hit = 287  (29.23%)
Number of 2-grams hit = 211  (21.49%)
Number of 1-grams hit = 50  (5.09%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article407.text
Perplexity = 168.92, Entropy = 7.40 bits
Computation based on 696 words.
Number of 5-grams hit = 160  (22.99%)
Number of 4-grams hit = 171  (24.57%)
Number of 3-grams hit = 207  (29.74%)
Number of 2-grams hit = 125  (17.96%)
Number of 1-grams hit = 33  (4.74%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article408.text
Perplexity = 114.04, Entropy = 6.83 bits
Computation based on 438 words.
Number of 5-grams hit = 137  (31.28%)
Number of 4-grams hit = 83  (18.95%)
Number of 3-grams hit = 111  (25.34%)
Number of 2-grams hit = 87  (19.86%)
Number of 1-grams hit = 20  (4.57%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article409.text
Perplexity = 169.66, Entropy = 7.41 bits
Computation based on 541 words.
Number of 5-grams hit = 107  (19.78%)
Number of 4-grams hit = 123  (22.74%)
Number of 3-grams hit = 190  (35.12%)
Number of 2-grams hit = 98  (18.11%)
Number of 1-grams hit = 23  (4.25%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article410.text
Perplexity = 111.45, Entropy = 6.80 bits
Computation based on 235 words.
Number of 5-grams hit = 67  (28.51%)
Number of 4-grams hit = 62  (26.38%)
Number of 3-grams hit = 68  (28.94%)
Number of 2-grams hit = 32  (13.62%)
Number of 1-grams hit = 6  (2.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article411.text
Perplexity = 98.51, Entropy = 6.62 bits
Computation based on 436 words.
Number of 5-grams hit = 139  (31.88%)
Number of 4-grams hit = 98  (22.48%)
Number of 3-grams hit = 105  (24.08%)
Number of 2-grams hit = 75  (17.20%)
Number of 1-grams hit = 19  (4.36%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article412.text
Perplexity = 77.46, Entropy = 6.28 bits
Computation based on 4559 words.
Number of 5-grams hit = 1552  (34.04%)
Number of 4-grams hit = 1149  (25.20%)
Number of 3-grams hit = 1160  (25.44%)
Number of 2-grams hit = 620  (13.60%)
Number of 1-grams hit = 78  (1.71%)
9 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article413.text
Perplexity = 158.08, Entropy = 7.30 bits
Computation based on 3853 words.
Number of 5-grams hit = 896  (23.25%)
Number of 4-grams hit = 994  (25.80%)
Number of 3-grams hit = 1124  (29.17%)
Number of 2-grams hit = 679  (17.62%)
Number of 1-grams hit = 160  (4.15%)
14 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article414.text
Perplexity = 142.15, Entropy = 7.15 bits
Computation based on 399 words.
Number of 5-grams hit = 97  (24.31%)
Number of 4-grams hit = 106  (26.57%)
Number of 3-grams hit = 114  (28.57%)
Number of 2-grams hit = 64  (16.04%)
Number of 1-grams hit = 18  (4.51%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article415.text
Perplexity = 240.57, Entropy = 7.91 bits
Computation based on 475 words.
Number of 5-grams hit = 85  (17.89%)
Number of 4-grams hit = 99  (20.84%)
Number of 3-grams hit = 142  (29.89%)
Number of 2-grams hit = 119  (25.05%)
Number of 1-grams hit = 30  (6.32%)
5 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article416.text
Perplexity = 176.44, Entropy = 7.46 bits
Computation based on 585 words.
Number of 5-grams hit = 113  (19.32%)
Number of 4-grams hit = 128  (21.88%)
Number of 3-grams hit = 173  (29.57%)
Number of 2-grams hit = 146  (24.96%)
Number of 1-grams hit = 25  (4.27%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article417.text
Perplexity = 103.48, Entropy = 6.69 bits
Computation based on 850 words.
Number of 5-grams hit = 274  (32.24%)
Number of 4-grams hit = 189  (22.24%)
Number of 3-grams hit = 211  (24.82%)
Number of 2-grams hit = 161  (18.94%)
Number of 1-grams hit = 15  (1.76%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article418.text
Perplexity = 148.29, Entropy = 7.21 bits
Computation based on 807 words.
Number of 5-grams hit = 207  (25.65%)
Number of 4-grams hit = 147  (18.22%)
Number of 3-grams hit = 221  (27.39%)
Number of 2-grams hit = 185  (22.92%)
Number of 1-grams hit = 47  (5.82%)
5 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article419.text
Perplexity = 143.27, Entropy = 7.16 bits
Computation based on 549 words.
Number of 5-grams hit = 128  (23.32%)
Number of 4-grams hit = 121  (22.04%)
Number of 3-grams hit = 153  (27.87%)
Number of 2-grams hit = 123  (22.40%)
Number of 1-grams hit = 24  (4.37%)
5 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article420.text
Perplexity = 174.04, Entropy = 7.44 bits
Computation based on 610 words.
Number of 5-grams hit = 120  (19.67%)
Number of 4-grams hit = 162  (26.56%)
Number of 3-grams hit = 187  (30.66%)
Number of 2-grams hit = 118  (19.34%)
Number of 1-grams hit = 23  (3.77%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article421.text
Perplexity = 126.91, Entropy = 6.99 bits
Computation based on 819 words.
Number of 5-grams hit = 216  (26.37%)
Number of 4-grams hit = 205  (25.03%)
Number of 3-grams hit = 236  (28.82%)
Number of 2-grams hit = 138  (16.85%)
Number of 1-grams hit = 24  (2.93%)
2 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article422.text
Perplexity = 134.17, Entropy = 7.07 bits
Computation based on 551 words.
Number of 5-grams hit = 168  (30.49%)
Number of 4-grams hit = 114  (20.69%)
Number of 3-grams hit = 131  (23.77%)
Number of 2-grams hit = 104  (18.87%)
Number of 1-grams hit = 34  (6.17%)
8 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article423.text
Perplexity = 248.96, Entropy = 7.96 bits
Computation based on 599 words.
Number of 5-grams hit = 80  (13.36%)
Number of 4-grams hit = 132  (22.04%)
Number of 3-grams hit = 189  (31.55%)
Number of 2-grams hit = 158  (26.38%)
Number of 1-grams hit = 40  (6.68%)
17 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article424.text
Perplexity = 139.55, Entropy = 7.12 bits
Computation based on 524 words.
Number of 5-grams hit = 117  (22.33%)
Number of 4-grams hit = 140  (26.72%)
Number of 3-grams hit = 154  (29.39%)
Number of 2-grams hit = 94  (17.94%)
Number of 1-grams hit = 19  (3.63%)
5 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article425.text
Perplexity = 151.12, Entropy = 7.24 bits
Computation based on 355 words.
Number of 5-grams hit = 67  (18.87%)
Number of 4-grams hit = 93  (26.20%)
Number of 3-grams hit = 112  (31.55%)
Number of 2-grams hit = 70  (19.72%)
Number of 1-grams hit = 13  (3.66%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article426.text
Perplexity = 131.22, Entropy = 7.04 bits
Computation based on 484 words.
Number of 5-grams hit = 129  (26.65%)
Number of 4-grams hit = 117  (24.17%)
Number of 3-grams hit = 141  (29.13%)
Number of 2-grams hit = 79  (16.32%)
Number of 1-grams hit = 18  (3.72%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article427.text
Perplexity = 104.42, Entropy = 6.71 bits
Computation based on 787 words.
Number of 5-grams hit = 269  (34.18%)
Number of 4-grams hit = 185  (23.51%)
Number of 3-grams hit = 163  (20.71%)
Number of 2-grams hit = 142  (18.04%)
Number of 1-grams hit = 28  (3.56%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article428.text
Perplexity = 238.86, Entropy = 7.90 bits
Computation based on 626 words.
Number of 5-grams hit = 111  (17.73%)
Number of 4-grams hit = 152  (24.28%)
Number of 3-grams hit = 187  (29.87%)
Number of 2-grams hit = 144  (23.00%)
Number of 1-grams hit = 32  (5.11%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article429.text
Perplexity = 126.72, Entropy = 6.99 bits
Computation based on 769 words.
Number of 5-grams hit = 194  (25.23%)
Number of 4-grams hit = 203  (26.40%)
Number of 3-grams hit = 221  (28.74%)
Number of 2-grams hit = 129  (16.78%)
Number of 1-grams hit = 22  (2.86%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article430.text
Perplexity = 161.48, Entropy = 7.34 bits
Computation based on 639 words.
Number of 5-grams hit = 134  (20.97%)
Number of 4-grams hit = 155  (24.26%)
Number of 3-grams hit = 197  (30.83%)
Number of 2-grams hit = 128  (20.03%)
Number of 1-grams hit = 25  (3.91%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article431.text
Perplexity = 135.59, Entropy = 7.08 bits
Computation based on 569 words.
Number of 5-grams hit = 128  (22.50%)
Number of 4-grams hit = 152  (26.71%)
Number of 3-grams hit = 179  (31.46%)
Number of 2-grams hit = 89  (15.64%)
Number of 1-grams hit = 21  (3.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article432.text
Perplexity = 157.79, Entropy = 7.30 bits
Computation based on 351 words.
Number of 5-grams hit = 77  (21.94%)
Number of 4-grams hit = 75  (21.37%)
Number of 3-grams hit = 114  (32.48%)
Number of 2-grams hit = 72  (20.51%)
Number of 1-grams hit = 13  (3.70%)
2 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article433.text
Perplexity = 132.80, Entropy = 7.05 bits
Computation based on 967 words.
Number of 5-grams hit = 227  (23.47%)
Number of 4-grams hit = 253  (26.16%)
Number of 3-grams hit = 293  (30.30%)
Number of 2-grams hit = 159  (16.44%)
Number of 1-grams hit = 35  (3.62%)
5 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article434.text
Perplexity = 74.77, Entropy = 6.22 bits
Computation based on 381 words.
Number of 5-grams hit = 158  (41.47%)
Number of 4-grams hit = 68  (17.85%)
Number of 3-grams hit = 75  (19.69%)
Number of 2-grams hit = 64  (16.80%)
Number of 1-grams hit = 16  (4.20%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article435.text
Perplexity = 107.36, Entropy = 6.75 bits
Computation based on 711 words.
Number of 5-grams hit = 253  (35.58%)
Number of 4-grams hit = 151  (21.24%)
Number of 3-grams hit = 165  (23.21%)
Number of 2-grams hit = 121  (17.02%)
Number of 1-grams hit = 21  (2.95%)
8 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article436.text
Perplexity = 163.51, Entropy = 7.35 bits
Computation based on 531 words.
Number of 5-grams hit = 115  (21.66%)
Number of 4-grams hit = 126  (23.73%)
Number of 3-grams hit = 168  (31.64%)
Number of 2-grams hit = 102  (19.21%)
Number of 1-grams hit = 20  (3.77%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article437.text
Perplexity = 186.26, Entropy = 7.54 bits
Computation based on 1007 words.
Number of 5-grams hit = 178  (17.68%)
Number of 4-grams hit = 249  (24.73%)
Number of 3-grams hit = 347  (34.46%)
Number of 2-grams hit = 183  (18.17%)
Number of 1-grams hit = 50  (4.97%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article438.text
Perplexity = 172.53, Entropy = 7.43 bits
Computation based on 290 words.
Number of 5-grams hit = 73  (25.17%)
Number of 4-grams hit = 67  (23.10%)
Number of 3-grams hit = 76  (26.21%)
Number of 2-grams hit = 57  (19.66%)
Number of 1-grams hit = 17  (5.86%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article439.text
Perplexity = 232.75, Entropy = 7.86 bits
Computation based on 516 words.
Number of 5-grams hit = 83  (16.09%)
Number of 4-grams hit = 114  (22.09%)
Number of 3-grams hit = 162  (31.40%)
Number of 2-grams hit = 124  (24.03%)
Number of 1-grams hit = 33  (6.40%)
8 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article440.text
Perplexity = 100.68, Entropy = 6.65 bits
Computation based on 688 words.
Number of 5-grams hit = 216  (31.40%)
Number of 4-grams hit = 164  (23.84%)
Number of 3-grams hit = 158  (22.97%)
Number of 2-grams hit = 128  (18.60%)
Number of 1-grams hit = 22  (3.20%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article441.text
Perplexity = 184.30, Entropy = 7.53 bits
Computation based on 400 words.
Number of 5-grams hit = 76  (19.00%)
Number of 4-grams hit = 87  (21.75%)
Number of 3-grams hit = 112  (28.00%)
Number of 2-grams hit = 103  (25.75%)
Number of 1-grams hit = 22  (5.50%)
13 OOVs (3.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article442.text
Perplexity = 82.30, Entropy = 6.36 bits
Computation based on 10008 words.
Number of 5-grams hit = 3649  (36.46%)
Number of 4-grams hit = 2441  (24.39%)
Number of 3-grams hit = 2448  (24.46%)
Number of 2-grams hit = 1268  (12.67%)
Number of 1-grams hit = 202  (2.02%)
23 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article443.text
Perplexity = 172.80, Entropy = 7.43 bits
Computation based on 11815 words.
Number of 5-grams hit = 2548  (21.57%)
Number of 4-grams hit = 3039  (25.72%)
Number of 3-grams hit = 3583  (30.33%)
Number of 2-grams hit = 2144  (18.15%)
Number of 1-grams hit = 501  (4.24%)
39 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article444.text
Perplexity = 139.03, Entropy = 7.12 bits
Computation based on 643 words.
Number of 5-grams hit = 171  (26.59%)
Number of 4-grams hit = 134  (20.84%)
Number of 3-grams hit = 169  (26.28%)
Number of 2-grams hit = 141  (21.93%)
Number of 1-grams hit = 28  (4.35%)
8 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article445.text
Perplexity = 116.44, Entropy = 6.86 bits
Computation based on 501 words.
Number of 5-grams hit = 146  (29.14%)
Number of 4-grams hit = 120  (23.95%)
Number of 3-grams hit = 142  (28.34%)
Number of 2-grams hit = 74  (14.77%)
Number of 1-grams hit = 19  (3.79%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article446.text
Perplexity = 173.05, Entropy = 7.44 bits
Computation based on 894 words.
Number of 5-grams hit = 165  (18.46%)
Number of 4-grams hit = 234  (26.17%)
Number of 3-grams hit = 285  (31.88%)
Number of 2-grams hit = 177  (19.80%)
Number of 1-grams hit = 33  (3.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article447.text
Perplexity = 166.42, Entropy = 7.38 bits
Computation based on 722 words.
Number of 5-grams hit = 180  (24.93%)
Number of 4-grams hit = 168  (23.27%)
Number of 3-grams hit = 183  (25.35%)
Number of 2-grams hit = 159  (22.02%)
Number of 1-grams hit = 32  (4.43%)
9 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article448.text
Perplexity = 93.17, Entropy = 6.54 bits
Computation based on 323 words.
Number of 5-grams hit = 104  (32.20%)
Number of 4-grams hit = 77  (23.84%)
Number of 3-grams hit = 90  (27.86%)
Number of 2-grams hit = 46  (14.24%)
Number of 1-grams hit = 6  (1.86%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article449.text
Perplexity = 150.85, Entropy = 7.24 bits
Computation based on 483 words.
Number of 5-grams hit = 96  (19.88%)
Number of 4-grams hit = 139  (28.78%)
Number of 3-grams hit = 159  (32.92%)
Number of 2-grams hit = 66  (13.66%)
Number of 1-grams hit = 23  (4.76%)
5 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article450.text
Perplexity = 121.87, Entropy = 6.93 bits
Computation based on 356 words.
Number of 5-grams hit = 91  (25.56%)
Number of 4-grams hit = 84  (23.60%)
Number of 3-grams hit = 105  (29.49%)
Number of 2-grams hit = 67  (18.82%)
Number of 1-grams hit = 9  (2.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article451.text
Perplexity = 117.40, Entropy = 6.88 bits
Computation based on 571 words.
Number of 5-grams hit = 155  (27.15%)
Number of 4-grams hit = 120  (21.02%)
Number of 3-grams hit = 155  (27.15%)
Number of 2-grams hit = 114  (19.96%)
Number of 1-grams hit = 27  (4.73%)
3 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article452.text
Perplexity = 152.95, Entropy = 7.26 bits
Computation based on 237 words.
Number of 5-grams hit = 49  (20.68%)
Number of 4-grams hit = 72  (30.38%)
Number of 3-grams hit = 67  (28.27%)
Number of 2-grams hit = 36  (15.19%)
Number of 1-grams hit = 13  (5.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article453.text
Perplexity = 141.43, Entropy = 7.14 bits
Computation based on 625 words.
Number of 5-grams hit = 160  (25.60%)
Number of 4-grams hit = 141  (22.56%)
Number of 3-grams hit = 170  (27.20%)
Number of 2-grams hit = 129  (20.64%)
Number of 1-grams hit = 25  (4.00%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article454.text
Perplexity = 161.52, Entropy = 7.34 bits
Computation based on 544 words.
Number of 5-grams hit = 122  (22.43%)
Number of 4-grams hit = 138  (25.37%)
Number of 3-grams hit = 161  (29.60%)
Number of 2-grams hit = 100  (18.38%)
Number of 1-grams hit = 23  (4.23%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article455.text
Perplexity = 82.44, Entropy = 6.37 bits
Computation based on 379 words.
Number of 5-grams hit = 133  (35.09%)
Number of 4-grams hit = 65  (17.15%)
Number of 3-grams hit = 104  (27.44%)
Number of 2-grams hit = 66  (17.41%)
Number of 1-grams hit = 11  (2.90%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article456.text
Perplexity = 170.96, Entropy = 7.42 bits
Computation based on 656 words.
Number of 5-grams hit = 99  (15.09%)
Number of 4-grams hit = 187  (28.51%)
Number of 3-grams hit = 220  (33.54%)
Number of 2-grams hit = 120  (18.29%)
Number of 1-grams hit = 30  (4.57%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article457.text
Perplexity = 186.80, Entropy = 7.55 bits
Computation based on 337 words.
Number of 5-grams hit = 52  (15.43%)
Number of 4-grams hit = 101  (29.97%)
Number of 3-grams hit = 108  (32.05%)
Number of 2-grams hit = 60  (17.80%)
Number of 1-grams hit = 16  (4.75%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article458.text
Perplexity = 153.00, Entropy = 7.26 bits
Computation based on 276 words.
Number of 5-grams hit = 64  (23.19%)
Number of 4-grams hit = 69  (25.00%)
Number of 3-grams hit = 86  (31.16%)
Number of 2-grams hit = 48  (17.39%)
Number of 1-grams hit = 9  (3.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article459.text
Perplexity = 160.01, Entropy = 7.32 bits
Computation based on 700 words.
Number of 5-grams hit = 170  (24.29%)
Number of 4-grams hit = 167  (23.86%)
Number of 3-grams hit = 209  (29.86%)
Number of 2-grams hit = 122  (17.43%)
Number of 1-grams hit = 32  (4.57%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article460.text
Perplexity = 200.05, Entropy = 7.64 bits
Computation based on 326 words.
Number of 5-grams hit = 75  (23.01%)
Number of 4-grams hit = 78  (23.93%)
Number of 3-grams hit = 91  (27.91%)
Number of 2-grams hit = 67  (20.55%)
Number of 1-grams hit = 15  (4.60%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article461.text
Perplexity = 177.42, Entropy = 7.47 bits
Computation based on 419 words.
Number of 5-grams hit = 95  (22.67%)
Number of 4-grams hit = 79  (18.85%)
Number of 3-grams hit = 117  (27.92%)
Number of 2-grams hit = 105  (25.06%)
Number of 1-grams hit = 23  (5.49%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article462.text
Perplexity = 204.10, Entropy = 7.67 bits
Computation based on 1406 words.
Number of 5-grams hit = 317  (22.55%)
Number of 4-grams hit = 320  (22.76%)
Number of 3-grams hit = 420  (29.87%)
Number of 2-grams hit = 280  (19.91%)
Number of 1-grams hit = 69  (4.91%)
4 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article463.text
Perplexity = 157.13, Entropy = 7.30 bits
Computation based on 543 words.
Number of 5-grams hit = 123  (22.65%)
Number of 4-grams hit = 136  (25.05%)
Number of 3-grams hit = 145  (26.70%)
Number of 2-grams hit = 117  (21.55%)
Number of 1-grams hit = 22  (4.05%)
12 OOVs (2.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article464.text
Perplexity = 211.94, Entropy = 7.73 bits
Computation based on 421 words.
Number of 5-grams hit = 77  (18.29%)
Number of 4-grams hit = 101  (23.99%)
Number of 3-grams hit = 125  (29.69%)
Number of 2-grams hit = 97  (23.04%)
Number of 1-grams hit = 21  (4.99%)
19 OOVs (4.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article465.text
Perplexity = 84.50, Entropy = 6.40 bits
Computation based on 1805 words.
Number of 5-grams hit = 665  (36.84%)
Number of 4-grams hit = 412  (22.83%)
Number of 3-grams hit = 429  (23.77%)
Number of 2-grams hit = 260  (14.40%)
Number of 1-grams hit = 39  (2.16%)
2 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article466.text
Perplexity = 140.60, Entropy = 7.14 bits
Computation based on 204 words.
Number of 5-grams hit = 49  (24.02%)
Number of 4-grams hit = 48  (23.53%)
Number of 3-grams hit = 66  (32.35%)
Number of 2-grams hit = 32  (15.69%)
Number of 1-grams hit = 9  (4.41%)
1 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article467.text
Perplexity = 134.47, Entropy = 7.07 bits
Computation based on 1028 words.
Number of 5-grams hit = 269  (26.17%)
Number of 4-grams hit = 258  (25.10%)
Number of 3-grams hit = 298  (28.99%)
Number of 2-grams hit = 167  (16.25%)
Number of 1-grams hit = 36  (3.50%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article468.text
Perplexity = 172.20, Entropy = 7.43 bits
Computation based on 776 words.
Number of 5-grams hit = 155  (19.97%)
Number of 4-grams hit = 203  (26.16%)
Number of 3-grams hit = 234  (30.15%)
Number of 2-grams hit = 146  (18.81%)
Number of 1-grams hit = 38  (4.90%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article469.text
Perplexity = 180.63, Entropy = 7.50 bits
Computation based on 355 words.
Number of 5-grams hit = 70  (19.72%)
Number of 4-grams hit = 86  (24.23%)
Number of 3-grams hit = 113  (31.83%)
Number of 2-grams hit = 65  (18.31%)
Number of 1-grams hit = 21  (5.92%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article470.text
Perplexity = 297.58, Entropy = 8.22 bits
Computation based on 235 words.
Number of 5-grams hit = 45  (19.15%)
Number of 4-grams hit = 37  (15.74%)
Number of 3-grams hit = 70  (29.79%)
Number of 2-grams hit = 63  (26.81%)
Number of 1-grams hit = 20  (8.51%)
5 OOVs (2.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article471.text
Perplexity = 157.15, Entropy = 7.30 bits
Computation based on 1994 words.
Number of 5-grams hit = 411  (20.61%)
Number of 4-grams hit = 545  (27.33%)
Number of 3-grams hit = 632  (31.70%)
Number of 2-grams hit = 334  (16.75%)
Number of 1-grams hit = 72  (3.61%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article472.text
Perplexity = 205.12, Entropy = 7.68 bits
Computation based on 930 words.
Number of 5-grams hit = 216  (23.23%)
Number of 4-grams hit = 188  (20.22%)
Number of 3-grams hit = 250  (26.88%)
Number of 2-grams hit = 217  (23.33%)
Number of 1-grams hit = 59  (6.34%)
6 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article473.text
Perplexity = 142.35, Entropy = 7.15 bits
Computation based on 219 words.
Number of 5-grams hit = 71  (32.42%)
Number of 4-grams hit = 53  (24.20%)
Number of 3-grams hit = 49  (22.37%)
Number of 2-grams hit = 34  (15.53%)
Number of 1-grams hit = 12  (5.48%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article474.text
Perplexity = 166.55, Entropy = 7.38 bits
Computation based on 775 words.
Number of 5-grams hit = 193  (24.90%)
Number of 4-grams hit = 173  (22.32%)
Number of 3-grams hit = 209  (26.97%)
Number of 2-grams hit = 166  (21.42%)
Number of 1-grams hit = 34  (4.39%)
9 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article475.text
Perplexity = 174.03, Entropy = 7.44 bits
Computation based on 546 words.
Number of 5-grams hit = 91  (16.67%)
Number of 4-grams hit = 125  (22.89%)
Number of 3-grams hit = 181  (33.15%)
Number of 2-grams hit = 129  (23.63%)
Number of 1-grams hit = 20  (3.66%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article476.text
Perplexity = 225.27, Entropy = 7.82 bits
Computation based on 582 words.
Number of 5-grams hit = 98  (16.84%)
Number of 4-grams hit = 141  (24.23%)
Number of 3-grams hit = 196  (33.68%)
Number of 2-grams hit = 109  (18.73%)
Number of 1-grams hit = 38  (6.53%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article477.text
Perplexity = 125.90, Entropy = 6.98 bits
Computation based on 444 words.
Number of 5-grams hit = 120  (27.03%)
Number of 4-grams hit = 97  (21.85%)
Number of 3-grams hit = 126  (28.38%)
Number of 2-grams hit = 79  (17.79%)
Number of 1-grams hit = 22  (4.95%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article478.text
Perplexity = 212.54, Entropy = 7.73 bits
Computation based on 507 words.
Number of 5-grams hit = 100  (19.72%)
Number of 4-grams hit = 119  (23.47%)
Number of 3-grams hit = 163  (32.15%)
Number of 2-grams hit = 93  (18.34%)
Number of 1-grams hit = 32  (6.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article479.text
Perplexity = 164.90, Entropy = 7.37 bits
Computation based on 421 words.
Number of 5-grams hit = 71  (16.86%)
Number of 4-grams hit = 105  (24.94%)
Number of 3-grams hit = 145  (34.44%)
Number of 2-grams hit = 85  (20.19%)
Number of 1-grams hit = 15  (3.56%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article480.text
Perplexity = 129.82, Entropy = 7.02 bits
Computation based on 684 words.
Number of 5-grams hit = 177  (25.88%)
Number of 4-grams hit = 165  (24.12%)
Number of 3-grams hit = 211  (30.85%)
Number of 2-grams hit = 112  (16.37%)
Number of 1-grams hit = 19  (2.78%)
5 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article481.text
Perplexity = 171.00, Entropy = 7.42 bits
Computation based on 349 words.
Number of 5-grams hit = 65  (18.62%)
Number of 4-grams hit = 92  (26.36%)
Number of 3-grams hit = 112  (32.09%)
Number of 2-grams hit = 63  (18.05%)
Number of 1-grams hit = 17  (4.87%)
2 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article482.text
Perplexity = 123.12, Entropy = 6.94 bits
Computation based on 494 words.
Number of 5-grams hit = 132  (26.72%)
Number of 4-grams hit = 136  (27.53%)
Number of 3-grams hit = 135  (27.33%)
Number of 2-grams hit = 75  (15.18%)
Number of 1-grams hit = 16  (3.24%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article483.text
Perplexity = 103.96, Entropy = 6.70 bits
Computation based on 1664 words.
Number of 5-grams hit = 566  (34.01%)
Number of 4-grams hit = 380  (22.84%)
Number of 3-grams hit = 391  (23.50%)
Number of 2-grams hit = 274  (16.47%)
Number of 1-grams hit = 53  (3.19%)
12 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article484.text
Perplexity = 178.99, Entropy = 7.48 bits
Computation based on 1427 words.
Number of 5-grams hit = 319  (22.35%)
Number of 4-grams hit = 348  (24.39%)
Number of 3-grams hit = 421  (29.50%)
Number of 2-grams hit = 268  (18.78%)
Number of 1-grams hit = 71  (4.98%)
5 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article485.text
Perplexity = 306.56, Entropy = 8.26 bits
Computation based on 372 words.
Number of 5-grams hit = 58  (15.59%)
Number of 4-grams hit = 69  (18.55%)
Number of 3-grams hit = 106  (28.49%)
Number of 2-grams hit = 103  (27.69%)
Number of 1-grams hit = 36  (9.68%)
5 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article486.text
Perplexity = 169.84, Entropy = 7.41 bits
Computation based on 331 words.
Number of 5-grams hit = 84  (25.38%)
Number of 4-grams hit = 76  (22.96%)
Number of 3-grams hit = 85  (25.68%)
Number of 2-grams hit = 71  (21.45%)
Number of 1-grams hit = 15  (4.53%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article487.text
Perplexity = 142.53, Entropy = 7.16 bits
Computation based on 429 words.
Number of 5-grams hit = 79  (18.41%)
Number of 4-grams hit = 121  (28.21%)
Number of 3-grams hit = 137  (31.93%)
Number of 2-grams hit = 78  (18.18%)
Number of 1-grams hit = 14  (3.26%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article488.text
Perplexity = 143.09, Entropy = 7.16 bits
Computation based on 710 words.
Number of 5-grams hit = 164  (23.10%)
Number of 4-grams hit = 192  (27.04%)
Number of 3-grams hit = 210  (29.58%)
Number of 2-grams hit = 126  (17.75%)
Number of 1-grams hit = 18  (2.54%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article489.text
Perplexity = 93.94, Entropy = 6.55 bits
Computation based on 537 words.
Number of 5-grams hit = 168  (31.28%)
Number of 4-grams hit = 115  (21.42%)
Number of 3-grams hit = 162  (30.17%)
Number of 2-grams hit = 83  (15.46%)
Number of 1-grams hit = 9  (1.68%)
9 OOVs (1.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article490.text
Perplexity = 224.29, Entropy = 7.81 bits
Computation based on 479 words.
Number of 5-grams hit = 91  (19.00%)
Number of 4-grams hit = 108  (22.55%)
Number of 3-grams hit = 135  (28.18%)
Number of 2-grams hit = 114  (23.80%)
Number of 1-grams hit = 31  (6.47%)
14 OOVs (2.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article491.text
Perplexity = 190.83, Entropy = 7.58 bits
Computation based on 1388 words.
Number of 5-grams hit = 303  (21.83%)
Number of 4-grams hit = 320  (23.05%)
Number of 3-grams hit = 427  (30.76%)
Number of 2-grams hit = 263  (18.95%)
Number of 1-grams hit = 75  (5.40%)
7 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article492.text
Perplexity = 123.20, Entropy = 6.94 bits
Computation based on 433 words.
Number of 5-grams hit = 111  (25.64%)
Number of 4-grams hit = 98  (22.63%)
Number of 3-grams hit = 119  (27.48%)
Number of 2-grams hit = 95  (21.94%)
Number of 1-grams hit = 10  (2.31%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article493.text
Perplexity = 177.04, Entropy = 7.47 bits
Computation based on 759 words.
Number of 5-grams hit = 153  (20.16%)
Number of 4-grams hit = 157  (20.69%)
Number of 3-grams hit = 233  (30.70%)
Number of 2-grams hit = 174  (22.92%)
Number of 1-grams hit = 42  (5.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article494.text
Perplexity = 182.78, Entropy = 7.51 bits
Computation based on 485 words.
Number of 5-grams hit = 99  (20.41%)
Number of 4-grams hit = 118  (24.33%)
Number of 3-grams hit = 150  (30.93%)
Number of 2-grams hit = 96  (19.79%)
Number of 1-grams hit = 22  (4.54%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article495.text
Perplexity = 126.58, Entropy = 6.98 bits
Computation based on 426 words.
Number of 5-grams hit = 118  (27.70%)
Number of 4-grams hit = 88  (20.66%)
Number of 3-grams hit = 108  (25.35%)
Number of 2-grams hit = 93  (21.83%)
Number of 1-grams hit = 19  (4.46%)
4 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article496.text
Perplexity = 269.33, Entropy = 8.07 bits
Computation based on 546 words.
Number of 5-grams hit = 93  (17.03%)
Number of 4-grams hit = 104  (19.05%)
Number of 3-grams hit = 153  (28.02%)
Number of 2-grams hit = 164  (30.04%)
Number of 1-grams hit = 32  (5.86%)
12 OOVs (2.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article497.text
Perplexity = 160.24, Entropy = 7.32 bits
Computation based on 647 words.
Number of 5-grams hit = 148  (22.87%)
Number of 4-grams hit = 175  (27.05%)
Number of 3-grams hit = 188  (29.06%)
Number of 2-grams hit = 98  (15.15%)
Number of 1-grams hit = 38  (5.87%)
5 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article498.text
Perplexity = 330.02, Entropy = 8.37 bits
Computation based on 1659 words.
Number of 5-grams hit = 291  (17.54%)
Number of 4-grams hit = 325  (19.59%)
Number of 3-grams hit = 465  (28.03%)
Number of 2-grams hit = 433  (26.10%)
Number of 1-grams hit = 145  (8.74%)
40 OOVs (2.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article499.text
Perplexity = 136.80, Entropy = 7.10 bits
Computation based on 495 words.
Number of 5-grams hit = 116  (23.43%)
Number of 4-grams hit = 136  (27.47%)
Number of 3-grams hit = 150  (30.30%)
Number of 2-grams hit = 79  (15.96%)
Number of 1-grams hit = 14  (2.83%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article500.text
Perplexity = 132.64, Entropy = 7.05 bits
Computation based on 457 words.
Number of 5-grams hit = 99  (21.66%)
Number of 4-grams hit = 122  (26.70%)
Number of 3-grams hit = 128  (28.01%)
Number of 2-grams hit = 88  (19.26%)
Number of 1-grams hit = 20  (4.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article501.text
Perplexity = 154.53, Entropy = 7.27 bits
Computation based on 403 words.
Number of 5-grams hit = 106  (26.30%)
Number of 4-grams hit = 99  (24.57%)
Number of 3-grams hit = 102  (25.31%)
Number of 2-grams hit = 82  (20.35%)
Number of 1-grams hit = 14  (3.47%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article502.text
Perplexity = 185.90, Entropy = 7.54 bits
Computation based on 505 words.
Number of 5-grams hit = 67  (13.27%)
Number of 4-grams hit = 121  (23.96%)
Number of 3-grams hit = 198  (39.21%)
Number of 2-grams hit = 96  (19.01%)
Number of 1-grams hit = 23  (4.55%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article503.text
Perplexity = 166.36, Entropy = 7.38 bits
Computation based on 1013 words.
Number of 5-grams hit = 229  (22.61%)
Number of 4-grams hit = 266  (26.26%)
Number of 3-grams hit = 301  (29.71%)
Number of 2-grams hit = 174  (17.18%)
Number of 1-grams hit = 43  (4.24%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article504.text
Perplexity = 167.02, Entropy = 7.38 bits
Computation based on 613 words.
Number of 5-grams hit = 141  (23.00%)
Number of 4-grams hit = 149  (24.31%)
Number of 3-grams hit = 181  (29.53%)
Number of 2-grams hit = 121  (19.74%)
Number of 1-grams hit = 21  (3.43%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article505.text
Perplexity = 225.86, Entropy = 7.82 bits
Computation based on 701 words.
Number of 5-grams hit = 146  (20.83%)
Number of 4-grams hit = 152  (21.68%)
Number of 3-grams hit = 200  (28.53%)
Number of 2-grams hit = 163  (23.25%)
Number of 1-grams hit = 40  (5.71%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article506.text
Perplexity = 237.62, Entropy = 7.89 bits
Computation based on 522 words.
Number of 5-grams hit = 122  (23.37%)
Number of 4-grams hit = 111  (21.26%)
Number of 3-grams hit = 134  (25.67%)
Number of 2-grams hit = 109  (20.88%)
Number of 1-grams hit = 46  (8.81%)
17 OOVs (3.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article507.text
Perplexity = 106.14, Entropy = 6.73 bits
Computation based on 1584 words.
Number of 5-grams hit = 489  (30.87%)
Number of 4-grams hit = 338  (21.34%)
Number of 3-grams hit = 416  (26.26%)
Number of 2-grams hit = 292  (18.43%)
Number of 1-grams hit = 49  (3.09%)
9 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article508.text
Perplexity = 50.12, Entropy = 5.65 bits
Computation based on 655 words.
Number of 5-grams hit = 302  (46.11%)
Number of 4-grams hit = 122  (18.63%)
Number of 3-grams hit = 138  (21.07%)
Number of 2-grams hit = 83  (12.67%)
Number of 1-grams hit = 10  (1.53%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article509.text
Perplexity = 206.05, Entropy = 7.69 bits
Computation based on 353 words.
Number of 5-grams hit = 88  (24.93%)
Number of 4-grams hit = 80  (22.66%)
Number of 3-grams hit = 95  (26.91%)
Number of 2-grams hit = 68  (19.26%)
Number of 1-grams hit = 22  (6.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article510.text
Perplexity = 149.68, Entropy = 7.23 bits
Computation based on 968 words.
Number of 5-grams hit = 254  (26.24%)
Number of 4-grams hit = 206  (21.28%)
Number of 3-grams hit = 259  (26.76%)
Number of 2-grams hit = 198  (20.45%)
Number of 1-grams hit = 51  (5.27%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article511.text
Perplexity = 198.37, Entropy = 7.63 bits
Computation based on 623 words.
Number of 5-grams hit = 118  (18.94%)
Number of 4-grams hit = 125  (20.06%)
Number of 3-grams hit = 186  (29.86%)
Number of 2-grams hit = 158  (25.36%)
Number of 1-grams hit = 36  (5.78%)
8 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article512.text
Perplexity = 165.62, Entropy = 7.37 bits
Computation based on 1900 words.
Number of 5-grams hit = 399  (21.00%)
Number of 4-grams hit = 478  (25.16%)
Number of 3-grams hit = 610  (32.11%)
Number of 2-grams hit = 339  (17.84%)
Number of 1-grams hit = 74  (3.89%)
8 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article513.text
Perplexity = 282.94, Entropy = 8.14 bits
Computation based on 414 words.
Number of 5-grams hit = 64  (15.46%)
Number of 4-grams hit = 81  (19.57%)
Number of 3-grams hit = 122  (29.47%)
Number of 2-grams hit = 114  (27.54%)
Number of 1-grams hit = 33  (7.97%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article514.text
Perplexity = 87.54, Entropy = 6.45 bits
Computation based on 397 words.
Number of 5-grams hit = 110  (27.71%)
Number of 4-grams hit = 94  (23.68%)
Number of 3-grams hit = 116  (29.22%)
Number of 2-grams hit = 63  (15.87%)
Number of 1-grams hit = 14  (3.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article515.text
Perplexity = 181.57, Entropy = 7.50 bits
Computation based on 972 words.
Number of 5-grams hit = 203  (20.88%)
Number of 4-grams hit = 225  (23.15%)
Number of 3-grams hit = 270  (27.78%)
Number of 2-grams hit = 235  (24.18%)
Number of 1-grams hit = 39  (4.01%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article516.text
Perplexity = 153.75, Entropy = 7.26 bits
Computation based on 412 words.
Number of 5-grams hit = 84  (20.39%)
Number of 4-grams hit = 106  (25.73%)
Number of 3-grams hit = 133  (32.28%)
Number of 2-grams hit = 73  (17.72%)
Number of 1-grams hit = 16  (3.88%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article517.text
Perplexity = 221.87, Entropy = 7.79 bits
Computation based on 1060 words.
Number of 5-grams hit = 194  (18.30%)
Number of 4-grams hit = 253  (23.87%)
Number of 3-grams hit = 328  (30.94%)
Number of 2-grams hit = 227  (21.42%)
Number of 1-grams hit = 58  (5.47%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article518.text
Perplexity = 110.10, Entropy = 6.78 bits
Computation based on 584 words.
Number of 5-grams hit = 169  (28.94%)
Number of 4-grams hit = 134  (22.95%)
Number of 3-grams hit = 155  (26.54%)
Number of 2-grams hit = 105  (17.98%)
Number of 1-grams hit = 21  (3.60%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article519.text
Perplexity = 159.75, Entropy = 7.32 bits
Computation based on 525 words.
Number of 5-grams hit = 139  (26.48%)
Number of 4-grams hit = 113  (21.52%)
Number of 3-grams hit = 140  (26.67%)
Number of 2-grams hit = 111  (21.14%)
Number of 1-grams hit = 22  (4.19%)
6 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article520.text
Perplexity = 138.55, Entropy = 7.11 bits
Computation based on 610 words.
Number of 5-grams hit = 140  (22.95%)
Number of 4-grams hit = 160  (26.23%)
Number of 3-grams hit = 189  (30.98%)
Number of 2-grams hit = 101  (16.56%)
Number of 1-grams hit = 20  (3.28%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article521.text
Perplexity = 186.85, Entropy = 7.55 bits
Computation based on 511 words.
Number of 5-grams hit = 107  (20.94%)
Number of 4-grams hit = 102  (19.96%)
Number of 3-grams hit = 154  (30.14%)
Number of 2-grams hit = 121  (23.68%)
Number of 1-grams hit = 27  (5.28%)
11 OOVs (2.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article522.text
Perplexity = 125.30, Entropy = 6.97 bits
Computation based on 2319 words.
Number of 5-grams hit = 654  (28.20%)
Number of 4-grams hit = 522  (22.51%)
Number of 3-grams hit = 665  (28.68%)
Number of 2-grams hit = 411  (17.72%)
Number of 1-grams hit = 67  (2.89%)
11 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article523.text
Perplexity = 161.73, Entropy = 7.34 bits
Computation based on 327 words.
Number of 5-grams hit = 65  (19.88%)
Number of 4-grams hit = 74  (22.63%)
Number of 3-grams hit = 114  (34.86%)
Number of 2-grams hit = 58  (17.74%)
Number of 1-grams hit = 16  (4.89%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article524.text
Perplexity = 193.15, Entropy = 7.59 bits
Computation based on 1383 words.
Number of 5-grams hit = 270  (19.52%)
Number of 4-grams hit = 309  (22.34%)
Number of 3-grams hit = 436  (31.53%)
Number of 2-grams hit = 301  (21.76%)
Number of 1-grams hit = 67  (4.84%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article525.text
Perplexity = 170.60, Entropy = 7.41 bits
Computation based on 1967 words.
Number of 5-grams hit = 405  (20.59%)
Number of 4-grams hit = 493  (25.06%)
Number of 3-grams hit = 608  (30.91%)
Number of 2-grams hit = 372  (18.91%)
Number of 1-grams hit = 89  (4.52%)
10 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article526.text
Perplexity = 162.86, Entropy = 7.35 bits
Computation based on 1900 words.
Number of 5-grams hit = 421  (22.16%)
Number of 4-grams hit = 496  (26.11%)
Number of 3-grams hit = 591  (31.11%)
Number of 2-grams hit = 310  (16.32%)
Number of 1-grams hit = 82  (4.32%)
9 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article527.text
Perplexity = 169.46, Entropy = 7.40 bits
Computation based on 733 words.
Number of 5-grams hit = 139  (18.96%)
Number of 4-grams hit = 180  (24.56%)
Number of 3-grams hit = 244  (33.29%)
Number of 2-grams hit = 141  (19.24%)
Number of 1-grams hit = 29  (3.96%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article528.text
Perplexity = 171.10, Entropy = 7.42 bits
Computation based on 723 words.
Number of 5-grams hit = 155  (21.44%)
Number of 4-grams hit = 180  (24.90%)
Number of 3-grams hit = 231  (31.95%)
Number of 2-grams hit = 125  (17.29%)
Number of 1-grams hit = 32  (4.43%)
4 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article529.text
Perplexity = 240.52, Entropy = 7.91 bits
Computation based on 886 words.
Number of 5-grams hit = 154  (17.38%)
Number of 4-grams hit = 223  (25.17%)
Number of 3-grams hit = 265  (29.91%)
Number of 2-grams hit = 190  (21.44%)
Number of 1-grams hit = 54  (6.09%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article530.text
Perplexity = 138.71, Entropy = 7.12 bits
Computation based on 420 words.
Number of 5-grams hit = 105  (25.00%)
Number of 4-grams hit = 93  (22.14%)
Number of 3-grams hit = 126  (30.00%)
Number of 2-grams hit = 82  (19.52%)
Number of 1-grams hit = 14  (3.33%)
6 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article531.text
Perplexity = 189.40, Entropy = 7.57 bits
Computation based on 944 words.
Number of 5-grams hit = 188  (19.92%)
Number of 4-grams hit = 223  (23.62%)
Number of 3-grams hit = 285  (30.19%)
Number of 2-grams hit = 202  (21.40%)
Number of 1-grams hit = 46  (4.87%)
4 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article532.text
Perplexity = 154.63, Entropy = 7.27 bits
Computation based on 933 words.
Number of 5-grams hit = 223  (23.90%)
Number of 4-grams hit = 249  (26.69%)
Number of 3-grams hit = 263  (28.19%)
Number of 2-grams hit = 163  (17.47%)
Number of 1-grams hit = 35  (3.75%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article533.text
Perplexity = 94.31, Entropy = 6.56 bits
Computation based on 1137 words.
Number of 5-grams hit = 383  (33.69%)
Number of 4-grams hit = 247  (21.72%)
Number of 3-grams hit = 288  (25.33%)
Number of 2-grams hit = 191  (16.80%)
Number of 1-grams hit = 28  (2.46%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article534.text
Perplexity = 108.92, Entropy = 6.77 bits
Computation based on 494 words.
Number of 5-grams hit = 146  (29.55%)
Number of 4-grams hit = 107  (21.66%)
Number of 3-grams hit = 126  (25.51%)
Number of 2-grams hit = 97  (19.64%)
Number of 1-grams hit = 18  (3.64%)
8 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article535.text
Perplexity = 264.20, Entropy = 8.05 bits
Computation based on 358 words.
Number of 5-grams hit = 51  (14.25%)
Number of 4-grams hit = 75  (20.95%)
Number of 3-grams hit = 100  (27.93%)
Number of 2-grams hit = 106  (29.61%)
Number of 1-grams hit = 26  (7.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article536.text
Perplexity = 107.88, Entropy = 6.75 bits
Computation based on 1511 words.
Number of 5-grams hit = 451  (29.85%)
Number of 4-grams hit = 337  (22.30%)
Number of 3-grams hit = 406  (26.87%)
Number of 2-grams hit = 271  (17.94%)
Number of 1-grams hit = 46  (3.04%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article537.text
Perplexity = 133.14, Entropy = 7.06 bits
Computation based on 999 words.
Number of 5-grams hit = 275  (27.53%)
Number of 4-grams hit = 268  (26.83%)
Number of 3-grams hit = 269  (26.93%)
Number of 2-grams hit = 148  (14.81%)
Number of 1-grams hit = 39  (3.90%)
4 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article538.text
Perplexity = 165.24, Entropy = 7.37 bits
Computation based on 3432 words.
Number of 5-grams hit = 788  (22.96%)
Number of 4-grams hit = 850  (24.77%)
Number of 3-grams hit = 1009  (29.40%)
Number of 2-grams hit = 630  (18.36%)
Number of 1-grams hit = 155  (4.52%)
9 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article539.text
Perplexity = 246.71, Entropy = 7.95 bits
Computation based on 534 words.
Number of 5-grams hit = 91  (17.04%)
Number of 4-grams hit = 134  (25.09%)
Number of 3-grams hit = 162  (30.34%)
Number of 2-grams hit = 105  (19.66%)
Number of 1-grams hit = 42  (7.87%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article540.text
Perplexity = 117.50, Entropy = 6.88 bits
Computation based on 2193 words.
Number of 5-grams hit = 591  (26.95%)
Number of 4-grams hit = 524  (23.89%)
Number of 3-grams hit = 618  (28.18%)
Number of 2-grams hit = 392  (17.88%)
Number of 1-grams hit = 68  (3.10%)
5 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article541.text
Perplexity = 201.05, Entropy = 7.65 bits
Computation based on 497 words.
Number of 5-grams hit = 86  (17.30%)
Number of 4-grams hit = 131  (26.36%)
Number of 3-grams hit = 154  (30.99%)
Number of 2-grams hit = 102  (20.52%)
Number of 1-grams hit = 24  (4.83%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article542.text
Perplexity = 87.48, Entropy = 6.45 bits
Computation based on 1590 words.
Number of 5-grams hit = 569  (35.79%)
Number of 4-grams hit = 362  (22.77%)
Number of 3-grams hit = 403  (25.35%)
Number of 2-grams hit = 220  (13.84%)
Number of 1-grams hit = 36  (2.26%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article543.text
Perplexity = 193.93, Entropy = 7.60 bits
Computation based on 534 words.
Number of 5-grams hit = 121  (22.66%)
Number of 4-grams hit = 125  (23.41%)
Number of 3-grams hit = 145  (27.15%)
Number of 2-grams hit = 111  (20.79%)
Number of 1-grams hit = 32  (5.99%)
7 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article544.text
Perplexity = 201.71, Entropy = 7.66 bits
Computation based on 1280 words.
Number of 5-grams hit = 275  (21.48%)
Number of 4-grams hit = 303  (23.67%)
Number of 3-grams hit = 373  (29.14%)
Number of 2-grams hit = 267  (20.86%)
Number of 1-grams hit = 62  (4.84%)
7 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article545.text
Perplexity = 139.31, Entropy = 7.12 bits
Computation based on 749 words.
Number of 5-grams hit = 182  (24.30%)
Number of 4-grams hit = 191  (25.50%)
Number of 3-grams hit = 225  (30.04%)
Number of 2-grams hit = 124  (16.56%)
Number of 1-grams hit = 27  (3.60%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article546.text
Perplexity = 34.02, Entropy = 5.09 bits
Computation based on 685 words.
Number of 5-grams hit = 403  (58.83%)
Number of 4-grams hit = 80  (11.68%)
Number of 3-grams hit = 113  (16.50%)
Number of 2-grams hit = 72  (10.51%)
Number of 1-grams hit = 17  (2.48%)
7 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article547.text
Perplexity = 283.88, Entropy = 8.15 bits
Computation based on 880 words.
Number of 5-grams hit = 119  (13.52%)
Number of 4-grams hit = 185  (21.02%)
Number of 3-grams hit = 297  (33.75%)
Number of 2-grams hit = 225  (25.57%)
Number of 1-grams hit = 54  (6.14%)
15 OOVs (1.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article548.text
Perplexity = 160.30, Entropy = 7.32 bits
Computation based on 623 words.
Number of 5-grams hit = 135  (21.67%)
Number of 4-grams hit = 167  (26.81%)
Number of 3-grams hit = 186  (29.86%)
Number of 2-grams hit = 115  (18.46%)
Number of 1-grams hit = 20  (3.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article549.text
Perplexity = 245.50, Entropy = 7.94 bits
Computation based on 358 words.
Number of 5-grams hit = 54  (15.08%)
Number of 4-grams hit = 81  (22.63%)
Number of 3-grams hit = 120  (33.52%)
Number of 2-grams hit = 86  (24.02%)
Number of 1-grams hit = 17  (4.75%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article550.text
Perplexity = 120.32, Entropy = 6.91 bits
Computation based on 185 words.
Number of 5-grams hit = 57  (30.81%)
Number of 4-grams hit = 58  (31.35%)
Number of 3-grams hit = 43  (23.24%)
Number of 2-grams hit = 20  (10.81%)
Number of 1-grams hit = 7  (3.78%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article551.text
Perplexity = 106.54, Entropy = 6.74 bits
Computation based on 918 words.
Number of 5-grams hit = 275  (29.96%)
Number of 4-grams hit = 225  (24.51%)
Number of 3-grams hit = 247  (26.91%)
Number of 2-grams hit = 154  (16.78%)
Number of 1-grams hit = 17  (1.85%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article552.text
Perplexity = 167.37, Entropy = 7.39 bits
Computation based on 1051 words.
Number of 5-grams hit = 223  (21.22%)
Number of 4-grams hit = 236  (22.45%)
Number of 3-grams hit = 301  (28.64%)
Number of 2-grams hit = 244  (23.22%)
Number of 1-grams hit = 47  (4.47%)
26 OOVs (2.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article553.text
Perplexity = 90.34, Entropy = 6.50 bits
Computation based on 1239 words.
Number of 5-grams hit = 407  (32.85%)
Number of 4-grams hit = 316  (25.50%)
Number of 3-grams hit = 325  (26.23%)
Number of 2-grams hit = 164  (13.24%)
Number of 1-grams hit = 27  (2.18%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article554.text
Perplexity = 132.93, Entropy = 7.05 bits
Computation based on 4043 words.
Number of 5-grams hit = 1111  (27.48%)
Number of 4-grams hit = 888  (21.96%)
Number of 3-grams hit = 1114  (27.55%)
Number of 2-grams hit = 773  (19.12%)
Number of 1-grams hit = 157  (3.88%)
32 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article555.text
Perplexity = 184.18, Entropy = 7.52 bits
Computation based on 505 words.
Number of 5-grams hit = 124  (24.55%)
Number of 4-grams hit = 117  (23.17%)
Number of 3-grams hit = 134  (26.53%)
Number of 2-grams hit = 106  (20.99%)
Number of 1-grams hit = 24  (4.75%)
3 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article556.text
Perplexity = 276.49, Entropy = 8.11 bits
Computation based on 555 words.
Number of 5-grams hit = 90  (16.22%)
Number of 4-grams hit = 120  (21.62%)
Number of 3-grams hit = 148  (26.67%)
Number of 2-grams hit = 154  (27.75%)
Number of 1-grams hit = 43  (7.75%)
12 OOVs (2.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article557.text
Perplexity = 110.60, Entropy = 6.79 bits
Computation based on 382 words.
Number of 5-grams hit = 107  (28.01%)
Number of 4-grams hit = 106  (27.75%)
Number of 3-grams hit = 97  (25.39%)
Number of 2-grams hit = 59  (15.45%)
Number of 1-grams hit = 13  (3.40%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article558.text
Perplexity = 136.40, Entropy = 7.09 bits
Computation based on 1710 words.
Number of 5-grams hit = 526  (30.76%)
Number of 4-grams hit = 327  (19.12%)
Number of 3-grams hit = 453  (26.49%)
Number of 2-grams hit = 337  (19.71%)
Number of 1-grams hit = 67  (3.92%)
19 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article559.text
Perplexity = 182.79, Entropy = 7.51 bits
Computation based on 629 words.
Number of 5-grams hit = 116  (18.44%)
Number of 4-grams hit = 142  (22.58%)
Number of 3-grams hit = 191  (30.37%)
Number of 2-grams hit = 148  (23.53%)
Number of 1-grams hit = 32  (5.09%)
9 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article560.text
Perplexity = 85.64, Entropy = 6.42 bits
Computation based on 547 words.
Number of 5-grams hit = 176  (32.18%)
Number of 4-grams hit = 144  (26.33%)
Number of 3-grams hit = 131  (23.95%)
Number of 2-grams hit = 78  (14.26%)
Number of 1-grams hit = 18  (3.29%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article561.text
Perplexity = 210.76, Entropy = 7.72 bits
Computation based on 330 words.
Number of 5-grams hit = 67  (20.30%)
Number of 4-grams hit = 60  (18.18%)
Number of 3-grams hit = 92  (27.88%)
Number of 2-grams hit = 88  (26.67%)
Number of 1-grams hit = 23  (6.97%)
3 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article562.text
Perplexity = 103.70, Entropy = 6.70 bits
Computation based on 312 words.
Number of 5-grams hit = 80  (25.64%)
Number of 4-grams hit = 75  (24.04%)
Number of 3-grams hit = 91  (29.17%)
Number of 2-grams hit = 55  (17.63%)
Number of 1-grams hit = 11  (3.53%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article563.text
Perplexity = 67.58, Entropy = 6.08 bits
Computation based on 492 words.
Number of 5-grams hit = 183  (37.20%)
Number of 4-grams hit = 118  (23.98%)
Number of 3-grams hit = 112  (22.76%)
Number of 2-grams hit = 68  (13.82%)
Number of 1-grams hit = 11  (2.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article564.text
Perplexity = 163.84, Entropy = 7.36 bits
Computation based on 1335 words.
Number of 5-grams hit = 297  (22.25%)
Number of 4-grams hit = 339  (25.39%)
Number of 3-grams hit = 395  (29.59%)
Number of 2-grams hit = 244  (18.28%)
Number of 1-grams hit = 60  (4.49%)
4 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article565.text
Perplexity = 153.30, Entropy = 7.26 bits
Computation based on 1286 words.
Number of 5-grams hit = 311  (24.18%)
Number of 4-grams hit = 272  (21.15%)
Number of 3-grams hit = 352  (27.37%)
Number of 2-grams hit = 284  (22.08%)
Number of 1-grams hit = 67  (5.21%)
15 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article566.text
Perplexity = 103.69, Entropy = 6.70 bits
Computation based on 4132 words.
Number of 5-grams hit = 1335  (32.31%)
Number of 4-grams hit = 877  (21.22%)
Number of 3-grams hit = 1021  (24.71%)
Number of 2-grams hit = 759  (18.37%)
Number of 1-grams hit = 140  (3.39%)
21 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article567.text
Perplexity = 169.65, Entropy = 7.41 bits
Computation based on 547 words.
Number of 5-grams hit = 153  (27.97%)
Number of 4-grams hit = 118  (21.57%)
Number of 3-grams hit = 119  (21.76%)
Number of 2-grams hit = 116  (21.21%)
Number of 1-grams hit = 41  (7.50%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article568.text
Perplexity = 76.96, Entropy = 6.27 bits
Computation based on 329 words.
Number of 5-grams hit = 118  (35.87%)
Number of 4-grams hit = 64  (19.45%)
Number of 3-grams hit = 82  (24.92%)
Number of 2-grams hit = 56  (17.02%)
Number of 1-grams hit = 9  (2.74%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article569.text
Perplexity = 159.21, Entropy = 7.31 bits
Computation based on 4344 words.
Number of 5-grams hit = 927  (21.34%)
Number of 4-grams hit = 1120  (25.78%)
Number of 3-grams hit = 1324  (30.48%)
Number of 2-grams hit = 806  (18.55%)
Number of 1-grams hit = 167  (3.84%)
12 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article570.text
Perplexity = 121.66, Entropy = 6.93 bits
Computation based on 815 words.
Number of 5-grams hit = 228  (27.98%)
Number of 4-grams hit = 201  (24.66%)
Number of 3-grams hit = 209  (25.64%)
Number of 2-grams hit = 145  (17.79%)
Number of 1-grams hit = 32  (3.93%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article571.text
Perplexity = 184.83, Entropy = 7.53 bits
Computation based on 534 words.
Number of 5-grams hit = 125  (23.41%)
Number of 4-grams hit = 111  (20.79%)
Number of 3-grams hit = 143  (26.78%)
Number of 2-grams hit = 121  (22.66%)
Number of 1-grams hit = 34  (6.37%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article572.text
Perplexity = 178.79, Entropy = 7.48 bits
Computation based on 570 words.
Number of 5-grams hit = 125  (21.93%)
Number of 4-grams hit = 153  (26.84%)
Number of 3-grams hit = 149  (26.14%)
Number of 2-grams hit = 112  (19.65%)
Number of 1-grams hit = 31  (5.44%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article573.text
Perplexity = 179.97, Entropy = 7.49 bits
Computation based on 251 words.
Number of 5-grams hit = 63  (25.10%)
Number of 4-grams hit = 66  (26.29%)
Number of 3-grams hit = 60  (23.90%)
Number of 2-grams hit = 50  (19.92%)
Number of 1-grams hit = 12  (4.78%)
1 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article574.text
Perplexity = 174.38, Entropy = 7.45 bits
Computation based on 1089 words.
Number of 5-grams hit = 215  (19.74%)
Number of 4-grams hit = 289  (26.54%)
Number of 3-grams hit = 333  (30.58%)
Number of 2-grams hit = 204  (18.73%)
Number of 1-grams hit = 48  (4.41%)
5 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article575.text
Perplexity = 154.18, Entropy = 7.27 bits
Computation based on 595 words.
Number of 5-grams hit = 133  (22.35%)
Number of 4-grams hit = 150  (25.21%)
Number of 3-grams hit = 175  (29.41%)
Number of 2-grams hit = 108  (18.15%)
Number of 1-grams hit = 29  (4.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article576.text
Perplexity = 383.85, Entropy = 8.58 bits
Computation based on 426 words.
Number of 5-grams hit = 59  (13.85%)
Number of 4-grams hit = 94  (22.07%)
Number of 3-grams hit = 113  (26.53%)
Number of 2-grams hit = 119  (27.93%)
Number of 1-grams hit = 41  (9.62%)
19 OOVs (4.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article577.text
Perplexity = 139.82, Entropy = 7.13 bits
Computation based on 363 words.
Number of 5-grams hit = 94  (25.90%)
Number of 4-grams hit = 99  (27.27%)
Number of 3-grams hit = 94  (25.90%)
Number of 2-grams hit = 62  (17.08%)
Number of 1-grams hit = 14  (3.86%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article578.text
Perplexity = 151.41, Entropy = 7.24 bits
Computation based on 406 words.
Number of 5-grams hit = 92  (22.66%)
Number of 4-grams hit = 104  (25.62%)
Number of 3-grams hit = 125  (30.79%)
Number of 2-grams hit = 66  (16.26%)
Number of 1-grams hit = 19  (4.68%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article579.text
Perplexity = 214.05, Entropy = 7.74 bits
Computation based on 440 words.
Number of 5-grams hit = 99  (22.50%)
Number of 4-grams hit = 103  (23.41%)
Number of 3-grams hit = 123  (27.95%)
Number of 2-grams hit = 89  (20.23%)
Number of 1-grams hit = 26  (5.91%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article580.text
Perplexity = 92.02, Entropy = 6.52 bits
Computation based on 532 words.
Number of 5-grams hit = 181  (34.02%)
Number of 4-grams hit = 103  (19.36%)
Number of 3-grams hit = 137  (25.75%)
Number of 2-grams hit = 93  (17.48%)
Number of 1-grams hit = 18  (3.38%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article581.text
Perplexity = 158.56, Entropy = 7.31 bits
Computation based on 533 words.
Number of 5-grams hit = 113  (21.20%)
Number of 4-grams hit = 128  (24.02%)
Number of 3-grams hit = 145  (27.20%)
Number of 2-grams hit = 113  (21.20%)
Number of 1-grams hit = 34  (6.38%)
8 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article582.text
Perplexity = 158.08, Entropy = 7.30 bits
Computation based on 500 words.
Number of 5-grams hit = 118  (23.60%)
Number of 4-grams hit = 97  (19.40%)
Number of 3-grams hit = 146  (29.20%)
Number of 2-grams hit = 112  (22.40%)
Number of 1-grams hit = 27  (5.40%)
9 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article583.text
Perplexity = 142.55, Entropy = 7.16 bits
Computation based on 664 words.
Number of 5-grams hit = 186  (28.01%)
Number of 4-grams hit = 132  (19.88%)
Number of 3-grams hit = 168  (25.30%)
Number of 2-grams hit = 144  (21.69%)
Number of 1-grams hit = 34  (5.12%)
11 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article584.text
Perplexity = 158.59, Entropy = 7.31 bits
Computation based on 433 words.
Number of 5-grams hit = 105  (24.25%)
Number of 4-grams hit = 101  (23.33%)
Number of 3-grams hit = 126  (29.10%)
Number of 2-grams hit = 83  (19.17%)
Number of 1-grams hit = 18  (4.16%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article585.text
Perplexity = 97.28, Entropy = 6.60 bits
Computation based on 946 words.
Number of 5-grams hit = 280  (29.60%)
Number of 4-grams hit = 251  (26.53%)
Number of 3-grams hit = 243  (25.69%)
Number of 2-grams hit = 145  (15.33%)
Number of 1-grams hit = 27  (2.85%)
8 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article586.text
Perplexity = 108.25, Entropy = 6.76 bits
Computation based on 733 words.
Number of 5-grams hit = 219  (29.88%)
Number of 4-grams hit = 187  (25.51%)
Number of 3-grams hit = 196  (26.74%)
Number of 2-grams hit = 107  (14.60%)
Number of 1-grams hit = 24  (3.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article587.text
Perplexity = 180.60, Entropy = 7.50 bits
Computation based on 522 words.
Number of 5-grams hit = 110  (21.07%)
Number of 4-grams hit = 132  (25.29%)
Number of 3-grams hit = 161  (30.84%)
Number of 2-grams hit = 101  (19.35%)
Number of 1-grams hit = 18  (3.45%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article588.text
Perplexity = 163.04, Entropy = 7.35 bits
Computation based on 955 words.
Number of 5-grams hit = 199  (20.84%)
Number of 4-grams hit = 230  (24.08%)
Number of 3-grams hit = 313  (32.77%)
Number of 2-grams hit = 169  (17.70%)
Number of 1-grams hit = 44  (4.61%)
5 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article589.text
Perplexity = 102.94, Entropy = 6.69 bits
Computation based on 504 words.
Number of 5-grams hit = 135  (26.79%)
Number of 4-grams hit = 131  (25.99%)
Number of 3-grams hit = 129  (25.60%)
Number of 2-grams hit = 93  (18.45%)
Number of 1-grams hit = 16  (3.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article590.text
Perplexity = 78.55, Entropy = 6.30 bits
Computation based on 1387 words.
Number of 5-grams hit = 488  (35.18%)
Number of 4-grams hit = 323  (23.29%)
Number of 3-grams hit = 332  (23.94%)
Number of 2-grams hit = 209  (15.07%)
Number of 1-grams hit = 35  (2.52%)
6 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article591.text
Perplexity = 136.57, Entropy = 7.09 bits
Computation based on 686 words.
Number of 5-grams hit = 161  (23.47%)
Number of 4-grams hit = 200  (29.15%)
Number of 3-grams hit = 193  (28.13%)
Number of 2-grams hit = 107  (15.60%)
Number of 1-grams hit = 25  (3.64%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article592.text
Perplexity = 172.42, Entropy = 7.43 bits
Computation based on 339 words.
Number of 5-grams hit = 71  (20.94%)
Number of 4-grams hit = 71  (20.94%)
Number of 3-grams hit = 100  (29.50%)
Number of 2-grams hit = 79  (23.30%)
Number of 1-grams hit = 18  (5.31%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article593.text
Perplexity = 117.10, Entropy = 6.87 bits
Computation based on 376 words.
Number of 5-grams hit = 102  (27.13%)
Number of 4-grams hit = 93  (24.73%)
Number of 3-grams hit = 112  (29.79%)
Number of 2-grams hit = 61  (16.22%)
Number of 1-grams hit = 8  (2.13%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article594.text
Perplexity = 116.30, Entropy = 6.86 bits
Computation based on 586 words.
Number of 5-grams hit = 178  (30.38%)
Number of 4-grams hit = 128  (21.84%)
Number of 3-grams hit = 154  (26.28%)
Number of 2-grams hit = 111  (18.94%)
Number of 1-grams hit = 15  (2.56%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article595.text
Perplexity = 105.03, Entropy = 6.71 bits
Computation based on 369 words.
Number of 5-grams hit = 110  (29.81%)
Number of 4-grams hit = 78  (21.14%)
Number of 3-grams hit = 101  (27.37%)
Number of 2-grams hit = 69  (18.70%)
Number of 1-grams hit = 11  (2.98%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article596.text
Perplexity = 195.03, Entropy = 7.61 bits
Computation based on 963 words.
Number of 5-grams hit = 210  (21.81%)
Number of 4-grams hit = 237  (24.61%)
Number of 3-grams hit = 285  (29.60%)
Number of 2-grams hit = 180  (18.69%)
Number of 1-grams hit = 51  (5.30%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article597.text
Perplexity = 184.58, Entropy = 7.53 bits
Computation based on 1027 words.
Number of 5-grams hit = 232  (22.59%)
Number of 4-grams hit = 211  (20.55%)
Number of 3-grams hit = 286  (27.85%)
Number of 2-grams hit = 241  (23.47%)
Number of 1-grams hit = 57  (5.55%)
7 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article598.text
Perplexity = 179.70, Entropy = 7.49 bits
Computation based on 296 words.
Number of 5-grams hit = 80  (27.03%)
Number of 4-grams hit = 68  (22.97%)
Number of 3-grams hit = 75  (25.34%)
Number of 2-grams hit = 59  (19.93%)
Number of 1-grams hit = 14  (4.73%)
3 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article599.text
Perplexity = 158.57, Entropy = 7.31 bits
Computation based on 802 words.
Number of 5-grams hit = 168  (20.95%)
Number of 4-grams hit = 191  (23.82%)
Number of 3-grams hit = 215  (26.81%)
Number of 2-grams hit = 188  (23.44%)
Number of 1-grams hit = 40  (4.99%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article600.text
Perplexity = 73.09, Entropy = 6.19 bits
Computation based on 4658 words.
Number of 5-grams hit = 2027  (43.52%)
Number of 4-grams hit = 976  (20.95%)
Number of 3-grams hit = 982  (21.08%)
Number of 2-grams hit = 575  (12.34%)
Number of 1-grams hit = 98  (2.10%)
15 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article601.text
Perplexity = 164.13, Entropy = 7.36 bits
Computation based on 1088 words.
Number of 5-grams hit = 227  (20.86%)
Number of 4-grams hit = 224  (20.59%)
Number of 3-grams hit = 335  (30.79%)
Number of 2-grams hit = 242  (22.24%)
Number of 1-grams hit = 60  (5.51%)
11 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article602.text
Perplexity = 145.95, Entropy = 7.19 bits
Computation based on 590 words.
Number of 5-grams hit = 151  (25.59%)
Number of 4-grams hit = 147  (24.92%)
Number of 3-grams hit = 175  (29.66%)
Number of 2-grams hit = 94  (15.93%)
Number of 1-grams hit = 23  (3.90%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article603.text
Perplexity = 219.42, Entropy = 7.78 bits
Computation based on 584 words.
Number of 5-grams hit = 100  (17.12%)
Number of 4-grams hit = 141  (24.14%)
Number of 3-grams hit = 195  (33.39%)
Number of 2-grams hit = 110  (18.84%)
Number of 1-grams hit = 38  (6.51%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article604.text
Perplexity = 82.51, Entropy = 6.37 bits
Computation based on 560 words.
Number of 5-grams hit = 193  (34.46%)
Number of 4-grams hit = 129  (23.04%)
Number of 3-grams hit = 145  (25.89%)
Number of 2-grams hit = 77  (13.75%)
Number of 1-grams hit = 16  (2.86%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article605.text
Perplexity = 118.55, Entropy = 6.89 bits
Computation based on 3116 words.
Number of 5-grams hit = 953  (30.58%)
Number of 4-grams hit = 682  (21.89%)
Number of 3-grams hit = 831  (26.67%)
Number of 2-grams hit = 546  (17.52%)
Number of 1-grams hit = 104  (3.34%)
40 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article606.text
Perplexity = 141.06, Entropy = 7.14 bits
Computation based on 900 words.
Number of 5-grams hit = 232  (25.78%)
Number of 4-grams hit = 182  (20.22%)
Number of 3-grams hit = 253  (28.11%)
Number of 2-grams hit = 197  (21.89%)
Number of 1-grams hit = 36  (4.00%)
13 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article607.text
Perplexity = 151.04, Entropy = 7.24 bits
Computation based on 1058 words.
Number of 5-grams hit = 213  (20.13%)
Number of 4-grams hit = 285  (26.94%)
Number of 3-grams hit = 343  (32.42%)
Number of 2-grams hit = 176  (16.64%)
Number of 1-grams hit = 41  (3.88%)
5 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article608.text
Perplexity = 154.79, Entropy = 7.27 bits
Computation based on 617 words.
Number of 5-grams hit = 157  (25.45%)
Number of 4-grams hit = 153  (24.80%)
Number of 3-grams hit = 177  (28.69%)
Number of 2-grams hit = 106  (17.18%)
Number of 1-grams hit = 24  (3.89%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article609.text
Perplexity = 121.98, Entropy = 6.93 bits
Computation based on 4371 words.
Number of 5-grams hit = 1304  (29.83%)
Number of 4-grams hit = 1014  (23.20%)
Number of 3-grams hit = 1110  (25.39%)
Number of 2-grams hit = 790  (18.07%)
Number of 1-grams hit = 153  (3.50%)
38 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article610.text
Perplexity = 165.78, Entropy = 7.37 bits
Computation based on 5950 words.
Number of 5-grams hit = 1315  (22.10%)
Number of 4-grams hit = 1584  (26.62%)
Number of 3-grams hit = 1745  (29.33%)
Number of 2-grams hit = 1052  (17.68%)
Number of 1-grams hit = 254  (4.27%)
13 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article611.text
Perplexity = 153.08, Entropy = 7.26 bits
Computation based on 776 words.
Number of 5-grams hit = 185  (23.84%)
Number of 4-grams hit = 204  (26.29%)
Number of 3-grams hit = 228  (29.38%)
Number of 2-grams hit = 121  (15.59%)
Number of 1-grams hit = 38  (4.90%)
4 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article612.text
Perplexity = 80.66, Entropy = 6.33 bits
Computation based on 1629 words.
Number of 5-grams hit = 549  (33.70%)
Number of 4-grams hit = 389  (23.88%)
Number of 3-grams hit = 404  (24.80%)
Number of 2-grams hit = 252  (15.47%)
Number of 1-grams hit = 35  (2.15%)
3 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article613.text
Perplexity = 116.00, Entropy = 6.86 bits
Computation based on 4146 words.
Number of 5-grams hit = 1299  (31.33%)
Number of 4-grams hit = 914  (22.05%)
Number of 3-grams hit = 1021  (24.63%)
Number of 2-grams hit = 767  (18.50%)
Number of 1-grams hit = 145  (3.50%)
42 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article614.text
Perplexity = 102.07, Entropy = 6.67 bits
Computation based on 1524 words.
Number of 5-grams hit = 419  (27.49%)
Number of 4-grams hit = 379  (24.87%)
Number of 3-grams hit = 423  (27.76%)
Number of 2-grams hit = 263  (17.26%)
Number of 1-grams hit = 40  (2.62%)
26 OOVs (1.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article615.text
Perplexity = 137.45, Entropy = 7.10 bits
Computation based on 452 words.
Number of 5-grams hit = 137  (30.31%)
Number of 4-grams hit = 101  (22.35%)
Number of 3-grams hit = 116  (25.66%)
Number of 2-grams hit = 80  (17.70%)
Number of 1-grams hit = 18  (3.98%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article616.text
Perplexity = 135.34, Entropy = 7.08 bits
Computation based on 767 words.
Number of 5-grams hit = 205  (26.73%)
Number of 4-grams hit = 179  (23.34%)
Number of 3-grams hit = 208  (27.12%)
Number of 2-grams hit = 140  (18.25%)
Number of 1-grams hit = 35  (4.56%)
5 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article617.text
Perplexity = 101.57, Entropy = 6.67 bits
Computation based on 1319 words.
Number of 5-grams hit = 369  (27.98%)
Number of 4-grams hit = 302  (22.90%)
Number of 3-grams hit = 375  (28.43%)
Number of 2-grams hit = 237  (17.97%)
Number of 1-grams hit = 36  (2.73%)
5 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article618.text
Perplexity = 171.68, Entropy = 7.42 bits
Computation based on 3769 words.
Number of 5-grams hit = 783  (20.77%)
Number of 4-grams hit = 944  (25.05%)
Number of 3-grams hit = 1170  (31.04%)
Number of 2-grams hit = 701  (18.60%)
Number of 1-grams hit = 171  (4.54%)
8 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article619.text
Perplexity = 137.94, Entropy = 7.11 bits
Computation based on 761 words.
Number of 5-grams hit = 175  (23.00%)
Number of 4-grams hit = 218  (28.65%)
Number of 3-grams hit = 211  (27.73%)
Number of 2-grams hit = 131  (17.21%)
Number of 1-grams hit = 26  (3.42%)
6 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article620.text
Perplexity = 172.67, Entropy = 7.43 bits
Computation based on 5314 words.
Number of 5-grams hit = 1188  (22.36%)
Number of 4-grams hit = 1328  (24.99%)
Number of 3-grams hit = 1561  (29.38%)
Number of 2-grams hit = 986  (18.55%)
Number of 1-grams hit = 251  (4.72%)
15 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article621.text
Perplexity = 96.35, Entropy = 6.59 bits
Computation based on 321 words.
Number of 5-grams hit = 103  (32.09%)
Number of 4-grams hit = 62  (19.31%)
Number of 3-grams hit = 76  (23.68%)
Number of 2-grams hit = 59  (18.38%)
Number of 1-grams hit = 21  (6.54%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article622.text
Perplexity = 180.61, Entropy = 7.50 bits
Computation based on 452 words.
Number of 5-grams hit = 89  (19.69%)
Number of 4-grams hit = 96  (21.24%)
Number of 3-grams hit = 149  (32.96%)
Number of 2-grams hit = 90  (19.91%)
Number of 1-grams hit = 28  (6.19%)
7 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article623.text
Perplexity = 160.72, Entropy = 7.33 bits
Computation based on 1306 words.
Number of 5-grams hit = 296  (22.66%)
Number of 4-grams hit = 333  (25.50%)
Number of 3-grams hit = 387  (29.63%)
Number of 2-grams hit = 234  (17.92%)
Number of 1-grams hit = 56  (4.29%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article624.text
Perplexity = 182.15, Entropy = 7.51 bits
Computation based on 5146 words.
Number of 5-grams hit = 1130  (21.96%)
Number of 4-grams hit = 1264  (24.56%)
Number of 3-grams hit = 1518  (29.50%)
Number of 2-grams hit = 1006  (19.55%)
Number of 1-grams hit = 228  (4.43%)
20 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article625.text
Perplexity = 183.28, Entropy = 7.52 bits
Computation based on 1088 words.
Number of 5-grams hit = 223  (20.50%)
Number of 4-grams hit = 267  (24.54%)
Number of 3-grams hit = 342  (31.43%)
Number of 2-grams hit = 204  (18.75%)
Number of 1-grams hit = 52  (4.78%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article626.text
Perplexity = 169.22, Entropy = 7.40 bits
Computation based on 446 words.
Number of 5-grams hit = 121  (27.13%)
Number of 4-grams hit = 96  (21.52%)
Number of 3-grams hit = 114  (25.56%)
Number of 2-grams hit = 88  (19.73%)
Number of 1-grams hit = 27  (6.05%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article627.text
Perplexity = 354.44, Entropy = 8.47 bits
Computation based on 141 words.
Number of 5-grams hit = 19  (13.48%)
Number of 4-grams hit = 31  (21.99%)
Number of 3-grams hit = 43  (30.50%)
Number of 2-grams hit = 35  (24.82%)
Number of 1-grams hit = 13  (9.22%)
8 OOVs (5.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article628.text
Perplexity = 143.23, Entropy = 7.16 bits
Computation based on 1057 words.
Number of 5-grams hit = 252  (23.84%)
Number of 4-grams hit = 224  (21.19%)
Number of 3-grams hit = 321  (30.37%)
Number of 2-grams hit = 206  (19.49%)
Number of 1-grams hit = 54  (5.11%)
18 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article629.text
Perplexity = 182.71, Entropy = 7.51 bits
Computation based on 1027 words.
Number of 5-grams hit = 226  (22.01%)
Number of 4-grams hit = 262  (25.51%)
Number of 3-grams hit = 287  (27.95%)
Number of 2-grams hit = 199  (19.38%)
Number of 1-grams hit = 53  (5.16%)
10 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article630.text
Perplexity = 202.21, Entropy = 7.66 bits
Computation based on 328 words.
Number of 5-grams hit = 62  (18.90%)
Number of 4-grams hit = 67  (20.43%)
Number of 3-grams hit = 99  (30.18%)
Number of 2-grams hit = 74  (22.56%)
Number of 1-grams hit = 26  (7.93%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article631.text
Perplexity = 217.83, Entropy = 7.77 bits
Computation based on 857 words.
Number of 5-grams hit = 193  (22.52%)
Number of 4-grams hit = 161  (18.79%)
Number of 3-grams hit = 222  (25.90%)
Number of 2-grams hit = 216  (25.20%)
Number of 1-grams hit = 65  (7.58%)
19 OOVs (2.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article632.text
Perplexity = 211.35, Entropy = 7.72 bits
Computation based on 408 words.
Number of 5-grams hit = 72  (17.65%)
Number of 4-grams hit = 105  (25.74%)
Number of 3-grams hit = 128  (31.37%)
Number of 2-grams hit = 85  (20.83%)
Number of 1-grams hit = 18  (4.41%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article633.text
Perplexity = 168.98, Entropy = 7.40 bits
Computation based on 529 words.
Number of 5-grams hit = 127  (24.01%)
Number of 4-grams hit = 124  (23.44%)
Number of 3-grams hit = 154  (29.11%)
Number of 2-grams hit = 99  (18.71%)
Number of 1-grams hit = 25  (4.73%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article634.text
Perplexity = 70.39, Entropy = 6.14 bits
Computation based on 560 words.
Number of 5-grams hit = 210  (37.50%)
Number of 4-grams hit = 134  (23.93%)
Number of 3-grams hit = 134  (23.93%)
Number of 2-grams hit = 72  (12.86%)
Number of 1-grams hit = 10  (1.79%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article635.text
Perplexity = 200.74, Entropy = 7.65 bits
Computation based on 151 words.
Number of 5-grams hit = 35  (23.18%)
Number of 4-grams hit = 27  (17.88%)
Number of 3-grams hit = 51  (33.77%)
Number of 2-grams hit = 33  (21.85%)
Number of 1-grams hit = 5  (3.31%)
1 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article636.text
Perplexity = 316.70, Entropy = 8.31 bits
Computation based on 1322 words.
Number of 5-grams hit = 227  (17.17%)
Number of 4-grams hit = 255  (19.29%)
Number of 3-grams hit = 388  (29.35%)
Number of 2-grams hit = 343  (25.95%)
Number of 1-grams hit = 109  (8.25%)
22 OOVs (1.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article637.text
Perplexity = 151.14, Entropy = 7.24 bits
Computation based on 1034 words.
Number of 5-grams hit = 226  (21.86%)
Number of 4-grams hit = 272  (26.31%)
Number of 3-grams hit = 315  (30.46%)
Number of 2-grams hit = 171  (16.54%)
Number of 1-grams hit = 50  (4.84%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article638.text
Perplexity = 236.28, Entropy = 7.88 bits
Computation based on 526 words.
Number of 5-grams hit = 106  (20.15%)
Number of 4-grams hit = 104  (19.77%)
Number of 3-grams hit = 140  (26.62%)
Number of 2-grams hit = 131  (24.90%)
Number of 1-grams hit = 45  (8.56%)
7 OOVs (1.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article639.text
Perplexity = 183.18, Entropy = 7.52 bits
Computation based on 522 words.
Number of 5-grams hit = 121  (23.18%)
Number of 4-grams hit = 106  (20.31%)
Number of 3-grams hit = 142  (27.20%)
Number of 2-grams hit = 119  (22.80%)
Number of 1-grams hit = 34  (6.51%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article640.text
Perplexity = 152.21, Entropy = 7.25 bits
Computation based on 204 words.
Number of 5-grams hit = 55  (26.96%)
Number of 4-grams hit = 52  (25.49%)
Number of 3-grams hit = 53  (25.98%)
Number of 2-grams hit = 35  (17.16%)
Number of 1-grams hit = 9  (4.41%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article641.text
Perplexity = 227.64, Entropy = 7.83 bits
Computation based on 825 words.
Number of 5-grams hit = 140  (16.97%)
Number of 4-grams hit = 201  (24.36%)
Number of 3-grams hit = 274  (33.21%)
Number of 2-grams hit = 167  (20.24%)
Number of 1-grams hit = 43  (5.21%)
5 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article642.text
Perplexity = 277.03, Entropy = 8.11 bits
Computation based on 429 words.
Number of 5-grams hit = 83  (19.35%)
Number of 4-grams hit = 84  (19.58%)
Number of 3-grams hit = 117  (27.27%)
Number of 2-grams hit = 112  (26.11%)
Number of 1-grams hit = 33  (7.69%)
15 OOVs (3.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article643.text
Perplexity = 159.30, Entropy = 7.32 bits
Computation based on 488 words.
Number of 5-grams hit = 71  (14.55%)
Number of 4-grams hit = 130  (26.64%)
Number of 3-grams hit = 180  (36.89%)
Number of 2-grams hit = 97  (19.88%)
Number of 1-grams hit = 10  (2.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article644.text
Perplexity = 152.40, Entropy = 7.25 bits
Computation based on 1116 words.
Number of 5-grams hit = 238  (21.33%)
Number of 4-grams hit = 290  (25.99%)
Number of 3-grams hit = 365  (32.71%)
Number of 2-grams hit = 183  (16.40%)
Number of 1-grams hit = 40  (3.58%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article645.text
Perplexity = 113.37, Entropy = 6.82 bits
Computation based on 440 words.
Number of 5-grams hit = 116  (26.36%)
Number of 4-grams hit = 108  (24.55%)
Number of 3-grams hit = 118  (26.82%)
Number of 2-grams hit = 85  (19.32%)
Number of 1-grams hit = 13  (2.95%)
6 OOVs (1.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article646.text
Perplexity = 163.07, Entropy = 7.35 bits
Computation based on 573 words.
Number of 5-grams hit = 118  (20.59%)
Number of 4-grams hit = 145  (25.31%)
Number of 3-grams hit = 181  (31.59%)
Number of 2-grams hit = 105  (18.32%)
Number of 1-grams hit = 24  (4.19%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article647.text
Perplexity = 245.53, Entropy = 7.94 bits
Computation based on 608 words.
Number of 5-grams hit = 97  (15.95%)
Number of 4-grams hit = 148  (24.34%)
Number of 3-grams hit = 194  (31.91%)
Number of 2-grams hit = 144  (23.68%)
Number of 1-grams hit = 25  (4.11%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article648.text
Perplexity = 162.60, Entropy = 7.35 bits
Computation based on 475 words.
Number of 5-grams hit = 101  (21.26%)
Number of 4-grams hit = 121  (25.47%)
Number of 3-grams hit = 143  (30.11%)
Number of 2-grams hit = 93  (19.58%)
Number of 1-grams hit = 17  (3.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article649.text
Perplexity = 198.84, Entropy = 7.64 bits
Computation based on 530 words.
Number of 5-grams hit = 103  (19.43%)
Number of 4-grams hit = 134  (25.28%)
Number of 3-grams hit = 163  (30.75%)
Number of 2-grams hit = 102  (19.25%)
Number of 1-grams hit = 28  (5.28%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article650.text
Perplexity = 156.58, Entropy = 7.29 bits
Computation based on 529 words.
Number of 5-grams hit = 130  (24.57%)
Number of 4-grams hit = 115  (21.74%)
Number of 3-grams hit = 150  (28.36%)
Number of 2-grams hit = 106  (20.04%)
Number of 1-grams hit = 28  (5.29%)
5 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article651.text
Perplexity = 154.31, Entropy = 7.27 bits
Computation based on 459 words.
Number of 5-grams hit = 107  (23.31%)
Number of 4-grams hit = 108  (23.53%)
Number of 3-grams hit = 135  (29.41%)
Number of 2-grams hit = 92  (20.04%)
Number of 1-grams hit = 17  (3.70%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article652.text
Perplexity = 78.99, Entropy = 6.30 bits
Computation based on 878 words.
Number of 5-grams hit = 333  (37.93%)
Number of 4-grams hit = 208  (23.69%)
Number of 3-grams hit = 207  (23.58%)
Number of 2-grams hit = 116  (13.21%)
Number of 1-grams hit = 14  (1.59%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article653.text
Perplexity = 147.17, Entropy = 7.20 bits
Computation based on 765 words.
Number of 5-grams hit = 205  (26.80%)
Number of 4-grams hit = 195  (25.49%)
Number of 3-grams hit = 214  (27.97%)
Number of 2-grams hit = 121  (15.82%)
Number of 1-grams hit = 30  (3.92%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article654.text
Perplexity = 184.91, Entropy = 7.53 bits
Computation based on 772 words.
Number of 5-grams hit = 171  (22.15%)
Number of 4-grams hit = 202  (26.17%)
Number of 3-grams hit = 219  (28.37%)
Number of 2-grams hit = 142  (18.39%)
Number of 1-grams hit = 38  (4.92%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article655.text
Perplexity = 153.78, Entropy = 7.26 bits
Computation based on 481 words.
Number of 5-grams hit = 143  (29.73%)
Number of 4-grams hit = 104  (21.62%)
Number of 3-grams hit = 119  (24.74%)
Number of 2-grams hit = 84  (17.46%)
Number of 1-grams hit = 31  (6.44%)
6 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article656.text
Perplexity = 315.49, Entropy = 8.30 bits
Computation based on 461 words.
Number of 5-grams hit = 75  (16.27%)
Number of 4-grams hit = 91  (19.74%)
Number of 3-grams hit = 129  (27.98%)
Number of 2-grams hit = 124  (26.90%)
Number of 1-grams hit = 42  (9.11%)
5 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article657.text
Perplexity = 151.58, Entropy = 7.24 bits
Computation based on 376 words.
Number of 5-grams hit = 84  (22.34%)
Number of 4-grams hit = 98  (26.06%)
Number of 3-grams hit = 111  (29.52%)
Number of 2-grams hit = 57  (15.16%)
Number of 1-grams hit = 26  (6.91%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article658.text
Perplexity = 143.56, Entropy = 7.17 bits
Computation based on 1004 words.
Number of 5-grams hit = 265  (26.39%)
Number of 4-grams hit = 243  (24.20%)
Number of 3-grams hit = 292  (29.08%)
Number of 2-grams hit = 167  (16.63%)
Number of 1-grams hit = 37  (3.69%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article659.text
Perplexity = 154.68, Entropy = 7.27 bits
Computation based on 964 words.
Number of 5-grams hit = 251  (26.04%)
Number of 4-grams hit = 215  (22.30%)
Number of 3-grams hit = 247  (25.62%)
Number of 2-grams hit = 197  (20.44%)
Number of 1-grams hit = 54  (5.60%)
26 OOVs (2.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article660.text
Perplexity = 255.43, Entropy = 8.00 bits
Computation based on 139 words.
Number of 5-grams hit = 18  (12.95%)
Number of 4-grams hit = 35  (25.18%)
Number of 3-grams hit = 46  (33.09%)
Number of 2-grams hit = 26  (18.71%)
Number of 1-grams hit = 14  (10.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article661.text
Perplexity = 181.27, Entropy = 7.50 bits
Computation based on 1244 words.
Number of 5-grams hit = 287  (23.07%)
Number of 4-grams hit = 310  (24.92%)
Number of 3-grams hit = 356  (28.62%)
Number of 2-grams hit = 224  (18.01%)
Number of 1-grams hit = 67  (5.39%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article662.text
Perplexity = 472.23, Entropy = 8.88 bits
Computation based on 123 words.
Number of 5-grams hit = 14  (11.38%)
Number of 4-grams hit = 21  (17.07%)
Number of 3-grams hit = 41  (33.33%)
Number of 2-grams hit = 32  (26.02%)
Number of 1-grams hit = 15  (12.20%)
3 OOVs (2.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article663.text
Perplexity = 227.69, Entropy = 7.83 bits
Computation based on 1338 words.
Number of 5-grams hit = 237  (17.71%)
Number of 4-grams hit = 291  (21.75%)
Number of 3-grams hit = 393  (29.37%)
Number of 2-grams hit = 346  (25.86%)
Number of 1-grams hit = 71  (5.31%)
22 OOVs (1.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article664.text
Perplexity = 85.84, Entropy = 6.42 bits
Computation based on 692 words.
Number of 5-grams hit = 237  (34.25%)
Number of 4-grams hit = 158  (22.83%)
Number of 3-grams hit = 176  (25.43%)
Number of 2-grams hit = 100  (14.45%)
Number of 1-grams hit = 21  (3.03%)
7 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article665.text
Perplexity = 143.86, Entropy = 7.17 bits
Computation based on 756 words.
Number of 5-grams hit = 197  (26.06%)
Number of 4-grams hit = 191  (25.26%)
Number of 3-grams hit = 213  (28.17%)
Number of 2-grams hit = 126  (16.67%)
Number of 1-grams hit = 29  (3.84%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article666.text
Perplexity = 180.78, Entropy = 7.50 bits
Computation based on 383 words.
Number of 5-grams hit = 99  (25.85%)
Number of 4-grams hit = 84  (21.93%)
Number of 3-grams hit = 97  (25.33%)
Number of 2-grams hit = 83  (21.67%)
Number of 1-grams hit = 20  (5.22%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article667.text
Perplexity = 76.45, Entropy = 6.26 bits
Computation based on 1292 words.
Number of 5-grams hit = 508  (39.32%)
Number of 4-grams hit = 319  (24.69%)
Number of 3-grams hit = 284  (21.98%)
Number of 2-grams hit = 166  (12.85%)
Number of 1-grams hit = 15  (1.16%)
9 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article668.text
Perplexity = 103.27, Entropy = 6.69 bits
Computation based on 466 words.
Number of 5-grams hit = 135  (28.97%)
Number of 4-grams hit = 102  (21.89%)
Number of 3-grams hit = 141  (30.26%)
Number of 2-grams hit = 78  (16.74%)
Number of 1-grams hit = 10  (2.15%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article669.text
Perplexity = 190.37, Entropy = 7.57 bits
Computation based on 485 words.
Number of 5-grams hit = 95  (19.59%)
Number of 4-grams hit = 115  (23.71%)
Number of 3-grams hit = 160  (32.99%)
Number of 2-grams hit = 90  (18.56%)
Number of 1-grams hit = 25  (5.15%)
4 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article670.text
Perplexity = 78.95, Entropy = 6.30 bits
Computation based on 448 words.
Number of 5-grams hit = 163  (36.38%)
Number of 4-grams hit = 93  (20.76%)
Number of 3-grams hit = 106  (23.66%)
Number of 2-grams hit = 72  (16.07%)
Number of 1-grams hit = 14  (3.12%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article671.text
Perplexity = 175.78, Entropy = 7.46 bits
Computation based on 2031 words.
Number of 5-grams hit = 438  (21.57%)
Number of 4-grams hit = 514  (25.31%)
Number of 3-grams hit = 616  (30.33%)
Number of 2-grams hit = 370  (18.22%)
Number of 1-grams hit = 93  (4.58%)
5 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article672.text
Perplexity = 220.35, Entropy = 7.78 bits
Computation based on 284 words.
Number of 5-grams hit = 61  (21.48%)
Number of 4-grams hit = 75  (26.41%)
Number of 3-grams hit = 79  (27.82%)
Number of 2-grams hit = 51  (17.96%)
Number of 1-grams hit = 18  (6.34%)
4 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article673.text
Perplexity = 143.25, Entropy = 7.16 bits
Computation based on 301 words.
Number of 5-grams hit = 67  (22.26%)
Number of 4-grams hit = 84  (27.91%)
Number of 3-grams hit = 93  (30.90%)
Number of 2-grams hit = 45  (14.95%)
Number of 1-grams hit = 12  (3.99%)
3 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article674.text
Perplexity = 110.54, Entropy = 6.79 bits
Computation based on 141 words.
Number of 5-grams hit = 46  (32.62%)
Number of 4-grams hit = 31  (21.99%)
Number of 3-grams hit = 36  (25.53%)
Number of 2-grams hit = 24  (17.02%)
Number of 1-grams hit = 4  (2.84%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article675.text
Perplexity = 166.27, Entropy = 7.38 bits
Computation based on 3859 words.
Number of 5-grams hit = 946  (24.51%)
Number of 4-grams hit = 997  (25.84%)
Number of 3-grams hit = 1069  (27.70%)
Number of 2-grams hit = 673  (17.44%)
Number of 1-grams hit = 174  (4.51%)
15 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article676.text
Perplexity = 177.94, Entropy = 7.48 bits
Computation based on 496 words.
Number of 5-grams hit = 99  (19.96%)
Number of 4-grams hit = 118  (23.79%)
Number of 3-grams hit = 169  (34.07%)
Number of 2-grams hit = 87  (17.54%)
Number of 1-grams hit = 23  (4.64%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article677.text
Perplexity = 192.82, Entropy = 7.59 bits
Computation based on 641 words.
Number of 5-grams hit = 127  (19.81%)
Number of 4-grams hit = 155  (24.18%)
Number of 3-grams hit = 185  (28.86%)
Number of 2-grams hit = 146  (22.78%)
Number of 1-grams hit = 28  (4.37%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article678.text
Perplexity = 178.78, Entropy = 7.48 bits
Computation based on 340 words.
Number of 5-grams hit = 64  (18.82%)
Number of 4-grams hit = 83  (24.41%)
Number of 3-grams hit = 113  (33.24%)
Number of 2-grams hit = 66  (19.41%)
Number of 1-grams hit = 14  (4.12%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article679.text
Perplexity = 163.92, Entropy = 7.36 bits
Computation based on 986 words.
Number of 5-grams hit = 208  (21.10%)
Number of 4-grams hit = 246  (24.95%)
Number of 3-grams hit = 305  (30.93%)
Number of 2-grams hit = 184  (18.66%)
Number of 1-grams hit = 43  (4.36%)
3 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article680.text
Perplexity = 131.22, Entropy = 7.04 bits
Computation based on 317 words.
Number of 5-grams hit = 108  (34.07%)
Number of 4-grams hit = 71  (22.40%)
Number of 3-grams hit = 71  (22.40%)
Number of 2-grams hit = 51  (16.09%)
Number of 1-grams hit = 16  (5.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article681.text
Perplexity = 127.33, Entropy = 6.99 bits
Computation based on 474 words.
Number of 5-grams hit = 106  (22.36%)
Number of 4-grams hit = 140  (29.54%)
Number of 3-grams hit = 140  (29.54%)
Number of 2-grams hit = 71  (14.98%)
Number of 1-grams hit = 17  (3.59%)
4 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article682.text
Perplexity = 156.87, Entropy = 7.29 bits
Computation based on 3680 words.
Number of 5-grams hit = 824  (22.39%)
Number of 4-grams hit = 945  (25.68%)
Number of 3-grams hit = 1127  (30.62%)
Number of 2-grams hit = 643  (17.47%)
Number of 1-grams hit = 141  (3.83%)
10 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article683.text
Perplexity = 138.46, Entropy = 7.11 bits
Computation based on 261 words.
Number of 5-grams hit = 79  (30.27%)
Number of 4-grams hit = 40  (15.33%)
Number of 3-grams hit = 70  (26.82%)
Number of 2-grams hit = 60  (22.99%)
Number of 1-grams hit = 12  (4.60%)
5 OOVs (1.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article684.text
Perplexity = 108.97, Entropy = 6.77 bits
Computation based on 4339 words.
Number of 5-grams hit = 1322  (30.47%)
Number of 4-grams hit = 975  (22.47%)
Number of 3-grams hit = 1137  (26.20%)
Number of 2-grams hit = 774  (17.84%)
Number of 1-grams hit = 131  (3.02%)
19 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article685.text
Perplexity = 208.34, Entropy = 7.70 bits
Computation based on 455 words.
Number of 5-grams hit = 86  (18.90%)
Number of 4-grams hit = 103  (22.64%)
Number of 3-grams hit = 144  (31.65%)
Number of 2-grams hit = 96  (21.10%)
Number of 1-grams hit = 26  (5.71%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article686.text
Perplexity = 184.46, Entropy = 7.53 bits
Computation based on 2213 words.
Number of 5-grams hit = 455  (20.56%)
Number of 4-grams hit = 554  (25.03%)
Number of 3-grams hit = 678  (30.64%)
Number of 2-grams hit = 425  (19.20%)
Number of 1-grams hit = 101  (4.56%)
16 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article687.text
Perplexity = 123.30, Entropy = 6.95 bits
Computation based on 512 words.
Number of 5-grams hit = 150  (29.30%)
Number of 4-grams hit = 97  (18.95%)
Number of 3-grams hit = 143  (27.93%)
Number of 2-grams hit = 101  (19.73%)
Number of 1-grams hit = 21  (4.10%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article688.text
Perplexity = 139.81, Entropy = 7.13 bits
Computation based on 303 words.
Number of 5-grams hit = 62  (20.46%)
Number of 4-grams hit = 83  (27.39%)
Number of 3-grams hit = 95  (31.35%)
Number of 2-grams hit = 54  (17.82%)
Number of 1-grams hit = 9  (2.97%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article689.text
Perplexity = 269.84, Entropy = 8.08 bits
Computation based on 508 words.
Number of 5-grams hit = 108  (21.26%)
Number of 4-grams hit = 108  (21.26%)
Number of 3-grams hit = 144  (28.35%)
Number of 2-grams hit = 112  (22.05%)
Number of 1-grams hit = 36  (7.09%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article690.text
Perplexity = 145.90, Entropy = 7.19 bits
Computation based on 754 words.
Number of 5-grams hit = 151  (20.03%)
Number of 4-grams hit = 193  (25.60%)
Number of 3-grams hit = 258  (34.22%)
Number of 2-grams hit = 130  (17.24%)
Number of 1-grams hit = 22  (2.92%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article691.text
Perplexity = 172.54, Entropy = 7.43 bits
Computation based on 872 words.
Number of 5-grams hit = 176  (20.18%)
Number of 4-grams hit = 222  (25.46%)
Number of 3-grams hit = 272  (31.19%)
Number of 2-grams hit = 156  (17.89%)
Number of 1-grams hit = 46  (5.28%)
6 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article692.text
Perplexity = 402.99, Entropy = 8.65 bits
Computation based on 427 words.
Number of 5-grams hit = 69  (16.16%)
Number of 4-grams hit = 71  (16.63%)
Number of 3-grams hit = 120  (28.10%)
Number of 2-grams hit = 127  (29.74%)
Number of 1-grams hit = 40  (9.37%)
6 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article693.text
Perplexity = 132.92, Entropy = 7.05 bits
Computation based on 922 words.
Number of 5-grams hit = 274  (29.72%)
Number of 4-grams hit = 188  (20.39%)
Number of 3-grams hit = 242  (26.25%)
Number of 2-grams hit = 181  (19.63%)
Number of 1-grams hit = 37  (4.01%)
22 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article694.text
Perplexity = 96.16, Entropy = 6.59 bits
Computation based on 626 words.
Number of 5-grams hit = 179  (28.59%)
Number of 4-grams hit = 157  (25.08%)
Number of 3-grams hit = 171  (27.32%)
Number of 2-grams hit = 103  (16.45%)
Number of 1-grams hit = 16  (2.56%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article695.text
Perplexity = 145.04, Entropy = 7.18 bits
Computation based on 849 words.
Number of 5-grams hit = 189  (22.26%)
Number of 4-grams hit = 221  (26.03%)
Number of 3-grams hit = 256  (30.15%)
Number of 2-grams hit = 145  (17.08%)
Number of 1-grams hit = 38  (4.48%)
5 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article696.text
Perplexity = 119.02, Entropy = 6.90 bits
Computation based on 496 words.
Number of 5-grams hit = 147  (29.64%)
Number of 4-grams hit = 97  (19.56%)
Number of 3-grams hit = 122  (24.60%)
Number of 2-grams hit = 115  (23.19%)
Number of 1-grams hit = 15  (3.02%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article697.text
Perplexity = 172.00, Entropy = 7.43 bits
Computation based on 942 words.
Number of 5-grams hit = 205  (21.76%)
Number of 4-grams hit = 238  (25.27%)
Number of 3-grams hit = 296  (31.42%)
Number of 2-grams hit = 160  (16.99%)
Number of 1-grams hit = 43  (4.56%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article698.text
Perplexity = 180.51, Entropy = 7.50 bits
Computation based on 572 words.
Number of 5-grams hit = 103  (18.01%)
Number of 4-grams hit = 147  (25.70%)
Number of 3-grams hit = 180  (31.47%)
Number of 2-grams hit = 118  (20.63%)
Number of 1-grams hit = 24  (4.20%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article699.text
Perplexity = 151.05, Entropy = 7.24 bits
Computation based on 352 words.
Number of 5-grams hit = 58  (16.48%)
Number of 4-grams hit = 88  (25.00%)
Number of 3-grams hit = 129  (36.65%)
Number of 2-grams hit = 65  (18.47%)
Number of 1-grams hit = 12  (3.41%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article700.text
Perplexity = 9.43, Entropy = 3.24 bits
Computation based on 3290 words.
Number of 5-grams hit = 3122  (94.89%)
Number of 4-grams hit = 72  (2.19%)
Number of 3-grams hit = 60  (1.82%)
Number of 2-grams hit = 35  (1.06%)
Number of 1-grams hit = 1  (0.03%)
53 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article701.text
Perplexity = 172.48, Entropy = 7.43 bits
Computation based on 807 words.
Number of 5-grams hit = 178  (22.06%)
Number of 4-grams hit = 211  (26.15%)
Number of 3-grams hit = 228  (28.25%)
Number of 2-grams hit = 149  (18.46%)
Number of 1-grams hit = 41  (5.08%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article702.text
Perplexity = 264.02, Entropy = 8.04 bits
Computation based on 1792 words.
Number of 5-grams hit = 358  (19.98%)
Number of 4-grams hit = 378  (21.09%)
Number of 3-grams hit = 500  (27.90%)
Number of 2-grams hit = 426  (23.77%)
Number of 1-grams hit = 130  (7.25%)
45 OOVs (2.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article703.text
Perplexity = 237.56, Entropy = 7.89 bits
Computation based on 421 words.
Number of 5-grams hit = 80  (19.00%)
Number of 4-grams hit = 96  (22.80%)
Number of 3-grams hit = 116  (27.55%)
Number of 2-grams hit = 100  (23.75%)
Number of 1-grams hit = 29  (6.89%)
5 OOVs (1.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article704.text
Perplexity = 160.67, Entropy = 7.33 bits
Computation based on 311 words.
Number of 5-grams hit = 66  (21.22%)
Number of 4-grams hit = 85  (27.33%)
Number of 3-grams hit = 93  (29.90%)
Number of 2-grams hit = 54  (17.36%)
Number of 1-grams hit = 13  (4.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article705.text
Perplexity = 175.21, Entropy = 7.45 bits
Computation based on 1010 words.
Number of 5-grams hit = 228  (22.57%)
Number of 4-grams hit = 264  (26.14%)
Number of 3-grams hit = 296  (29.31%)
Number of 2-grams hit = 169  (16.73%)
Number of 1-grams hit = 53  (5.25%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article706.text
Perplexity = 197.56, Entropy = 7.63 bits
Computation based on 476 words.
Number of 5-grams hit = 108  (22.69%)
Number of 4-grams hit = 98  (20.59%)
Number of 3-grams hit = 145  (30.46%)
Number of 2-grams hit = 98  (20.59%)
Number of 1-grams hit = 27  (5.67%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article707.text
Perplexity = 303.64, Entropy = 8.25 bits
Computation based on 869 words.
Number of 5-grams hit = 121  (13.92%)
Number of 4-grams hit = 196  (22.55%)
Number of 3-grams hit = 275  (31.65%)
Number of 2-grams hit = 218  (25.09%)
Number of 1-grams hit = 59  (6.79%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article708.text
Perplexity = 157.84, Entropy = 7.30 bits
Computation based on 523 words.
Number of 5-grams hit = 131  (25.05%)
Number of 4-grams hit = 123  (23.52%)
Number of 3-grams hit = 136  (26.00%)
Number of 2-grams hit = 111  (21.22%)
Number of 1-grams hit = 22  (4.21%)
5 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article709.text
Perplexity = 213.40, Entropy = 7.74 bits
Computation based on 548 words.
Number of 5-grams hit = 95  (17.34%)
Number of 4-grams hit = 130  (23.72%)
Number of 3-grams hit = 175  (31.93%)
Number of 2-grams hit = 121  (22.08%)
Number of 1-grams hit = 27  (4.93%)
10 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article710.text
Perplexity = 145.83, Entropy = 7.19 bits
Computation based on 753 words.
Number of 5-grams hit = 176  (23.37%)
Number of 4-grams hit = 187  (24.83%)
Number of 3-grams hit = 217  (28.82%)
Number of 2-grams hit = 139  (18.46%)
Number of 1-grams hit = 34  (4.52%)
21 OOVs (2.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article711.text
Perplexity = 121.32, Entropy = 6.92 bits
Computation based on 959 words.
Number of 5-grams hit = 266  (27.74%)
Number of 4-grams hit = 201  (20.96%)
Number of 3-grams hit = 249  (25.96%)
Number of 2-grams hit = 209  (21.79%)
Number of 1-grams hit = 34  (3.55%)
3 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article712.text
Perplexity = 203.71, Entropy = 7.67 bits
Computation based on 365 words.
Number of 5-grams hit = 77  (21.10%)
Number of 4-grams hit = 93  (25.48%)
Number of 3-grams hit = 97  (26.58%)
Number of 2-grams hit = 71  (19.45%)
Number of 1-grams hit = 27  (7.40%)
4 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article713.text
Perplexity = 102.51, Entropy = 6.68 bits
Computation based on 887 words.
Number of 5-grams hit = 310  (34.95%)
Number of 4-grams hit = 189  (21.31%)
Number of 3-grams hit = 212  (23.90%)
Number of 2-grams hit = 149  (16.80%)
Number of 1-grams hit = 27  (3.04%)
3 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article714.text
Perplexity = 171.49, Entropy = 7.42 bits
Computation based on 364 words.
Number of 5-grams hit = 83  (22.80%)
Number of 4-grams hit = 90  (24.73%)
Number of 3-grams hit = 101  (27.75%)
Number of 2-grams hit = 74  (20.33%)
Number of 1-grams hit = 16  (4.40%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article715.text
Perplexity = 167.50, Entropy = 7.39 bits
Computation based on 486 words.
Number of 5-grams hit = 113  (23.25%)
Number of 4-grams hit = 119  (24.49%)
Number of 3-grams hit = 143  (29.42%)
Number of 2-grams hit = 92  (18.93%)
Number of 1-grams hit = 19  (3.91%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article716.text
Perplexity = 211.70, Entropy = 7.73 bits
Computation based on 452 words.
Number of 5-grams hit = 117  (25.88%)
Number of 4-grams hit = 88  (19.47%)
Number of 3-grams hit = 108  (23.89%)
Number of 2-grams hit = 102  (22.57%)
Number of 1-grams hit = 37  (8.19%)
16 OOVs (3.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article717.text
Perplexity = 103.58, Entropy = 6.69 bits
Computation based on 377 words.
Number of 5-grams hit = 124  (32.89%)
Number of 4-grams hit = 77  (20.42%)
Number of 3-grams hit = 91  (24.14%)
Number of 2-grams hit = 65  (17.24%)
Number of 1-grams hit = 20  (5.31%)
4 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article718.text
Perplexity = 179.38, Entropy = 7.49 bits
Computation based on 6958 words.
Number of 5-grams hit = 1537  (22.09%)
Number of 4-grams hit = 1732  (24.89%)
Number of 3-grams hit = 2099  (30.17%)
Number of 2-grams hit = 1274  (18.31%)
Number of 1-grams hit = 316  (4.54%)
27 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article719.text
Perplexity = 137.71, Entropy = 7.11 bits
Computation based on 1162 words.
Number of 5-grams hit = 279  (24.01%)
Number of 4-grams hit = 312  (26.85%)
Number of 3-grams hit = 338  (29.09%)
Number of 2-grams hit = 190  (16.35%)
Number of 1-grams hit = 43  (3.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article720.text
Perplexity = 129.77, Entropy = 7.02 bits
Computation based on 806 words.
Number of 5-grams hit = 191  (23.70%)
Number of 4-grams hit = 189  (23.45%)
Number of 3-grams hit = 246  (30.52%)
Number of 2-grams hit = 154  (19.11%)
Number of 1-grams hit = 26  (3.23%)
8 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article721.text
Perplexity = 188.71, Entropy = 7.56 bits
Computation based on 423 words.
Number of 5-grams hit = 74  (17.49%)
Number of 4-grams hit = 100  (23.64%)
Number of 3-grams hit = 137  (32.39%)
Number of 2-grams hit = 96  (22.70%)
Number of 1-grams hit = 16  (3.78%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article722.text
Perplexity = 96.72, Entropy = 6.60 bits
Computation based on 1012 words.
Number of 5-grams hit = 314  (31.03%)
Number of 4-grams hit = 235  (23.22%)
Number of 3-grams hit = 278  (27.47%)
Number of 2-grams hit = 154  (15.22%)
Number of 1-grams hit = 31  (3.06%)
8 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article723.text
Perplexity = 161.67, Entropy = 7.34 bits
Computation based on 427 words.
Number of 5-grams hit = 120  (28.10%)
Number of 4-grams hit = 83  (19.44%)
Number of 3-grams hit = 101  (23.65%)
Number of 2-grams hit = 99  (23.19%)
Number of 1-grams hit = 24  (5.62%)
13 OOVs (2.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article724.text
Perplexity = 297.00, Entropy = 8.21 bits
Computation based on 458 words.
Number of 5-grams hit = 71  (15.50%)
Number of 4-grams hit = 110  (24.02%)
Number of 3-grams hit = 139  (30.35%)
Number of 2-grams hit = 106  (23.14%)
Number of 1-grams hit = 32  (6.99%)
4 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article725.text
Perplexity = 163.27, Entropy = 7.35 bits
Computation based on 710 words.
Number of 5-grams hit = 176  (24.79%)
Number of 4-grams hit = 161  (22.68%)
Number of 3-grams hit = 197  (27.75%)
Number of 2-grams hit = 134  (18.87%)
Number of 1-grams hit = 42  (5.92%)
12 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article726.text
Perplexity = 125.15, Entropy = 6.97 bits
Computation based on 519 words.
Number of 5-grams hit = 133  (25.63%)
Number of 4-grams hit = 140  (26.97%)
Number of 3-grams hit = 146  (28.13%)
Number of 2-grams hit = 88  (16.96%)
Number of 1-grams hit = 12  (2.31%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article727.text
Perplexity = 217.10, Entropy = 7.76 bits
Computation based on 482 words.
Number of 5-grams hit = 91  (18.88%)
Number of 4-grams hit = 96  (19.92%)
Number of 3-grams hit = 140  (29.05%)
Number of 2-grams hit = 128  (26.56%)
Number of 1-grams hit = 27  (5.60%)
11 OOVs (2.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article728.text
Perplexity = 161.83, Entropy = 7.34 bits
Computation based on 416 words.
Number of 5-grams hit = 108  (25.96%)
Number of 4-grams hit = 92  (22.12%)
Number of 3-grams hit = 113  (27.16%)
Number of 2-grams hit = 78  (18.75%)
Number of 1-grams hit = 25  (6.01%)
5 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article729.text
Perplexity = 212.27, Entropy = 7.73 bits
Computation based on 437 words.
Number of 5-grams hit = 97  (22.20%)
Number of 4-grams hit = 94  (21.51%)
Number of 3-grams hit = 116  (26.54%)
Number of 2-grams hit = 103  (23.57%)
Number of 1-grams hit = 27  (6.18%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article730.text
Perplexity = 117.05, Entropy = 6.87 bits
Computation based on 8423 words.
Number of 5-grams hit = 2547  (30.24%)
Number of 4-grams hit = 1912  (22.70%)
Number of 3-grams hit = 2239  (26.58%)
Number of 2-grams hit = 1462  (17.36%)
Number of 1-grams hit = 263  (3.12%)
54 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article731.text
Perplexity = 90.18, Entropy = 6.49 bits
Computation based on 1450 words.
Number of 5-grams hit = 450  (31.03%)
Number of 4-grams hit = 352  (24.28%)
Number of 3-grams hit = 397  (27.38%)
Number of 2-grams hit = 215  (14.83%)
Number of 1-grams hit = 36  (2.48%)
6 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article732.text
Perplexity = 164.51, Entropy = 7.36 bits
Computation based on 702 words.
Number of 5-grams hit = 148  (21.08%)
Number of 4-grams hit = 180  (25.64%)
Number of 3-grams hit = 215  (30.63%)
Number of 2-grams hit = 133  (18.95%)
Number of 1-grams hit = 26  (3.70%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article733.text
Perplexity = 142.85, Entropy = 7.16 bits
Computation based on 559 words.
Number of 5-grams hit = 131  (23.43%)
Number of 4-grams hit = 143  (25.58%)
Number of 3-grams hit = 168  (30.05%)
Number of 2-grams hit = 94  (16.82%)
Number of 1-grams hit = 23  (4.11%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article734.text
Perplexity = 397.11, Entropy = 8.63 bits
Computation based on 414 words.
Number of 5-grams hit = 75  (18.12%)
Number of 4-grams hit = 75  (18.12%)
Number of 3-grams hit = 96  (23.19%)
Number of 2-grams hit = 124  (29.95%)
Number of 1-grams hit = 44  (10.63%)
7 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article735.text
Perplexity = 148.18, Entropy = 7.21 bits
Computation based on 1178 words.
Number of 5-grams hit = 255  (21.65%)
Number of 4-grams hit = 323  (27.42%)
Number of 3-grams hit = 370  (31.41%)
Number of 2-grams hit = 192  (16.30%)
Number of 1-grams hit = 38  (3.23%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article736.text
Perplexity = 185.38, Entropy = 7.53 bits
Computation based on 667 words.
Number of 5-grams hit = 137  (20.54%)
Number of 4-grams hit = 159  (23.84%)
Number of 3-grams hit = 205  (30.73%)
Number of 2-grams hit = 138  (20.69%)
Number of 1-grams hit = 28  (4.20%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article737.text
Perplexity = 74.22, Entropy = 6.21 bits
Computation based on 633 words.
Number of 5-grams hit = 260  (41.07%)
Number of 4-grams hit = 118  (18.64%)
Number of 3-grams hit = 143  (22.59%)
Number of 2-grams hit = 90  (14.22%)
Number of 1-grams hit = 22  (3.48%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article738.text
Perplexity = 219.08, Entropy = 7.78 bits
Computation based on 1002 words.
Number of 5-grams hit = 189  (18.86%)
Number of 4-grams hit = 216  (21.56%)
Number of 3-grams hit = 291  (29.04%)
Number of 2-grams hit = 263  (26.25%)
Number of 1-grams hit = 43  (4.29%)
9 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article739.text
Perplexity = 129.15, Entropy = 7.01 bits
Computation based on 433 words.
Number of 5-grams hit = 121  (27.94%)
Number of 4-grams hit = 101  (23.33%)
Number of 3-grams hit = 110  (25.40%)
Number of 2-grams hit = 81  (18.71%)
Number of 1-grams hit = 20  (4.62%)
7 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article740.text
Perplexity = 100.46, Entropy = 6.65 bits
Computation based on 1093 words.
Number of 5-grams hit = 340  (31.11%)
Number of 4-grams hit = 254  (23.24%)
Number of 3-grams hit = 292  (26.72%)
Number of 2-grams hit = 178  (16.29%)
Number of 1-grams hit = 29  (2.65%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article741.text
Perplexity = 173.84, Entropy = 7.44 bits
Computation based on 756 words.
Number of 5-grams hit = 165  (21.83%)
Number of 4-grams hit = 169  (22.35%)
Number of 3-grams hit = 233  (30.82%)
Number of 2-grams hit = 158  (20.90%)
Number of 1-grams hit = 31  (4.10%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article742.text
Perplexity = 259.35, Entropy = 8.02 bits
Computation based on 609 words.
Number of 5-grams hit = 113  (18.56%)
Number of 4-grams hit = 124  (20.36%)
Number of 3-grams hit = 167  (27.42%)
Number of 2-grams hit = 168  (27.59%)
Number of 1-grams hit = 37  (6.08%)
22 OOVs (3.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article743.text
Perplexity = 194.56, Entropy = 7.60 bits
Computation based on 1973 words.
Number of 5-grams hit = 369  (18.70%)
Number of 4-grams hit = 495  (25.09%)
Number of 3-grams hit = 629  (31.88%)
Number of 2-grams hit = 387  (19.61%)
Number of 1-grams hit = 93  (4.71%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article744.text
Perplexity = 128.57, Entropy = 7.01 bits
Computation based on 525 words.
Number of 5-grams hit = 138  (26.29%)
Number of 4-grams hit = 137  (26.10%)
Number of 3-grams hit = 141  (26.86%)
Number of 2-grams hit = 83  (15.81%)
Number of 1-grams hit = 26  (4.95%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article745.text
Perplexity = 107.62, Entropy = 6.75 bits
Computation based on 687 words.
Number of 5-grams hit = 221  (32.17%)
Number of 4-grams hit = 155  (22.56%)
Number of 3-grams hit = 153  (22.27%)
Number of 2-grams hit = 130  (18.92%)
Number of 1-grams hit = 28  (4.08%)
6 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article746.text
Perplexity = 115.66, Entropy = 6.85 bits
Computation based on 1313 words.
Number of 5-grams hit = 392  (29.86%)
Number of 4-grams hit = 335  (25.51%)
Number of 3-grams hit = 346  (26.35%)
Number of 2-grams hit = 212  (16.15%)
Number of 1-grams hit = 28  (2.13%)
19 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article747.text
Perplexity = 57.15, Entropy = 5.84 bits
Computation based on 518 words.
Number of 5-grams hit = 208  (40.15%)
Number of 4-grams hit = 117  (22.59%)
Number of 3-grams hit = 116  (22.39%)
Number of 2-grams hit = 70  (13.51%)
Number of 1-grams hit = 7  (1.35%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article748.text
Perplexity = 197.85, Entropy = 7.63 bits
Computation based on 500 words.
Number of 5-grams hit = 117  (23.40%)
Number of 4-grams hit = 108  (21.60%)
Number of 3-grams hit = 145  (29.00%)
Number of 2-grams hit = 93  (18.60%)
Number of 1-grams hit = 37  (7.40%)
9 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article749.text
Perplexity = 111.11, Entropy = 6.80 bits
Computation based on 550 words.
Number of 5-grams hit = 170  (30.91%)
Number of 4-grams hit = 107  (19.45%)
Number of 3-grams hit = 145  (26.36%)
Number of 2-grams hit = 105  (19.09%)
Number of 1-grams hit = 23  (4.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article750.text
Perplexity = 139.03, Entropy = 7.12 bits
Computation based on 467 words.
Number of 5-grams hit = 110  (23.55%)
Number of 4-grams hit = 108  (23.13%)
Number of 3-grams hit = 166  (35.55%)
Number of 2-grams hit = 63  (13.49%)
Number of 1-grams hit = 20  (4.28%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article751.text
Perplexity = 132.65, Entropy = 7.05 bits
Computation based on 2005 words.
Number of 5-grams hit = 546  (27.23%)
Number of 4-grams hit = 424  (21.15%)
Number of 3-grams hit = 535  (26.68%)
Number of 2-grams hit = 403  (20.10%)
Number of 1-grams hit = 97  (4.84%)
11 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article752.text
Perplexity = 160.20, Entropy = 7.32 bits
Computation based on 427 words.
Number of 5-grams hit = 105  (24.59%)
Number of 4-grams hit = 105  (24.59%)
Number of 3-grams hit = 132  (30.91%)
Number of 2-grams hit = 67  (15.69%)
Number of 1-grams hit = 18  (4.22%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article753.text
Perplexity = 110.67, Entropy = 6.79 bits
Computation based on 531 words.
Number of 5-grams hit = 155  (29.19%)
Number of 4-grams hit = 124  (23.35%)
Number of 3-grams hit = 134  (25.24%)
Number of 2-grams hit = 97  (18.27%)
Number of 1-grams hit = 21  (3.95%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article754.text
Perplexity = 157.18, Entropy = 7.30 bits
Computation based on 1850 words.
Number of 5-grams hit = 463  (25.03%)
Number of 4-grams hit = 484  (26.16%)
Number of 3-grams hit = 511  (27.62%)
Number of 2-grams hit = 311  (16.81%)
Number of 1-grams hit = 81  (4.38%)
12 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article755.text
Perplexity = 192.17, Entropy = 7.59 bits
Computation based on 448 words.
Number of 5-grams hit = 106  (23.66%)
Number of 4-grams hit = 108  (24.11%)
Number of 3-grams hit = 118  (26.34%)
Number of 2-grams hit = 94  (20.98%)
Number of 1-grams hit = 22  (4.91%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article756.text
Perplexity = 203.96, Entropy = 7.67 bits
Computation based on 709 words.
Number of 5-grams hit = 110  (15.51%)
Number of 4-grams hit = 166  (23.41%)
Number of 3-grams hit = 245  (34.56%)
Number of 2-grams hit = 160  (22.57%)
Number of 1-grams hit = 28  (3.95%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article757.text
Perplexity = 13.87, Entropy = 3.79 bits
Computation based on 506 words.
Number of 5-grams hit = 405  (80.04%)
Number of 4-grams hit = 41  (8.10%)
Number of 3-grams hit = 37  (7.31%)
Number of 2-grams hit = 18  (3.56%)
Number of 1-grams hit = 5  (0.99%)
4 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article758.text
Perplexity = 176.52, Entropy = 7.46 bits
Computation based on 2506 words.
Number of 5-grams hit = 523  (20.87%)
Number of 4-grams hit = 628  (25.06%)
Number of 3-grams hit = 783  (31.25%)
Number of 2-grams hit = 473  (18.87%)
Number of 1-grams hit = 99  (3.95%)
10 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article759.text
Perplexity = 102.62, Entropy = 6.68 bits
Computation based on 3315 words.
Number of 5-grams hit = 1001  (30.20%)
Number of 4-grams hit = 809  (24.40%)
Number of 3-grams hit = 870  (26.24%)
Number of 2-grams hit = 555  (16.74%)
Number of 1-grams hit = 80  (2.41%)
7 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article760.text
Perplexity = 12.40, Entropy = 3.63 bits
Computation based on 550 words.
Number of 5-grams hit = 476  (86.55%)
Number of 4-grams hit = 28  (5.09%)
Number of 3-grams hit = 26  (4.73%)
Number of 2-grams hit = 14  (2.55%)
Number of 1-grams hit = 6  (1.09%)
6 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article761.text
Perplexity = 82.79, Entropy = 6.37 bits
Computation based on 629 words.
Number of 5-grams hit = 235  (37.36%)
Number of 4-grams hit = 121  (19.24%)
Number of 3-grams hit = 158  (25.12%)
Number of 2-grams hit = 96  (15.26%)
Number of 1-grams hit = 19  (3.02%)
6 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article762.text
Perplexity = 181.50, Entropy = 7.50 bits
Computation based on 790 words.
Number of 5-grams hit = 171  (21.65%)
Number of 4-grams hit = 182  (23.04%)
Number of 3-grams hit = 251  (31.77%)
Number of 2-grams hit = 144  (18.23%)
Number of 1-grams hit = 42  (5.32%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article763.text
Perplexity = 148.57, Entropy = 7.21 bits
Computation based on 722 words.
Number of 5-grams hit = 158  (21.88%)
Number of 4-grams hit = 187  (25.90%)
Number of 3-grams hit = 236  (32.69%)
Number of 2-grams hit = 115  (15.93%)
Number of 1-grams hit = 26  (3.60%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article764.text
Perplexity = 191.29, Entropy = 7.58 bits
Computation based on 534 words.
Number of 5-grams hit = 124  (23.22%)
Number of 4-grams hit = 113  (21.16%)
Number of 3-grams hit = 149  (27.90%)
Number of 2-grams hit = 117  (21.91%)
Number of 1-grams hit = 31  (5.81%)
7 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article765.text
Perplexity = 262.35, Entropy = 8.04 bits
Computation based on 828 words.
Number of 5-grams hit = 130  (15.70%)
Number of 4-grams hit = 165  (19.93%)
Number of 3-grams hit = 260  (31.40%)
Number of 2-grams hit = 226  (27.29%)
Number of 1-grams hit = 47  (5.68%)
10 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article766.text
Perplexity = 116.91, Entropy = 6.87 bits
Computation based on 263 words.
Number of 5-grams hit = 81  (30.80%)
Number of 4-grams hit = 57  (21.67%)
Number of 3-grams hit = 62  (23.57%)
Number of 2-grams hit = 49  (18.63%)
Number of 1-grams hit = 14  (5.32%)
5 OOVs (1.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article767.text
Perplexity = 129.47, Entropy = 7.02 bits
Computation based on 4386 words.
Number of 5-grams hit = 1044  (23.80%)
Number of 4-grams hit = 1018  (23.21%)
Number of 3-grams hit = 1311  (29.89%)
Number of 2-grams hit = 876  (19.97%)
Number of 1-grams hit = 137  (3.12%)
33 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article768.text
Perplexity = 131.41, Entropy = 7.04 bits
Computation based on 461 words.
Number of 5-grams hit = 125  (27.11%)
Number of 4-grams hit = 119  (25.81%)
Number of 3-grams hit = 129  (27.98%)
Number of 2-grams hit = 69  (14.97%)
Number of 1-grams hit = 19  (4.12%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article769.text
Perplexity = 65.08, Entropy = 6.02 bits
Computation based on 677 words.
Number of 5-grams hit = 249  (36.78%)
Number of 4-grams hit = 180  (26.59%)
Number of 3-grams hit = 163  (24.08%)
Number of 2-grams hit = 78  (11.52%)
Number of 1-grams hit = 7  (1.03%)
4 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article770.text
Perplexity = 188.69, Entropy = 7.56 bits
Computation based on 606 words.
Number of 5-grams hit = 124  (20.46%)
Number of 4-grams hit = 159  (26.24%)
Number of 3-grams hit = 174  (28.71%)
Number of 2-grams hit = 117  (19.31%)
Number of 1-grams hit = 32  (5.28%)
4 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article771.text
Perplexity = 158.58, Entropy = 7.31 bits
Computation based on 468 words.
Number of 5-grams hit = 89  (19.02%)
Number of 4-grams hit = 119  (25.43%)
Number of 3-grams hit = 157  (33.55%)
Number of 2-grams hit = 89  (19.02%)
Number of 1-grams hit = 14  (2.99%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article772.text
Perplexity = 107.60, Entropy = 6.75 bits
Computation based on 541 words.
Number of 5-grams hit = 142  (26.25%)
Number of 4-grams hit = 129  (23.84%)
Number of 3-grams hit = 143  (26.43%)
Number of 2-grams hit = 103  (19.04%)
Number of 1-grams hit = 24  (4.44%)
6 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article773.text
Perplexity = 162.99, Entropy = 7.35 bits
Computation based on 2984 words.
Number of 5-grams hit = 658  (22.05%)
Number of 4-grams hit = 743  (24.90%)
Number of 3-grams hit = 915  (30.66%)
Number of 2-grams hit = 554  (18.57%)
Number of 1-grams hit = 114  (3.82%)
14 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article774.text
Perplexity = 97.13, Entropy = 6.60 bits
Computation based on 2009 words.
Number of 5-grams hit = 595  (29.62%)
Number of 4-grams hit = 515  (25.63%)
Number of 3-grams hit = 541  (26.93%)
Number of 2-grams hit = 310  (15.43%)
Number of 1-grams hit = 48  (2.39%)
13 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article775.text
Perplexity = 101.69, Entropy = 6.67 bits
Computation based on 498 words.
Number of 5-grams hit = 155  (31.12%)
Number of 4-grams hit = 114  (22.89%)
Number of 3-grams hit = 131  (26.31%)
Number of 2-grams hit = 84  (16.87%)
Number of 1-grams hit = 14  (2.81%)
7 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article776.text
Perplexity = 109.51, Entropy = 6.77 bits
Computation based on 1575 words.
Number of 5-grams hit = 498  (31.62%)
Number of 4-grams hit = 348  (22.10%)
Number of 3-grams hit = 416  (26.41%)
Number of 2-grams hit = 270  (17.14%)
Number of 1-grams hit = 43  (2.73%)
9 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article777.text
Perplexity = 108.16, Entropy = 6.76 bits
Computation based on 9319 words.
Number of 5-grams hit = 3091  (33.17%)
Number of 4-grams hit = 2081  (22.33%)
Number of 3-grams hit = 2312  (24.81%)
Number of 2-grams hit = 1562  (16.76%)
Number of 1-grams hit = 273  (2.93%)
59 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article778.text
Perplexity = 186.47, Entropy = 7.54 bits
Computation based on 503 words.
Number of 5-grams hit = 111  (22.07%)
Number of 4-grams hit = 123  (24.45%)
Number of 3-grams hit = 161  (32.01%)
Number of 2-grams hit = 84  (16.70%)
Number of 1-grams hit = 24  (4.77%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article779.text
Perplexity = 125.46, Entropy = 6.97 bits
Computation based on 932 words.
Number of 5-grams hit = 240  (25.75%)
Number of 4-grams hit = 223  (23.93%)
Number of 3-grams hit = 272  (29.18%)
Number of 2-grams hit = 163  (17.49%)
Number of 1-grams hit = 34  (3.65%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article780.text
Perplexity = 134.48, Entropy = 7.07 bits
Computation based on 401 words.
Number of 5-grams hit = 97  (24.19%)
Number of 4-grams hit = 108  (26.93%)
Number of 3-grams hit = 115  (28.68%)
Number of 2-grams hit = 66  (16.46%)
Number of 1-grams hit = 15  (3.74%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article781.text
Perplexity = 126.78, Entropy = 6.99 bits
Computation based on 732 words.
Number of 5-grams hit = 180  (24.59%)
Number of 4-grams hit = 175  (23.91%)
Number of 3-grams hit = 212  (28.96%)
Number of 2-grams hit = 140  (19.13%)
Number of 1-grams hit = 25  (3.42%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article782.text
Perplexity = 144.23, Entropy = 7.17 bits
Computation based on 1362 words.
Number of 5-grams hit = 333  (24.45%)
Number of 4-grams hit = 349  (25.62%)
Number of 3-grams hit = 407  (29.88%)
Number of 2-grams hit = 223  (16.37%)
Number of 1-grams hit = 50  (3.67%)
1 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article783.text
Perplexity = 157.27, Entropy = 7.30 bits
Computation based on 501 words.
Number of 5-grams hit = 118  (23.55%)
Number of 4-grams hit = 119  (23.75%)
Number of 3-grams hit = 159  (31.74%)
Number of 2-grams hit = 87  (17.37%)
Number of 1-grams hit = 18  (3.59%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article784.text
Perplexity = 140.63, Entropy = 7.14 bits
Computation based on 1721 words.
Number of 5-grams hit = 384  (22.31%)
Number of 4-grams hit = 471  (27.37%)
Number of 3-grams hit = 531  (30.85%)
Number of 2-grams hit = 266  (15.46%)
Number of 1-grams hit = 69  (4.01%)
8 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article785.text
Perplexity = 110.99, Entropy = 6.79 bits
Computation based on 2613 words.
Number of 5-grams hit = 846  (32.38%)
Number of 4-grams hit = 602  (23.04%)
Number of 3-grams hit = 647  (24.76%)
Number of 2-grams hit = 431  (16.49%)
Number of 1-grams hit = 87  (3.33%)
30 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article786.text
Perplexity = 95.78, Entropy = 6.58 bits
Computation based on 353 words.
Number of 5-grams hit = 107  (30.31%)
Number of 4-grams hit = 83  (23.51%)
Number of 3-grams hit = 93  (26.35%)
Number of 2-grams hit = 63  (17.85%)
Number of 1-grams hit = 7  (1.98%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article787.text
Perplexity = 255.29, Entropy = 8.00 bits
Computation based on 472 words.
Number of 5-grams hit = 80  (16.95%)
Number of 4-grams hit = 86  (18.22%)
Number of 3-grams hit = 151  (31.99%)
Number of 2-grams hit = 129  (27.33%)
Number of 1-grams hit = 26  (5.51%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article788.text
Perplexity = 69.68, Entropy = 6.12 bits
Computation based on 1453 words.
Number of 5-grams hit = 557  (38.33%)
Number of 4-grams hit = 371  (25.53%)
Number of 3-grams hit = 351  (24.16%)
Number of 2-grams hit = 160  (11.01%)
Number of 1-grams hit = 14  (0.96%)
3 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article789.text
Perplexity = 167.33, Entropy = 7.39 bits
Computation based on 11445 words.
Number of 5-grams hit = 2560  (22.37%)
Number of 4-grams hit = 2939  (25.68%)
Number of 3-grams hit = 3443  (30.08%)
Number of 2-grams hit = 2005  (17.52%)
Number of 1-grams hit = 498  (4.35%)
47 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article790.text
Perplexity = 180.43, Entropy = 7.50 bits
Computation based on 935 words.
Number of 5-grams hit = 191  (20.43%)
Number of 4-grams hit = 238  (25.45%)
Number of 3-grams hit = 281  (30.05%)
Number of 2-grams hit = 180  (19.25%)
Number of 1-grams hit = 45  (4.81%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article791.text
Perplexity = 153.35, Entropy = 7.26 bits
Computation based on 696 words.
Number of 5-grams hit = 142  (20.40%)
Number of 4-grams hit = 179  (25.72%)
Number of 3-grams hit = 215  (30.89%)
Number of 2-grams hit = 132  (18.97%)
Number of 1-grams hit = 28  (4.02%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article792.text
Perplexity = 175.17, Entropy = 7.45 bits
Computation based on 2871 words.
Number of 5-grams hit = 704  (24.52%)
Number of 4-grams hit = 667  (23.23%)
Number of 3-grams hit = 842  (29.33%)
Number of 2-grams hit = 524  (18.25%)
Number of 1-grams hit = 134  (4.67%)
13 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article793.text
Perplexity = 144.20, Entropy = 7.17 bits
Computation based on 180 words.
Number of 5-grams hit = 37  (20.56%)
Number of 4-grams hit = 55  (30.56%)
Number of 3-grams hit = 48  (26.67%)
Number of 2-grams hit = 32  (17.78%)
Number of 1-grams hit = 8  (4.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article794.text
Perplexity = 215.98, Entropy = 7.75 bits
Computation based on 500 words.
Number of 5-grams hit = 98  (19.60%)
Number of 4-grams hit = 102  (20.40%)
Number of 3-grams hit = 141  (28.20%)
Number of 2-grams hit = 130  (26.00%)
Number of 1-grams hit = 29  (5.80%)
4 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article795.text
Perplexity = 309.53, Entropy = 8.27 bits
Computation based on 426 words.
Number of 5-grams hit = 68  (15.96%)
Number of 4-grams hit = 97  (22.77%)
Number of 3-grams hit = 132  (30.99%)
Number of 2-grams hit = 94  (22.07%)
Number of 1-grams hit = 35  (8.22%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article796.text
Perplexity = 194.71, Entropy = 7.61 bits
Computation based on 1490 words.
Number of 5-grams hit = 318  (21.34%)
Number of 4-grams hit = 403  (27.05%)
Number of 3-grams hit = 409  (27.45%)
Number of 2-grams hit = 286  (19.19%)
Number of 1-grams hit = 74  (4.97%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article797.text
Perplexity = 115.48, Entropy = 6.85 bits
Computation based on 375 words.
Number of 5-grams hit = 102  (27.20%)
Number of 4-grams hit = 104  (27.73%)
Number of 3-grams hit = 110  (29.33%)
Number of 2-grams hit = 50  (13.33%)
Number of 1-grams hit = 9  (2.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article798.text
Perplexity = 119.29, Entropy = 6.90 bits
Computation based on 458 words.
Number of 5-grams hit = 125  (27.29%)
Number of 4-grams hit = 106  (23.14%)
Number of 3-grams hit = 125  (27.29%)
Number of 2-grams hit = 87  (19.00%)
Number of 1-grams hit = 15  (3.28%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article799.text
Perplexity = 188.32, Entropy = 7.56 bits
Computation based on 741 words.
Number of 5-grams hit = 170  (22.94%)
Number of 4-grams hit = 162  (21.86%)
Number of 3-grams hit = 202  (27.26%)
Number of 2-grams hit = 167  (22.54%)
Number of 1-grams hit = 40  (5.40%)
10 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article800.text
Perplexity = 162.35, Entropy = 7.34 bits
Computation based on 431 words.
Number of 5-grams hit = 98  (22.74%)
Number of 4-grams hit = 104  (24.13%)
Number of 3-grams hit = 133  (30.86%)
Number of 2-grams hit = 79  (18.33%)
Number of 1-grams hit = 17  (3.94%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article801.text
Perplexity = 94.66, Entropy = 6.56 bits
Computation based on 3087 words.
Number of 5-grams hit = 1064  (34.47%)
Number of 4-grams hit = 725  (23.49%)
Number of 3-grams hit = 805  (26.08%)
Number of 2-grams hit = 446  (14.45%)
Number of 1-grams hit = 47  (1.52%)
28 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article802.text
Perplexity = 181.48, Entropy = 7.50 bits
Computation based on 2082 words.
Number of 5-grams hit = 389  (18.68%)
Number of 4-grams hit = 474  (22.77%)
Number of 3-grams hit = 622  (29.88%)
Number of 2-grams hit = 487  (23.39%)
Number of 1-grams hit = 110  (5.28%)
28 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article803.text
Perplexity = 119.81, Entropy = 6.90 bits
Computation based on 4196 words.
Number of 5-grams hit = 1283  (30.58%)
Number of 4-grams hit = 951  (22.66%)
Number of 3-grams hit = 1072  (25.55%)
Number of 2-grams hit = 750  (17.87%)
Number of 1-grams hit = 140  (3.34%)
30 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article804.text
Perplexity = 334.33, Entropy = 8.39 bits
Computation based on 1431 words.
Number of 5-grams hit = 241  (16.84%)
Number of 4-grams hit = 276  (19.29%)
Number of 3-grams hit = 397  (27.74%)
Number of 2-grams hit = 393  (27.46%)
Number of 1-grams hit = 124  (8.67%)
37 OOVs (2.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article805.text
Perplexity = 174.95, Entropy = 7.45 bits
Computation based on 919 words.
Number of 5-grams hit = 180  (19.59%)
Number of 4-grams hit = 254  (27.64%)
Number of 3-grams hit = 275  (29.92%)
Number of 2-grams hit = 173  (18.82%)
Number of 1-grams hit = 37  (4.03%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article806.text
Perplexity = 157.35, Entropy = 7.30 bits
Computation based on 3287 words.
Number of 5-grams hit = 754  (22.94%)
Number of 4-grams hit = 820  (24.95%)
Number of 3-grams hit = 1011  (30.76%)
Number of 2-grams hit = 595  (18.10%)
Number of 1-grams hit = 107  (3.26%)
16 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article807.text
Perplexity = 93.56, Entropy = 6.55 bits
Computation based on 708 words.
Number of 5-grams hit = 185  (26.13%)
Number of 4-grams hit = 176  (24.86%)
Number of 3-grams hit = 211  (29.80%)
Number of 2-grams hit = 118  (16.67%)
Number of 1-grams hit = 18  (2.54%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article808.text
Perplexity = 181.18, Entropy = 7.50 bits
Computation based on 1682 words.
Number of 5-grams hit = 342  (20.33%)
Number of 4-grams hit = 412  (24.49%)
Number of 3-grams hit = 537  (31.93%)
Number of 2-grams hit = 306  (18.19%)
Number of 1-grams hit = 85  (5.05%)
12 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article809.text
Perplexity = 145.68, Entropy = 7.19 bits
Computation based on 2017 words.
Number of 5-grams hit = 545  (27.02%)
Number of 4-grams hit = 435  (21.57%)
Number of 3-grams hit = 578  (28.66%)
Number of 2-grams hit = 378  (18.74%)
Number of 1-grams hit = 81  (4.02%)
9 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article810.text
Perplexity = 108.84, Entropy = 6.77 bits
Computation based on 869 words.
Number of 5-grams hit = 235  (27.04%)
Number of 4-grams hit = 213  (24.51%)
Number of 3-grams hit = 238  (27.39%)
Number of 2-grams hit = 158  (18.18%)
Number of 1-grams hit = 25  (2.88%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article811.text
Perplexity = 165.53, Entropy = 7.37 bits
Computation based on 4497 words.
Number of 5-grams hit = 1055  (23.46%)
Number of 4-grams hit = 1113  (24.75%)
Number of 3-grams hit = 1337  (29.73%)
Number of 2-grams hit = 800  (17.79%)
Number of 1-grams hit = 192  (4.27%)
14 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article812.text
Perplexity = 112.71, Entropy = 6.82 bits
Computation based on 3353 words.
Number of 5-grams hit = 994  (29.65%)
Number of 4-grams hit = 797  (23.77%)
Number of 3-grams hit = 897  (26.75%)
Number of 2-grams hit = 575  (17.15%)
Number of 1-grams hit = 90  (2.68%)
16 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article813.text
Perplexity = 242.45, Entropy = 7.92 bits
Computation based on 1357 words.
Number of 5-grams hit = 267  (19.68%)
Number of 4-grams hit = 262  (19.31%)
Number of 3-grams hit = 410  (30.21%)
Number of 2-grams hit = 330  (24.32%)
Number of 1-grams hit = 88  (6.48%)
23 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article814.text
Perplexity = 74.29, Entropy = 6.22 bits
Computation based on 851 words.
Number of 5-grams hit = 288  (33.84%)
Number of 4-grams hit = 209  (24.56%)
Number of 3-grams hit = 207  (24.32%)
Number of 2-grams hit = 128  (15.04%)
Number of 1-grams hit = 19  (2.23%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article815.text
Perplexity = 158.15, Entropy = 7.31 bits
Computation based on 1264 words.
Number of 5-grams hit = 327  (25.87%)
Number of 4-grams hit = 296  (23.42%)
Number of 3-grams hit = 357  (28.24%)
Number of 2-grams hit = 229  (18.12%)
Number of 1-grams hit = 55  (4.35%)
10 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article816.text
Perplexity = 88.89, Entropy = 6.47 bits
Computation based on 1331 words.
Number of 5-grams hit = 451  (33.88%)
Number of 4-grams hit = 299  (22.46%)
Number of 3-grams hit = 334  (25.09%)
Number of 2-grams hit = 210  (15.78%)
Number of 1-grams hit = 37  (2.78%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article817.text
Perplexity = 182.06, Entropy = 7.51 bits
Computation based on 533 words.
Number of 5-grams hit = 113  (21.20%)
Number of 4-grams hit = 138  (25.89%)
Number of 3-grams hit = 157  (29.46%)
Number of 2-grams hit = 100  (18.76%)
Number of 1-grams hit = 25  (4.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article818.text
Perplexity = 248.40, Entropy = 7.96 bits
Computation based on 532 words.
Number of 5-grams hit = 115  (21.62%)
Number of 4-grams hit = 117  (21.99%)
Number of 3-grams hit = 136  (25.56%)
Number of 2-grams hit = 117  (21.99%)
Number of 1-grams hit = 47  (8.83%)
21 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article819.text
Perplexity = 169.04, Entropy = 7.40 bits
Computation based on 1803 words.
Number of 5-grams hit = 394  (21.85%)
Number of 4-grams hit = 438  (24.29%)
Number of 3-grams hit = 559  (31.00%)
Number of 2-grams hit = 324  (17.97%)
Number of 1-grams hit = 88  (4.88%)
6 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article820.text
Perplexity = 176.01, Entropy = 7.46 bits
Computation based on 816 words.
Number of 5-grams hit = 158  (19.36%)
Number of 4-grams hit = 197  (24.14%)
Number of 3-grams hit = 242  (29.66%)
Number of 2-grams hit = 184  (22.55%)
Number of 1-grams hit = 35  (4.29%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article821.text
Perplexity = 80.90, Entropy = 6.34 bits
Computation based on 1100 words.
Number of 5-grams hit = 369  (33.55%)
Number of 4-grams hit = 282  (25.64%)
Number of 3-grams hit = 282  (25.64%)
Number of 2-grams hit = 144  (13.09%)
Number of 1-grams hit = 23  (2.09%)
4 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article822.text
Perplexity = 168.81, Entropy = 7.40 bits
Computation based on 2420 words.
Number of 5-grams hit = 524  (21.65%)
Number of 4-grams hit = 569  (23.51%)
Number of 3-grams hit = 771  (31.86%)
Number of 2-grams hit = 454  (18.76%)
Number of 1-grams hit = 102  (4.21%)
4 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article823.text
Perplexity = 164.45, Entropy = 7.36 bits
Computation based on 1307 words.
Number of 5-grams hit = 265  (20.28%)
Number of 4-grams hit = 328  (25.10%)
Number of 3-grams hit = 407  (31.14%)
Number of 2-grams hit = 256  (19.59%)
Number of 1-grams hit = 51  (3.90%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article824.text
Perplexity = 233.61, Entropy = 7.87 bits
Computation based on 486 words.
Number of 5-grams hit = 83  (17.08%)
Number of 4-grams hit = 114  (23.46%)
Number of 3-grams hit = 135  (27.78%)
Number of 2-grams hit = 122  (25.10%)
Number of 1-grams hit = 32  (6.58%)
16 OOVs (3.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article825.text
Perplexity = 148.55, Entropy = 7.21 bits
Computation based on 571 words.
Number of 5-grams hit = 113  (19.79%)
Number of 4-grams hit = 149  (26.09%)
Number of 3-grams hit = 184  (32.22%)
Number of 2-grams hit = 104  (18.21%)
Number of 1-grams hit = 21  (3.68%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article826.text
Perplexity = 117.20, Entropy = 6.87 bits
Computation based on 889 words.
Number of 5-grams hit = 284  (31.95%)
Number of 4-grams hit = 201  (22.61%)
Number of 3-grams hit = 231  (25.98%)
Number of 2-grams hit = 146  (16.42%)
Number of 1-grams hit = 27  (3.04%)
4 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article827.text
Perplexity = 202.41, Entropy = 7.66 bits
Computation based on 627 words.
Number of 5-grams hit = 152  (24.24%)
Number of 4-grams hit = 131  (20.89%)
Number of 3-grams hit = 168  (26.79%)
Number of 2-grams hit = 139  (22.17%)
Number of 1-grams hit = 37  (5.90%)
21 OOVs (3.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article828.text
Perplexity = 131.69, Entropy = 7.04 bits
Computation based on 11850 words.
Number of 5-grams hit = 3537  (29.85%)
Number of 4-grams hit = 2665  (22.49%)
Number of 3-grams hit = 3096  (26.13%)
Number of 2-grams hit = 2111  (17.81%)
Number of 1-grams hit = 441  (3.72%)
147 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article829.text
Perplexity = 251.80, Entropy = 7.98 bits
Computation based on 531 words.
Number of 5-grams hit = 106  (19.96%)
Number of 4-grams hit = 103  (19.40%)
Number of 3-grams hit = 142  (26.74%)
Number of 2-grams hit = 141  (26.55%)
Number of 1-grams hit = 39  (7.34%)
9 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article830.text
Perplexity = 230.28, Entropy = 7.85 bits
Computation based on 2276 words.
Number of 5-grams hit = 503  (22.10%)
Number of 4-grams hit = 486  (21.35%)
Number of 3-grams hit = 615  (27.02%)
Number of 2-grams hit = 499  (21.92%)
Number of 1-grams hit = 173  (7.60%)
43 OOVs (1.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article831.text
Perplexity = 205.10, Entropy = 7.68 bits
Computation based on 1025 words.
Number of 5-grams hit = 233  (22.73%)
Number of 4-grams hit = 234  (22.83%)
Number of 3-grams hit = 297  (28.98%)
Number of 2-grams hit = 202  (19.71%)
Number of 1-grams hit = 59  (5.76%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article832.text
Perplexity = 124.84, Entropy = 6.96 bits
Computation based on 419 words.
Number of 5-grams hit = 122  (29.12%)
Number of 4-grams hit = 87  (20.76%)
Number of 3-grams hit = 114  (27.21%)
Number of 2-grams hit = 78  (18.62%)
Number of 1-grams hit = 18  (4.30%)
3 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article833.text
Perplexity = 166.80, Entropy = 7.38 bits
Computation based on 826 words.
Number of 5-grams hit = 187  (22.64%)
Number of 4-grams hit = 205  (24.82%)
Number of 3-grams hit = 249  (30.15%)
Number of 2-grams hit = 148  (17.92%)
Number of 1-grams hit = 37  (4.48%)
3 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article834.text
Perplexity = 147.50, Entropy = 7.20 bits
Computation based on 881 words.
Number of 5-grams hit = 198  (22.47%)
Number of 4-grams hit = 235  (26.67%)
Number of 3-grams hit = 262  (29.74%)
Number of 2-grams hit = 158  (17.93%)
Number of 1-grams hit = 28  (3.18%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article835.text
Perplexity = 171.43, Entropy = 7.42 bits
Computation based on 460 words.
Number of 5-grams hit = 85  (18.48%)
Number of 4-grams hit = 127  (27.61%)
Number of 3-grams hit = 135  (29.35%)
Number of 2-grams hit = 94  (20.43%)
Number of 1-grams hit = 19  (4.13%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article836.text
Perplexity = 109.42, Entropy = 6.77 bits
Computation based on 483 words.
Number of 5-grams hit = 130  (26.92%)
Number of 4-grams hit = 116  (24.02%)
Number of 3-grams hit = 133  (27.54%)
Number of 2-grams hit = 86  (17.81%)
Number of 1-grams hit = 18  (3.73%)
8 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article837.text
Perplexity = 209.63, Entropy = 7.71 bits
Computation based on 845 words.
Number of 5-grams hit = 197  (23.31%)
Number of 4-grams hit = 148  (17.51%)
Number of 3-grams hit = 212  (25.09%)
Number of 2-grams hit = 224  (26.51%)
Number of 1-grams hit = 64  (7.57%)
7 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article838.text
Perplexity = 190.10, Entropy = 7.57 bits
Computation based on 2236 words.
Number of 5-grams hit = 468  (20.93%)
Number of 4-grams hit = 482  (21.56%)
Number of 3-grams hit = 674  (30.14%)
Number of 2-grams hit = 502  (22.45%)
Number of 1-grams hit = 110  (4.92%)
35 OOVs (1.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article839.text
Perplexity = 139.90, Entropy = 7.13 bits
Computation based on 461 words.
Number of 5-grams hit = 120  (26.03%)
Number of 4-grams hit = 99  (21.48%)
Number of 3-grams hit = 125  (27.11%)
Number of 2-grams hit = 101  (21.91%)
Number of 1-grams hit = 16  (3.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article840.text
Perplexity = 106.94, Entropy = 6.74 bits
Computation based on 521 words.
Number of 5-grams hit = 154  (29.56%)
Number of 4-grams hit = 118  (22.65%)
Number of 3-grams hit = 130  (24.95%)
Number of 2-grams hit = 95  (18.23%)
Number of 1-grams hit = 24  (4.61%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article841.text
Perplexity = 159.00, Entropy = 7.31 bits
Computation based on 916 words.
Number of 5-grams hit = 204  (22.27%)
Number of 4-grams hit = 233  (25.44%)
Number of 3-grams hit = 283  (30.90%)
Number of 2-grams hit = 155  (16.92%)
Number of 1-grams hit = 41  (4.48%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article842.text
Perplexity = 159.23, Entropy = 7.31 bits
Computation based on 473 words.
Number of 5-grams hit = 110  (23.26%)
Number of 4-grams hit = 98  (20.72%)
Number of 3-grams hit = 148  (31.29%)
Number of 2-grams hit = 97  (20.51%)
Number of 1-grams hit = 20  (4.23%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article843.text
Perplexity = 141.72, Entropy = 7.15 bits
Computation based on 1310 words.
Number of 5-grams hit = 320  (24.43%)
Number of 4-grams hit = 337  (25.73%)
Number of 3-grams hit = 415  (31.68%)
Number of 2-grams hit = 191  (14.58%)
Number of 1-grams hit = 47  (3.59%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article844.text
Perplexity = 159.23, Entropy = 7.31 bits
Computation based on 15758 words.
Number of 5-grams hit = 3524  (22.36%)
Number of 4-grams hit = 4067  (25.81%)
Number of 3-grams hit = 4750  (30.14%)
Number of 2-grams hit = 2776  (17.62%)
Number of 1-grams hit = 641  (4.07%)
53 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article845.text
Perplexity = 152.77, Entropy = 7.26 bits
Computation based on 601 words.
Number of 5-grams hit = 116  (19.30%)
Number of 4-grams hit = 156  (25.96%)
Number of 3-grams hit = 201  (33.44%)
Number of 2-grams hit = 102  (16.97%)
Number of 1-grams hit = 26  (4.33%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article846.text
Perplexity = 151.05, Entropy = 7.24 bits
Computation based on 556 words.
Number of 5-grams hit = 146  (26.26%)
Number of 4-grams hit = 115  (20.68%)
Number of 3-grams hit = 157  (28.24%)
Number of 2-grams hit = 108  (19.42%)
Number of 1-grams hit = 30  (5.40%)
8 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article847.text
Perplexity = 130.72, Entropy = 7.03 bits
Computation based on 1613 words.
Number of 5-grams hit = 420  (26.04%)
Number of 4-grams hit = 423  (26.22%)
Number of 3-grams hit = 463  (28.70%)
Number of 2-grams hit = 256  (15.87%)
Number of 1-grams hit = 51  (3.16%)
8 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article848.text
Perplexity = 78.86, Entropy = 6.30 bits
Computation based on 366 words.
Number of 5-grams hit = 128  (34.97%)
Number of 4-grams hit = 76  (20.77%)
Number of 3-grams hit = 87  (23.77%)
Number of 2-grams hit = 66  (18.03%)
Number of 1-grams hit = 9  (2.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article849.text
Perplexity = 147.62, Entropy = 7.21 bits
Computation based on 1550 words.
Number of 5-grams hit = 437  (28.19%)
Number of 4-grams hit = 335  (21.61%)
Number of 3-grams hit = 394  (25.42%)
Number of 2-grams hit = 318  (20.52%)
Number of 1-grams hit = 66  (4.26%)
5 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article850.text
Perplexity = 169.70, Entropy = 7.41 bits
Computation based on 443 words.
Number of 5-grams hit = 81  (18.28%)
Number of 4-grams hit = 120  (27.09%)
Number of 3-grams hit = 146  (32.96%)
Number of 2-grams hit = 82  (18.51%)
Number of 1-grams hit = 14  (3.16%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article851.text
Perplexity = 186.52, Entropy = 7.54 bits
Computation based on 558 words.
Number of 5-grams hit = 105  (18.82%)
Number of 4-grams hit = 143  (25.63%)
Number of 3-grams hit = 185  (33.15%)
Number of 2-grams hit = 96  (17.20%)
Number of 1-grams hit = 29  (5.20%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article852.text
Perplexity = 156.73, Entropy = 7.29 bits
Computation based on 622 words.
Number of 5-grams hit = 119  (19.13%)
Number of 4-grams hit = 169  (27.17%)
Number of 3-grams hit = 204  (32.80%)
Number of 2-grams hit = 106  (17.04%)
Number of 1-grams hit = 24  (3.86%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article853.text
Perplexity = 160.57, Entropy = 7.33 bits
Computation based on 1476 words.
Number of 5-grams hit = 375  (25.41%)
Number of 4-grams hit = 367  (24.86%)
Number of 3-grams hit = 410  (27.78%)
Number of 2-grams hit = 252  (17.07%)
Number of 1-grams hit = 72  (4.88%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article854.text
Perplexity = 140.95, Entropy = 7.14 bits
Computation based on 2651 words.
Number of 5-grams hit = 718  (27.08%)
Number of 4-grams hit = 596  (22.48%)
Number of 3-grams hit = 709  (26.74%)
Number of 2-grams hit = 515  (19.43%)
Number of 1-grams hit = 113  (4.26%)
18 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article855.text
Perplexity = 116.97, Entropy = 6.87 bits
Computation based on 423 words.
Number of 5-grams hit = 112  (26.48%)
Number of 4-grams hit = 121  (28.61%)
Number of 3-grams hit = 125  (29.55%)
Number of 2-grams hit = 55  (13.00%)
Number of 1-grams hit = 10  (2.36%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article856.text
Perplexity = 145.45, Entropy = 7.18 bits
Computation based on 824 words.
Number of 5-grams hit = 224  (27.18%)
Number of 4-grams hit = 183  (22.21%)
Number of 3-grams hit = 212  (25.73%)
Number of 2-grams hit = 160  (19.42%)
Number of 1-grams hit = 45  (5.46%)
9 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article857.text
Perplexity = 168.76, Entropy = 7.40 bits
Computation based on 354 words.
Number of 5-grams hit = 64  (18.08%)
Number of 4-grams hit = 77  (21.75%)
Number of 3-grams hit = 103  (29.10%)
Number of 2-grams hit = 89  (25.14%)
Number of 1-grams hit = 21  (5.93%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article858.text
Perplexity = 99.77, Entropy = 6.64 bits
Computation based on 237 words.
Number of 5-grams hit = 72  (30.38%)
Number of 4-grams hit = 62  (26.16%)
Number of 3-grams hit = 68  (28.69%)
Number of 2-grams hit = 31  (13.08%)
Number of 1-grams hit = 4  (1.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article859.text
Perplexity = 239.21, Entropy = 7.90 bits
Computation based on 1095 words.
Number of 5-grams hit = 181  (16.53%)
Number of 4-grams hit = 226  (20.64%)
Number of 3-grams hit = 335  (30.59%)
Number of 2-grams hit = 298  (27.21%)
Number of 1-grams hit = 55  (5.02%)
16 OOVs (1.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article860.text
Perplexity = 123.14, Entropy = 6.94 bits
Computation based on 450 words.
Number of 5-grams hit = 94  (20.89%)
Number of 4-grams hit = 121  (26.89%)
Number of 3-grams hit = 136  (30.22%)
Number of 2-grams hit = 85  (18.89%)
Number of 1-grams hit = 14  (3.11%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article861.text
Perplexity = 162.87, Entropy = 7.35 bits
Computation based on 393 words.
Number of 5-grams hit = 89  (22.65%)
Number of 4-grams hit = 99  (25.19%)
Number of 3-grams hit = 106  (26.97%)
Number of 2-grams hit = 79  (20.10%)
Number of 1-grams hit = 20  (5.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article862.text
Perplexity = 122.18, Entropy = 6.93 bits
Computation based on 452 words.
Number of 5-grams hit = 120  (26.55%)
Number of 4-grams hit = 127  (28.10%)
Number of 3-grams hit = 131  (28.98%)
Number of 2-grams hit = 57  (12.61%)
Number of 1-grams hit = 17  (3.76%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article863.text
Perplexity = 159.08, Entropy = 7.31 bits
Computation based on 662 words.
Number of 5-grams hit = 131  (19.79%)
Number of 4-grams hit = 178  (26.89%)
Number of 3-grams hit = 204  (30.82%)
Number of 2-grams hit = 122  (18.43%)
Number of 1-grams hit = 27  (4.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article864.text
Perplexity = 451.97, Entropy = 8.82 bits
Computation based on 442 words.
Number of 5-grams hit = 76  (17.19%)
Number of 4-grams hit = 81  (18.33%)
Number of 3-grams hit = 113  (25.57%)
Number of 2-grams hit = 139  (31.45%)
Number of 1-grams hit = 33  (7.47%)
7 OOVs (1.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article865.text
Perplexity = 72.51, Entropy = 6.18 bits
Computation based on 6140 words.
Number of 5-grams hit = 2576  (41.95%)
Number of 4-grams hit = 1398  (22.77%)
Number of 3-grams hit = 1267  (20.64%)
Number of 2-grams hit = 759  (12.36%)
Number of 1-grams hit = 140  (2.28%)
25 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article866.text
Perplexity = 234.63, Entropy = 7.87 bits
Computation based on 615 words.
Number of 5-grams hit = 117  (19.02%)
Number of 4-grams hit = 135  (21.95%)
Number of 3-grams hit = 173  (28.13%)
Number of 2-grams hit = 140  (22.76%)
Number of 1-grams hit = 50  (8.13%)
12 OOVs (1.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article867.text
Perplexity = 168.10, Entropy = 7.39 bits
Computation based on 341 words.
Number of 5-grams hit = 69  (20.23%)
Number of 4-grams hit = 90  (26.39%)
Number of 3-grams hit = 112  (32.84%)
Number of 2-grams hit = 52  (15.25%)
Number of 1-grams hit = 18  (5.28%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article868.text
Perplexity = 149.82, Entropy = 7.23 bits
Computation based on 1652 words.
Number of 5-grams hit = 391  (23.67%)
Number of 4-grams hit = 416  (25.18%)
Number of 3-grams hit = 503  (30.45%)
Number of 2-grams hit = 275  (16.65%)
Number of 1-grams hit = 67  (4.06%)
5 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article869.text
Perplexity = 155.37, Entropy = 7.28 bits
Computation based on 3106 words.
Number of 5-grams hit = 666  (21.44%)
Number of 4-grams hit = 817  (26.30%)
Number of 3-grams hit = 975  (31.39%)
Number of 2-grams hit = 547  (17.61%)
Number of 1-grams hit = 101  (3.25%)
10 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article870.text
Perplexity = 196.53, Entropy = 7.62 bits
Computation based on 768 words.
Number of 5-grams hit = 162  (21.09%)
Number of 4-grams hit = 177  (23.05%)
Number of 3-grams hit = 241  (31.38%)
Number of 2-grams hit = 151  (19.66%)
Number of 1-grams hit = 37  (4.82%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article871.text
Perplexity = 187.97, Entropy = 7.55 bits
Computation based on 412 words.
Number of 5-grams hit = 75  (18.20%)
Number of 4-grams hit = 85  (20.63%)
Number of 3-grams hit = 154  (37.38%)
Number of 2-grams hit = 87  (21.12%)
Number of 1-grams hit = 11  (2.67%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article872.text
Perplexity = 200.30, Entropy = 7.65 bits
Computation based on 294 words.
Number of 5-grams hit = 48  (16.33%)
Number of 4-grams hit = 70  (23.81%)
Number of 3-grams hit = 97  (32.99%)
Number of 2-grams hit = 70  (23.81%)
Number of 1-grams hit = 9  (3.06%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article873.text
Perplexity = 198.86, Entropy = 7.64 bits
Computation based on 930 words.
Number of 5-grams hit = 188  (20.22%)
Number of 4-grams hit = 228  (24.52%)
Number of 3-grams hit = 287  (30.86%)
Number of 2-grams hit = 177  (19.03%)
Number of 1-grams hit = 50  (5.38%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article874.text
Perplexity = 159.70, Entropy = 7.32 bits
Computation based on 429 words.
Number of 5-grams hit = 86  (20.05%)
Number of 4-grams hit = 106  (24.71%)
Number of 3-grams hit = 137  (31.93%)
Number of 2-grams hit = 85  (19.81%)
Number of 1-grams hit = 15  (3.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article875.text
Perplexity = 225.14, Entropy = 7.81 bits
Computation based on 786 words.
Number of 5-grams hit = 169  (21.50%)
Number of 4-grams hit = 157  (19.97%)
Number of 3-grams hit = 236  (30.03%)
Number of 2-grams hit = 174  (22.14%)
Number of 1-grams hit = 50  (6.36%)
7 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article876.text
Perplexity = 172.52, Entropy = 7.43 bits
Computation based on 9198 words.
Number of 5-grams hit = 2053  (22.32%)
Number of 4-grams hit = 2311  (25.13%)
Number of 3-grams hit = 2739  (29.78%)
Number of 2-grams hit = 1691  (18.38%)
Number of 1-grams hit = 404  (4.39%)
33 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article877.text
Perplexity = 115.71, Entropy = 6.85 bits
Computation based on 458 words.
Number of 5-grams hit = 145  (31.66%)
Number of 4-grams hit = 107  (23.36%)
Number of 3-grams hit = 123  (26.86%)
Number of 2-grams hit = 63  (13.76%)
Number of 1-grams hit = 20  (4.37%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article878.text
Perplexity = 102.57, Entropy = 6.68 bits
Computation based on 333 words.
Number of 5-grams hit = 112  (33.63%)
Number of 4-grams hit = 75  (22.52%)
Number of 3-grams hit = 81  (24.32%)
Number of 2-grams hit = 52  (15.62%)
Number of 1-grams hit = 13  (3.90%)
5 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article879.text
Perplexity = 168.51, Entropy = 7.40 bits
Computation based on 539 words.
Number of 5-grams hit = 125  (23.19%)
Number of 4-grams hit = 126  (23.38%)
Number of 3-grams hit = 160  (29.68%)
Number of 2-grams hit = 106  (19.67%)
Number of 1-grams hit = 22  (4.08%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article880.text
Perplexity = 108.07, Entropy = 6.76 bits
Computation based on 459 words.
Number of 5-grams hit = 122  (26.58%)
Number of 4-grams hit = 128  (27.89%)
Number of 3-grams hit = 122  (26.58%)
Number of 2-grams hit = 72  (15.69%)
Number of 1-grams hit = 15  (3.27%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article881.text
Perplexity = 240.59, Entropy = 7.91 bits
Computation based on 345 words.
Number of 5-grams hit = 65  (18.84%)
Number of 4-grams hit = 73  (21.16%)
Number of 3-grams hit = 109  (31.59%)
Number of 2-grams hit = 82  (23.77%)
Number of 1-grams hit = 16  (4.64%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article882.text
Perplexity = 149.10, Entropy = 7.22 bits
Computation based on 591 words.
Number of 5-grams hit = 116  (19.63%)
Number of 4-grams hit = 161  (27.24%)
Number of 3-grams hit = 200  (33.84%)
Number of 2-grams hit = 94  (15.91%)
Number of 1-grams hit = 20  (3.38%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article883.text
Perplexity = 140.86, Entropy = 7.14 bits
Computation based on 591 words.
Number of 5-grams hit = 130  (22.00%)
Number of 4-grams hit = 158  (26.73%)
Number of 3-grams hit = 188  (31.81%)
Number of 2-grams hit = 98  (16.58%)
Number of 1-grams hit = 17  (2.88%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article884.text
Perplexity = 104.74, Entropy = 6.71 bits
Computation based on 727 words.
Number of 5-grams hit = 207  (28.47%)
Number of 4-grams hit = 167  (22.97%)
Number of 3-grams hit = 190  (26.13%)
Number of 2-grams hit = 140  (19.26%)
Number of 1-grams hit = 23  (3.16%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article885.text
Perplexity = 129.81, Entropy = 7.02 bits
Computation based on 482 words.
Number of 5-grams hit = 129  (26.76%)
Number of 4-grams hit = 134  (27.80%)
Number of 3-grams hit = 139  (28.84%)
Number of 2-grams hit = 70  (14.52%)
Number of 1-grams hit = 10  (2.07%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article886.text
Perplexity = 174.90, Entropy = 7.45 bits
Computation based on 473 words.
Number of 5-grams hit = 92  (19.45%)
Number of 4-grams hit = 132  (27.91%)
Number of 3-grams hit = 146  (30.87%)
Number of 2-grams hit = 85  (17.97%)
Number of 1-grams hit = 18  (3.81%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article887.text
Perplexity = 100.07, Entropy = 6.64 bits
Computation based on 481 words.
Number of 5-grams hit = 120  (24.95%)
Number of 4-grams hit = 125  (25.99%)
Number of 3-grams hit = 147  (30.56%)
Number of 2-grams hit = 80  (16.63%)
Number of 1-grams hit = 9  (1.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article888.text
Perplexity = 184.29, Entropy = 7.53 bits
Computation based on 1207 words.
Number of 5-grams hit = 241  (19.97%)
Number of 4-grams hit = 294  (24.36%)
Number of 3-grams hit = 371  (30.74%)
Number of 2-grams hit = 238  (19.72%)
Number of 1-grams hit = 63  (5.22%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article889.text
Perplexity = 184.28, Entropy = 7.53 bits
Computation based on 4303 words.
Number of 5-grams hit = 962  (22.36%)
Number of 4-grams hit = 1047  (24.33%)
Number of 3-grams hit = 1282  (29.79%)
Number of 2-grams hit = 817  (18.99%)
Number of 1-grams hit = 195  (4.53%)
10 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article890.text
Perplexity = 89.44, Entropy = 6.48 bits
Computation based on 1038 words.
Number of 5-grams hit = 369  (35.55%)
Number of 4-grams hit = 198  (19.08%)
Number of 3-grams hit = 229  (22.06%)
Number of 2-grams hit = 185  (17.82%)
Number of 1-grams hit = 57  (5.49%)
13 OOVs (1.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article891.text
Perplexity = 151.82, Entropy = 7.25 bits
Computation based on 804 words.
Number of 5-grams hit = 196  (24.38%)
Number of 4-grams hit = 218  (27.11%)
Number of 3-grams hit = 219  (27.24%)
Number of 2-grams hit = 134  (16.67%)
Number of 1-grams hit = 37  (4.60%)
4 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article892.text
Perplexity = 84.25, Entropy = 6.40 bits
Computation based on 3156 words.
Number of 5-grams hit = 1130  (35.80%)
Number of 4-grams hit = 732  (23.19%)
Number of 3-grams hit = 744  (23.57%)
Number of 2-grams hit = 474  (15.02%)
Number of 1-grams hit = 76  (2.41%)
7 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article893.text
Perplexity = 83.86, Entropy = 6.39 bits
Computation based on 625 words.
Number of 5-grams hit = 227  (36.32%)
Number of 4-grams hit = 121  (19.36%)
Number of 3-grams hit = 159  (25.44%)
Number of 2-grams hit = 94  (15.04%)
Number of 1-grams hit = 24  (3.84%)
8 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article894.text
Perplexity = 204.80, Entropy = 7.68 bits
Computation based on 743 words.
Number of 5-grams hit = 144  (19.38%)
Number of 4-grams hit = 178  (23.96%)
Number of 3-grams hit = 227  (30.55%)
Number of 2-grams hit = 158  (21.27%)
Number of 1-grams hit = 36  (4.85%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article895.text
Perplexity = 155.44, Entropy = 7.28 bits
Computation based on 169 words.
Number of 5-grams hit = 28  (16.57%)
Number of 4-grams hit = 54  (31.95%)
Number of 3-grams hit = 50  (29.59%)
Number of 2-grams hit = 30  (17.75%)
Number of 1-grams hit = 7  (4.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article896.text
Perplexity = 177.21, Entropy = 7.47 bits
Computation based on 413 words.
Number of 5-grams hit = 86  (20.82%)
Number of 4-grams hit = 113  (27.36%)
Number of 3-grams hit = 120  (29.06%)
Number of 2-grams hit = 68  (16.46%)
Number of 1-grams hit = 26  (6.30%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article897.text
Perplexity = 102.04, Entropy = 6.67 bits
Computation based on 696 words.
Number of 5-grams hit = 198  (28.45%)
Number of 4-grams hit = 176  (25.29%)
Number of 3-grams hit = 188  (27.01%)
Number of 2-grams hit = 115  (16.52%)
Number of 1-grams hit = 19  (2.73%)
4 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article898.text
Perplexity = 158.86, Entropy = 7.31 bits
Computation based on 1592 words.
Number of 5-grams hit = 365  (22.93%)
Number of 4-grams hit = 433  (27.20%)
Number of 3-grams hit = 456  (28.64%)
Number of 2-grams hit = 272  (17.09%)
Number of 1-grams hit = 66  (4.15%)
10 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article899.text
Perplexity = 258.94, Entropy = 8.02 bits
Computation based on 199 words.
Number of 5-grams hit = 45  (22.61%)
Number of 4-grams hit = 41  (20.60%)
Number of 3-grams hit = 46  (23.12%)
Number of 2-grams hit = 51  (25.63%)
Number of 1-grams hit = 16  (8.04%)
5 OOVs (2.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article900.text
Perplexity = 194.44, Entropy = 7.60 bits
Computation based on 1046 words.
Number of 5-grams hit = 235  (22.47%)
Number of 4-grams hit = 236  (22.56%)
Number of 3-grams hit = 301  (28.78%)
Number of 2-grams hit = 222  (21.22%)
Number of 1-grams hit = 52  (4.97%)
5 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article901.text
Perplexity = 206.06, Entropy = 7.69 bits
Computation based on 388 words.
Number of 5-grams hit = 85  (21.91%)
Number of 4-grams hit = 85  (21.91%)
Number of 3-grams hit = 104  (26.80%)
Number of 2-grams hit = 88  (22.68%)
Number of 1-grams hit = 26  (6.70%)
11 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article902.text
Perplexity = 119.14, Entropy = 6.90 bits
Computation based on 1995 words.
Number of 5-grams hit = 550  (27.57%)
Number of 4-grams hit = 467  (23.41%)
Number of 3-grams hit = 557  (27.92%)
Number of 2-grams hit = 359  (17.99%)
Number of 1-grams hit = 62  (3.11%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article903.text
Perplexity = 127.72, Entropy = 7.00 bits
Computation based on 1184 words.
Number of 5-grams hit = 298  (25.17%)
Number of 4-grams hit = 271  (22.89%)
Number of 3-grams hit = 346  (29.22%)
Number of 2-grams hit = 222  (18.75%)
Number of 1-grams hit = 47  (3.97%)
5 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article904.text
Perplexity = 85.72, Entropy = 6.42 bits
Computation based on 623 words.
Number of 5-grams hit = 192  (30.82%)
Number of 4-grams hit = 152  (24.40%)
Number of 3-grams hit = 174  (27.93%)
Number of 2-grams hit = 94  (15.09%)
Number of 1-grams hit = 11  (1.77%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article905.text
Perplexity = 303.02, Entropy = 8.24 bits
Computation based on 354 words.
Number of 5-grams hit = 54  (15.25%)
Number of 4-grams hit = 78  (22.03%)
Number of 3-grams hit = 108  (30.51%)
Number of 2-grams hit = 89  (25.14%)
Number of 1-grams hit = 25  (7.06%)
3 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article906.text
Perplexity = 99.30, Entropy = 6.63 bits
Computation based on 429 words.
Number of 5-grams hit = 161  (37.53%)
Number of 4-grams hit = 102  (23.78%)
Number of 3-grams hit = 94  (21.91%)
Number of 2-grams hit = 52  (12.12%)
Number of 1-grams hit = 20  (4.66%)
7 OOVs (1.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article907.text
Perplexity = 149.29, Entropy = 7.22 bits
Computation based on 445 words.
Number of 5-grams hit = 103  (23.15%)
Number of 4-grams hit = 114  (25.62%)
Number of 3-grams hit = 137  (30.79%)
Number of 2-grams hit = 75  (16.85%)
Number of 1-grams hit = 16  (3.60%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article908.text
Perplexity = 106.59, Entropy = 6.74 bits
Computation based on 524 words.
Number of 5-grams hit = 136  (25.95%)
Number of 4-grams hit = 109  (20.80%)
Number of 3-grams hit = 156  (29.77%)
Number of 2-grams hit = 103  (19.66%)
Number of 1-grams hit = 20  (3.82%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article909.text
Perplexity = 250.18, Entropy = 7.97 bits
Computation based on 413 words.
Number of 5-grams hit = 73  (17.68%)
Number of 4-grams hit = 99  (23.97%)
Number of 3-grams hit = 129  (31.23%)
Number of 2-grams hit = 82  (19.85%)
Number of 1-grams hit = 30  (7.26%)
4 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article910.text
Perplexity = 157.54, Entropy = 7.30 bits
Computation based on 667 words.
Number of 5-grams hit = 154  (23.09%)
Number of 4-grams hit = 162  (24.29%)
Number of 3-grams hit = 213  (31.93%)
Number of 2-grams hit = 109  (16.34%)
Number of 1-grams hit = 29  (4.35%)
4 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article911.text
Perplexity = 146.20, Entropy = 7.19 bits
Computation based on 635 words.
Number of 5-grams hit = 150  (23.62%)
Number of 4-grams hit = 156  (24.57%)
Number of 3-grams hit = 197  (31.02%)
Number of 2-grams hit = 109  (17.17%)
Number of 1-grams hit = 23  (3.62%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article912.text
Perplexity = 84.64, Entropy = 6.40 bits
Computation based on 675 words.
Number of 5-grams hit = 228  (33.78%)
Number of 4-grams hit = 165  (24.44%)
Number of 3-grams hit = 166  (24.59%)
Number of 2-grams hit = 95  (14.07%)
Number of 1-grams hit = 21  (3.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article913.text
Perplexity = 137.00, Entropy = 7.10 bits
Computation based on 670 words.
Number of 5-grams hit = 179  (26.72%)
Number of 4-grams hit = 181  (27.01%)
Number of 3-grams hit = 176  (26.27%)
Number of 2-grams hit = 110  (16.42%)
Number of 1-grams hit = 24  (3.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article914.text
Perplexity = 149.86, Entropy = 7.23 bits
Computation based on 529 words.
Number of 5-grams hit = 134  (25.33%)
Number of 4-grams hit = 120  (22.68%)
Number of 3-grams hit = 130  (24.57%)
Number of 2-grams hit = 119  (22.50%)
Number of 1-grams hit = 26  (4.91%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article915.text
Perplexity = 183.02, Entropy = 7.52 bits
Computation based on 1206 words.
Number of 5-grams hit = 244  (20.23%)
Number of 4-grams hit = 300  (24.88%)
Number of 3-grams hit = 380  (31.51%)
Number of 2-grams hit = 234  (19.40%)
Number of 1-grams hit = 48  (3.98%)
5 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article916.text
Perplexity = 164.57, Entropy = 7.36 bits
Computation based on 939 words.
Number of 5-grams hit = 217  (23.11%)
Number of 4-grams hit = 211  (22.47%)
Number of 3-grams hit = 256  (27.26%)
Number of 2-grams hit = 202  (21.51%)
Number of 1-grams hit = 53  (5.64%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article917.text
Perplexity = 78.66, Entropy = 6.30 bits
Computation based on 1076 words.
Number of 5-grams hit = 380  (35.32%)
Number of 4-grams hit = 231  (21.47%)
Number of 3-grams hit = 268  (24.91%)
Number of 2-grams hit = 160  (14.87%)
Number of 1-grams hit = 37  (3.44%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article918.text
Perplexity = 189.65, Entropy = 7.57 bits
Computation based on 482 words.
Number of 5-grams hit = 109  (22.61%)
Number of 4-grams hit = 130  (26.97%)
Number of 3-grams hit = 133  (27.59%)
Number of 2-grams hit = 82  (17.01%)
Number of 1-grams hit = 28  (5.81%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article919.text
Perplexity = 167.96, Entropy = 7.39 bits
Computation based on 3300 words.
Number of 5-grams hit = 716  (21.70%)
Number of 4-grams hit = 830  (25.15%)
Number of 3-grams hit = 1028  (31.15%)
Number of 2-grams hit = 593  (17.97%)
Number of 1-grams hit = 133  (4.03%)
15 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article920.text
Perplexity = 166.31, Entropy = 7.38 bits
Computation based on 915 words.
Number of 5-grams hit = 224  (24.48%)
Number of 4-grams hit = 252  (27.54%)
Number of 3-grams hit = 246  (26.89%)
Number of 2-grams hit = 151  (16.50%)
Number of 1-grams hit = 42  (4.59%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article921.text
Perplexity = 140.22, Entropy = 7.13 bits
Computation based on 795 words.
Number of 5-grams hit = 185  (23.27%)
Number of 4-grams hit = 180  (22.64%)
Number of 3-grams hit = 237  (29.81%)
Number of 2-grams hit = 161  (20.25%)
Number of 1-grams hit = 32  (4.03%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article922.text
Perplexity = 139.37, Entropy = 7.12 bits
Computation based on 474 words.
Number of 5-grams hit = 148  (31.22%)
Number of 4-grams hit = 100  (21.10%)
Number of 3-grams hit = 128  (27.00%)
Number of 2-grams hit = 75  (15.82%)
Number of 1-grams hit = 23  (4.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article923.text
Perplexity = 140.86, Entropy = 7.14 bits
Computation based on 519 words.
Number of 5-grams hit = 98  (18.88%)
Number of 4-grams hit = 142  (27.36%)
Number of 3-grams hit = 176  (33.91%)
Number of 2-grams hit = 88  (16.96%)
Number of 1-grams hit = 15  (2.89%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article924.text
Perplexity = 108.66, Entropy = 6.76 bits
Computation based on 341 words.
Number of 5-grams hit = 98  (28.74%)
Number of 4-grams hit = 88  (25.81%)
Number of 3-grams hit = 94  (27.57%)
Number of 2-grams hit = 50  (14.66%)
Number of 1-grams hit = 11  (3.23%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article925.text
Perplexity = 92.54, Entropy = 6.53 bits
Computation based on 3813 words.
Number of 5-grams hit = 1321  (34.64%)
Number of 4-grams hit = 873  (22.90%)
Number of 3-grams hit = 982  (25.75%)
Number of 2-grams hit = 545  (14.29%)
Number of 1-grams hit = 92  (2.41%)
21 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article926.text
Perplexity = 87.82, Entropy = 6.46 bits
Computation based on 1337 words.
Number of 5-grams hit = 457  (34.18%)
Number of 4-grams hit = 303  (22.66%)
Number of 3-grams hit = 361  (27.00%)
Number of 2-grams hit = 184  (13.76%)
Number of 1-grams hit = 32  (2.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article927.text
Perplexity = 225.76, Entropy = 7.82 bits
Computation based on 403 words.
Number of 5-grams hit = 76  (18.86%)
Number of 4-grams hit = 81  (20.10%)
Number of 3-grams hit = 126  (31.27%)
Number of 2-grams hit = 90  (22.33%)
Number of 1-grams hit = 30  (7.44%)
4 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article928.text
Perplexity = 124.90, Entropy = 6.96 bits
Computation based on 483 words.
Number of 5-grams hit = 120  (24.84%)
Number of 4-grams hit = 116  (24.02%)
Number of 3-grams hit = 138  (28.57%)
Number of 2-grams hit = 95  (19.67%)
Number of 1-grams hit = 14  (2.90%)
6 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article929.text
Perplexity = 231.89, Entropy = 7.86 bits
Computation based on 502 words.
Number of 5-grams hit = 86  (17.13%)
Number of 4-grams hit = 107  (21.31%)
Number of 3-grams hit = 170  (33.86%)
Number of 2-grams hit = 120  (23.90%)
Number of 1-grams hit = 19  (3.78%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article930.text
Perplexity = 251.93, Entropy = 7.98 bits
Computation based on 417 words.
Number of 5-grams hit = 59  (14.15%)
Number of 4-grams hit = 87  (20.86%)
Number of 3-grams hit = 150  (35.97%)
Number of 2-grams hit = 101  (24.22%)
Number of 1-grams hit = 20  (4.80%)
17 OOVs (3.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article931.text
Perplexity = 181.34, Entropy = 7.50 bits
Computation based on 595 words.
Number of 5-grams hit = 115  (19.33%)
Number of 4-grams hit = 154  (25.88%)
Number of 3-grams hit = 183  (30.76%)
Number of 2-grams hit = 122  (20.50%)
Number of 1-grams hit = 21  (3.53%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article932.text
Perplexity = 186.72, Entropy = 7.54 bits
Computation based on 211 words.
Number of 5-grams hit = 44  (20.85%)
Number of 4-grams hit = 48  (22.75%)
Number of 3-grams hit = 57  (27.01%)
Number of 2-grams hit = 51  (24.17%)
Number of 1-grams hit = 11  (5.21%)
1 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article933.text
Perplexity = 160.73, Entropy = 7.33 bits
Computation based on 410 words.
Number of 5-grams hit = 109  (26.59%)
Number of 4-grams hit = 97  (23.66%)
Number of 3-grams hit = 113  (27.56%)
Number of 2-grams hit = 73  (17.80%)
Number of 1-grams hit = 18  (4.39%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article934.text
Perplexity = 201.96, Entropy = 7.66 bits
Computation based on 384 words.
Number of 5-grams hit = 82  (21.35%)
Number of 4-grams hit = 70  (18.23%)
Number of 3-grams hit = 101  (26.30%)
Number of 2-grams hit = 106  (27.60%)
Number of 1-grams hit = 25  (6.51%)
7 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article935.text
Perplexity = 154.96, Entropy = 7.28 bits
Computation based on 1153 words.
Number of 5-grams hit = 250  (21.68%)
Number of 4-grams hit = 298  (25.85%)
Number of 3-grams hit = 364  (31.57%)
Number of 2-grams hit = 194  (16.83%)
Number of 1-grams hit = 47  (4.08%)
3 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article936.text
Perplexity = 203.27, Entropy = 7.67 bits
Computation based on 435 words.
Number of 5-grams hit = 107  (24.60%)
Number of 4-grams hit = 91  (20.92%)
Number of 3-grams hit = 104  (23.91%)
Number of 2-grams hit = 97  (22.30%)
Number of 1-grams hit = 36  (8.28%)
6 OOVs (1.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article937.text
Perplexity = 182.49, Entropy = 7.51 bits
Computation based on 228 words.
Number of 5-grams hit = 30  (13.16%)
Number of 4-grams hit = 52  (22.81%)
Number of 3-grams hit = 80  (35.09%)
Number of 2-grams hit = 51  (22.37%)
Number of 1-grams hit = 15  (6.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article938.text
Perplexity = 173.56, Entropy = 7.44 bits
Computation based on 1295 words.
Number of 5-grams hit = 282  (21.78%)
Number of 4-grams hit = 313  (24.17%)
Number of 3-grams hit = 395  (30.50%)
Number of 2-grams hit = 242  (18.69%)
Number of 1-grams hit = 63  (4.86%)
5 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article939.text
Perplexity = 103.79, Entropy = 6.70 bits
Computation based on 476 words.
Number of 5-grams hit = 141  (29.62%)
Number of 4-grams hit = 110  (23.11%)
Number of 3-grams hit = 126  (26.47%)
Number of 2-grams hit = 82  (17.23%)
Number of 1-grams hit = 17  (3.57%)
5 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article940.text
Perplexity = 162.24, Entropy = 7.34 bits
Computation based on 1898 words.
Number of 5-grams hit = 425  (22.39%)
Number of 4-grams hit = 509  (26.82%)
Number of 3-grams hit = 575  (30.30%)
Number of 2-grams hit = 307  (16.17%)
Number of 1-grams hit = 82  (4.32%)
3 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article941.text
Perplexity = 111.37, Entropy = 6.80 bits
Computation based on 1216 words.
Number of 5-grams hit = 325  (26.73%)
Number of 4-grams hit = 276  (22.70%)
Number of 3-grams hit = 341  (28.04%)
Number of 2-grams hit = 223  (18.34%)
Number of 1-grams hit = 51  (4.19%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article942.text
Perplexity = 149.02, Entropy = 7.22 bits
Computation based on 524 words.
Number of 5-grams hit = 139  (26.53%)
Number of 4-grams hit = 138  (26.34%)
Number of 3-grams hit = 140  (26.72%)
Number of 2-grams hit = 84  (16.03%)
Number of 1-grams hit = 23  (4.39%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article943.text
Perplexity = 122.09, Entropy = 6.93 bits
Computation based on 1395 words.
Number of 5-grams hit = 363  (26.02%)
Number of 4-grams hit = 348  (24.95%)
Number of 3-grams hit = 406  (29.10%)
Number of 2-grams hit = 232  (16.63%)
Number of 1-grams hit = 46  (3.30%)
10 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article944.text
Perplexity = 146.26, Entropy = 7.19 bits
Computation based on 436 words.
Number of 5-grams hit = 96  (22.02%)
Number of 4-grams hit = 101  (23.17%)
Number of 3-grams hit = 150  (34.40%)
Number of 2-grams hit = 75  (17.20%)
Number of 1-grams hit = 14  (3.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article945.text
Perplexity = 109.66, Entropy = 6.78 bits
Computation based on 3522 words.
Number of 5-grams hit = 937  (26.60%)
Number of 4-grams hit = 842  (23.91%)
Number of 3-grams hit = 1042  (29.59%)
Number of 2-grams hit = 609  (17.29%)
Number of 1-grams hit = 92  (2.61%)
22 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article946.text
Perplexity = 217.41, Entropy = 7.76 bits
Computation based on 551 words.
Number of 5-grams hit = 119  (21.60%)
Number of 4-grams hit = 125  (22.69%)
Number of 3-grams hit = 156  (28.31%)
Number of 2-grams hit = 112  (20.33%)
Number of 1-grams hit = 39  (7.08%)
15 OOVs (2.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article947.text
Perplexity = 175.54, Entropy = 7.46 bits
Computation based on 422 words.
Number of 5-grams hit = 91  (21.56%)
Number of 4-grams hit = 83  (19.67%)
Number of 3-grams hit = 131  (31.04%)
Number of 2-grams hit = 97  (22.99%)
Number of 1-grams hit = 20  (4.74%)
7 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article948.text
Perplexity = 168.71, Entropy = 7.40 bits
Computation based on 581 words.
Number of 5-grams hit = 111  (19.10%)
Number of 4-grams hit = 149  (25.65%)
Number of 3-grams hit = 183  (31.50%)
Number of 2-grams hit = 113  (19.45%)
Number of 1-grams hit = 25  (4.30%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article949.text
Perplexity = 193.15, Entropy = 7.59 bits
Computation based on 1701 words.
Number of 5-grams hit = 333  (19.58%)
Number of 4-grams hit = 435  (25.57%)
Number of 3-grams hit = 520  (30.57%)
Number of 2-grams hit = 335  (19.69%)
Number of 1-grams hit = 78  (4.59%)
9 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article950.text
Perplexity = 306.77, Entropy = 8.26 bits
Computation based on 500 words.
Number of 5-grams hit = 73  (14.60%)
Number of 4-grams hit = 100  (20.00%)
Number of 3-grams hit = 157  (31.40%)
Number of 2-grams hit = 126  (25.20%)
Number of 1-grams hit = 44  (8.80%)
7 OOVs (1.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article951.text
Perplexity = 132.35, Entropy = 7.05 bits
Computation based on 564 words.
Number of 5-grams hit = 155  (27.48%)
Number of 4-grams hit = 125  (22.16%)
Number of 3-grams hit = 164  (29.08%)
Number of 2-grams hit = 107  (18.97%)
Number of 1-grams hit = 13  (2.30%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article952.text
Perplexity = 179.64, Entropy = 7.49 bits
Computation based on 1283 words.
Number of 5-grams hit = 252  (19.64%)
Number of 4-grams hit = 314  (24.47%)
Number of 3-grams hit = 422  (32.89%)
Number of 2-grams hit = 233  (18.16%)
Number of 1-grams hit = 62  (4.83%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article953.text
Perplexity = 124.34, Entropy = 6.96 bits
Computation based on 2117 words.
Number of 5-grams hit = 558  (26.36%)
Number of 4-grams hit = 511  (24.14%)
Number of 3-grams hit = 584  (27.59%)
Number of 2-grams hit = 401  (18.94%)
Number of 1-grams hit = 63  (2.98%)
18 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article954.text
Perplexity = 127.06, Entropy = 6.99 bits
Computation based on 730 words.
Number of 5-grams hit = 190  (26.03%)
Number of 4-grams hit = 177  (24.25%)
Number of 3-grams hit = 203  (27.81%)
Number of 2-grams hit = 131  (17.95%)
Number of 1-grams hit = 29  (3.97%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article955.text
Perplexity = 77.46, Entropy = 6.28 bits
Computation based on 1475 words.
Number of 5-grams hit = 524  (35.53%)
Number of 4-grams hit = 370  (25.08%)
Number of 3-grams hit = 361  (24.47%)
Number of 2-grams hit = 194  (13.15%)
Number of 1-grams hit = 26  (1.76%)
2 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article956.text
Perplexity = 243.57, Entropy = 7.93 bits
Computation based on 1026 words.
Number of 5-grams hit = 195  (19.01%)
Number of 4-grams hit = 179  (17.45%)
Number of 3-grams hit = 289  (28.17%)
Number of 2-grams hit = 284  (27.68%)
Number of 1-grams hit = 79  (7.70%)
8 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article957.text
Perplexity = 231.76, Entropy = 7.86 bits
Computation based on 302 words.
Number of 5-grams hit = 54  (17.88%)
Number of 4-grams hit = 63  (20.86%)
Number of 3-grams hit = 97  (32.12%)
Number of 2-grams hit = 62  (20.53%)
Number of 1-grams hit = 26  (8.61%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article958.text
Perplexity = 176.22, Entropy = 7.46 bits
Computation based on 940 words.
Number of 5-grams hit = 177  (18.83%)
Number of 4-grams hit = 256  (27.23%)
Number of 3-grams hit = 305  (32.45%)
Number of 2-grams hit = 160  (17.02%)
Number of 1-grams hit = 42  (4.47%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article959.text
Perplexity = 154.57, Entropy = 7.27 bits
Computation based on 201 words.
Number of 5-grams hit = 50  (24.88%)
Number of 4-grams hit = 41  (20.40%)
Number of 3-grams hit = 59  (29.35%)
Number of 2-grams hit = 40  (19.90%)
Number of 1-grams hit = 11  (5.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article960.text
Perplexity = 145.52, Entropy = 7.19 bits
Computation based on 605 words.
Number of 5-grams hit = 136  (22.48%)
Number of 4-grams hit = 155  (25.62%)
Number of 3-grams hit = 195  (32.23%)
Number of 2-grams hit = 100  (16.53%)
Number of 1-grams hit = 19  (3.14%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article961.text
Perplexity = 177.56, Entropy = 7.47 bits
Computation based on 475 words.
Number of 5-grams hit = 92  (19.37%)
Number of 4-grams hit = 117  (24.63%)
Number of 3-grams hit = 138  (29.05%)
Number of 2-grams hit = 109  (22.95%)
Number of 1-grams hit = 19  (4.00%)
17 OOVs (3.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article962.text
Perplexity = 109.34, Entropy = 6.77 bits
Computation based on 2235 words.
Number of 5-grams hit = 694  (31.05%)
Number of 4-grams hit = 537  (24.03%)
Number of 3-grams hit = 579  (25.91%)
Number of 2-grams hit = 360  (16.11%)
Number of 1-grams hit = 65  (2.91%)
1 OOVs (0.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article963.text
Perplexity = 136.93, Entropy = 7.10 bits
Computation based on 2012 words.
Number of 5-grams hit = 496  (24.65%)
Number of 4-grams hit = 536  (26.64%)
Number of 3-grams hit = 583  (28.98%)
Number of 2-grams hit = 322  (16.00%)
Number of 1-grams hit = 75  (3.73%)
5 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article964.text
Perplexity = 130.90, Entropy = 7.03 bits
Computation based on 1010 words.
Number of 5-grams hit = 239  (23.66%)
Number of 4-grams hit = 281  (27.82%)
Number of 3-grams hit = 299  (29.60%)
Number of 2-grams hit = 166  (16.44%)
Number of 1-grams hit = 25  (2.48%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article965.text
Perplexity = 189.00, Entropy = 7.56 bits
Computation based on 643 words.
Number of 5-grams hit = 136  (21.15%)
Number of 4-grams hit = 143  (22.24%)
Number of 3-grams hit = 196  (30.48%)
Number of 2-grams hit = 128  (19.91%)
Number of 1-grams hit = 40  (6.22%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article966.text
Perplexity = 217.54, Entropy = 7.77 bits
Computation based on 789 words.
Number of 5-grams hit = 178  (22.56%)
Number of 4-grams hit = 165  (20.91%)
Number of 3-grams hit = 241  (30.54%)
Number of 2-grams hit = 161  (20.41%)
Number of 1-grams hit = 44  (5.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article967.text
Perplexity = 152.41, Entropy = 7.25 bits
Computation based on 388 words.
Number of 5-grams hit = 86  (22.16%)
Number of 4-grams hit = 103  (26.55%)
Number of 3-grams hit = 120  (30.93%)
Number of 2-grams hit = 66  (17.01%)
Number of 1-grams hit = 13  (3.35%)
3 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article968.text
Perplexity = 73.36, Entropy = 6.20 bits
Computation based on 1057 words.
Number of 5-grams hit = 390  (36.90%)
Number of 4-grams hit = 274  (25.92%)
Number of 3-grams hit = 232  (21.95%)
Number of 2-grams hit = 136  (12.87%)
Number of 1-grams hit = 25  (2.37%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article969.text
Perplexity = 125.24, Entropy = 6.97 bits
Computation based on 719 words.
Number of 5-grams hit = 183  (25.45%)
Number of 4-grams hit = 170  (23.64%)
Number of 3-grams hit = 185  (25.73%)
Number of 2-grams hit = 147  (20.45%)
Number of 1-grams hit = 34  (4.73%)
7 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article970.text
Perplexity = 156.82, Entropy = 7.29 bits
Computation based on 739 words.
Number of 5-grams hit = 171  (23.14%)
Number of 4-grams hit = 188  (25.44%)
Number of 3-grams hit = 220  (29.77%)
Number of 2-grams hit = 126  (17.05%)
Number of 1-grams hit = 34  (4.60%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article971.text
Perplexity = 171.48, Entropy = 7.42 bits
Computation based on 468 words.
Number of 5-grams hit = 102  (21.79%)
Number of 4-grams hit = 125  (26.71%)
Number of 3-grams hit = 135  (28.85%)
Number of 2-grams hit = 84  (17.95%)
Number of 1-grams hit = 22  (4.70%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article972.text
Perplexity = 164.50, Entropy = 7.36 bits
Computation based on 1855 words.
Number of 5-grams hit = 398  (21.46%)
Number of 4-grams hit = 470  (25.34%)
Number of 3-grams hit = 579  (31.21%)
Number of 2-grams hit = 330  (17.79%)
Number of 1-grams hit = 78  (4.20%)
7 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article973.text
Perplexity = 100.85, Entropy = 6.66 bits
Computation based on 605 words.
Number of 5-grams hit = 184  (30.41%)
Number of 4-grams hit = 149  (24.63%)
Number of 3-grams hit = 161  (26.61%)
Number of 2-grams hit = 100  (16.53%)
Number of 1-grams hit = 11  (1.82%)
6 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article974.text
Perplexity = 208.28, Entropy = 7.70 bits
Computation based on 942 words.
Number of 5-grams hit = 156  (16.56%)
Number of 4-grams hit = 231  (24.52%)
Number of 3-grams hit = 312  (33.12%)
Number of 2-grams hit = 194  (20.59%)
Number of 1-grams hit = 49  (5.20%)
9 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article975.text
Perplexity = 144.24, Entropy = 7.17 bits
Computation based on 353 words.
Number of 5-grams hit = 83  (23.51%)
Number of 4-grams hit = 86  (24.36%)
Number of 3-grams hit = 103  (29.18%)
Number of 2-grams hit = 63  (17.85%)
Number of 1-grams hit = 18  (5.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article976.text
Perplexity = 256.21, Entropy = 8.00 bits
Computation based on 347 words.
Number of 5-grams hit = 56  (16.14%)
Number of 4-grams hit = 65  (18.73%)
Number of 3-grams hit = 98  (28.24%)
Number of 2-grams hit = 107  (30.84%)
Number of 1-grams hit = 21  (6.05%)
13 OOVs (3.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article977.text
Perplexity = 86.57, Entropy = 6.44 bits
Computation based on 857 words.
Number of 5-grams hit = 289  (33.72%)
Number of 4-grams hit = 202  (23.57%)
Number of 3-grams hit = 209  (24.39%)
Number of 2-grams hit = 129  (15.05%)
Number of 1-grams hit = 28  (3.27%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article978.text
Perplexity = 126.76, Entropy = 6.99 bits
Computation based on 464 words.
Number of 5-grams hit = 112  (24.14%)
Number of 4-grams hit = 116  (25.00%)
Number of 3-grams hit = 129  (27.80%)
Number of 2-grams hit = 86  (18.53%)
Number of 1-grams hit = 21  (4.53%)
10 OOVs (2.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article979.text
Perplexity = 93.43, Entropy = 6.55 bits
Computation based on 1554 words.
Number of 5-grams hit = 560  (36.04%)
Number of 4-grams hit = 319  (20.53%)
Number of 3-grams hit = 369  (23.75%)
Number of 2-grams hit = 259  (16.67%)
Number of 1-grams hit = 47  (3.02%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article980.text
Perplexity = 166.48, Entropy = 7.38 bits
Computation based on 598 words.
Number of 5-grams hit = 154  (25.75%)
Number of 4-grams hit = 117  (19.57%)
Number of 3-grams hit = 164  (27.42%)
Number of 2-grams hit = 131  (21.91%)
Number of 1-grams hit = 32  (5.35%)
13 OOVs (2.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article981.text
Perplexity = 167.03, Entropy = 7.38 bits
Computation based on 345 words.
Number of 5-grams hit = 74  (21.45%)
Number of 4-grams hit = 77  (22.32%)
Number of 3-grams hit = 100  (28.99%)
Number of 2-grams hit = 77  (22.32%)
Number of 1-grams hit = 17  (4.93%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article982.text
Perplexity = 106.74, Entropy = 6.74 bits
Computation based on 825 words.
Number of 5-grams hit = 247  (29.94%)
Number of 4-grams hit = 168  (20.36%)
Number of 3-grams hit = 220  (26.67%)
Number of 2-grams hit = 165  (20.00%)
Number of 1-grams hit = 25  (3.03%)
12 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article983.text
Perplexity = 178.44, Entropy = 7.48 bits
Computation based on 468 words.
Number of 5-grams hit = 108  (23.08%)
Number of 4-grams hit = 123  (26.28%)
Number of 3-grams hit = 125  (26.71%)
Number of 2-grams hit = 85  (18.16%)
Number of 1-grams hit = 27  (5.77%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article984.text
Perplexity = 66.43, Entropy = 6.05 bits
Computation based on 458 words.
Number of 5-grams hit = 165  (36.03%)
Number of 4-grams hit = 104  (22.71%)
Number of 3-grams hit = 113  (24.67%)
Number of 2-grams hit = 66  (14.41%)
Number of 1-grams hit = 10  (2.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article985.text
Perplexity = 140.61, Entropy = 7.14 bits
Computation based on 1233 words.
Number of 5-grams hit = 317  (25.71%)
Number of 4-grams hit = 285  (23.11%)
Number of 3-grams hit = 333  (27.01%)
Number of 2-grams hit = 249  (20.19%)
Number of 1-grams hit = 49  (3.97%)
13 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article986.text
Perplexity = 136.70, Entropy = 7.09 bits
Computation based on 269 words.
Number of 5-grams hit = 56  (20.82%)
Number of 4-grams hit = 69  (25.65%)
Number of 3-grams hit = 89  (33.09%)
Number of 2-grams hit = 47  (17.47%)
Number of 1-grams hit = 8  (2.97%)
2 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article987.text
Perplexity = 77.77, Entropy = 6.28 bits
Computation based on 1737 words.
Number of 5-grams hit = 599  (34.48%)
Number of 4-grams hit = 432  (24.87%)
Number of 3-grams hit = 417  (24.01%)
Number of 2-grams hit = 264  (15.20%)
Number of 1-grams hit = 25  (1.44%)
7 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article988.text
Perplexity = 177.43, Entropy = 7.47 bits
Computation based on 1241 words.
Number of 5-grams hit = 256  (20.63%)
Number of 4-grams hit = 360  (29.01%)
Number of 3-grams hit = 364  (29.33%)
Number of 2-grams hit = 205  (16.52%)
Number of 1-grams hit = 56  (4.51%)
8 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article989.text
Perplexity = 64.32, Entropy = 6.01 bits
Computation based on 2270 words.
Number of 5-grams hit = 959  (42.25%)
Number of 4-grams hit = 455  (20.04%)
Number of 3-grams hit = 504  (22.20%)
Number of 2-grams hit = 294  (12.95%)
Number of 1-grams hit = 58  (2.56%)
9 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article990.text
Perplexity = 67.51, Entropy = 6.08 bits
Computation based on 653 words.
Number of 5-grams hit = 247  (37.83%)
Number of 4-grams hit = 158  (24.20%)
Number of 3-grams hit = 145  (22.21%)
Number of 2-grams hit = 87  (13.32%)
Number of 1-grams hit = 16  (2.45%)
7 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article991.text
Perplexity = 144.39, Entropy = 7.17 bits
Computation based on 1540 words.
Number of 5-grams hit = 393  (25.52%)
Number of 4-grams hit = 390  (25.32%)
Number of 3-grams hit = 431  (27.99%)
Number of 2-grams hit = 273  (17.73%)
Number of 1-grams hit = 53  (3.44%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article992.text
Perplexity = 216.98, Entropy = 7.76 bits
Computation based on 510 words.
Number of 5-grams hit = 107  (20.98%)
Number of 4-grams hit = 114  (22.35%)
Number of 3-grams hit = 141  (27.65%)
Number of 2-grams hit = 119  (23.33%)
Number of 1-grams hit = 29  (5.69%)
25 OOVs (4.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article993.text
Perplexity = 165.59, Entropy = 7.37 bits
Computation based on 2457 words.
Number of 5-grams hit = 517  (21.04%)
Number of 4-grams hit = 640  (26.05%)
Number of 3-grams hit = 761  (30.97%)
Number of 2-grams hit = 434  (17.66%)
Number of 1-grams hit = 105  (4.27%)
9 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article994.text
Perplexity = 142.71, Entropy = 7.16 bits
Computation based on 477 words.
Number of 5-grams hit = 122  (25.58%)
Number of 4-grams hit = 123  (25.79%)
Number of 3-grams hit = 139  (29.14%)
Number of 2-grams hit = 73  (15.30%)
Number of 1-grams hit = 20  (4.19%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article995.text
Perplexity = 239.54, Entropy = 7.90 bits
Computation based on 7311 words.
Number of 5-grams hit = 1261  (17.25%)
Number of 4-grams hit = 1645  (22.50%)
Number of 3-grams hit = 2296  (31.40%)
Number of 2-grams hit = 1726  (23.61%)
Number of 1-grams hit = 383  (5.24%)
246 OOVs (3.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article996.text
Perplexity = 200.49, Entropy = 7.65 bits
Computation based on 580 words.
Number of 5-grams hit = 122  (21.03%)
Number of 4-grams hit = 136  (23.45%)
Number of 3-grams hit = 174  (30.00%)
Number of 2-grams hit = 120  (20.69%)
Number of 1-grams hit = 28  (4.83%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article997.text
Perplexity = 170.39, Entropy = 7.41 bits
Computation based on 5318 words.
Number of 5-grams hit = 1144  (21.51%)
Number of 4-grams hit = 1346  (25.31%)
Number of 3-grams hit = 1578  (29.67%)
Number of 2-grams hit = 1016  (19.10%)
Number of 1-grams hit = 234  (4.40%)
13 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article998.text
Perplexity = 133.40, Entropy = 7.06 bits
Computation based on 523 words.
Number of 5-grams hit = 134  (25.62%)
Number of 4-grams hit = 133  (25.43%)
Number of 3-grams hit = 157  (30.02%)
Number of 2-grams hit = 82  (15.68%)
Number of 1-grams hit = 17  (3.25%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article999.text
Perplexity = 95.91, Entropy = 6.58 bits
Computation based on 468 words.
Number of 5-grams hit = 152  (32.48%)
Number of 4-grams hit = 116  (24.79%)
Number of 3-grams hit = 121  (25.85%)
Number of 2-grams hit = 67  (14.32%)
Number of 1-grams hit = 12  (2.56%)
4 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : 