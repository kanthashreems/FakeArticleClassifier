evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article0.text
Will force inclusive back-off from OOVs.
Perplexity = 602.71, Entropy = 9.24 bits
Computation based on 28 words.
Number of 2-grams hit = 23  (82.14%)
Number of 1-grams hit = 5  (17.86%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article1.text
Will force inclusive back-off from OOVs.
Perplexity = 124.70, Entropy = 6.96 bits
Computation based on 27 words.
Number of 2-grams hit = 26  (96.30%)
Number of 1-grams hit = 1  (3.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article2.text
Will force inclusive back-off from OOVs.
Perplexity = 50.34, Entropy = 5.65 bits
Computation based on 16 words.
Number of 2-grams hit = 15  (93.75%)
Number of 1-grams hit = 1  (6.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article3.text
Will force inclusive back-off from OOVs.
Perplexity = 131.83, Entropy = 7.04 bits
Computation based on 22 words.
Number of 2-grams hit = 21  (95.45%)
Number of 1-grams hit = 1  (4.55%)
2 OOVs (8.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article4.text
Will force inclusive back-off from OOVs.
Perplexity = 249.47, Entropy = 7.96 bits
Computation based on 19 words.
Number of 2-grams hit = 17  (89.47%)
Number of 1-grams hit = 2  (10.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article5.text
Will force inclusive back-off from OOVs.
Perplexity = 190.95, Entropy = 7.58 bits
Computation based on 8 words.
Number of 2-grams hit = 7  (87.50%)
Number of 1-grams hit = 1  (12.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article6.text
Will force inclusive back-off from OOVs.
Perplexity = 260.76, Entropy = 8.03 bits
Computation based on 9 words.
Number of 2-grams hit = 7  (77.78%)
Number of 1-grams hit = 2  (22.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article7.text
Will force inclusive back-off from OOVs.
Perplexity = 191.15, Entropy = 7.58 bits
Computation based on 26 words.
Number of 2-grams hit = 23  (88.46%)
Number of 1-grams hit = 3  (11.54%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article8.text
Will force inclusive back-off from OOVs.
Perplexity = 373.79, Entropy = 8.55 bits
Computation based on 17 words.
Number of 2-grams hit = 16  (94.12%)
Number of 1-grams hit = 1  (5.88%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article9.text
Will force inclusive back-off from OOVs.
Perplexity = 55.01, Entropy = 5.78 bits
Computation based on 12 words.
Number of 2-grams hit = 11  (91.67%)
Number of 1-grams hit = 1  (8.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article10.text
Will force inclusive back-off from OOVs.
Perplexity = 60.11, Entropy = 5.91 bits
Computation based on 8 words.
Number of 2-grams hit = 7  (87.50%)
Number of 1-grams hit = 1  (12.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article11.text
Will force inclusive back-off from OOVs.
Perplexity = 85.90, Entropy = 6.42 bits
Computation based on 47 words.
Number of 2-grams hit = 45  (95.74%)
Number of 1-grams hit = 2  (4.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article12.text
Will force inclusive back-off from OOVs.
Perplexity = 74.01, Entropy = 6.21 bits
Computation based on 27 words.
Number of 2-grams hit = 25  (92.59%)
Number of 1-grams hit = 2  (7.41%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article13.text
Will force inclusive back-off from OOVs.
Perplexity = 85.27, Entropy = 6.41 bits
Computation based on 30 words.
Number of 2-grams hit = 29  (96.67%)
Number of 1-grams hit = 1  (3.33%)
1 OOVs (3.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article14.text
Will force inclusive back-off from OOVs.
Perplexity = 173.52, Entropy = 7.44 bits
Computation based on 23 words.
Number of 2-grams hit = 22  (95.65%)
Number of 1-grams hit = 1  (4.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article15.text
Will force inclusive back-off from OOVs.
Perplexity = 46.51, Entropy = 5.54 bits
Computation based on 4 words.
Number of 2-grams hit = 3  (75.00%)
Number of 1-grams hit = 1  (25.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article16.text
Will force inclusive back-off from OOVs.
Perplexity = 294.56, Entropy = 8.20 bits
Computation based on 22 words.
Number of 2-grams hit = 20  (90.91%)
Number of 1-grams hit = 2  (9.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article17.text
Will force inclusive back-off from OOVs.
Perplexity = 61.04, Entropy = 5.93 bits
Computation based on 12 words.
Number of 2-grams hit = 11  (91.67%)
Number of 1-grams hit = 1  (8.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article18.text
Will force inclusive back-off from OOVs.
Perplexity = 188.33, Entropy = 7.56 bits
Computation based on 22 words.
Number of 2-grams hit = 19  (86.36%)
Number of 1-grams hit = 3  (13.64%)
2 OOVs (8.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article19.text
Will force inclusive back-off from OOVs.
Perplexity = 151.20, Entropy = 7.24 bits
Computation based on 52 words.
Number of 2-grams hit = 48  (92.31%)
Number of 1-grams hit = 4  (7.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article20.text
Will force inclusive back-off from OOVs.
Perplexity = 163.87, Entropy = 7.36 bits
Computation based on 18 words.
Number of 2-grams hit = 17  (94.44%)
Number of 1-grams hit = 1  (5.56%)
1 OOVs (5.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article21.text
Will force inclusive back-off from OOVs.
Perplexity = 190.33, Entropy = 7.57 bits
Computation based on 13 words.
Number of 2-grams hit = 12  (92.31%)
Number of 1-grams hit = 1  (7.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article22.text
Will force inclusive back-off from OOVs.
Perplexity = 403.70, Entropy = 8.66 bits
Computation based on 29 words.
Number of 2-grams hit = 25  (86.21%)
Number of 1-grams hit = 4  (13.79%)
1 OOVs (3.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article23.text
Will force inclusive back-off from OOVs.
Perplexity = 144.68, Entropy = 7.18 bits
Computation based on 11 words.
Number of 2-grams hit = 9  (81.82%)
Number of 1-grams hit = 2  (18.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article24.text
Will force inclusive back-off from OOVs.
Perplexity = 145.14, Entropy = 7.18 bits
Computation based on 23 words.
Number of 2-grams hit = 21  (91.30%)
Number of 1-grams hit = 2  (8.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article25.text
Will force inclusive back-off from OOVs.
Perplexity = 123.97, Entropy = 6.95 bits
Computation based on 35 words.
Number of 2-grams hit = 34  (97.14%)
Number of 1-grams hit = 1  (2.86%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article26.text
Will force inclusive back-off from OOVs.
Perplexity = 94.05, Entropy = 6.56 bits
Computation based on 25 words.
Number of 2-grams hit = 24  (96.00%)
Number of 1-grams hit = 1  (4.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article27.text
Will force inclusive back-off from OOVs.
Perplexity = 51.98, Entropy = 5.70 bits
Computation based on 10 words.
Number of 2-grams hit = 9  (90.00%)
Number of 1-grams hit = 1  (10.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article28.text
Will force inclusive back-off from OOVs.
Perplexity = 88.72, Entropy = 6.47 bits
Computation based on 27 words.
Number of 2-grams hit = 26  (96.30%)
Number of 1-grams hit = 1  (3.70%)
3 OOVs (10.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article29.text
Will force inclusive back-off from OOVs.
Perplexity = 88.30, Entropy = 6.46 bits
Computation based on 26 words.
Number of 2-grams hit = 25  (96.15%)
Number of 1-grams hit = 1  (3.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article30.text
Will force inclusive back-off from OOVs.
Perplexity = 59.68, Entropy = 5.90 bits
Computation based on 12 words.
Number of 2-grams hit = 11  (91.67%)
Number of 1-grams hit = 1  (8.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article31.text
Will force inclusive back-off from OOVs.
Perplexity = 293.36, Entropy = 8.20 bits
Computation based on 16 words.
Number of 2-grams hit = 14  (87.50%)
Number of 1-grams hit = 2  (12.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article32.text
Will force inclusive back-off from OOVs.
Perplexity = 105.39, Entropy = 6.72 bits
Computation based on 20 words.
Number of 2-grams hit = 18  (90.00%)
Number of 1-grams hit = 2  (10.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article33.text
Will force inclusive back-off from OOVs.
Perplexity = 98.34, Entropy = 6.62 bits
Computation based on 18 words.
Number of 2-grams hit = 17  (94.44%)
Number of 1-grams hit = 1  (5.56%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article34.text
Will force inclusive back-off from OOVs.
Perplexity = 152.61, Entropy = 7.25 bits
Computation based on 31 words.
Number of 2-grams hit = 27  (87.10%)
Number of 1-grams hit = 4  (12.90%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article35.text
Will force inclusive back-off from OOVs.
Perplexity = 204.35, Entropy = 7.67 bits
Computation based on 77 words.
Number of 2-grams hit = 70  (90.91%)
Number of 1-grams hit = 7  (9.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article36.text
Will force inclusive back-off from OOVs.
Perplexity = 197.06, Entropy = 7.62 bits
Computation based on 25 words.
Number of 2-grams hit = 22  (88.00%)
Number of 1-grams hit = 3  (12.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article37.text
Will force inclusive back-off from OOVs.
Perplexity = 183.20, Entropy = 7.52 bits
Computation based on 14 words.
Number of 2-grams hit = 13  (92.86%)
Number of 1-grams hit = 1  (7.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article38.text
Will force inclusive back-off from OOVs.
Perplexity = 37.99, Entropy = 5.25 bits
Computation based on 22 words.
Number of 2-grams hit = 21  (95.45%)
Number of 1-grams hit = 1  (4.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article39.text
Will force inclusive back-off from OOVs.
Perplexity = 95.80, Entropy = 6.58 bits
Computation based on 22 words.
Number of 2-grams hit = 21  (95.45%)
Number of 1-grams hit = 1  (4.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article40.text
Will force inclusive back-off from OOVs.
Perplexity = 60.83, Entropy = 5.93 bits
Computation based on 32 words.
Number of 2-grams hit = 31  (96.88%)
Number of 1-grams hit = 1  (3.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article41.text
Will force inclusive back-off from OOVs.
Perplexity = 80.73, Entropy = 6.34 bits
Computation based on 19 words.
Number of 2-grams hit = 18  (94.74%)
Number of 1-grams hit = 1  (5.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article42.text
Will force inclusive back-off from OOVs.
Perplexity = 86.97, Entropy = 6.44 bits
Computation based on 48 words.
Number of 2-grams hit = 46  (95.83%)
Number of 1-grams hit = 2  (4.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article43.text
Will force inclusive back-off from OOVs.
Perplexity = 331.17, Entropy = 8.37 bits
Computation based on 39 words.
Number of 2-grams hit = 34  (87.18%)
Number of 1-grams hit = 5  (12.82%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article44.text
Will force inclusive back-off from OOVs.
Perplexity = 125.96, Entropy = 6.98 bits
Computation based on 59 words.
Number of 2-grams hit = 57  (96.61%)
Number of 1-grams hit = 2  (3.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article45.text
Will force inclusive back-off from OOVs.
Perplexity = 185.81, Entropy = 7.54 bits
Computation based on 23 words.
Number of 2-grams hit = 20  (86.96%)
Number of 1-grams hit = 3  (13.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article46.text
Will force inclusive back-off from OOVs.
Perplexity = 229.51, Entropy = 7.84 bits
Computation based on 63 words.
Number of 2-grams hit = 57  (90.48%)
Number of 1-grams hit = 6  (9.52%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article47.text
Will force inclusive back-off from OOVs.
Perplexity = 146.60, Entropy = 7.20 bits
Computation based on 59 words.
Number of 2-grams hit = 56  (94.92%)
Number of 1-grams hit = 3  (5.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article48.text
Will force inclusive back-off from OOVs.
Perplexity = 248.54, Entropy = 7.96 bits
Computation based on 35 words.
Number of 2-grams hit = 31  (88.57%)
Number of 1-grams hit = 4  (11.43%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article49.text
Will force inclusive back-off from OOVs.
Perplexity = 200.32, Entropy = 7.65 bits
Computation based on 76 words.
Number of 2-grams hit = 72  (94.74%)
Number of 1-grams hit = 4  (5.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article50.text
Will force inclusive back-off from OOVs.
Perplexity = 94.20, Entropy = 6.56 bits
Computation based on 32 words.
Number of 2-grams hit = 31  (96.88%)
Number of 1-grams hit = 1  (3.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article51.text
Will force inclusive back-off from OOVs.
Perplexity = 168.10, Entropy = 7.39 bits
Computation based on 22 words.
Number of 2-grams hit = 19  (86.36%)
Number of 1-grams hit = 3  (13.64%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article52.text
Will force inclusive back-off from OOVs.
Perplexity = 106.46, Entropy = 6.73 bits
Computation based on 52 words.
Number of 2-grams hit = 50  (96.15%)
Number of 1-grams hit = 2  (3.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article53.text
Will force inclusive back-off from OOVs.
Perplexity = 169.09, Entropy = 7.40 bits
Computation based on 48 words.
Number of 2-grams hit = 44  (91.67%)
Number of 1-grams hit = 4  (8.33%)
2 OOVs (4.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article54.text
Will force inclusive back-off from OOVs.
Perplexity = 93.94, Entropy = 6.55 bits
Computation based on 43 words.
Number of 2-grams hit = 41  (95.35%)
Number of 1-grams hit = 2  (4.65%)
1 OOVs (2.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article55.text
Will force inclusive back-off from OOVs.
Perplexity = 243.60, Entropy = 7.93 bits
Computation based on 40 words.
Number of 2-grams hit = 35  (87.50%)
Number of 1-grams hit = 5  (12.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article56.text
Will force inclusive back-off from OOVs.
Perplexity = 125.10, Entropy = 6.97 bits
Computation based on 44 words.
Number of 2-grams hit = 41  (93.18%)
Number of 1-grams hit = 3  (6.82%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article57.text
Will force inclusive back-off from OOVs.
Perplexity = 96.35, Entropy = 6.59 bits
Computation based on 16 words.
Number of 2-grams hit = 14  (87.50%)
Number of 1-grams hit = 2  (12.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article58.text
Will force inclusive back-off from OOVs.
Perplexity = 594.00, Entropy = 9.21 bits
Computation based on 39 words.
Number of 2-grams hit = 32  (82.05%)
Number of 1-grams hit = 7  (17.95%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article59.text
Will force inclusive back-off from OOVs.
Perplexity = 125.18, Entropy = 6.97 bits
Computation based on 45 words.
Number of 2-grams hit = 40  (88.89%)
Number of 1-grams hit = 5  (11.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article60.text
Will force inclusive back-off from OOVs.
Perplexity = 96.65, Entropy = 6.59 bits
Computation based on 45 words.
Number of 2-grams hit = 43  (95.56%)
Number of 1-grams hit = 2  (4.44%)
1 OOVs (2.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article61.text
Will force inclusive back-off from OOVs.
Perplexity = 98.67, Entropy = 6.62 bits
Computation based on 40 words.
Number of 2-grams hit = 39  (97.50%)
Number of 1-grams hit = 1  (2.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article62.text
Will force inclusive back-off from OOVs.
Perplexity = 254.47, Entropy = 7.99 bits
Computation based on 57 words.
Number of 2-grams hit = 50  (87.72%)
Number of 1-grams hit = 7  (12.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article63.text
Will force inclusive back-off from OOVs.
Perplexity = 150.85, Entropy = 7.24 bits
Computation based on 36 words.
Number of 2-grams hit = 33  (91.67%)
Number of 1-grams hit = 3  (8.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article64.text
Will force inclusive back-off from OOVs.
Perplexity = 14.16, Entropy = 3.82 bits
Computation based on 18 words.
Number of 2-grams hit = 17  (94.44%)
Number of 1-grams hit = 1  (5.56%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article65.text
Will force inclusive back-off from OOVs.
Perplexity = 34.57, Entropy = 5.11 bits
Computation based on 19 words.
Number of 2-grams hit = 18  (94.74%)
Number of 1-grams hit = 1  (5.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article66.text
Will force inclusive back-off from OOVs.
Perplexity = 212.31, Entropy = 7.73 bits
Computation based on 65 words.
Number of 2-grams hit = 63  (96.92%)
Number of 1-grams hit = 2  (3.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article67.text
Will force inclusive back-off from OOVs.
Perplexity = 176.92, Entropy = 7.47 bits
Computation based on 54 words.
Number of 2-grams hit = 51  (94.44%)
Number of 1-grams hit = 3  (5.56%)
1 OOVs (1.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article68.text
Will force inclusive back-off from OOVs.
Perplexity = 221.81, Entropy = 7.79 bits
Computation based on 64 words.
Number of 2-grams hit = 59  (92.19%)
Number of 1-grams hit = 5  (7.81%)
5 OOVs (7.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article69.text
Will force inclusive back-off from OOVs.
Perplexity = 122.99, Entropy = 6.94 bits
Computation based on 48 words.
Number of 2-grams hit = 47  (97.92%)
Number of 1-grams hit = 1  (2.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article70.text
Will force inclusive back-off from OOVs.
Perplexity = 221.53, Entropy = 7.79 bits
Computation based on 48 words.
Number of 2-grams hit = 46  (95.83%)
Number of 1-grams hit = 2  (4.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article71.text
Will force inclusive back-off from OOVs.
Perplexity = 127.68, Entropy = 7.00 bits
Computation based on 26 words.
Number of 2-grams hit = 24  (92.31%)
Number of 1-grams hit = 2  (7.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article72.text
Will force inclusive back-off from OOVs.
Perplexity = 79.38, Entropy = 6.31 bits
Computation based on 46 words.
Number of 2-grams hit = 45  (97.83%)
Number of 1-grams hit = 1  (2.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article73.text
Will force inclusive back-off from OOVs.
Perplexity = 89.10, Entropy = 6.48 bits
Computation based on 57 words.
Number of 2-grams hit = 56  (98.25%)
Number of 1-grams hit = 1  (1.75%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article74.text
Will force inclusive back-off from OOVs.
Perplexity = 169.19, Entropy = 7.40 bits
Computation based on 55 words.
Number of 2-grams hit = 52  (94.55%)
Number of 1-grams hit = 3  (5.45%)
1 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article75.text
Will force inclusive back-off from OOVs.
Perplexity = 55.44, Entropy = 5.79 bits
Computation based on 30 words.
Number of 2-grams hit = 29  (96.67%)
Number of 1-grams hit = 1  (3.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article76.text
Will force inclusive back-off from OOVs.
Perplexity = 59.40, Entropy = 5.89 bits
Computation based on 45 words.
Number of 2-grams hit = 44  (97.78%)
Number of 1-grams hit = 1  (2.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article77.text
Will force inclusive back-off from OOVs.
Perplexity = 183.42, Entropy = 7.52 bits
Computation based on 71 words.
Number of 2-grams hit = 68  (95.77%)
Number of 1-grams hit = 3  (4.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article78.text
Will force inclusive back-off from OOVs.
Perplexity = 66.21, Entropy = 6.05 bits
Computation based on 54 words.
Number of 2-grams hit = 53  (98.15%)
Number of 1-grams hit = 1  (1.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article79.text
Will force inclusive back-off from OOVs.
Perplexity = 89.19, Entropy = 6.48 bits
Computation based on 54 words.
Number of 2-grams hit = 51  (94.44%)
Number of 1-grams hit = 3  (5.56%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article80.text
Will force inclusive back-off from OOVs.
Perplexity = 170.05, Entropy = 7.41 bits
Computation based on 50 words.
Number of 2-grams hit = 47  (94.00%)
Number of 1-grams hit = 3  (6.00%)
3 OOVs (5.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article81.text
Will force inclusive back-off from OOVs.
Perplexity = 284.65, Entropy = 8.15 bits
Computation based on 58 words.
Number of 2-grams hit = 50  (86.21%)
Number of 1-grams hit = 8  (13.79%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article82.text
Will force inclusive back-off from OOVs.
Perplexity = 114.28, Entropy = 6.84 bits
Computation based on 111 words.
Number of 2-grams hit = 108  (97.30%)
Number of 1-grams hit = 3  (2.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article83.text
Will force inclusive back-off from OOVs.
Perplexity = 88.29, Entropy = 6.46 bits
Computation based on 57 words.
Number of 2-grams hit = 55  (96.49%)
Number of 1-grams hit = 2  (3.51%)
1 OOVs (1.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article84.text
Will force inclusive back-off from OOVs.
Perplexity = 208.38, Entropy = 7.70 bits
Computation based on 76 words.
Number of 2-grams hit = 70  (92.11%)
Number of 1-grams hit = 6  (7.89%)
3 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article85.text
Will force inclusive back-off from OOVs.
Perplexity = 87.32, Entropy = 6.45 bits
Computation based on 36 words.
Number of 2-grams hit = 32  (88.89%)
Number of 1-grams hit = 4  (11.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article86.text
Will force inclusive back-off from OOVs.
Perplexity = 122.76, Entropy = 6.94 bits
Computation based on 74 words.
Number of 2-grams hit = 69  (93.24%)
Number of 1-grams hit = 5  (6.76%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article87.text
Will force inclusive back-off from OOVs.
Perplexity = 123.27, Entropy = 6.95 bits
Computation based on 43 words.
Number of 2-grams hit = 40  (93.02%)
Number of 1-grams hit = 3  (6.98%)
1 OOVs (2.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article88.text
Will force inclusive back-off from OOVs.
Perplexity = 130.96, Entropy = 7.03 bits
Computation based on 100 words.
Number of 2-grams hit = 97  (97.00%)
Number of 1-grams hit = 3  (3.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article89.text
Will force inclusive back-off from OOVs.
Perplexity = 168.05, Entropy = 7.39 bits
Computation based on 78 words.
Number of 2-grams hit = 74  (94.87%)
Number of 1-grams hit = 4  (5.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article90.text
Will force inclusive back-off from OOVs.
Perplexity = 167.25, Entropy = 7.39 bits
Computation based on 70 words.
Number of 2-grams hit = 67  (95.71%)
Number of 1-grams hit = 3  (4.29%)
2 OOVs (2.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article91.text
Will force inclusive back-off from OOVs.
Perplexity = 122.01, Entropy = 6.93 bits
Computation based on 45 words.
Number of 2-grams hit = 44  (97.78%)
Number of 1-grams hit = 1  (2.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article92.text
Will force inclusive back-off from OOVs.
Perplexity = 179.04, Entropy = 7.48 bits
Computation based on 85 words.
Number of 2-grams hit = 81  (95.29%)
Number of 1-grams hit = 4  (4.71%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article93.text
Will force inclusive back-off from OOVs.
Perplexity = 218.28, Entropy = 7.77 bits
Computation based on 138 words.
Number of 2-grams hit = 129  (93.48%)
Number of 1-grams hit = 9  (6.52%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article94.text
Will force inclusive back-off from OOVs.
Perplexity = 26.13, Entropy = 4.71 bits
Computation based on 32 words.
Number of 2-grams hit = 31  (96.88%)
Number of 1-grams hit = 1  (3.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article95.text
Will force inclusive back-off from OOVs.
Perplexity = 193.61, Entropy = 7.60 bits
Computation based on 154 words.
Number of 2-grams hit = 140  (90.91%)
Number of 1-grams hit = 14  (9.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article96.text
Will force inclusive back-off from OOVs.
Perplexity = 373.49, Entropy = 8.54 bits
Computation based on 82 words.
Number of 2-grams hit = 74  (90.24%)
Number of 1-grams hit = 8  (9.76%)
6 OOVs (6.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article97.text
Will force inclusive back-off from OOVs.
Perplexity = 64.17, Entropy = 6.00 bits
Computation based on 42 words.
Number of 2-grams hit = 41  (97.62%)
Number of 1-grams hit = 1  (2.38%)
1 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article98.text
Will force inclusive back-off from OOVs.
Perplexity = 319.71, Entropy = 8.32 bits
Computation based on 95 words.
Number of 2-grams hit = 84  (88.42%)
Number of 1-grams hit = 11  (11.58%)
1 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article99.text
Will force inclusive back-off from OOVs.
Perplexity = 202.62, Entropy = 7.66 bits
Computation based on 74 words.
Number of 2-grams hit = 66  (89.19%)
Number of 1-grams hit = 8  (10.81%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article100.text
Will force inclusive back-off from OOVs.
Perplexity = 105.64, Entropy = 6.72 bits
Computation based on 97 words.
Number of 2-grams hit = 93  (95.88%)
Number of 1-grams hit = 4  (4.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article101.text
Will force inclusive back-off from OOVs.
Perplexity = 108.58, Entropy = 6.76 bits
Computation based on 78 words.
Number of 2-grams hit = 76  (97.44%)
Number of 1-grams hit = 2  (2.56%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article102.text
Will force inclusive back-off from OOVs.
Perplexity = 148.23, Entropy = 7.21 bits
Computation based on 87 words.
Number of 2-grams hit = 78  (89.66%)
Number of 1-grams hit = 9  (10.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article103.text
Will force inclusive back-off from OOVs.
Perplexity = 92.41, Entropy = 6.53 bits
Computation based on 67 words.
Number of 2-grams hit = 63  (94.03%)
Number of 1-grams hit = 4  (5.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article104.text
Will force inclusive back-off from OOVs.
Perplexity = 236.90, Entropy = 7.89 bits
Computation based on 68 words.
Number of 2-grams hit = 62  (91.18%)
Number of 1-grams hit = 6  (8.82%)
1 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article105.text
Will force inclusive back-off from OOVs.
Perplexity = 144.40, Entropy = 7.17 bits
Computation based on 90 words.
Number of 2-grams hit = 83  (92.22%)
Number of 1-grams hit = 7  (7.78%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article106.text
Will force inclusive back-off from OOVs.
Perplexity = 103.30, Entropy = 6.69 bits
Computation based on 57 words.
Number of 2-grams hit = 55  (96.49%)
Number of 1-grams hit = 2  (3.51%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article107.text
Will force inclusive back-off from OOVs.
Perplexity = 75.68, Entropy = 6.24 bits
Computation based on 40 words.
Number of 2-grams hit = 37  (92.50%)
Number of 1-grams hit = 3  (7.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article108.text
Will force inclusive back-off from OOVs.
Perplexity = 71.85, Entropy = 6.17 bits
Computation based on 125 words.
Number of 2-grams hit = 124  (99.20%)
Number of 1-grams hit = 1  (0.80%)
1 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article109.text
Will force inclusive back-off from OOVs.
Perplexity = 179.86, Entropy = 7.49 bits
Computation based on 96 words.
Number of 2-grams hit = 89  (92.71%)
Number of 1-grams hit = 7  (7.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article110.text
Will force inclusive back-off from OOVs.
Perplexity = 168.14, Entropy = 7.39 bits
Computation based on 120 words.
Number of 2-grams hit = 116  (96.67%)
Number of 1-grams hit = 4  (3.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article111.text
Will force inclusive back-off from OOVs.
Perplexity = 103.24, Entropy = 6.69 bits
Computation based on 68 words.
Number of 2-grams hit = 65  (95.59%)
Number of 1-grams hit = 3  (4.41%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article112.text
Will force inclusive back-off from OOVs.
Perplexity = 101.60, Entropy = 6.67 bits
Computation based on 89 words.
Number of 2-grams hit = 86  (96.63%)
Number of 1-grams hit = 3  (3.37%)
1 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article113.text
Will force inclusive back-off from OOVs.
Perplexity = 221.90, Entropy = 7.79 bits
Computation based on 94 words.
Number of 2-grams hit = 89  (94.68%)
Number of 1-grams hit = 5  (5.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article114.text
Will force inclusive back-off from OOVs.
Perplexity = 211.83, Entropy = 7.73 bits
Computation based on 143 words.
Number of 2-grams hit = 130  (90.91%)
Number of 1-grams hit = 13  (9.09%)
1 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article115.text
Will force inclusive back-off from OOVs.
Perplexity = 78.89, Entropy = 6.30 bits
Computation based on 71 words.
Number of 2-grams hit = 69  (97.18%)
Number of 1-grams hit = 2  (2.82%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article116.text
Will force inclusive back-off from OOVs.
Perplexity = 194.33, Entropy = 7.60 bits
Computation based on 112 words.
Number of 2-grams hit = 106  (94.64%)
Number of 1-grams hit = 6  (5.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article117.text
Will force inclusive back-off from OOVs.
Perplexity = 81.08, Entropy = 6.34 bits
Computation based on 82 words.
Number of 2-grams hit = 80  (97.56%)
Number of 1-grams hit = 2  (2.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article118.text
Will force inclusive back-off from OOVs.
Perplexity = 140.05, Entropy = 7.13 bits
Computation based on 91 words.
Number of 2-grams hit = 87  (95.60%)
Number of 1-grams hit = 4  (4.40%)
2 OOVs (2.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article119.text
Will force inclusive back-off from OOVs.
Perplexity = 108.57, Entropy = 6.76 bits
Computation based on 48 words.
Number of 2-grams hit = 45  (93.75%)
Number of 1-grams hit = 3  (6.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article120.text
Will force inclusive back-off from OOVs.
Perplexity = 118.55, Entropy = 6.89 bits
Computation based on 125 words.
Number of 2-grams hit = 120  (96.00%)
Number of 1-grams hit = 5  (4.00%)
2 OOVs (1.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article121.text
Will force inclusive back-off from OOVs.
Perplexity = 264.92, Entropy = 8.05 bits
Computation based on 156 words.
Number of 2-grams hit = 142  (91.03%)
Number of 1-grams hit = 14  (8.97%)
2 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article122.text
Will force inclusive back-off from OOVs.
Perplexity = 76.91, Entropy = 6.27 bits
Computation based on 97 words.
Number of 2-grams hit = 93  (95.88%)
Number of 1-grams hit = 4  (4.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article123.text
Will force inclusive back-off from OOVs.
Perplexity = 50.40, Entropy = 5.66 bits
Computation based on 56 words.
Number of 2-grams hit = 55  (98.21%)
Number of 1-grams hit = 1  (1.79%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article124.text
Will force inclusive back-off from OOVs.
Perplexity = 138.53, Entropy = 7.11 bits
Computation based on 143 words.
Number of 2-grams hit = 140  (97.90%)
Number of 1-grams hit = 3  (2.10%)
2 OOVs (1.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article125.text
Will force inclusive back-off from OOVs.
Perplexity = 91.99, Entropy = 6.52 bits
Computation based on 95 words.
Number of 2-grams hit = 91  (95.79%)
Number of 1-grams hit = 4  (4.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article126.text
Will force inclusive back-off from OOVs.
Perplexity = 225.51, Entropy = 7.82 bits
Computation based on 135 words.
Number of 2-grams hit = 127  (94.07%)
Number of 1-grams hit = 8  (5.93%)
11 OOVs (7.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article127.text
Will force inclusive back-off from OOVs.
Perplexity = 136.78, Entropy = 7.10 bits
Computation based on 155 words.
Number of 2-grams hit = 145  (93.55%)
Number of 1-grams hit = 10  (6.45%)
5 OOVs (3.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article128.text
Will force inclusive back-off from OOVs.
Perplexity = 120.65, Entropy = 6.91 bits
Computation based on 82 words.
Number of 2-grams hit = 79  (96.34%)
Number of 1-grams hit = 3  (3.66%)
1 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article129.text
Will force inclusive back-off from OOVs.
Perplexity = 107.21, Entropy = 6.74 bits
Computation based on 116 words.
Number of 2-grams hit = 111  (95.69%)
Number of 1-grams hit = 5  (4.31%)
1 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article130.text
Will force inclusive back-off from OOVs.
Perplexity = 94.93, Entropy = 6.57 bits
Computation based on 135 words.
Number of 2-grams hit = 130  (96.30%)
Number of 1-grams hit = 5  (3.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article131.text
Will force inclusive back-off from OOVs.
Perplexity = 67.48, Entropy = 6.08 bits
Computation based on 66 words.
Number of 2-grams hit = 65  (98.48%)
Number of 1-grams hit = 1  (1.52%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article132.text
Will force inclusive back-off from OOVs.
Perplexity = 115.11, Entropy = 6.85 bits
Computation based on 128 words.
Number of 2-grams hit = 127  (99.22%)
Number of 1-grams hit = 1  (0.78%)
1 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article133.text
Will force inclusive back-off from OOVs.
Perplexity = 115.07, Entropy = 6.85 bits
Computation based on 99 words.
Number of 2-grams hit = 97  (97.98%)
Number of 1-grams hit = 2  (2.02%)
1 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article134.text
Will force inclusive back-off from OOVs.
Perplexity = 89.17, Entropy = 6.48 bits
Computation based on 79 words.
Number of 2-grams hit = 78  (98.73%)
Number of 1-grams hit = 1  (1.27%)
1 OOVs (1.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article135.text
Will force inclusive back-off from OOVs.
Perplexity = 121.48, Entropy = 6.92 bits
Computation based on 74 words.
Number of 2-grams hit = 70  (94.59%)
Number of 1-grams hit = 4  (5.41%)
1 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article136.text
Will force inclusive back-off from OOVs.
Perplexity = 230.25, Entropy = 7.85 bits
Computation based on 121 words.
Number of 2-grams hit = 109  (90.08%)
Number of 1-grams hit = 12  (9.92%)
8 OOVs (6.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article137.text
Will force inclusive back-off from OOVs.
Perplexity = 151.55, Entropy = 7.24 bits
Computation based on 141 words.
Number of 2-grams hit = 136  (96.45%)
Number of 1-grams hit = 5  (3.55%)
1 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article138.text
Will force inclusive back-off from OOVs.
Perplexity = 87.89, Entropy = 6.46 bits
Computation based on 94 words.
Number of 2-grams hit = 90  (95.74%)
Number of 1-grams hit = 4  (4.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article139.text
Will force inclusive back-off from OOVs.
Perplexity = 178.15, Entropy = 7.48 bits
Computation based on 111 words.
Number of 2-grams hit = 104  (93.69%)
Number of 1-grams hit = 7  (6.31%)
2 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article140.text
Will force inclusive back-off from OOVs.
Perplexity = 238.35, Entropy = 7.90 bits
Computation based on 200 words.
Number of 2-grams hit = 187  (93.50%)
Number of 1-grams hit = 13  (6.50%)
1 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article141.text
Will force inclusive back-off from OOVs.
Perplexity = 125.41, Entropy = 6.97 bits
Computation based on 132 words.
Number of 2-grams hit = 125  (94.70%)
Number of 1-grams hit = 7  (5.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article142.text
Will force inclusive back-off from OOVs.
Perplexity = 105.10, Entropy = 6.72 bits
Computation based on 100 words.
Number of 2-grams hit = 95  (95.00%)
Number of 1-grams hit = 5  (5.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article143.text
Will force inclusive back-off from OOVs.
Perplexity = 118.35, Entropy = 6.89 bits
Computation based on 185 words.
Number of 2-grams hit = 178  (96.22%)
Number of 1-grams hit = 7  (3.78%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article144.text
Will force inclusive back-off from OOVs.
Perplexity = 177.60, Entropy = 7.47 bits
Computation based on 103 words.
Number of 2-grams hit = 95  (92.23%)
Number of 1-grams hit = 8  (7.77%)
1 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article145.text
Will force inclusive back-off from OOVs.
Perplexity = 189.28, Entropy = 7.56 bits
Computation based on 278 words.
Number of 2-grams hit = 263  (94.60%)
Number of 1-grams hit = 15  (5.40%)
1 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article146.text
Will force inclusive back-off from OOVs.
Perplexity = 106.87, Entropy = 6.74 bits
Computation based on 187 words.
Number of 2-grams hit = 185  (98.93%)
Number of 1-grams hit = 2  (1.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article147.text
Will force inclusive back-off from OOVs.
Perplexity = 161.95, Entropy = 7.34 bits
Computation based on 178 words.
Number of 2-grams hit = 170  (95.51%)
Number of 1-grams hit = 8  (4.49%)
1 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article148.text
Will force inclusive back-off from OOVs.
Perplexity = 51.21, Entropy = 5.68 bits
Computation based on 114 words.
Number of 2-grams hit = 113  (99.12%)
Number of 1-grams hit = 1  (0.88%)
1 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article149.text
Will force inclusive back-off from OOVs.
Perplexity = 83.93, Entropy = 6.39 bits
Computation based on 134 words.
Number of 2-grams hit = 132  (98.51%)
Number of 1-grams hit = 2  (1.49%)
2 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article150.text
Will force inclusive back-off from OOVs.
Perplexity = 86.00, Entropy = 6.43 bits
Computation based on 219 words.
Number of 2-grams hit = 214  (97.72%)
Number of 1-grams hit = 5  (2.28%)
1 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article151.text
Will force inclusive back-off from OOVs.
Perplexity = 122.75, Entropy = 6.94 bits
Computation based on 151 words.
Number of 2-grams hit = 143  (94.70%)
Number of 1-grams hit = 8  (5.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article152.text
Will force inclusive back-off from OOVs.
Perplexity = 101.45, Entropy = 6.66 bits
Computation based on 123 words.
Number of 2-grams hit = 121  (98.37%)
Number of 1-grams hit = 2  (1.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article153.text
Will force inclusive back-off from OOVs.
Perplexity = 254.19, Entropy = 7.99 bits
Computation based on 250 words.
Number of 2-grams hit = 225  (90.00%)
Number of 1-grams hit = 25  (10.00%)
2 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article154.text
Will force inclusive back-off from OOVs.
Perplexity = 175.92, Entropy = 7.46 bits
Computation based on 183 words.
Number of 2-grams hit = 175  (95.63%)
Number of 1-grams hit = 8  (4.37%)
7 OOVs (3.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article155.text
Will force inclusive back-off from OOVs.
Perplexity = 128.08, Entropy = 7.00 bits
Computation based on 156 words.
Number of 2-grams hit = 150  (96.15%)
Number of 1-grams hit = 6  (3.85%)
1 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article156.text
Will force inclusive back-off from OOVs.
Perplexity = 130.17, Entropy = 7.02 bits
Computation based on 171 words.
Number of 2-grams hit = 163  (95.32%)
Number of 1-grams hit = 8  (4.68%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article157.text
Will force inclusive back-off from OOVs.
Perplexity = 98.29, Entropy = 6.62 bits
Computation based on 137 words.
Number of 2-grams hit = 133  (97.08%)
Number of 1-grams hit = 4  (2.92%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article158.text
Will force inclusive back-off from OOVs.
Perplexity = 122.42, Entropy = 6.94 bits
Computation based on 174 words.
Number of 2-grams hit = 168  (96.55%)
Number of 1-grams hit = 6  (3.45%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article159.text
Will force inclusive back-off from OOVs.
Perplexity = 159.35, Entropy = 7.32 bits
Computation based on 173 words.
Number of 2-grams hit = 162  (93.64%)
Number of 1-grams hit = 11  (6.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article160.text
Will force inclusive back-off from OOVs.
Perplexity = 104.78, Entropy = 6.71 bits
Computation based on 249 words.
Number of 2-grams hit = 238  (95.58%)
Number of 1-grams hit = 11  (4.42%)
4 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article161.text
Will force inclusive back-off from OOVs.
Perplexity = 135.81, Entropy = 7.09 bits
Computation based on 276 words.
Number of 2-grams hit = 265  (96.01%)
Number of 1-grams hit = 11  (3.99%)
1 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article162.text
Will force inclusive back-off from OOVs.
Perplexity = 121.11, Entropy = 6.92 bits
Computation based on 208 words.
Number of 2-grams hit = 199  (95.67%)
Number of 1-grams hit = 9  (4.33%)
7 OOVs (3.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article163.text
Will force inclusive back-off from OOVs.
Perplexity = 129.15, Entropy = 7.01 bits
Computation based on 275 words.
Number of 2-grams hit = 263  (95.64%)
Number of 1-grams hit = 12  (4.36%)
2 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article164.text
Will force inclusive back-off from OOVs.
Perplexity = 86.81, Entropy = 6.44 bits
Computation based on 236 words.
Number of 2-grams hit = 232  (98.31%)
Number of 1-grams hit = 4  (1.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article165.text
Will force inclusive back-off from OOVs.
Perplexity = 172.45, Entropy = 7.43 bits
Computation based on 280 words.
Number of 2-grams hit = 265  (94.64%)
Number of 1-grams hit = 15  (5.36%)
1 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article166.text
Will force inclusive back-off from OOVs.
Perplexity = 286.58, Entropy = 8.16 bits
Computation based on 289 words.
Number of 2-grams hit = 264  (91.35%)
Number of 1-grams hit = 25  (8.65%)
10 OOVs (3.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article167.text
Will force inclusive back-off from OOVs.
Perplexity = 95.42, Entropy = 6.58 bits
Computation based on 231 words.
Number of 2-grams hit = 216  (93.51%)
Number of 1-grams hit = 15  (6.49%)
1 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article168.text
Will force inclusive back-off from OOVs.
Perplexity = 51.48, Entropy = 5.69 bits
Computation based on 118 words.
Number of 2-grams hit = 115  (97.46%)
Number of 1-grams hit = 3  (2.54%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article169.text
Will force inclusive back-off from OOVs.
Perplexity = 153.06, Entropy = 7.26 bits
Computation based on 339 words.
Number of 2-grams hit = 324  (95.58%)
Number of 1-grams hit = 15  (4.42%)
6 OOVs (1.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article170.text
Will force inclusive back-off from OOVs.
Perplexity = 101.42, Entropy = 6.66 bits
Computation based on 263 words.
Number of 2-grams hit = 254  (96.58%)
Number of 1-grams hit = 9  (3.42%)
2 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article171.text
Will force inclusive back-off from OOVs.
Perplexity = 125.67, Entropy = 6.97 bits
Computation based on 288 words.
Number of 2-grams hit = 276  (95.83%)
Number of 1-grams hit = 12  (4.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article172.text
Will force inclusive back-off from OOVs.
Perplexity = 102.61, Entropy = 6.68 bits
Computation based on 195 words.
Number of 2-grams hit = 184  (94.36%)
Number of 1-grams hit = 11  (5.64%)
2 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article173.text
Will force inclusive back-off from OOVs.
Perplexity = 125.92, Entropy = 6.98 bits
Computation based on 243 words.
Number of 2-grams hit = 228  (93.83%)
Number of 1-grams hit = 15  (6.17%)
1 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article174.text
Will force inclusive back-off from OOVs.
Perplexity = 93.34, Entropy = 6.54 bits
Computation based on 203 words.
Number of 2-grams hit = 199  (98.03%)
Number of 1-grams hit = 4  (1.97%)
1 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article175.text
Will force inclusive back-off from OOVs.
Perplexity = 156.99, Entropy = 7.29 bits
Computation based on 428 words.
Number of 2-grams hit = 405  (94.63%)
Number of 1-grams hit = 23  (5.37%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article176.text
Will force inclusive back-off from OOVs.
Perplexity = 121.85, Entropy = 6.93 bits
Computation based on 256 words.
Number of 2-grams hit = 248  (96.88%)
Number of 1-grams hit = 8  (3.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article177.text
Will force inclusive back-off from OOVs.
Perplexity = 164.74, Entropy = 7.36 bits
Computation based on 293 words.
Number of 2-grams hit = 277  (94.54%)
Number of 1-grams hit = 16  (5.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article178.text
Will force inclusive back-off from OOVs.
Perplexity = 114.38, Entropy = 6.84 bits
Computation based on 165 words.
Number of 2-grams hit = 157  (95.15%)
Number of 1-grams hit = 8  (4.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article179.text
Will force inclusive back-off from OOVs.
Perplexity = 124.91, Entropy = 6.96 bits
Computation based on 243 words.
Number of 2-grams hit = 229  (94.24%)
Number of 1-grams hit = 14  (5.76%)
1 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article180.text
Will force inclusive back-off from OOVs.
Perplexity = 83.85, Entropy = 6.39 bits
Computation based on 192 words.
Number of 2-grams hit = 190  (98.96%)
Number of 1-grams hit = 2  (1.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article181.text
Will force inclusive back-off from OOVs.
Perplexity = 131.77, Entropy = 7.04 bits
Computation based on 298 words.
Number of 2-grams hit = 284  (95.30%)
Number of 1-grams hit = 14  (4.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article182.text
Will force inclusive back-off from OOVs.
Perplexity = 73.92, Entropy = 6.21 bits
Computation based on 254 words.
Number of 2-grams hit = 253  (99.61%)
Number of 1-grams hit = 1  (0.39%)
5 OOVs (1.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article183.text
Will force inclusive back-off from OOVs.
Perplexity = 109.95, Entropy = 6.78 bits
Computation based on 291 words.
Number of 2-grams hit = 282  (96.91%)
Number of 1-grams hit = 9  (3.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article184.text
Will force inclusive back-off from OOVs.
Perplexity = 90.57, Entropy = 6.50 bits
Computation based on 315 words.
Number of 2-grams hit = 310  (98.41%)
Number of 1-grams hit = 5  (1.59%)
4 OOVs (1.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article185.text
Will force inclusive back-off from OOVs.
Perplexity = 156.40, Entropy = 7.29 bits
Computation based on 443 words.
Number of 2-grams hit = 413  (93.23%)
Number of 1-grams hit = 30  (6.77%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article186.text
Will force inclusive back-off from OOVs.
Perplexity = 60.25, Entropy = 5.91 bits
Computation based on 230 words.
Number of 2-grams hit = 226  (98.26%)
Number of 1-grams hit = 4  (1.74%)
2 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article187.text
Will force inclusive back-off from OOVs.
Perplexity = 148.76, Entropy = 7.22 bits
Computation based on 327 words.
Number of 2-grams hit = 306  (93.58%)
Number of 1-grams hit = 21  (6.42%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article188.text
Will force inclusive back-off from OOVs.
Perplexity = 126.61, Entropy = 6.98 bits
Computation based on 302 words.
Number of 2-grams hit = 286  (94.70%)
Number of 1-grams hit = 16  (5.30%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article189.text
Will force inclusive back-off from OOVs.
Perplexity = 176.44, Entropy = 7.46 bits
Computation based on 377 words.
Number of 2-grams hit = 353  (93.63%)
Number of 1-grams hit = 24  (6.37%)
3 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article190.text
Will force inclusive back-off from OOVs.
Perplexity = 56.39, Entropy = 5.82 bits
Computation based on 206 words.
Number of 2-grams hit = 205  (99.51%)
Number of 1-grams hit = 1  (0.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article191.text
Will force inclusive back-off from OOVs.
Perplexity = 140.10, Entropy = 7.13 bits
Computation based on 341 words.
Number of 2-grams hit = 325  (95.31%)
Number of 1-grams hit = 16  (4.69%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article192.text
Will force inclusive back-off from OOVs.
Perplexity = 65.42, Entropy = 6.03 bits
Computation based on 276 words.
Number of 2-grams hit = 273  (98.91%)
Number of 1-grams hit = 3  (1.09%)
3 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article193.text
Will force inclusive back-off from OOVs.
Perplexity = 116.63, Entropy = 6.87 bits
Computation based on 272 words.
Number of 2-grams hit = 261  (95.96%)
Number of 1-grams hit = 11  (4.04%)
1 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article194.text
Will force inclusive back-off from OOVs.
Perplexity = 150.19, Entropy = 7.23 bits
Computation based on 237 words.
Number of 2-grams hit = 226  (95.36%)
Number of 1-grams hit = 11  (4.64%)
8 OOVs (3.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article195.text
Will force inclusive back-off from OOVs.
Perplexity = 166.51, Entropy = 7.38 bits
Computation based on 391 words.
Number of 2-grams hit = 371  (94.88%)
Number of 1-grams hit = 20  (5.12%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article196.text
Will force inclusive back-off from OOVs.
Perplexity = 231.93, Entropy = 7.86 bits
Computation based on 499 words.
Number of 2-grams hit = 470  (94.19%)
Number of 1-grams hit = 29  (5.81%)
5 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article197.text
Will force inclusive back-off from OOVs.
Perplexity = 150.83, Entropy = 7.24 bits
Computation based on 363 words.
Number of 2-grams hit = 343  (94.49%)
Number of 1-grams hit = 20  (5.51%)
3 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article198.text
Will force inclusive back-off from OOVs.
Perplexity = 95.10, Entropy = 6.57 bits
Computation based on 309 words.
Number of 2-grams hit = 303  (98.06%)
Number of 1-grams hit = 6  (1.94%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/devArticles/article199.text
Will force inclusive back-off from OOVs.
Perplexity = 126.89, Entropy = 6.99 bits
Computation based on 353 words.
Number of 2-grams hit = 333  (94.33%)
Number of 1-grams hit = 20  (5.67%)
4 OOVs (1.12%) and 0 context cues were removed from the calculation.
evallm : 