evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle0.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1366 words.
Number of 2-grams hit = 1365  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle1.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 1620 words.
Number of 2-grams hit = 1619  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle2.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 591 words.
Number of 2-grams hit = 590  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle3.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 661 words.
Number of 2-grams hit = 660  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle4.out
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 425 words.
Number of 2-grams hit = 424  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle5.out
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 957 words.
Number of 2-grams hit = 956  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle6.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 317 words.
Number of 2-grams hit = 316  (99.68%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle7.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 665 words.
Number of 2-grams hit = 664  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle8.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 528 words.
Number of 2-grams hit = 527  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle9.out
Perplexity = 4.20, Entropy = 2.07 bits
Computation based on 345 words.
Number of 2-grams hit = 344  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle10.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 327 words.
Number of 2-grams hit = 326  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle11.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 325 words.
Number of 2-grams hit = 324  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle12.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 326 words.
Number of 2-grams hit = 325  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle13.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 488 words.
Number of 2-grams hit = 487  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle14.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 523 words.
Number of 2-grams hit = 522  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle15.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 543 words.
Number of 2-grams hit = 542  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle16.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 400 words.
Number of 2-grams hit = 399  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle17.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 722 words.
Number of 2-grams hit = 721  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle18.out
Perplexity = 4.13, Entropy = 2.04 bits
Computation based on 445 words.
Number of 2-grams hit = 444  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle19.out
Perplexity = 4.33, Entropy = 2.11 bits
Computation based on 560 words.
Number of 2-grams hit = 559  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle20.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 386 words.
Number of 2-grams hit = 385  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle21.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 500 words.
Number of 2-grams hit = 499  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle22.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 387 words.
Number of 2-grams hit = 386  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle23.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 378 words.
Number of 2-grams hit = 377  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle24.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 599 words.
Number of 2-grams hit = 598  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle25.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 413 words.
Number of 2-grams hit = 412  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle26.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 1649 words.
Number of 2-grams hit = 1648  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle27.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 424 words.
Number of 2-grams hit = 423  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle28.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1511 words.
Number of 2-grams hit = 1510  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle29.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 346 words.
Number of 2-grams hit = 345  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle30.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 523 words.
Number of 2-grams hit = 522  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle31.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 481 words.
Number of 2-grams hit = 480  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle32.out
Perplexity = 3.99, Entropy = 1.99 bits
Computation based on 516 words.
Number of 2-grams hit = 515  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle33.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 476 words.
Number of 2-grams hit = 475  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle34.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 326 words.
Number of 2-grams hit = 325  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle35.out
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 402 words.
Number of 2-grams hit = 401  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle36.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 1088 words.
Number of 2-grams hit = 1087  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle37.out
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 604 words.
Number of 2-grams hit = 603  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle38.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 444 words.
Number of 2-grams hit = 443  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle39.out
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 2871 words.
Number of 2-grams hit = 2870  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle40.out
Perplexity = 4.42, Entropy = 2.14 bits
Computation based on 442 words.
Number of 2-grams hit = 441  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle41.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1060 words.
Number of 2-grams hit = 1059  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle42.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 591 words.
Number of 2-grams hit = 590  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle43.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 490 words.
Number of 2-grams hit = 489  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle44.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 549 words.
Number of 2-grams hit = 548  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle45.out
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 502 words.
Number of 2-grams hit = 501  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle46.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 436 words.
Number of 2-grams hit = 435  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle47.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 2914 words.
Number of 2-grams hit = 2913  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle48.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 400 words.
Number of 2-grams hit = 399  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle49.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 395 words.
Number of 2-grams hit = 394  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle50.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 563 words.
Number of 2-grams hit = 562  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle51.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1184 words.
Number of 2-grams hit = 1183  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle52.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 401 words.
Number of 2-grams hit = 400  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle53.out
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 521 words.
Number of 2-grams hit = 520  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle54.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1201 words.
Number of 2-grams hit = 1200  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle55.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 578 words.
Number of 2-grams hit = 577  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle56.out
Perplexity = 4.34, Entropy = 2.12 bits
Computation based on 988 words.
Number of 2-grams hit = 987  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle57.out
Perplexity = 3.96, Entropy = 1.98 bits
Computation based on 1691 words.
Number of 2-grams hit = 1690  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle58.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 416 words.
Number of 2-grams hit = 415  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle59.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 231 words.
Number of 2-grams hit = 230  (99.57%)
Number of 1-grams hit = 1  (0.43%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle60.out
Perplexity = 3.17, Entropy = 1.66 bits
Computation based on 354 words.
Number of 2-grams hit = 353  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle61.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 530 words.
Number of 2-grams hit = 529  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle62.out
Perplexity = 4.18, Entropy = 2.06 bits
Computation based on 1398 words.
Number of 2-grams hit = 1397  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle63.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 1066 words.
Number of 2-grams hit = 1065  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle64.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 4550 words.
Number of 2-grams hit = 4549  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle65.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 697 words.
Number of 2-grams hit = 696  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle66.out
Perplexity = 4.22, Entropy = 2.08 bits
Computation based on 514 words.
Number of 2-grams hit = 513  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle67.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 484 words.
Number of 2-grams hit = 483  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle68.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 1206 words.
Number of 2-grams hit = 1205  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle69.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 407 words.
Number of 2-grams hit = 406  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle70.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 1071 words.
Number of 2-grams hit = 1070  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle71.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 591 words.
Number of 2-grams hit = 590  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle72.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 1423 words.
Number of 2-grams hit = 1422  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle73.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 566 words.
Number of 2-grams hit = 565  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle74.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 901 words.
Number of 2-grams hit = 900  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle75.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 1288 words.
Number of 2-grams hit = 1287  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle76.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1515 words.
Number of 2-grams hit = 1514  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle77.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 322 words.
Number of 2-grams hit = 321  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle78.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1223 words.
Number of 2-grams hit = 1222  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle79.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1163 words.
Number of 2-grams hit = 1162  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle80.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 412 words.
Number of 2-grams hit = 411  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle81.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 4418 words.
Number of 2-grams hit = 4417  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle82.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 510 words.
Number of 2-grams hit = 509  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle83.out
Perplexity = 4.20, Entropy = 2.07 bits
Computation based on 476 words.
Number of 2-grams hit = 474  (99.58%)
Number of 1-grams hit = 2  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle84.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 431 words.
Number of 2-grams hit = 430  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle85.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1317 words.
Number of 2-grams hit = 1316  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle86.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 309 words.
Number of 2-grams hit = 308  (99.68%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle87.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 676 words.
Number of 2-grams hit = 675  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle88.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1583 words.
Number of 2-grams hit = 1582  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle89.out
Perplexity = 4.59, Entropy = 2.20 bits
Computation based on 414 words.
Number of 2-grams hit = 413  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle90.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 555 words.
Number of 2-grams hit = 554  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle91.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 371 words.
Number of 2-grams hit = 370  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle92.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1134 words.
Number of 2-grams hit = 1133  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle93.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 745 words.
Number of 2-grams hit = 744  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle94.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1395 words.
Number of 2-grams hit = 1394  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle95.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 546 words.
Number of 2-grams hit = 545  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle96.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 439 words.
Number of 2-grams hit = 438  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle97.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 601 words.
Number of 2-grams hit = 600  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle98.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 391 words.
Number of 2-grams hit = 390  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle99.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 497 words.
Number of 2-grams hit = 496  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle100.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 513 words.
Number of 2-grams hit = 512  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle101.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 554 words.
Number of 2-grams hit = 553  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle102.out
Perplexity = 4.13, Entropy = 2.05 bits
Computation based on 448 words.
Number of 2-grams hit = 447  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle103.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 536 words.
Number of 2-grams hit = 535  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle104.out
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 6983 words.
Number of 2-grams hit = 6982  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle105.out
Perplexity = 4.74, Entropy = 2.24 bits
Computation based on 485 words.
Number of 2-grams hit = 484  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle106.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 875 words.
Number of 2-grams hit = 874  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle107.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 529 words.
Number of 2-grams hit = 528  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle108.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 568 words.
Number of 2-grams hit = 567  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle109.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 1223 words.
Number of 2-grams hit = 1222  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle110.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 981 words.
Number of 2-grams hit = 980  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle111.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1359 words.
Number of 2-grams hit = 1358  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle112.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 355 words.
Number of 2-grams hit = 354  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle113.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 6382 words.
Number of 2-grams hit = 6381  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle114.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 552 words.
Number of 2-grams hit = 551  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle115.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 698 words.
Number of 2-grams hit = 697  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle116.out
Perplexity = 4.37, Entropy = 2.13 bits
Computation based on 1647 words.
Number of 2-grams hit = 1646  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle117.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 557 words.
Number of 2-grams hit = 556  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle118.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 667 words.
Number of 2-grams hit = 666  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle119.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 424 words.
Number of 2-grams hit = 423  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle120.out
Perplexity = 4.11, Entropy = 2.04 bits
Computation based on 608 words.
Number of 2-grams hit = 607  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle121.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 7778 words.
Number of 2-grams hit = 7777  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle122.out
Perplexity = 4.31, Entropy = 2.11 bits
Computation based on 353 words.
Number of 2-grams hit = 352  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle123.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 553 words.
Number of 2-grams hit = 552  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle124.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 873 words.
Number of 2-grams hit = 872  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle125.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 525 words.
Number of 2-grams hit = 524  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle126.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 587 words.
Number of 2-grams hit = 586  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle127.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1001 words.
Number of 2-grams hit = 1000  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle128.out
Perplexity = 4.53, Entropy = 2.18 bits
Computation based on 462 words.
Number of 2-grams hit = 461  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle129.out
Perplexity = 4.32, Entropy = 2.11 bits
Computation based on 511 words.
Number of 2-grams hit = 510  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle130.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 1659 words.
Number of 2-grams hit = 1658  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle131.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 7398 words.
Number of 2-grams hit = 7397  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle132.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1222 words.
Number of 2-grams hit = 1221  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle133.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 262 words.
Number of 2-grams hit = 261  (99.62%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle134.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 539 words.
Number of 2-grams hit = 538  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle135.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 521 words.
Number of 2-grams hit = 520  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle136.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 567 words.
Number of 2-grams hit = 566  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle137.out
Perplexity = 4.39, Entropy = 2.13 bits
Computation based on 361 words.
Number of 2-grams hit = 360  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle138.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 342 words.
Number of 2-grams hit = 341  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle139.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 298 words.
Number of 2-grams hit = 297  (99.66%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle140.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 400 words.
Number of 2-grams hit = 399  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle141.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 467 words.
Number of 2-grams hit = 466  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle142.out
Perplexity = 4.13, Entropy = 2.05 bits
Computation based on 758 words.
Number of 2-grams hit = 757  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle143.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 412 words.
Number of 2-grams hit = 411  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle144.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 541 words.
Number of 2-grams hit = 540  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle145.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 2107 words.
Number of 2-grams hit = 2106  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle146.out
Perplexity = 4.49, Entropy = 2.17 bits
Computation based on 262 words.
Number of 2-grams hit = 261  (99.62%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle147.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 714 words.
Number of 2-grams hit = 713  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle148.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1886 words.
Number of 2-grams hit = 1885  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle149.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 612 words.
Number of 2-grams hit = 611  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle150.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 686 words.
Number of 2-grams hit = 685  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle151.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 188 words.
Number of 2-grams hit = 187  (99.47%)
Number of 1-grams hit = 1  (0.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle152.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1984 words.
Number of 2-grams hit = 1983  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle153.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 629 words.
Number of 2-grams hit = 628  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle154.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1160 words.
Number of 2-grams hit = 1159  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle155.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 695 words.
Number of 2-grams hit = 694  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle156.out
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 1549 words.
Number of 2-grams hit = 1548  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle157.out
Perplexity = 4.35, Entropy = 2.12 bits
Computation based on 431 words.
Number of 2-grams hit = 430  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle158.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 623 words.
Number of 2-grams hit = 622  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle159.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 1362 words.
Number of 2-grams hit = 1361  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle160.out
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 584 words.
Number of 2-grams hit = 583  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle161.out
Perplexity = 4.13, Entropy = 2.05 bits
Computation based on 211 words.
Number of 2-grams hit = 210  (99.53%)
Number of 1-grams hit = 1  (0.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle162.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1245 words.
Number of 2-grams hit = 1244  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle163.out
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 752 words.
Number of 2-grams hit = 751  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle164.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 1948 words.
Number of 2-grams hit = 1947  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle165.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 593 words.
Number of 2-grams hit = 592  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle166.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 555 words.
Number of 2-grams hit = 554  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle167.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1260 words.
Number of 2-grams hit = 1259  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle168.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 571 words.
Number of 2-grams hit = 570  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle169.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 551 words.
Number of 2-grams hit = 550  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle170.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1235 words.
Number of 2-grams hit = 1234  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle171.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 656 words.
Number of 2-grams hit = 655  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle172.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 734 words.
Number of 2-grams hit = 733  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle173.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1110 words.
Number of 2-grams hit = 1109  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle174.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 493 words.
Number of 2-grams hit = 492  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle175.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 486 words.
Number of 2-grams hit = 485  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle176.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 800 words.
Number of 2-grams hit = 799  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle177.out
Perplexity = 4.24, Entropy = 2.08 bits
Computation based on 557 words.
Number of 2-grams hit = 556  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle178.out
Perplexity = 4.23, Entropy = 2.08 bits
Computation based on 183 words.
Number of 2-grams hit = 182  (99.45%)
Number of 1-grams hit = 1  (0.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle179.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 491 words.
Number of 2-grams hit = 490  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle180.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 473 words.
Number of 2-grams hit = 472  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle181.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 955 words.
Number of 2-grams hit = 954  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle182.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 407 words.
Number of 2-grams hit = 406  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle183.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 504 words.
Number of 2-grams hit = 503  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle184.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 5958 words.
Number of 2-grams hit = 5957  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle185.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1070 words.
Number of 2-grams hit = 1069  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle186.out
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 529 words.
Number of 2-grams hit = 528  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle187.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 3137 words.
Number of 2-grams hit = 3136  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle188.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 729 words.
Number of 2-grams hit = 728  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle189.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 580 words.
Number of 2-grams hit = 579  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle190.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 421 words.
Number of 2-grams hit = 420  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle191.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 5916 words.
Number of 2-grams hit = 5913  (99.95%)
Number of 1-grams hit = 3  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle192.out
Perplexity = 2.86, Entropy = 1.52 bits
Computation based on 818 words.
Number of 2-grams hit = 817  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle193.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 170 words.
Number of 2-grams hit = 169  (99.41%)
Number of 1-grams hit = 1  (0.59%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle194.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 661 words.
Number of 2-grams hit = 660  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle195.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 864 words.
Number of 2-grams hit = 863  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle196.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 730 words.
Number of 2-grams hit = 729  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle197.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 251 words.
Number of 2-grams hit = 250  (99.60%)
Number of 1-grams hit = 1  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle198.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 768 words.
Number of 2-grams hit = 767  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle199.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 4967 words.
Number of 2-grams hit = 4966  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle200.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 702 words.
Number of 2-grams hit = 701  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle201.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 507 words.
Number of 2-grams hit = 506  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle202.out
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 1262 words.
Number of 2-grams hit = 1261  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle203.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 5293 words.
Number of 2-grams hit = 5292  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle204.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 5882 words.
Number of 2-grams hit = 5881  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle205.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 603 words.
Number of 2-grams hit = 602  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle206.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 636 words.
Number of 2-grams hit = 635  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle207.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 779 words.
Number of 2-grams hit = 778  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle208.out
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 951 words.
Number of 2-grams hit = 950  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle209.out
Perplexity = 4.10, Entropy = 2.04 bits
Computation based on 283 words.
Number of 2-grams hit = 282  (99.65%)
Number of 1-grams hit = 1  (0.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle210.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 886 words.
Number of 2-grams hit = 885  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle211.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 504 words.
Number of 2-grams hit = 503  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle212.out
Perplexity = 4.07, Entropy = 2.02 bits
Computation based on 449 words.
Number of 2-grams hit = 448  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle213.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 695 words.
Number of 2-grams hit = 694  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle214.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 509 words.
Number of 2-grams hit = 508  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle215.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 742 words.
Number of 2-grams hit = 741  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle216.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 525 words.
Number of 2-grams hit = 524  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle217.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 323 words.
Number of 2-grams hit = 322  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle218.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 792 words.
Number of 2-grams hit = 791  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle219.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 439 words.
Number of 2-grams hit = 438  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle220.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 4706 words.
Number of 2-grams hit = 4705  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle221.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1255 words.
Number of 2-grams hit = 1254  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle222.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 4839 words.
Number of 2-grams hit = 4838  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle223.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 895 words.
Number of 2-grams hit = 894  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle224.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 406 words.
Number of 2-grams hit = 405  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle225.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 777 words.
Number of 2-grams hit = 776  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle226.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1314 words.
Number of 2-grams hit = 1313  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle227.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 1017 words.
Number of 2-grams hit = 1016  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle228.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 851 words.
Number of 2-grams hit = 850  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle229.out
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 518 words.
Number of 2-grams hit = 517  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle230.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 337 words.
Number of 2-grams hit = 336  (99.70%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle231.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 850 words.
Number of 2-grams hit = 849  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle232.out
Perplexity = 4.23, Entropy = 2.08 bits
Computation based on 391 words.
Number of 2-grams hit = 390  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle233.out
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 734 words.
Number of 2-grams hit = 733  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle234.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 1446 words.
Number of 2-grams hit = 1445  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle235.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 379 words.
Number of 2-grams hit = 378  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle236.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 801 words.
Number of 2-grams hit = 800  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle237.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 423 words.
Number of 2-grams hit = 422  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle238.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 339 words.
Number of 2-grams hit = 338  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle239.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 530 words.
Number of 2-grams hit = 529  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle240.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 350 words.
Number of 2-grams hit = 349  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle241.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 334 words.
Number of 2-grams hit = 333  (99.70%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle242.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 319 words.
Number of 2-grams hit = 318  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle243.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 813 words.
Number of 2-grams hit = 812  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle244.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 457 words.
Number of 2-grams hit = 456  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle245.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 5533 words.
Number of 2-grams hit = 5532  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle246.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 304 words.
Number of 2-grams hit = 303  (99.67%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle247.out
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 492 words.
Number of 2-grams hit = 491  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle248.out
Perplexity = 4.22, Entropy = 2.08 bits
Computation based on 388 words.
Number of 2-grams hit = 387  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle249.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 928 words.
Number of 2-grams hit = 927  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle250.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 5273 words.
Number of 2-grams hit = 5272  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle251.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 5439 words.
Number of 2-grams hit = 5438  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle252.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 579 words.
Number of 2-grams hit = 578  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle253.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 5348 words.
Number of 2-grams hit = 5347  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle254.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 836 words.
Number of 2-grams hit = 835  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle255.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 526 words.
Number of 2-grams hit = 525  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle256.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 837 words.
Number of 2-grams hit = 836  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle257.out
Perplexity = 4.10, Entropy = 2.04 bits
Computation based on 6431 words.
Number of 2-grams hit = 6428  (99.95%)
Number of 1-grams hit = 3  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle258.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 444 words.
Number of 2-grams hit = 443  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle259.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 764 words.
Number of 2-grams hit = 763  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle260.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 7634 words.
Number of 2-grams hit = 7633  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle261.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 717 words.
Number of 2-grams hit = 716  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle262.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 721 words.
Number of 2-grams hit = 720  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle263.out
Perplexity = 4.12, Entropy = 2.04 bits
Computation based on 411 words.
Number of 2-grams hit = 410  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle264.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 857 words.
Number of 2-grams hit = 856  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle265.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 602 words.
Number of 2-grams hit = 601  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle266.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 468 words.
Number of 2-grams hit = 467  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle267.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 6787 words.
Number of 2-grams hit = 6786  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle268.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 6349 words.
Number of 2-grams hit = 6348  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle269.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 457 words.
Number of 2-grams hit = 456  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle270.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 404 words.
Number of 2-grams hit = 403  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle271.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 7909 words.
Number of 2-grams hit = 7908  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle272.out
Perplexity = 4.12, Entropy = 2.04 bits
Computation based on 4304 words.
Number of 2-grams hit = 4303  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle273.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 683 words.
Number of 2-grams hit = 682  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle274.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 704 words.
Number of 2-grams hit = 703  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle275.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 2091 words.
Number of 2-grams hit = 2090  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle276.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 506 words.
Number of 2-grams hit = 505  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle277.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 835 words.
Number of 2-grams hit = 834  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle278.out
Perplexity = 4.22, Entropy = 2.08 bits
Computation based on 856 words.
Number of 2-grams hit = 855  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle279.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 446 words.
Number of 2-grams hit = 445  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle280.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 1844 words.
Number of 2-grams hit = 1843  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle281.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 610 words.
Number of 2-grams hit = 609  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle282.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 779 words.
Number of 2-grams hit = 778  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle283.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1294 words.
Number of 2-grams hit = 1293  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle284.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 1757 words.
Number of 2-grams hit = 1756  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle285.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1159 words.
Number of 2-grams hit = 1158  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle286.out
Perplexity = 4.50, Entropy = 2.17 bits
Computation based on 485 words.
Number of 2-grams hit = 484  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle287.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 1778 words.
Number of 2-grams hit = 1777  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle288.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 635 words.
Number of 2-grams hit = 634  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle289.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 774 words.
Number of 2-grams hit = 773  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle290.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1396 words.
Number of 2-grams hit = 1395  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle291.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 552 words.
Number of 2-grams hit = 551  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle292.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 575 words.
Number of 2-grams hit = 574  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle293.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 593 words.
Number of 2-grams hit = 592  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle294.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 2240 words.
Number of 2-grams hit = 2239  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle295.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 644 words.
Number of 2-grams hit = 643  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle296.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 1670 words.
Number of 2-grams hit = 1669  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle297.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 550 words.
Number of 2-grams hit = 549  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle298.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 3737 words.
Number of 2-grams hit = 3736  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle299.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 3001 words.
Number of 2-grams hit = 3000  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle300.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 2733 words.
Number of 2-grams hit = 2732  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle301.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 2305 words.
Number of 2-grams hit = 2303  (99.91%)
Number of 1-grams hit = 2  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle302.out
Perplexity = 3.82, Entropy = 1.94 bits
Computation based on 698 words.
Number of 2-grams hit = 697  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle303.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 941 words.
Number of 2-grams hit = 940  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle304.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 446 words.
Number of 2-grams hit = 445  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle305.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 331 words.
Number of 2-grams hit = 330  (99.70%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle306.out
Perplexity = 2.96, Entropy = 1.57 bits
Computation based on 635 words.
Number of 2-grams hit = 634  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle307.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 557 words.
Number of 2-grams hit = 556  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle308.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 417 words.
Number of 2-grams hit = 416  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle309.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 265 words.
Number of 2-grams hit = 264  (99.62%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle310.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 522 words.
Number of 2-grams hit = 521  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle311.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 497 words.
Number of 2-grams hit = 496  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle312.out
Perplexity = 3.02, Entropy = 1.59 bits
Computation based on 651 words.
Number of 2-grams hit = 650  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle313.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 512 words.
Number of 2-grams hit = 511  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle314.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 3615 words.
Number of 2-grams hit = 3614  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle315.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 1186 words.
Number of 2-grams hit = 1185  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle316.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 363 words.
Number of 2-grams hit = 362  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle317.out
Perplexity = 3.38, Entropy = 1.75 bits
Computation based on 675 words.
Number of 2-grams hit = 674  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle318.out
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 3867 words.
Number of 2-grams hit = 3866  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle319.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 641 words.
Number of 2-grams hit = 640  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle320.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 405 words.
Number of 2-grams hit = 404  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle321.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1172 words.
Number of 2-grams hit = 1171  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle322.out
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 1851 words.
Number of 2-grams hit = 1850  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle323.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 877 words.
Number of 2-grams hit = 876  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle324.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1808 words.
Number of 2-grams hit = 1807  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle325.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 1207 words.
Number of 2-grams hit = 1206  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle326.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 655 words.
Number of 2-grams hit = 654  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle327.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 1223 words.
Number of 2-grams hit = 1222  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle328.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 277 words.
Number of 2-grams hit = 276  (99.64%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle329.out
Perplexity = 4.32, Entropy = 2.11 bits
Computation based on 445 words.
Number of 2-grams hit = 444  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle330.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 1141 words.
Number of 2-grams hit = 1139  (99.82%)
Number of 1-grams hit = 2  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle331.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 2113 words.
Number of 2-grams hit = 2112  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle332.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 626 words.
Number of 2-grams hit = 625  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle333.out
Perplexity = 3.96, Entropy = 1.98 bits
Computation based on 1432 words.
Number of 2-grams hit = 1431  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle334.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1057 words.
Number of 2-grams hit = 1056  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle335.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1850 words.
Number of 2-grams hit = 1849  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle336.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 1239 words.
Number of 2-grams hit = 1238  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle337.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 438 words.
Number of 2-grams hit = 437  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle338.out
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 3804 words.
Number of 2-grams hit = 3803  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle339.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 408 words.
Number of 2-grams hit = 407  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle340.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1243 words.
Number of 2-grams hit = 1242  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle341.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1498 words.
Number of 2-grams hit = 1497  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle342.out
Perplexity = 4.10, Entropy = 2.04 bits
Computation based on 1604 words.
Number of 2-grams hit = 1603  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle343.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 522 words.
Number of 2-grams hit = 521  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle344.out
Perplexity = 3.17, Entropy = 1.66 bits
Computation based on 620 words.
Number of 2-grams hit = 619  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle345.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 3421 words.
Number of 2-grams hit = 3420  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle346.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1725 words.
Number of 2-grams hit = 1724  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle347.out
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 690 words.
Number of 2-grams hit = 689  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle348.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 2288 words.
Number of 2-grams hit = 2287  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle349.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 2145 words.
Number of 2-grams hit = 2144  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle350.out
Perplexity = 4.18, Entropy = 2.06 bits
Computation based on 400 words.
Number of 2-grams hit = 399  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle351.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 464 words.
Number of 2-grams hit = 463  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle352.out
Perplexity = 4.23, Entropy = 2.08 bits
Computation based on 488 words.
Number of 2-grams hit = 487  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle353.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 471 words.
Number of 2-grams hit = 470  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle354.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 551 words.
Number of 2-grams hit = 550  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle355.out
Perplexity = 4.24, Entropy = 2.08 bits
Computation based on 448 words.
Number of 2-grams hit = 447  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle356.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 743 words.
Number of 2-grams hit = 742  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle357.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 834 words.
Number of 2-grams hit = 833  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle358.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 1334 words.
Number of 2-grams hit = 1333  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle359.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1380 words.
Number of 2-grams hit = 1379  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle360.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 760 words.
Number of 2-grams hit = 759  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle361.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1178 words.
Number of 2-grams hit = 1177  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle362.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 1543 words.
Number of 2-grams hit = 1542  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle363.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 1251 words.
Number of 2-grams hit = 1250  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle364.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 345 words.
Number of 2-grams hit = 344  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle365.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 430 words.
Number of 2-grams hit = 429  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle366.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 390 words.
Number of 2-grams hit = 389  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle367.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1013 words.
Number of 2-grams hit = 1012  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle368.out
Perplexity = 4.74, Entropy = 2.25 bits
Computation based on 345 words.
Number of 2-grams hit = 344  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle369.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 352 words.
Number of 2-grams hit = 351  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle370.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1781 words.
Number of 2-grams hit = 1780  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle371.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 280 words.
Number of 2-grams hit = 279  (99.64%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle372.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 456 words.
Number of 2-grams hit = 455  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle373.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 546 words.
Number of 2-grams hit = 545  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle374.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1668 words.
Number of 2-grams hit = 1667  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle375.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1770 words.
Number of 2-grams hit = 1769  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle376.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 621 words.
Number of 2-grams hit = 620  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle377.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 809 words.
Number of 2-grams hit = 808  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle378.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 671 words.
Number of 2-grams hit = 670  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle379.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 2784 words.
Number of 2-grams hit = 2782  (99.93%)
Number of 1-grams hit = 2  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle380.out
Perplexity = 3.96, Entropy = 1.98 bits
Computation based on 377 words.
Number of 2-grams hit = 376  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle381.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 700 words.
Number of 2-grams hit = 699  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle382.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 1415 words.
Number of 2-grams hit = 1414  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle383.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1120 words.
Number of 2-grams hit = 1119  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle384.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 1098 words.
Number of 2-grams hit = 1097  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle385.out
Perplexity = 3.93, Entropy = 1.98 bits
Computation based on 2797 words.
Number of 2-grams hit = 2796  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle386.out
Perplexity = 3.82, Entropy = 1.94 bits
Computation based on 442 words.
Number of 2-grams hit = 441  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle387.out
Perplexity = 4.32, Entropy = 2.11 bits
Computation based on 1354 words.
Number of 2-grams hit = 1353  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle388.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 4692 words.
Number of 2-grams hit = 4691  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle389.out
Perplexity = 4.12, Entropy = 2.04 bits
Computation based on 1197 words.
Number of 2-grams hit = 1196  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle390.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1281 words.
Number of 2-grams hit = 1280  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle391.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 3885 words.
Number of 2-grams hit = 3883  (99.95%)
Number of 1-grams hit = 2  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle392.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 256 words.
Number of 2-grams hit = 255  (99.61%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle393.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 508 words.
Number of 2-grams hit = 507  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle394.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 417 words.
Number of 2-grams hit = 416  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle395.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 332 words.
Number of 2-grams hit = 331  (99.70%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle396.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 3378 words.
Number of 2-grams hit = 3377  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle397.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 4002 words.
Number of 2-grams hit = 4001  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle398.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 878 words.
Number of 2-grams hit = 877  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle399.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 3969 words.
Number of 2-grams hit = 3967  (99.95%)
Number of 1-grams hit = 2  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle400.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 634 words.
Number of 2-grams hit = 633  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle401.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 721 words.
Number of 2-grams hit = 720  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle402.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 3978 words.
Number of 2-grams hit = 3977  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle403.out
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 668 words.
Number of 2-grams hit = 667  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle404.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 228 words.
Number of 2-grams hit = 227  (99.56%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle405.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 172 words.
Number of 2-grams hit = 171  (99.42%)
Number of 1-grams hit = 1  (0.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle406.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 1046 words.
Number of 2-grams hit = 1045  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle407.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 755 words.
Number of 2-grams hit = 754  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle408.out
Perplexity = 4.47, Entropy = 2.16 bits
Computation based on 469 words.
Number of 2-grams hit = 468  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle409.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 595 words.
Number of 2-grams hit = 594  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle410.out
Perplexity = 3.90, Entropy = 1.97 bits
Computation based on 261 words.
Number of 2-grams hit = 260  (99.62%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle411.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 465 words.
Number of 2-grams hit = 464  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle412.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 4945 words.
Number of 2-grams hit = 4944  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle413.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 4175 words.
Number of 2-grams hit = 4174  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle414.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 430 words.
Number of 2-grams hit = 429  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle415.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 509 words.
Number of 2-grams hit = 508  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle416.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 635 words.
Number of 2-grams hit = 634  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle417.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 947 words.
Number of 2-grams hit = 946  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle418.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 863 words.
Number of 2-grams hit = 862  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle419.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 581 words.
Number of 2-grams hit = 580  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle420.out
Perplexity = 4.10, Entropy = 2.04 bits
Computation based on 667 words.
Number of 2-grams hit = 666  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle421.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 893 words.
Number of 2-grams hit = 892  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle422.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 602 words.
Number of 2-grams hit = 601  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle423.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 652 words.
Number of 2-grams hit = 651  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle424.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 582 words.
Number of 2-grams hit = 581  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle425.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 385 words.
Number of 2-grams hit = 384  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle426.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 537 words.
Number of 2-grams hit = 536  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle427.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 844 words.
Number of 2-grams hit = 843  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle428.out
Perplexity = 3.10, Entropy = 1.63 bits
Computation based on 680 words.
Number of 2-grams hit = 679  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle429.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 812 words.
Number of 2-grams hit = 811  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle430.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 699 words.
Number of 2-grams hit = 698  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle431.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 612 words.
Number of 2-grams hit = 611  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle432.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 388 words.
Number of 2-grams hit = 387  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle433.out
Perplexity = 3.96, Entropy = 1.98 bits
Computation based on 1055 words.
Number of 2-grams hit = 1054  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle434.out
Perplexity = 4.19, Entropy = 2.07 bits
Computation based on 414 words.
Number of 2-grams hit = 413  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle435.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 797 words.
Number of 2-grams hit = 796  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle436.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 585 words.
Number of 2-grams hit = 584  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle437.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 1077 words.
Number of 2-grams hit = 1076  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle438.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 318 words.
Number of 2-grams hit = 317  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle439.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 569 words.
Number of 2-grams hit = 568  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle440.out
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 753 words.
Number of 2-grams hit = 752  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle441.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 439 words.
Number of 2-grams hit = 438  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle442.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 11076 words.
Number of 2-grams hit = 11075  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle443.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 9530 words.
Number of 2-grams hit = 9529  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle444.out
Perplexity = 3.69, Entropy = 1.89 bits
Computation based on 703 words.
Number of 2-grams hit = 702  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle445.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 556 words.
Number of 2-grams hit = 555  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle446.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 969 words.
Number of 2-grams hit = 968  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle447.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 787 words.
Number of 2-grams hit = 786  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle448.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 346 words.
Number of 2-grams hit = 345  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle449.out
Perplexity = 2.93, Entropy = 1.55 bits
Computation based on 520 words.
Number of 2-grams hit = 519  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle450.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 386 words.
Number of 2-grams hit = 385  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle451.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 615 words.
Number of 2-grams hit = 614  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle452.out
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 262 words.
Number of 2-grams hit = 261  (99.62%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle453.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 679 words.
Number of 2-grams hit = 678  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle454.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 598 words.
Number of 2-grams hit = 597  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle455.out
Perplexity = 3.31, Entropy = 1.72 bits
Computation based on 405 words.
Number of 2-grams hit = 404  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle456.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 719 words.
Number of 2-grams hit = 718  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle457.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 366 words.
Number of 2-grams hit = 365  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle458.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 299 words.
Number of 2-grams hit = 298  (99.67%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle459.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 752 words.
Number of 2-grams hit = 751  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle460.out
Perplexity = 4.16, Entropy = 2.06 bits
Computation based on 354 words.
Number of 2-grams hit = 353  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle461.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 440 words.
Number of 2-grams hit = 439  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle462.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1537 words.
Number of 2-grams hit = 1536  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle463.out
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 610 words.
Number of 2-grams hit = 609  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle464.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 466 words.
Number of 2-grams hit = 465  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle465.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 1935 words.
Number of 2-grams hit = 1934  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle466.out
Perplexity = 4.32, Entropy = 2.11 bits
Computation based on 225 words.
Number of 2-grams hit = 224  (99.56%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle467.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 1127 words.
Number of 2-grams hit = 1126  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle468.out
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 843 words.
Number of 2-grams hit = 842  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle469.out
Perplexity = 2.98, Entropy = 1.58 bits
Computation based on 381 words.
Number of 2-grams hit = 380  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle470.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 259 words.
Number of 2-grams hit = 258  (99.61%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle471.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 2170 words.
Number of 2-grams hit = 2169  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle472.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1013 words.
Number of 2-grams hit = 1012  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle473.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 247 words.
Number of 2-grams hit = 246  (99.60%)
Number of 1-grams hit = 1  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle474.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 838 words.
Number of 2-grams hit = 837  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle475.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 588 words.
Number of 2-grams hit = 587  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle476.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 626 words.
Number of 2-grams hit = 625  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle477.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 474 words.
Number of 2-grams hit = 473  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle478.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 547 words.
Number of 2-grams hit = 546  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle479.out
Perplexity = 4.54, Entropy = 2.18 bits
Computation based on 458 words.
Number of 2-grams hit = 456  (99.56%)
Number of 1-grams hit = 2  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle480.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 749 words.
Number of 2-grams hit = 748  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle481.out
Perplexity = 4.12, Entropy = 2.04 bits
Computation based on 379 words.
Number of 2-grams hit = 378  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle482.out
Perplexity = 3.26, Entropy = 1.71 bits
Computation based on 542 words.
Number of 2-grams hit = 541  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle483.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1863 words.
Number of 2-grams hit = 1862  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle484.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1569 words.
Number of 2-grams hit = 1568  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle485.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 402 words.
Number of 2-grams hit = 401  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle486.out
Perplexity = 4.52, Entropy = 2.18 bits
Computation based on 369 words.
Number of 2-grams hit = 368  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle487.out
Perplexity = 4.16, Entropy = 2.06 bits
Computation based on 462 words.
Number of 2-grams hit = 461  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle488.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 770 words.
Number of 2-grams hit = 769  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle489.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 577 words.
Number of 2-grams hit = 576  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle490.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 533 words.
Number of 2-grams hit = 532  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle491.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1520 words.
Number of 2-grams hit = 1519  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle492.out
Perplexity = 4.33, Entropy = 2.11 bits
Computation based on 473 words.
Number of 2-grams hit = 472  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle493.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 827 words.
Number of 2-grams hit = 826  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle494.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 540 words.
Number of 2-grams hit = 539  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle495.out
Perplexity = 4.43, Entropy = 2.15 bits
Computation based on 454 words.
Number of 2-grams hit = 453  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle496.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 608 words.
Number of 2-grams hit = 607  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle497.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 711 words.
Number of 2-grams hit = 710  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle498.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1824 words.
Number of 2-grams hit = 1823  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle499.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 542 words.
Number of 2-grams hit = 541  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle500.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 486 words.
Number of 2-grams hit = 485  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle501.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 448 words.
Number of 2-grams hit = 447  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle502.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 535 words.
Number of 2-grams hit = 534  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle503.out
Perplexity = 4.33, Entropy = 2.11 bits
Computation based on 1109 words.
Number of 2-grams hit = 1108  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle504.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 658 words.
Number of 2-grams hit = 657  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle505.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 768 words.
Number of 2-grams hit = 767  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle506.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 594 words.
Number of 2-grams hit = 593  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle507.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1702 words.
Number of 2-grams hit = 1701  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle508.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 700 words.
Number of 2-grams hit = 699  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle509.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 383 words.
Number of 2-grams hit = 382  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle510.out
Perplexity = 3.96, Entropy = 1.98 bits
Computation based on 1032 words.
Number of 2-grams hit = 1031  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle511.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 677 words.
Number of 2-grams hit = 676  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle512.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 2057 words.
Number of 2-grams hit = 2056  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle513.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 448 words.
Number of 2-grams hit = 447  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle514.out
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 422 words.
Number of 2-grams hit = 421  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle515.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 1070 words.
Number of 2-grams hit = 1069  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle516.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 440 words.
Number of 2-grams hit = 439  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle517.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1145 words.
Number of 2-grams hit = 1144  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle518.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 639 words.
Number of 2-grams hit = 638  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle519.out
Perplexity = 4.19, Entropy = 2.07 bits
Computation based on 592 words.
Number of 2-grams hit = 591  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle520.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 663 words.
Number of 2-grams hit = 662  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle521.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 548 words.
Number of 2-grams hit = 547  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle522.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 1741 words.
Number of 2-grams hit = 1740  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle523.out
Perplexity = 4.40, Entropy = 2.14 bits
Computation based on 354 words.
Number of 2-grams hit = 353  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle524.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 1488 words.
Number of 2-grams hit = 1487  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle525.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 2121 words.
Number of 2-grams hit = 2120  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle526.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 2067 words.
Number of 2-grams hit = 2066  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle527.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 793 words.
Number of 2-grams hit = 792  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle528.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 788 words.
Number of 2-grams hit = 787  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle529.out
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 957 words.
Number of 2-grams hit = 956  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle530.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 451 words.
Number of 2-grams hit = 450  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle531.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 1046 words.
Number of 2-grams hit = 1045  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle532.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1019 words.
Number of 2-grams hit = 1018  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle533.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1210 words.
Number of 2-grams hit = 1209  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle534.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 549 words.
Number of 2-grams hit = 548  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle535.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 375 words.
Number of 2-grams hit = 374  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle536.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 1602 words.
Number of 2-grams hit = 1601  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle537.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 1098 words.
Number of 2-grams hit = 1097  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle538.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 3722 words.
Number of 2-grams hit = 3721  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle539.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 581 words.
Number of 2-grams hit = 580  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle540.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 2359 words.
Number of 2-grams hit = 2358  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle541.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 544 words.
Number of 2-grams hit = 543  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle542.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1758 words.
Number of 2-grams hit = 1757  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle543.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 592 words.
Number of 2-grams hit = 591  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle544.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 1399 words.
Number of 2-grams hit = 1398  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle545.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 802 words.
Number of 2-grams hit = 801  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle546.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 753 words.
Number of 2-grams hit = 752  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle547.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 953 words.
Number of 2-grams hit = 952  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle548.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 671 words.
Number of 2-grams hit = 669  (99.70%)
Number of 1-grams hit = 2  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle549.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 387 words.
Number of 2-grams hit = 386  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle550.out
Perplexity = 4.44, Entropy = 2.15 bits
Computation based on 204 words.
Number of 2-grams hit = 203  (99.51%)
Number of 1-grams hit = 1  (0.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle551.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 996 words.
Number of 2-grams hit = 995  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle552.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1143 words.
Number of 2-grams hit = 1142  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle553.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 1347 words.
Number of 2-grams hit = 1346  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle554.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 4358 words.
Number of 2-grams hit = 4357  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle555.out
Perplexity = 4.26, Entropy = 2.09 bits
Computation based on 544 words.
Number of 2-grams hit = 543  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle556.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 605 words.
Number of 2-grams hit = 604  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle557.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 418 words.
Number of 2-grams hit = 417  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle558.out
Perplexity = 4.15, Entropy = 2.05 bits
Computation based on 1850 words.
Number of 2-grams hit = 1849  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle559.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 694 words.
Number of 2-grams hit = 693  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle560.out
Perplexity = 4.22, Entropy = 2.08 bits
Computation based on 595 words.
Number of 2-grams hit = 594  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle561.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 362 words.
Number of 2-grams hit = 361  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle562.out
Perplexity = 4.23, Entropy = 2.08 bits
Computation based on 331 words.
Number of 2-grams hit = 330  (99.70%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle563.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 524 words.
Number of 2-grams hit = 523  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle564.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1451 words.
Number of 2-grams hit = 1450  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle565.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1392 words.
Number of 2-grams hit = 1391  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle566.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 4543 words.
Number of 2-grams hit = 4541  (99.96%)
Number of 1-grams hit = 2  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle567.out
Perplexity = 4.45, Entropy = 2.15 bits
Computation based on 596 words.
Number of 2-grams hit = 595  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle568.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 355 words.
Number of 2-grams hit = 354  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle569.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 4706 words.
Number of 2-grams hit = 4705  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle570.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 897 words.
Number of 2-grams hit = 896  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle571.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 576 words.
Number of 2-grams hit = 575  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle572.out
Perplexity = 4.07, Entropy = 2.02 bits
Computation based on 619 words.
Number of 2-grams hit = 618  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle573.out
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 279 words.
Number of 2-grams hit = 278  (99.64%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle574.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1179 words.
Number of 2-grams hit = 1178  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle575.out
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 643 words.
Number of 2-grams hit = 642  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle576.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 489 words.
Number of 2-grams hit = 488  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle577.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 405 words.
Number of 2-grams hit = 404  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle578.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 447 words.
Number of 2-grams hit = 446  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle579.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 479 words.
Number of 2-grams hit = 478  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle580.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 585 words.
Number of 2-grams hit = 584  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle581.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 584 words.
Number of 2-grams hit = 583  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle582.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 547 words.
Number of 2-grams hit = 546  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle583.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 722 words.
Number of 2-grams hit = 721  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle584.out
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 477 words.
Number of 2-grams hit = 476  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle585.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 1036 words.
Number of 2-grams hit = 1035  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle586.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 782 words.
Number of 2-grams hit = 781  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle587.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 576 words.
Number of 2-grams hit = 575  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle588.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 1045 words.
Number of 2-grams hit = 1044  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle589.out
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 538 words.
Number of 2-grams hit = 537  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle590.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1468 words.
Number of 2-grams hit = 1467  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle591.out
Perplexity = 4.11, Entropy = 2.04 bits
Computation based on 749 words.
Number of 2-grams hit = 748  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle592.out
Perplexity = 4.35, Entropy = 2.12 bits
Computation based on 367 words.
Number of 2-grams hit = 366  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle593.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 409 words.
Number of 2-grams hit = 408  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle594.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 653 words.
Number of 2-grams hit = 652  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle595.out
Perplexity = 4.24, Entropy = 2.09 bits
Computation based on 398 words.
Number of 2-grams hit = 397  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle596.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 1041 words.
Number of 2-grams hit = 1040  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle597.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1109 words.
Number of 2-grams hit = 1108  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle598.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 327 words.
Number of 2-grams hit = 326  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle599.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 879 words.
Number of 2-grams hit = 878  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle600.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 5209 words.
Number of 2-grams hit = 5208  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle601.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1181 words.
Number of 2-grams hit = 1180  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle602.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 652 words.
Number of 2-grams hit = 651  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle603.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 94 words.
Number of 2-grams hit = 93  (98.94%)
Number of 1-grams hit = 1  (1.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle604.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 612 words.
Number of 2-grams hit = 611  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle605.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 3406 words.
Number of 2-grams hit = 3405  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle606.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 978 words.
Number of 2-grams hit = 977  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle607.out
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 1157 words.
Number of 2-grams hit = 1156  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle608.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 674 words.
Number of 2-grams hit = 673  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle609.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 4849 words.
Number of 2-grams hit = 4848  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle610.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 6480 words.
Number of 2-grams hit = 6479  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle611.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 856 words.
Number of 2-grams hit = 855  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle612.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 1757 words.
Number of 2-grams hit = 1756  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle613.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 4584 words.
Number of 2-grams hit = 4583  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle614.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1629 words.
Number of 2-grams hit = 1628  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle615.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 494 words.
Number of 2-grams hit = 493  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle616.out
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 827 words.
Number of 2-grams hit = 826  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle617.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1418 words.
Number of 2-grams hit = 1417  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle618.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 4055 words.
Number of 2-grams hit = 4054  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle619.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 853 words.
Number of 2-grams hit = 852  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle620.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 5770 words.
Number of 2-grams hit = 5769  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle621.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 347 words.
Number of 2-grams hit = 346  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle622.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 503 words.
Number of 2-grams hit = 501  (99.60%)
Number of 1-grams hit = 2  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle623.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 1430 words.
Number of 2-grams hit = 1429  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle624.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 5610 words.
Number of 2-grams hit = 5608  (99.96%)
Number of 1-grams hit = 2  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle625.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1191 words.
Number of 2-grams hit = 1190  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle626.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 483 words.
Number of 2-grams hit = 482  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle627.out
Perplexity = 3.15, Entropy = 1.66 bits
Computation based on 159 words.
Number of 2-grams hit = 158  (99.37%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle628.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1142 words.
Number of 2-grams hit = 1141  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle629.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 1114 words.
Number of 2-grams hit = 1113  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle630.out
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 349 words.
Number of 2-grams hit = 348  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle631.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 931 words.
Number of 2-grams hit = 930  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle632.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 439 words.
Number of 2-grams hit = 438  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle633.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 579 words.
Number of 2-grams hit = 578  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle634.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 603 words.
Number of 2-grams hit = 602  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle635.out
Perplexity = 4.10, Entropy = 2.03 bits
Computation based on 169 words.
Number of 2-grams hit = 168  (99.41%)
Number of 1-grams hit = 1  (0.59%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle636.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 1435 words.
Number of 2-grams hit = 1434  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle637.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1118 words.
Number of 2-grams hit = 1117  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle638.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 586 words.
Number of 2-grams hit = 585  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle639.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 561 words.
Number of 2-grams hit = 560  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle640.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 227 words.
Number of 2-grams hit = 226  (99.56%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle641.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 888 words.
Number of 2-grams hit = 887  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle642.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 477 words.
Number of 2-grams hit = 476  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle643.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 522 words.
Number of 2-grams hit = 521  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle644.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1207 words.
Number of 2-grams hit = 1206  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle645.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 475 words.
Number of 2-grams hit = 474  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle646.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 615 words.
Number of 2-grams hit = 614  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle647.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 653 words.
Number of 2-grams hit = 652  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle648.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 511 words.
Number of 2-grams hit = 510  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle649.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 577 words.
Number of 2-grams hit = 576  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle650.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 579 words.
Number of 2-grams hit = 578  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle651.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 503 words.
Number of 2-grams hit = 502  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle652.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 956 words.
Number of 2-grams hit = 955  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle653.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 848 words.
Number of 2-grams hit = 847  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle654.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 827 words.
Number of 2-grams hit = 826  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle655.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 547 words.
Number of 2-grams hit = 546  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle656.out
Perplexity = 4.38, Entropy = 2.13 bits
Computation based on 501 words.
Number of 2-grams hit = 500  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle657.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 412 words.
Number of 2-grams hit = 411  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle658.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 1097 words.
Number of 2-grams hit = 1096  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle659.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 1082 words.
Number of 2-grams hit = 1081  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle660.out
Perplexity = 2.49, Entropy = 1.32 bits
Computation based on 151 words.
Number of 2-grams hit = 150  (99.34%)
Number of 1-grams hit = 1  (0.66%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle661.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1330 words.
Number of 2-grams hit = 1329  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle662.out
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 131 words.
Number of 2-grams hit = 130  (99.24%)
Number of 1-grams hit = 1  (0.76%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle663.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1445 words.
Number of 2-grams hit = 1444  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle664.out
Perplexity = 4.46, Entropy = 2.16 bits
Computation based on 770 words.
Number of 2-grams hit = 769  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle665.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 830 words.
Number of 2-grams hit = 829  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle666.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 413 words.
Number of 2-grams hit = 412  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle667.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 1455 words.
Number of 2-grams hit = 1454  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle668.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 504 words.
Number of 2-grams hit = 503  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle669.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 524 words.
Number of 2-grams hit = 523  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle670.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 481 words.
Number of 2-grams hit = 480  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle671.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 693 words.
Number of 2-grams hit = 692  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle672.out
Perplexity = 4.13, Entropy = 2.05 bits
Computation based on 316 words.
Number of 2-grams hit = 315  (99.68%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle673.out
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 336 words.
Number of 2-grams hit = 335  (99.70%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle674.out
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 159 words.
Number of 2-grams hit = 158  (99.37%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle675.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 4196 words.
Number of 2-grams hit = 4195  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle676.out
Perplexity = 2.93, Entropy = 1.55 bits
Computation based on 531 words.
Number of 2-grams hit = 530  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle677.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 684 words.
Number of 2-grams hit = 683  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle678.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 373 words.
Number of 2-grams hit = 372  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle679.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1075 words.
Number of 2-grams hit = 1074  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle680.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 356 words.
Number of 2-grams hit = 355  (99.72%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle681.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 522 words.
Number of 2-grams hit = 521  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle682.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 4014 words.
Number of 2-grams hit = 4013  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle683.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 279 words.
Number of 2-grams hit = 278  (99.64%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle684.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 4719 words.
Number of 2-grams hit = 4718  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle685.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 485 words.
Number of 2-grams hit = 484  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle686.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 2419 words.
Number of 2-grams hit = 2418  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle687.out
Perplexity = 4.16, Entropy = 2.06 bits
Computation based on 556 words.
Number of 2-grams hit = 555  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle688.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 332 words.
Number of 2-grams hit = 331  (99.70%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle689.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 554 words.
Number of 2-grams hit = 553  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle690.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 824 words.
Number of 2-grams hit = 823  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle691.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 138 words.
Number of 2-grams hit = 137  (99.28%)
Number of 1-grams hit = 1  (0.72%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle692.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 460 words.
Number of 2-grams hit = 459  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle693.out
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 1025 words.
Number of 2-grams hit = 1024  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle694.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 669 words.
Number of 2-grams hit = 668  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle695.out
Perplexity = 3.24, Entropy = 1.69 bits
Computation based on 924 words.
Number of 2-grams hit = 923  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle696.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 541 words.
Number of 2-grams hit = 540  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle697.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1021 words.
Number of 2-grams hit = 1020  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle698.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 630 words.
Number of 2-grams hit = 629  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle699.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 377 words.
Number of 2-grams hit = 376  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle700.out
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 3659 words.
Number of 2-grams hit = 3658  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle701.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 878 words.
Number of 2-grams hit = 877  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle702.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 2030 words.
Number of 2-grams hit = 2029  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle703.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 470 words.
Number of 2-grams hit = 469  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle704.out
Perplexity = 3.88, Entropy = 1.95 bits
Computation based on 341 words.
Number of 2-grams hit = 340  (99.71%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle705.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 1100 words.
Number of 2-grams hit = 1099  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle706.out
Perplexity = 3.09, Entropy = 1.63 bits
Computation based on 509 words.
Number of 2-grams hit = 508  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle707.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 941 words.
Number of 2-grams hit = 940  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle708.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 562 words.
Number of 2-grams hit = 561  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle709.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 602 words.
Number of 2-grams hit = 601  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle710.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 823 words.
Number of 2-grams hit = 822  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle711.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 1017 words.
Number of 2-grams hit = 1016  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle712.out
Perplexity = 4.22, Entropy = 2.08 bits
Computation based on 406 words.
Number of 2-grams hit = 405  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle713.out
Perplexity = 4.61, Entropy = 2.20 bits
Computation based on 971 words.
Number of 2-grams hit = 970  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle714.out
Perplexity = 4.19, Entropy = 2.07 bits
Computation based on 404 words.
Number of 2-grams hit = 403  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle715.out
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 525 words.
Number of 2-grams hit = 524  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle716.out
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 521 words.
Number of 2-grams hit = 520  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle717.out
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 403 words.
Number of 2-grams hit = 402  (99.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle718.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 7638 words.
Number of 2-grams hit = 7636  (99.97%)
Number of 1-grams hit = 2  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle719.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 1275 words.
Number of 2-grams hit = 1274  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle720.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 872 words.
Number of 2-grams hit = 871  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle721.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 452 words.
Number of 2-grams hit = 451  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle722.out
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 1116 words.
Number of 2-grams hit = 1115  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle723.out
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 472 words.
Number of 2-grams hit = 471  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle724.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 500 words.
Number of 2-grams hit = 499  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle725.out
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 792 words.
Number of 2-grams hit = 791  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle726.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 573 words.
Number of 2-grams hit = 572  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle727.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 519 words.
Number of 2-grams hit = 518  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle728.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 453 words.
Number of 2-grams hit = 452  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle729.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 468 words.
Number of 2-grams hit = 467  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle730.out
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 5476 words.
Number of 2-grams hit = 5475  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle731.out
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 1562 words.
Number of 2-grams hit = 1561  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle732.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 769 words.
Number of 2-grams hit = 768  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle733.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 609 words.
Number of 2-grams hit = 608  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle734.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 450 words.
Number of 2-grams hit = 449  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle735.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 1274 words.
Number of 2-grams hit = 1271  (99.76%)
Number of 1-grams hit = 3  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle736.out
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 733 words.
Number of 2-grams hit = 732  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle737.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 697 words.
Number of 2-grams hit = 696  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle738.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1069 words.
Number of 2-grams hit = 1068  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle739.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 484 words.
Number of 2-grams hit = 483  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle740.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 1194 words.
Number of 2-grams hit = 1193  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle741.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 824 words.
Number of 2-grams hit = 823  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle742.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 690 words.
Number of 2-grams hit = 689  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle743.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 2142 words.
Number of 2-grams hit = 2141  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle744.out
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 580 words.
Number of 2-grams hit = 579  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle745.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 740 words.
Number of 2-grams hit = 739  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle746.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 1498 words.
Number of 2-grams hit = 1497  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle747.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 571 words.
Number of 2-grams hit = 570  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle748.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 544 words.
Number of 2-grams hit = 543  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle749.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 590 words.
Number of 2-grams hit = 589  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle750.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 513 words.
Number of 2-grams hit = 512  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle751.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 2179 words.
Number of 2-grams hit = 2178  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle752.out
Perplexity = 4.47, Entropy = 2.16 bits
Computation based on 468 words.
Number of 2-grams hit = 467  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle753.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 572 words.
Number of 2-grams hit = 571  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle754.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 2031 words.
Number of 2-grams hit = 2030  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle755.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 487 words.
Number of 2-grams hit = 486  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle756.out
Perplexity = 4.36, Entropy = 2.12 bits
Computation based on 74 words.
Number of 2-grams hit = 73  (98.65%)
Number of 1-grams hit = 1  (1.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle757.out
Perplexity = 4.11, Entropy = 2.04 bits
Computation based on 557 words.
Number of 2-grams hit = 556  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle758.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 2743 words.
Number of 2-grams hit = 2742  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle759.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 3515 words.
Number of 2-grams hit = 3514  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle760.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 618 words.
Number of 2-grams hit = 617  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle761.out
Perplexity = 4.22, Entropy = 2.08 bits
Computation based on 684 words.
Number of 2-grams hit = 683  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle762.out
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 861 words.
Number of 2-grams hit = 860  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle763.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 782 words.
Number of 2-grams hit = 781  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle764.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 582 words.
Number of 2-grams hit = 581  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle765.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 888 words.
Number of 2-grams hit = 887  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle766.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 293 words.
Number of 2-grams hit = 292  (99.66%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle767.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 4625 words.
Number of 2-grams hit = 4624  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle768.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 503 words.
Number of 2-grams hit = 502  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle769.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 730 words.
Number of 2-grams hit = 729  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle770.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 658 words.
Number of 2-grams hit = 657  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle771.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 501 words.
Number of 2-grams hit = 499  (99.60%)
Number of 1-grams hit = 2  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle772.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 590 words.
Number of 2-grams hit = 589  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle773.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 3261 words.
Number of 2-grams hit = 3260  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle774.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 2158 words.
Number of 2-grams hit = 2157  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle775.out
Perplexity = 3.96, Entropy = 1.98 bits
Computation based on 558 words.
Number of 2-grams hit = 557  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle776.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 48 words.
Number of 2-grams hit = 47  (97.92%)
Number of 1-grams hit = 1  (2.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle777.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 10354 words.
Number of 2-grams hit = 10353  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle778.out
Perplexity = 4.06, Entropy = 2.02 bits
Computation based on 545 words.
Number of 2-grams hit = 544  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle779.out
Perplexity = 4.28, Entropy = 2.10 bits
Computation based on 1014 words.
Number of 2-grams hit = 1013  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle780.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 445 words.
Number of 2-grams hit = 444  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle781.out
Perplexity = 4.06, Entropy = 2.02 bits
Computation based on 786 words.
Number of 2-grams hit = 785  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle782.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1485 words.
Number of 2-grams hit = 1484  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle783.out
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 549 words.
Number of 2-grams hit = 548  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle784.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1881 words.
Number of 2-grams hit = 1880  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle785.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 2868 words.
Number of 2-grams hit = 2867  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle786.out
Perplexity = 4.07, Entropy = 2.03 bits
Computation based on 370 words.
Number of 2-grams hit = 369  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle787.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 512 words.
Number of 2-grams hit = 511  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle788.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1616 words.
Number of 2-grams hit = 1615  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle789.out
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 12475 words.
Number of 2-grams hit = 12474  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle790.out
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 1019 words.
Number of 2-grams hit = 1018  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle791.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 749 words.
Number of 2-grams hit = 748  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle792.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 3144 words.
Number of 2-grams hit = 3143  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle793.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 197 words.
Number of 2-grams hit = 196  (99.49%)
Number of 1-grams hit = 1  (0.51%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle794.out
Perplexity = 4.58, Entropy = 2.20 bits
Computation based on 548 words.
Number of 2-grams hit = 547  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle795.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 460 words.
Number of 2-grams hit = 459  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle796.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1620 words.
Number of 2-grams hit = 1619  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle797.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 413 words.
Number of 2-grams hit = 412  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle798.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 503 words.
Number of 2-grams hit = 502  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle799.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 816 words.
Number of 2-grams hit = 815  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle800.out
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 473 words.
Number of 2-grams hit = 472  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle801.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 3418 words.
Number of 2-grams hit = 3417  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle802.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 2237 words.
Number of 2-grams hit = 2236  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle803.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 4594 words.
Number of 2-grams hit = 4593  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle804.out
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 1564 words.
Number of 2-grams hit = 1563  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle805.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 993 words.
Number of 2-grams hit = 991  (99.80%)
Number of 1-grams hit = 2  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle806.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 3603 words.
Number of 2-grams hit = 3602  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle807.out
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 756 words.
Number of 2-grams hit = 755  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle808.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 1830 words.
Number of 2-grams hit = 1829  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle809.out
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 2165 words.
Number of 2-grams hit = 2164  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle810.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 937 words.
Number of 2-grams hit = 936  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle811.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 4883 words.
Number of 2-grams hit = 4882  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle812.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 3604 words.
Number of 2-grams hit = 3603  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle813.out
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 1497 words.
Number of 2-grams hit = 1496  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle814.out
Perplexity = 3.14, Entropy = 1.65 bits
Computation based on 908 words.
Number of 2-grams hit = 907  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle815.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 1390 words.
Number of 2-grams hit = 1389  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle816.out
Perplexity = 4.15, Entropy = 2.05 bits
Computation based on 1434 words.
Number of 2-grams hit = 1433  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle817.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 580 words.
Number of 2-grams hit = 579  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle818.out
Perplexity = 4.07, Entropy = 2.03 bits
Computation based on 604 words.
Number of 2-grams hit = 603  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle819.out
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1947 words.
Number of 2-grams hit = 1946  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle820.out
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 880 words.
Number of 2-grams hit = 879  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle821.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1187 words.
Number of 2-grams hit = 1186  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle822.out
Perplexity = 3.69, Entropy = 1.89 bits
Computation based on 2654 words.
Number of 2-grams hit = 2652  (99.92%)
Number of 1-grams hit = 2  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle823.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 1435 words.
Number of 2-grams hit = 1434  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle824.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 533 words.
Number of 2-grams hit = 532  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle825.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 639 words.
Number of 2-grams hit = 638  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle826.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 978 words.
Number of 2-grams hit = 977  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle827.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 734 words.
Number of 2-grams hit = 733  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle828.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 13188 words.
Number of 2-grams hit = 13185  (99.98%)
Number of 1-grams hit = 3  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle829.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 578 words.
Number of 2-grams hit = 577  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle830.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 2488 words.
Number of 2-grams hit = 2487  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle831.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 1106 words.
Number of 2-grams hit = 1105  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle832.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 460 words.
Number of 2-grams hit = 459  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle833.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 888 words.
Number of 2-grams hit = 887  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle834.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 963 words.
Number of 2-grams hit = 962  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle835.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 491 words.
Number of 2-grams hit = 490  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle836.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 528 words.
Number of 2-grams hit = 527  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle837.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 900 words.
Number of 2-grams hit = 899  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle838.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 2446 words.
Number of 2-grams hit = 2445  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle839.out
Perplexity = 4.32, Entropy = 2.11 bits
Computation based on 496 words.
Number of 2-grams hit = 495  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle840.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 560 words.
Number of 2-grams hit = 559  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle841.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 1009 words.
Number of 2-grams hit = 1008  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle842.out
Perplexity = 3.00, Entropy = 1.58 bits
Computation based on 503 words.
Number of 2-grams hit = 502  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle843.out
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1419 words.
Number of 2-grams hit = 1418  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle844.out
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 9009 words.
Number of 2-grams hit = 9008  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle845.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 648 words.
Number of 2-grams hit = 647  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle846.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 609 words.
Number of 2-grams hit = 608  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle847.out
Perplexity = 3.93, Entropy = 1.98 bits
Computation based on 1795 words.
Number of 2-grams hit = 1794  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle848.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 389 words.
Number of 2-grams hit = 388  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle849.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 1705 words.
Number of 2-grams hit = 1704  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle850.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 477 words.
Number of 2-grams hit = 476  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle851.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 604 words.
Number of 2-grams hit = 603  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle852.out
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 684 words.
Number of 2-grams hit = 683  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle853.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 1621 words.
Number of 2-grams hit = 1620  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle854.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 2917 words.
Number of 2-grams hit = 2916  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle855.out
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 465 words.
Number of 2-grams hit = 464  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle856.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 889 words.
Number of 2-grams hit = 888  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle857.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 379 words.
Number of 2-grams hit = 378  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle858.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 255 words.
Number of 2-grams hit = 254  (99.61%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle859.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 1192 words.
Number of 2-grams hit = 1191  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle860.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 503 words.
Number of 2-grams hit = 502  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle861.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 432 words.
Number of 2-grams hit = 431  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle862.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 487 words.
Number of 2-grams hit = 486  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle863.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 712 words.
Number of 2-grams hit = 711  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle864.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 494 words.
Number of 2-grams hit = 493  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle865.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 6948 words.
Number of 2-grams hit = 6947  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle866.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 668 words.
Number of 2-grams hit = 667  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle867.out
Perplexity = 4.15, Entropy = 2.05 bits
Computation based on 377 words.
Number of 2-grams hit = 376  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle868.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1799 words.
Number of 2-grams hit = 1798  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle869.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 3365 words.
Number of 2-grams hit = 3364  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle870.out
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 831 words.
Number of 2-grams hit = 830  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle871.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 444 words.
Number of 2-grams hit = 443  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle872.out
Perplexity = 4.35, Entropy = 2.12 bits
Computation based on 316 words.
Number of 2-grams hit = 315  (99.68%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle873.out
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1015 words.
Number of 2-grams hit = 1014  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle874.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 460 words.
Number of 2-grams hit = 459  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle875.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 855 words.
Number of 2-grams hit = 854  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle876.out
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 10045 words.
Number of 2-grams hit = 10043  (99.98%)
Number of 1-grams hit = 2  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle877.out
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 502 words.
Number of 2-grams hit = 501  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle878.out
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 364 words.
Number of 2-grams hit = 363  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle879.out
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 591 words.
Number of 2-grams hit = 590  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle880.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 509 words.
Number of 2-grams hit = 508  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle881.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 376 words.
Number of 2-grams hit = 375  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle882.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 638 words.
Number of 2-grams hit = 637  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle883.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 638 words.
Number of 2-grams hit = 637  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle884.out
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 777 words.
Number of 2-grams hit = 776  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle885.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 534 words.
Number of 2-grams hit = 533  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle886.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 508 words.
Number of 2-grams hit = 507  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle887.out
Perplexity = 3.93, Entropy = 1.98 bits
Computation based on 517 words.
Number of 2-grams hit = 516  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle888.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1311 words.
Number of 2-grams hit = 1310  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle889.out
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 4700 words.
Number of 2-grams hit = 4699  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle890.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1148 words.
Number of 2-grams hit = 1147  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle891.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 873 words.
Number of 2-grams hit = 872  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle892.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 3489 words.
Number of 2-grams hit = 3488  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle893.out
Perplexity = 4.45, Entropy = 2.15 bits
Computation based on 699 words.
Number of 2-grams hit = 698  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle894.out
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 805 words.
Number of 2-grams hit = 804  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle895.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 188 words.
Number of 2-grams hit = 187  (99.47%)
Number of 1-grams hit = 1  (0.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle896.out
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 452 words.
Number of 2-grams hit = 451  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle897.out
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 767 words.
Number of 2-grams hit = 766  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle898.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 1746 words.
Number of 2-grams hit = 1745  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle899.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 219 words.
Number of 2-grams hit = 218  (99.54%)
Number of 1-grams hit = 1  (0.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle900.out
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1135 words.
Number of 2-grams hit = 1134  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle901.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 428 words.
Number of 2-grams hit = 427  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle902.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 2155 words.
Number of 2-grams hit = 2154  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle903.out
Perplexity = 3.93, Entropy = 1.97 bits
Computation based on 1273 words.
Number of 2-grams hit = 1272  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle904.out
Perplexity = 2.73, Entropy = 1.45 bits
Computation based on 649 words.
Number of 2-grams hit = 648  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle905.out
Perplexity = 4.06, Entropy = 2.02 bits
Computation based on 388 words.
Number of 2-grams hit = 387  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle906.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 476 words.
Number of 2-grams hit = 475  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle907.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 488 words.
Number of 2-grams hit = 487  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle908.out
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 552 words.
Number of 2-grams hit = 551  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle909.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 456 words.
Number of 2-grams hit = 455  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle910.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 737 words.
Number of 2-grams hit = 736  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle911.out
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 688 words.
Number of 2-grams hit = 687  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle912.out
Perplexity = 4.21, Entropy = 2.07 bits
Computation based on 743 words.
Number of 2-grams hit = 742  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle913.out
Perplexity = 4.30, Entropy = 2.10 bits
Computation based on 743 words.
Number of 2-grams hit = 742  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle914.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 575 words.
Number of 2-grams hit = 574  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle915.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 1309 words.
Number of 2-grams hit = 1308  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle916.out
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 1003 words.
Number of 2-grams hit = 1002  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle917.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1180 words.
Number of 2-grams hit = 1179  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle918.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 535 words.
Number of 2-grams hit = 534  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle919.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 3585 words.
Number of 2-grams hit = 3583  (99.94%)
Number of 1-grams hit = 2  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle920.out
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1001 words.
Number of 2-grams hit = 1000  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle921.out
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 844 words.
Number of 2-grams hit = 843  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle922.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 515 words.
Number of 2-grams hit = 514  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle923.out
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 560 words.
Number of 2-grams hit = 559  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle924.out
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 376 words.
Number of 2-grams hit = 375  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle925.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 4136 words.
Number of 2-grams hit = 4135  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle926.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 1421 words.
Number of 2-grams hit = 1420  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle927.out
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 440 words.
Number of 2-grams hit = 439  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle928.out
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 516 words.
Number of 2-grams hit = 515  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle929.out
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 538 words.
Number of 2-grams hit = 537  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle930.out
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 465 words.
Number of 2-grams hit = 464  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle931.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 645 words.
Number of 2-grams hit = 644  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle932.out
Perplexity = 2.95, Entropy = 1.56 bits
Computation based on 228 words.
Number of 2-grams hit = 227  (99.56%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle933.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 453 words.
Number of 2-grams hit = 452  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle934.out
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 416 words.
Number of 2-grams hit = 415  (99.76%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle935.out
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1259 words.
Number of 2-grams hit = 1258  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle936.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 483 words.
Number of 2-grams hit = 482  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle937.out
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 239 words.
Number of 2-grams hit = 238  (99.58%)
Number of 1-grams hit = 1  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle938.out
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1402 words.
Number of 2-grams hit = 1401  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle939.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 519 words.
Number of 2-grams hit = 518  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle940.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 2076 words.
Number of 2-grams hit = 2075  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle941.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 1305 words.
Number of 2-grams hit = 1304  (99.92%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle942.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 571 words.
Number of 2-grams hit = 570  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle943.out
Perplexity = 3.17, Entropy = 1.66 bits
Computation based on 1491 words.
Number of 2-grams hit = 1490  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle944.out
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 477 words.
Number of 2-grams hit = 476  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle945.out
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 3747 words.
Number of 2-grams hit = 3746  (99.97%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle946.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 611 words.
Number of 2-grams hit = 610  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle947.out
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 459 words.
Number of 2-grams hit = 458  (99.78%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle948.out
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 627 words.
Number of 2-grams hit = 626  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle949.out
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1862 words.
Number of 2-grams hit = 1861  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle950.out
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 543 words.
Number of 2-grams hit = 542  (99.82%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle951.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 622 words.
Number of 2-grams hit = 621  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle952.out
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1389 words.
Number of 2-grams hit = 1388  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle953.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 2294 words.
Number of 2-grams hit = 2293  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle954.out
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 800 words.
Number of 2-grams hit = 799  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle955.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1574 words.
Number of 2-grams hit = 1573  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle956.out
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 1107 words.
Number of 2-grams hit = 1106  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle957.out
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 323 words.
Number of 2-grams hit = 322  (99.69%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle958.out
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 1014 words.
Number of 2-grams hit = 1013  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle959.out
Perplexity = 4.55, Entropy = 2.19 bits
Computation based on 218 words.
Number of 2-grams hit = 217  (99.54%)
Number of 1-grams hit = 1  (0.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle960.out
Perplexity = 2.97, Entropy = 1.57 bits
Computation based on 652 words.
Number of 2-grams hit = 651  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle961.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 534 words.
Number of 2-grams hit = 533  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle962.out
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 2369 words.
Number of 2-grams hit = 2368  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle963.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 2187 words.
Number of 2-grams hit = 2186  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle964.out
Perplexity = 4.07, Entropy = 2.02 bits
Computation based on 1090 words.
Number of 2-grams hit = 1089  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle965.out
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 701 words.
Number of 2-grams hit = 700  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle966.out
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 850 words.
Number of 2-grams hit = 849  (99.88%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle967.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 426 words.
Number of 2-grams hit = 425  (99.77%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle968.out
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1147 words.
Number of 2-grams hit = 1146  (99.91%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle969.out
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 773 words.
Number of 2-grams hit = 772  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle970.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 798 words.
Number of 2-grams hit = 797  (99.87%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle971.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 504 words.
Number of 2-grams hit = 503  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle972.out
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 2041 words.
Number of 2-grams hit = 2040  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle973.out
Perplexity = 3.88, Entropy = 1.95 bits
Computation based on 663 words.
Number of 2-grams hit = 662  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle974.out
Perplexity = 2.94, Entropy = 1.55 bits
Computation based on 1022 words.
Number of 2-grams hit = 1021  (99.90%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle975.out
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 380 words.
Number of 2-grams hit = 379  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle976.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 385 words.
Number of 2-grams hit = 384  (99.74%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle977.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 917 words.
Number of 2-grams hit = 916  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle978.out
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 503 words.
Number of 2-grams hit = 502  (99.80%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle979.out
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 1716 words.
Number of 2-grams hit = 1715  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle980.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 681 words.
Number of 2-grams hit = 680  (99.85%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle981.out
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 371 words.
Number of 2-grams hit = 370  (99.73%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle982.out
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 890 words.
Number of 2-grams hit = 889  (99.89%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle983.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 520 words.
Number of 2-grams hit = 519  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle984.out
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 479 words.
Number of 2-grams hit = 478  (99.79%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle985.out
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 1351 words.
Number of 2-grams hit = 1350  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle986.out
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 297 words.
Number of 2-grams hit = 296  (99.66%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle987.out
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 1891 words.
Number of 2-grams hit = 1890  (99.95%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle988.out
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1353 words.
Number of 2-grams hit = 1352  (99.93%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle989.out
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 2476 words.
Number of 2-grams hit = 2475  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle990.out
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 710 words.
Number of 2-grams hit = 709  (99.86%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle991.out
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 1688 words.
Number of 2-grams hit = 1687  (99.94%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle992.out
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 578 words.
Number of 2-grams hit = 577  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle993.out
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 2666 words.
Number of 2-grams hit = 2665  (99.96%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle994.out
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 527 words.
Number of 2-grams hit = 526  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle995.out
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 7968 words.
Number of 2-grams hit = 7967  (99.99%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle996.out
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 627 words.
Number of 2-grams hit = 626  (99.84%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle997.out
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 5785 words.
Number of 2-grams hit = 5784  (99.98%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle998.out
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 572 words.
Number of 2-grams hit = 571  (99.83%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle999.out
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 523 words.
Number of 2-grams hit = 522  (99.81%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : 