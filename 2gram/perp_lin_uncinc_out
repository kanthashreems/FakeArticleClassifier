evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article0.text
Will force inclusive back-off from OOVs.
Perplexity = 131.50, Entropy = 7.04 bits
Computation based on 1255 words.
Number of 2-grams hit = 1208  (96.25%)
Number of 1-grams hit = 47  (3.75%)
7 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article1.text
Will force inclusive back-off from OOVs.
Perplexity = 112.67, Entropy = 6.82 bits
Computation based on 1499 words.
Number of 2-grams hit = 1453  (96.93%)
Number of 1-grams hit = 46  (3.07%)
6 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article2.text
Will force inclusive back-off from OOVs.
Perplexity = 129.49, Entropy = 7.02 bits
Computation based on 540 words.
Number of 2-grams hit = 524  (97.04%)
Number of 1-grams hit = 16  (2.96%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article3.text
Will force inclusive back-off from OOVs.
Perplexity = 118.50, Entropy = 6.89 bits
Computation based on 620 words.
Number of 2-grams hit = 609  (98.23%)
Number of 1-grams hit = 11  (1.77%)
5 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article4.text
Will force inclusive back-off from OOVs.
Perplexity = 204.29, Entropy = 7.67 bits
Computation based on 395 words.
Number of 2-grams hit = 370  (93.67%)
Number of 1-grams hit = 25  (6.33%)
3 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article5.text
Will force inclusive back-off from OOVs.
Perplexity = 160.61, Entropy = 7.33 bits
Computation based on 888 words.
Number of 2-grams hit = 842  (94.82%)
Number of 1-grams hit = 46  (5.18%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article6.text
Will force inclusive back-off from OOVs.
Perplexity = 361.18, Entropy = 8.50 bits
Computation based on 281 words.
Number of 2-grams hit = 251  (89.32%)
Number of 1-grams hit = 30  (10.68%)
6 OOVs (2.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article7.text
Will force inclusive back-off from OOVs.
Perplexity = 153.64, Entropy = 7.26 bits
Computation based on 605 words.
Number of 2-grams hit = 591  (97.69%)
Number of 1-grams hit = 14  (2.31%)
6 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article8.text
Will force inclusive back-off from OOVs.
Perplexity = 247.97, Entropy = 7.95 bits
Computation based on 496 words.
Number of 2-grams hit = 459  (92.54%)
Number of 1-grams hit = 37  (7.46%)
8 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article9.text
Will force inclusive back-off from OOVs.
Perplexity = 122.40, Entropy = 6.94 bits
Computation based on 314 words.
Number of 2-grams hit = 301  (95.86%)
Number of 1-grams hit = 13  (4.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article10.text
Will force inclusive back-off from OOVs.
Perplexity = 158.41, Entropy = 7.31 bits
Computation based on 304 words.
Number of 2-grams hit = 286  (94.08%)
Number of 1-grams hit = 18  (5.92%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article11.text
Will force inclusive back-off from OOVs.
Perplexity = 174.21, Entropy = 7.44 bits
Computation based on 302 words.
Number of 2-grams hit = 285  (94.37%)
Number of 1-grams hit = 17  (5.63%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article12.text
Will force inclusive back-off from OOVs.
Perplexity = 98.71, Entropy = 6.63 bits
Computation based on 300 words.
Number of 2-grams hit = 292  (97.33%)
Number of 1-grams hit = 8  (2.67%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article13.text
Will force inclusive back-off from OOVs.
Perplexity = 227.83, Entropy = 7.83 bits
Computation based on 453 words.
Number of 2-grams hit = 423  (93.38%)
Number of 1-grams hit = 30  (6.62%)
11 OOVs (2.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article14.text
Will force inclusive back-off from OOVs.
Perplexity = 159.37, Entropy = 7.32 bits
Computation based on 468 words.
Number of 2-grams hit = 437  (93.38%)
Number of 1-grams hit = 31  (6.62%)
7 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article15.text
Will force inclusive back-off from OOVs.
Perplexity = 140.40, Entropy = 7.13 bits
Computation based on 493 words.
Number of 2-grams hit = 478  (96.96%)
Number of 1-grams hit = 15  (3.04%)
4 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article16.text
Will force inclusive back-off from OOVs.
Perplexity = 218.20, Entropy = 7.77 bits
Computation based on 365 words.
Number of 2-grams hit = 343  (93.97%)
Number of 1-grams hit = 22  (6.03%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article17.text
Will force inclusive back-off from OOVs.
Perplexity = 196.07, Entropy = 7.62 bits
Computation based on 673 words.
Number of 2-grams hit = 628  (93.31%)
Number of 1-grams hit = 45  (6.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article18.text
Will force inclusive back-off from OOVs.
Perplexity = 133.51, Entropy = 7.06 bits
Computation based on 406 words.
Number of 2-grams hit = 392  (96.55%)
Number of 1-grams hit = 14  (3.45%)
6 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article19.text
Will force inclusive back-off from OOVs.
Perplexity = 178.83, Entropy = 7.48 bits
Computation based on 525 words.
Number of 2-grams hit = 500  (95.24%)
Number of 1-grams hit = 25  (4.76%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article20.text
Will force inclusive back-off from OOVs.
Perplexity = 171.71, Entropy = 7.42 bits
Computation based on 348 words.
Number of 2-grams hit = 325  (93.39%)
Number of 1-grams hit = 23  (6.61%)
4 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article21.text
Will force inclusive back-off from OOVs.
Perplexity = 193.66, Entropy = 7.60 bits
Computation based on 461 words.
Number of 2-grams hit = 426  (92.41%)
Number of 1-grams hit = 35  (7.59%)
4 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article22.text
Will force inclusive back-off from OOVs.
Perplexity = 181.78, Entropy = 7.51 bits
Computation based on 360 words.
Number of 2-grams hit = 340  (94.44%)
Number of 1-grams hit = 20  (5.56%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article23.text
Will force inclusive back-off from OOVs.
Perplexity = 218.53, Entropy = 7.77 bits
Computation based on 345 words.
Number of 2-grams hit = 325  (94.20%)
Number of 1-grams hit = 20  (5.80%)
12 OOVs (3.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article24.text
Will force inclusive back-off from OOVs.
Perplexity = 160.08, Entropy = 7.32 bits
Computation based on 561 words.
Number of 2-grams hit = 530  (94.47%)
Number of 1-grams hit = 31  (5.53%)
5 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article25.text
Will force inclusive back-off from OOVs.
Perplexity = 230.46, Entropy = 7.85 bits
Computation based on 373 words.
Number of 2-grams hit = 348  (93.30%)
Number of 1-grams hit = 25  (6.70%)
7 OOVs (1.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article26.text
Will force inclusive back-off from OOVs.
Perplexity = 201.12, Entropy = 7.65 bits
Computation based on 1519 words.
Number of 2-grams hit = 1437  (94.60%)
Number of 1-grams hit = 82  (5.40%)
13 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article27.text
Will force inclusive back-off from OOVs.
Perplexity = 223.43, Entropy = 7.80 bits
Computation based on 389 words.
Number of 2-grams hit = 354  (91.00%)
Number of 1-grams hit = 35  (9.00%)
4 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article28.text
Will force inclusive back-off from OOVs.
Perplexity = 151.13, Entropy = 7.24 bits
Computation based on 1388 words.
Number of 2-grams hit = 1317  (94.88%)
Number of 1-grams hit = 71  (5.12%)
11 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article29.text
Will force inclusive back-off from OOVs.
Perplexity = 184.51, Entropy = 7.53 bits
Computation based on 317 words.
Number of 2-grams hit = 300  (94.64%)
Number of 1-grams hit = 17  (5.36%)
2 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article30.text
Will force inclusive back-off from OOVs.
Perplexity = 141.92, Entropy = 7.15 bits
Computation based on 475 words.
Number of 2-grams hit = 456  (96.00%)
Number of 1-grams hit = 19  (4.00%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article31.text
Will force inclusive back-off from OOVs.
Perplexity = 165.14, Entropy = 7.37 bits
Computation based on 439 words.
Number of 2-grams hit = 414  (94.31%)
Number of 1-grams hit = 25  (5.69%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article32.text
Will force inclusive back-off from OOVs.
Perplexity = 190.06, Entropy = 7.57 bits
Computation based on 475 words.
Number of 2-grams hit = 442  (93.05%)
Number of 1-grams hit = 33  (6.95%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article33.text
Will force inclusive back-off from OOVs.
Perplexity = 215.65, Entropy = 7.75 bits
Computation based on 441 words.
Number of 2-grams hit = 410  (92.97%)
Number of 1-grams hit = 31  (7.03%)
3 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article34.text
Will force inclusive back-off from OOVs.
Perplexity = 141.76, Entropy = 7.15 bits
Computation based on 304 words.
Number of 2-grams hit = 294  (96.71%)
Number of 1-grams hit = 10  (3.29%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article35.text
Will force inclusive back-off from OOVs.
Perplexity = 178.21, Entropy = 7.48 bits
Computation based on 378 words.
Number of 2-grams hit = 364  (96.30%)
Number of 1-grams hit = 14  (3.70%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article36.text
Will force inclusive back-off from OOVs.
Perplexity = 134.34, Entropy = 7.07 bits
Computation based on 1000 words.
Number of 2-grams hit = 960  (96.00%)
Number of 1-grams hit = 40  (4.00%)
11 OOVs (1.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article37.text
Will force inclusive back-off from OOVs.
Perplexity = 154.11, Entropy = 7.27 bits
Computation based on 559 words.
Number of 2-grams hit = 532  (95.17%)
Number of 1-grams hit = 27  (4.83%)
10 OOVs (1.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article38.text
Will force inclusive back-off from OOVs.
Perplexity = 200.32, Entropy = 7.65 bits
Computation based on 408 words.
Number of 2-grams hit = 387  (94.85%)
Number of 1-grams hit = 21  (5.15%)
4 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article39.text
Will force inclusive back-off from OOVs.
Perplexity = 129.21, Entropy = 7.01 bits
Computation based on 2623 words.
Number of 2-grams hit = 2537  (96.72%)
Number of 1-grams hit = 86  (3.28%)
37 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article40.text
Will force inclusive back-off from OOVs.
Perplexity = 90.79, Entropy = 6.50 bits
Computation based on 405 words.
Number of 2-grams hit = 398  (98.27%)
Number of 1-grams hit = 7  (1.73%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article41.text
Will force inclusive back-off from OOVs.
Perplexity = 115.02, Entropy = 6.85 bits
Computation based on 969 words.
Number of 2-grams hit = 936  (96.59%)
Number of 1-grams hit = 33  (3.41%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article42.text
Will force inclusive back-off from OOVs.
Perplexity = 146.35, Entropy = 7.19 bits
Computation based on 547 words.
Number of 2-grams hit = 525  (95.98%)
Number of 1-grams hit = 22  (4.02%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article43.text
Will force inclusive back-off from OOVs.
Perplexity = 138.57, Entropy = 7.11 bits
Computation based on 449 words.
Number of 2-grams hit = 428  (95.32%)
Number of 1-grams hit = 21  (4.68%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article44.text
Will force inclusive back-off from OOVs.
Perplexity = 143.14, Entropy = 7.16 bits
Computation based on 492 words.
Number of 2-grams hit = 467  (94.92%)
Number of 1-grams hit = 25  (5.08%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article45.text
Will force inclusive back-off from OOVs.
Perplexity = 189.14, Entropy = 7.56 bits
Computation based on 464 words.
Number of 2-grams hit = 440  (94.83%)
Number of 1-grams hit = 24  (5.17%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article46.text
Will force inclusive back-off from OOVs.
Perplexity = 218.22, Entropy = 7.77 bits
Computation based on 401 words.
Number of 2-grams hit = 375  (93.52%)
Number of 1-grams hit = 26  (6.48%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article47.text
Will force inclusive back-off from OOVs.
Perplexity = 136.00, Entropy = 7.09 bits
Computation based on 2678 words.
Number of 2-grams hit = 2577  (96.23%)
Number of 1-grams hit = 101  (3.77%)
6 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article48.text
Will force inclusive back-off from OOVs.
Perplexity = 253.14, Entropy = 7.98 bits
Computation based on 366 words.
Number of 2-grams hit = 349  (95.36%)
Number of 1-grams hit = 17  (4.64%)
7 OOVs (1.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article49.text
Will force inclusive back-off from OOVs.
Perplexity = 106.11, Entropy = 6.73 bits
Computation based on 364 words.
Number of 2-grams hit = 354  (97.25%)
Number of 1-grams hit = 10  (2.75%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article50.text
Will force inclusive back-off from OOVs.
Perplexity = 130.21, Entropy = 7.02 bits
Computation based on 522 words.
Number of 2-grams hit = 500  (95.79%)
Number of 1-grams hit = 22  (4.21%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article51.text
Will force inclusive back-off from OOVs.
Perplexity = 238.43, Entropy = 7.90 bits
Computation based on 1102 words.
Number of 2-grams hit = 1033  (93.74%)
Number of 1-grams hit = 69  (6.26%)
19 OOVs (1.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article52.text
Will force inclusive back-off from OOVs.
Perplexity = 103.53, Entropy = 6.69 bits
Computation based on 360 words.
Number of 2-grams hit = 353  (98.06%)
Number of 1-grams hit = 7  (1.94%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article53.text
Will force inclusive back-off from OOVs.
Perplexity = 189.38, Entropy = 7.57 bits
Computation based on 472 words.
Number of 2-grams hit = 438  (92.80%)
Number of 1-grams hit = 34  (7.20%)
9 OOVs (1.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article54.text
Will force inclusive back-off from OOVs.
Perplexity = 210.71, Entropy = 7.72 bits
Computation based on 1063 words.
Number of 2-grams hit = 986  (92.76%)
Number of 1-grams hit = 77  (7.24%)
21 OOVs (1.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article55.text
Will force inclusive back-off from OOVs.
Perplexity = 92.48, Entropy = 6.53 bits
Computation based on 533 words.
Number of 2-grams hit = 510  (95.68%)
Number of 1-grams hit = 23  (4.32%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article56.text
Will force inclusive back-off from OOVs.
Perplexity = 166.02, Entropy = 7.38 bits
Computation based on 898 words.
Number of 2-grams hit = 840  (93.54%)
Number of 1-grams hit = 58  (6.46%)
7 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article57.text
Will force inclusive back-off from OOVs.
Perplexity = 106.32, Entropy = 6.73 bits
Computation based on 1537 words.
Number of 2-grams hit = 1498  (97.46%)
Number of 1-grams hit = 39  (2.54%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article58.text
Will force inclusive back-off from OOVs.
Perplexity = 151.36, Entropy = 7.24 bits
Computation based on 383 words.
Number of 2-grams hit = 360  (93.99%)
Number of 1-grams hit = 23  (6.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article59.text
Will force inclusive back-off from OOVs.
Perplexity = 93.38, Entropy = 6.55 bits
Computation based on 208 words.
Number of 2-grams hit = 202  (97.12%)
Number of 1-grams hit = 6  (2.88%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article60.text
Will force inclusive back-off from OOVs.
Perplexity = 153.39, Entropy = 7.26 bits
Computation based on 321 words.
Number of 2-grams hit = 309  (96.26%)
Number of 1-grams hit = 12  (3.74%)
13 OOVs (3.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article61.text
Will force inclusive back-off from OOVs.
Perplexity = 163.13, Entropy = 7.35 bits
Computation based on 489 words.
Number of 2-grams hit = 469  (95.91%)
Number of 1-grams hit = 20  (4.09%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article62.text
Will force inclusive back-off from OOVs.
Perplexity = 198.31, Entropy = 7.63 bits
Computation based on 1282 words.
Number of 2-grams hit = 1210  (94.38%)
Number of 1-grams hit = 72  (5.62%)
11 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article63.text
Will force inclusive back-off from OOVs.
Perplexity = 183.17, Entropy = 7.52 bits
Computation based on 965 words.
Number of 2-grams hit = 914  (94.72%)
Number of 1-grams hit = 51  (5.28%)
25 OOVs (2.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article64.text
Will force inclusive back-off from OOVs.
Perplexity = 142.58, Entropy = 7.16 bits
Computation based on 4162 words.
Number of 2-grams hit = 3984  (95.72%)
Number of 1-grams hit = 178  (4.28%)
54 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article65.text
Will force inclusive back-off from OOVs.
Perplexity = 125.84, Entropy = 6.98 bits
Computation based on 628 words.
Number of 2-grams hit = 595  (94.75%)
Number of 1-grams hit = 33  (5.25%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article66.text
Will force inclusive back-off from OOVs.
Perplexity = 112.77, Entropy = 6.82 bits
Computation based on 472 words.
Number of 2-grams hit = 460  (97.46%)
Number of 1-grams hit = 12  (2.54%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article67.text
Will force inclusive back-off from OOVs.
Perplexity = 221.12, Entropy = 7.79 bits
Computation based on 449 words.
Number of 2-grams hit = 422  (93.99%)
Number of 1-grams hit = 27  (6.01%)
5 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article68.text
Will force inclusive back-off from OOVs.
Perplexity = 134.92, Entropy = 7.08 bits
Computation based on 1109 words.
Number of 2-grams hit = 1068  (96.30%)
Number of 1-grams hit = 41  (3.70%)
11 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article69.text
Will force inclusive back-off from OOVs.
Perplexity = 237.49, Entropy = 7.89 bits
Computation based on 380 words.
Number of 2-grams hit = 366  (96.32%)
Number of 1-grams hit = 14  (3.68%)
3 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article70.text
Will force inclusive back-off from OOVs.
Perplexity = 102.76, Entropy = 6.68 bits
Computation based on 959 words.
Number of 2-grams hit = 943  (98.33%)
Number of 1-grams hit = 16  (1.67%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article71.text
Will force inclusive back-off from OOVs.
Perplexity = 140.36, Entropy = 7.13 bits
Computation based on 550 words.
Number of 2-grams hit = 532  (96.73%)
Number of 1-grams hit = 18  (3.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article72.text
Will force inclusive back-off from OOVs.
Perplexity = 158.44, Entropy = 7.31 bits
Computation based on 1302 words.
Number of 2-grams hit = 1240  (95.24%)
Number of 1-grams hit = 62  (4.76%)
6 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article73.text
Will force inclusive back-off from OOVs.
Perplexity = 120.15, Entropy = 6.91 bits
Computation based on 509 words.
Number of 2-grams hit = 492  (96.66%)
Number of 1-grams hit = 17  (3.34%)
4 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article74.text
Will force inclusive back-off from OOVs.
Perplexity = 168.78, Entropy = 7.40 bits
Computation based on 818 words.
Number of 2-grams hit = 778  (95.11%)
Number of 1-grams hit = 40  (4.89%)
5 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article75.text
Will force inclusive back-off from OOVs.
Perplexity = 149.45, Entropy = 7.22 bits
Computation based on 1190 words.
Number of 2-grams hit = 1143  (96.05%)
Number of 1-grams hit = 47  (3.95%)
4 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article76.text
Will force inclusive back-off from OOVs.
Perplexity = 106.38, Entropy = 6.73 bits
Computation based on 1377 words.
Number of 2-grams hit = 1337  (97.10%)
Number of 1-grams hit = 40  (2.90%)
5 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article77.text
Will force inclusive back-off from OOVs.
Perplexity = 175.81, Entropy = 7.46 bits
Computation based on 288 words.
Number of 2-grams hit = 267  (92.71%)
Number of 1-grams hit = 21  (7.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article78.text
Will force inclusive back-off from OOVs.
Perplexity = 128.50, Entropy = 7.01 bits
Computation based on 1118 words.
Number of 2-grams hit = 1075  (96.15%)
Number of 1-grams hit = 43  (3.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article79.text
Will force inclusive back-off from OOVs.
Perplexity = 197.34, Entropy = 7.62 bits
Computation based on 1065 words.
Number of 2-grams hit = 992  (93.15%)
Number of 1-grams hit = 73  (6.85%)
8 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article80.text
Will force inclusive back-off from OOVs.
Perplexity = 132.78, Entropy = 7.05 bits
Computation based on 354 words.
Number of 2-grams hit = 336  (94.92%)
Number of 1-grams hit = 18  (5.08%)
16 OOVs (4.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article81.text
Will force inclusive back-off from OOVs.
Perplexity = 131.11, Entropy = 7.03 bits
Computation based on 4029 words.
Number of 2-grams hit = 3879  (96.28%)
Number of 1-grams hit = 150  (3.72%)
21 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article82.text
Will force inclusive back-off from OOVs.
Perplexity = 150.39, Entropy = 7.23 bits
Computation based on 467 words.
Number of 2-grams hit = 453  (97.00%)
Number of 1-grams hit = 14  (3.00%)
4 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article83.text
Will force inclusive back-off from OOVs.
Perplexity = 122.89, Entropy = 6.94 bits
Computation based on 431 words.
Number of 2-grams hit = 415  (96.29%)
Number of 1-grams hit = 16  (3.71%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article84.text
Will force inclusive back-off from OOVs.
Perplexity = 157.32, Entropy = 7.30 bits
Computation based on 399 words.
Number of 2-grams hit = 382  (95.74%)
Number of 1-grams hit = 17  (4.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article85.text
Will force inclusive back-off from OOVs.
Perplexity = 150.71, Entropy = 7.24 bits
Computation based on 1214 words.
Number of 2-grams hit = 1159  (95.47%)
Number of 1-grams hit = 55  (4.53%)
5 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article86.text
Will force inclusive back-off from OOVs.
Perplexity = 169.13, Entropy = 7.40 bits
Computation based on 286 words.
Number of 2-grams hit = 269  (94.06%)
Number of 1-grams hit = 17  (5.94%)
3 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article87.text
Will force inclusive back-off from OOVs.
Perplexity = 260.41, Entropy = 8.02 bits
Computation based on 622 words.
Number of 2-grams hit = 576  (92.60%)
Number of 1-grams hit = 46  (7.40%)
10 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article88.text
Will force inclusive back-off from OOVs.
Perplexity = 135.42, Entropy = 7.08 bits
Computation based on 1476 words.
Number of 2-grams hit = 1419  (96.14%)
Number of 1-grams hit = 57  (3.86%)
23 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article89.text
Will force inclusive back-off from OOVs.
Perplexity = 196.92, Entropy = 7.62 bits
Computation based on 390 words.
Number of 2-grams hit = 370  (94.87%)
Number of 1-grams hit = 20  (5.13%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article90.text
Will force inclusive back-off from OOVs.
Perplexity = 166.40, Entropy = 7.38 bits
Computation based on 506 words.
Number of 2-grams hit = 481  (95.06%)
Number of 1-grams hit = 25  (4.94%)
14 OOVs (2.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article91.text
Will force inclusive back-off from OOVs.
Perplexity = 133.92, Entropy = 7.07 bits
Computation based on 342 words.
Number of 2-grams hit = 329  (96.20%)
Number of 1-grams hit = 13  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article92.text
Will force inclusive back-off from OOVs.
Perplexity = 133.40, Entropy = 7.06 bits
Computation based on 1024 words.
Number of 2-grams hit = 981  (95.80%)
Number of 1-grams hit = 43  (4.20%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article93.text
Will force inclusive back-off from OOVs.
Perplexity = 171.73, Entropy = 7.42 bits
Computation based on 682 words.
Number of 2-grams hit = 645  (94.57%)
Number of 1-grams hit = 37  (5.43%)
4 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article94.text
Will force inclusive back-off from OOVs.
Perplexity = 121.81, Entropy = 6.93 bits
Computation based on 1286 words.
Number of 2-grams hit = 1245  (96.81%)
Number of 1-grams hit = 41  (3.19%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article95.text
Will force inclusive back-off from OOVs.
Perplexity = 121.89, Entropy = 6.93 bits
Computation based on 499 words.
Number of 2-grams hit = 481  (96.39%)
Number of 1-grams hit = 18  (3.61%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article96.text
Will force inclusive back-off from OOVs.
Perplexity = 134.43, Entropy = 7.07 bits
Computation based on 397 words.
Number of 2-grams hit = 380  (95.72%)
Number of 1-grams hit = 17  (4.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article97.text
Will force inclusive back-off from OOVs.
Perplexity = 133.14, Entropy = 7.06 bits
Computation based on 544 words.
Number of 2-grams hit = 527  (96.88%)
Number of 1-grams hit = 17  (3.12%)
9 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article98.text
Will force inclusive back-off from OOVs.
Perplexity = 143.23, Entropy = 7.16 bits
Computation based on 363 words.
Number of 2-grams hit = 350  (96.42%)
Number of 1-grams hit = 13  (3.58%)
3 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article99.text
Will force inclusive back-off from OOVs.
Perplexity = 196.18, Entropy = 7.62 bits
Computation based on 431 words.
Number of 2-grams hit = 405  (93.97%)
Number of 1-grams hit = 26  (6.03%)
25 OOVs (5.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article100.text
Will force inclusive back-off from OOVs.
Perplexity = 139.99, Entropy = 7.13 bits
Computation based on 474 words.
Number of 2-grams hit = 455  (95.99%)
Number of 1-grams hit = 19  (4.01%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article101.text
Will force inclusive back-off from OOVs.
Perplexity = 160.46, Entropy = 7.33 bits
Computation based on 486 words.
Number of 2-grams hit = 462  (95.06%)
Number of 1-grams hit = 24  (4.94%)
21 OOVs (4.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article102.text
Will force inclusive back-off from OOVs.
Perplexity = 198.12, Entropy = 7.63 bits
Computation based on 412 words.
Number of 2-grams hit = 390  (94.66%)
Number of 1-grams hit = 22  (5.34%)
3 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article103.text
Will force inclusive back-off from OOVs.
Perplexity = 247.11, Entropy = 7.95 bits
Computation based on 476 words.
Number of 2-grams hit = 436  (91.60%)
Number of 1-grams hit = 40  (8.40%)
22 OOVs (4.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article104.text
Will force inclusive back-off from OOVs.
Perplexity = 113.47, Entropy = 6.83 bits
Computation based on 6340 words.
Number of 2-grams hit = 6108  (96.34%)
Number of 1-grams hit = 232  (3.66%)
19 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article105.text
Will force inclusive back-off from OOVs.
Perplexity = 194.12, Entropy = 7.60 bits
Computation based on 446 words.
Number of 2-grams hit = 417  (93.50%)
Number of 1-grams hit = 29  (6.50%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article106.text
Will force inclusive back-off from OOVs.
Perplexity = 235.30, Entropy = 7.88 bits
Computation based on 794 words.
Number of 2-grams hit = 743  (93.58%)
Number of 1-grams hit = 51  (6.42%)
8 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article107.text
Will force inclusive back-off from OOVs.
Perplexity = 360.53, Entropy = 8.49 bits
Computation based on 474 words.
Number of 2-grams hit = 419  (88.40%)
Number of 1-grams hit = 55  (11.60%)
11 OOVs (2.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article108.text
Will force inclusive back-off from OOVs.
Perplexity = 124.70, Entropy = 6.96 bits
Computation based on 528 words.
Number of 2-grams hit = 515  (97.54%)
Number of 1-grams hit = 13  (2.46%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article109.text
Will force inclusive back-off from OOVs.
Perplexity = 135.77, Entropy = 7.09 bits
Computation based on 1123 words.
Number of 2-grams hit = 1074  (95.64%)
Number of 1-grams hit = 49  (4.36%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article110.text
Will force inclusive back-off from OOVs.
Perplexity = 154.52, Entropy = 7.27 bits
Computation based on 908 words.
Number of 2-grams hit = 877  (96.59%)
Number of 1-grams hit = 31  (3.41%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article111.text
Will force inclusive back-off from OOVs.
Perplexity = 141.06, Entropy = 7.14 bits
Computation based on 1223 words.
Number of 2-grams hit = 1167  (95.42%)
Number of 1-grams hit = 56  (4.58%)
10 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article112.text
Will force inclusive back-off from OOVs.
Perplexity = 154.04, Entropy = 7.27 bits
Computation based on 328 words.
Number of 2-grams hit = 312  (95.12%)
Number of 1-grams hit = 16  (4.88%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article113.text
Will force inclusive back-off from OOVs.
Perplexity = 91.65, Entropy = 6.52 bits
Computation based on 5818 words.
Number of 2-grams hit = 5693  (97.85%)
Number of 1-grams hit = 125  (2.15%)
13 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article114.text
Will force inclusive back-off from OOVs.
Perplexity = 137.40, Entropy = 7.10 bits
Computation based on 499 words.
Number of 2-grams hit = 483  (96.79%)
Number of 1-grams hit = 16  (3.21%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article115.text
Will force inclusive back-off from OOVs.
Perplexity = 138.21, Entropy = 7.11 bits
Computation based on 641 words.
Number of 2-grams hit = 612  (95.48%)
Number of 1-grams hit = 29  (4.52%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article116.text
Will force inclusive back-off from OOVs.
Perplexity = 129.31, Entropy = 7.01 bits
Computation based on 1526 words.
Number of 2-grams hit = 1482  (97.12%)
Number of 1-grams hit = 44  (2.88%)
13 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article117.text
Will force inclusive back-off from OOVs.
Perplexity = 153.65, Entropy = 7.26 bits
Computation based on 517 words.
Number of 2-grams hit = 493  (95.36%)
Number of 1-grams hit = 24  (4.64%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article118.text
Will force inclusive back-off from OOVs.
Perplexity = 122.96, Entropy = 6.94 bits
Computation based on 610 words.
Number of 2-grams hit = 585  (95.90%)
Number of 1-grams hit = 25  (4.10%)
5 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article119.text
Will force inclusive back-off from OOVs.
Perplexity = 134.07, Entropy = 7.07 bits
Computation based on 390 words.
Number of 2-grams hit = 375  (96.15%)
Number of 1-grams hit = 15  (3.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article120.text
Will force inclusive back-off from OOVs.
Perplexity = 147.05, Entropy = 7.20 bits
Computation based on 557 words.
Number of 2-grams hit = 529  (94.97%)
Number of 1-grams hit = 28  (5.03%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article121.text
Will force inclusive back-off from OOVs.
Perplexity = 138.12, Entropy = 7.11 bits
Computation based on 7137 words.
Number of 2-grams hit = 6859  (96.10%)
Number of 1-grams hit = 278  (3.90%)
24 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article122.text
Will force inclusive back-off from OOVs.
Perplexity = 79.07, Entropy = 6.31 bits
Computation based on 325 words.
Number of 2-grams hit = 319  (98.15%)
Number of 1-grams hit = 6  (1.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article123.text
Will force inclusive back-off from OOVs.
Perplexity = 202.26, Entropy = 7.66 bits
Computation based on 524 words.
Number of 2-grams hit = 505  (96.37%)
Number of 1-grams hit = 19  (3.63%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article124.text
Will force inclusive back-off from OOVs.
Perplexity = 114.89, Entropy = 6.84 bits
Computation based on 798 words.
Number of 2-grams hit = 768  (96.24%)
Number of 1-grams hit = 30  (3.76%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article125.text
Will force inclusive back-off from OOVs.
Perplexity = 161.75, Entropy = 7.34 bits
Computation based on 484 words.
Number of 2-grams hit = 457  (94.42%)
Number of 1-grams hit = 27  (5.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article126.text
Will force inclusive back-off from OOVs.
Perplexity = 129.50, Entropy = 7.02 bits
Computation based on 540 words.
Number of 2-grams hit = 517  (95.74%)
Number of 1-grams hit = 23  (4.26%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article127.text
Will force inclusive back-off from OOVs.
Perplexity = 136.99, Entropy = 7.10 bits
Computation based on 923 words.
Number of 2-grams hit = 883  (95.67%)
Number of 1-grams hit = 40  (4.33%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article128.text
Will force inclusive back-off from OOVs.
Perplexity = 295.61, Entropy = 8.21 bits
Computation based on 427 words.
Number of 2-grams hit = 390  (91.33%)
Number of 1-grams hit = 37  (8.67%)
4 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article129.text
Will force inclusive back-off from OOVs.
Perplexity = 206.85, Entropy = 7.69 bits
Computation based on 469 words.
Number of 2-grams hit = 438  (93.39%)
Number of 1-grams hit = 31  (6.61%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article130.text
Will force inclusive back-off from OOVs.
Perplexity = 136.69, Entropy = 7.09 bits
Computation based on 1527 words.
Number of 2-grams hit = 1452  (95.09%)
Number of 1-grams hit = 75  (4.91%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article131.text
Will force inclusive back-off from OOVs.
Perplexity = 143.00, Entropy = 7.16 bits
Computation based on 6797 words.
Number of 2-grams hit = 6514  (95.84%)
Number of 1-grams hit = 283  (4.16%)
19 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article132.text
Will force inclusive back-off from OOVs.
Perplexity = 128.50, Entropy = 7.01 bits
Computation based on 1115 words.
Number of 2-grams hit = 1071  (96.05%)
Number of 1-grams hit = 44  (3.95%)
8 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article133.text
Will force inclusive back-off from OOVs.
Perplexity = 85.71, Entropy = 6.42 bits
Computation based on 243 words.
Number of 2-grams hit = 235  (96.71%)
Number of 1-grams hit = 8  (3.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article134.text
Will force inclusive back-off from OOVs.
Perplexity = 147.90, Entropy = 7.21 bits
Computation based on 500 words.
Number of 2-grams hit = 479  (95.80%)
Number of 1-grams hit = 21  (4.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article135.text
Will force inclusive back-off from OOVs.
Perplexity = 311.55, Entropy = 8.28 bits
Computation based on 482 words.
Number of 2-grams hit = 443  (91.91%)
Number of 1-grams hit = 39  (8.09%)
9 OOVs (1.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article136.text
Will force inclusive back-off from OOVs.
Perplexity = 129.06, Entropy = 7.01 bits
Computation based on 526 words.
Number of 2-grams hit = 506  (96.20%)
Number of 1-grams hit = 20  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article137.text
Will force inclusive back-off from OOVs.
Perplexity = 102.79, Entropy = 6.68 bits
Computation based on 329 words.
Number of 2-grams hit = 321  (97.57%)
Number of 1-grams hit = 8  (2.43%)
4 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article138.text
Will force inclusive back-off from OOVs.
Perplexity = 103.94, Entropy = 6.70 bits
Computation based on 307 words.
Number of 2-grams hit = 301  (98.05%)
Number of 1-grams hit = 6  (1.95%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article139.text
Will force inclusive back-off from OOVs.
Perplexity = 112.93, Entropy = 6.82 bits
Computation based on 271 words.
Number of 2-grams hit = 263  (97.05%)
Number of 1-grams hit = 8  (2.95%)
4 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article140.text
Will force inclusive back-off from OOVs.
Perplexity = 215.78, Entropy = 7.75 bits
Computation based on 371 words.
Number of 2-grams hit = 348  (93.80%)
Number of 1-grams hit = 23  (6.20%)
3 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article141.text
Will force inclusive back-off from OOVs.
Perplexity = 160.27, Entropy = 7.32 bits
Computation based on 434 words.
Number of 2-grams hit = 419  (96.54%)
Number of 1-grams hit = 15  (3.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article142.text
Will force inclusive back-off from OOVs.
Perplexity = 160.78, Entropy = 7.33 bits
Computation based on 695 words.
Number of 2-grams hit = 659  (94.82%)
Number of 1-grams hit = 36  (5.18%)
5 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article143.text
Will force inclusive back-off from OOVs.
Perplexity = 158.28, Entropy = 7.31 bits
Computation based on 377 words.
Number of 2-grams hit = 359  (95.23%)
Number of 1-grams hit = 18  (4.77%)
4 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article144.text
Will force inclusive back-off from OOVs.
Perplexity = 227.76, Entropy = 7.83 bits
Computation based on 506 words.
Number of 2-grams hit = 475  (93.87%)
Number of 1-grams hit = 31  (6.13%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article145.text
Will force inclusive back-off from OOVs.
Perplexity = 156.82, Entropy = 7.29 bits
Computation based on 1941 words.
Number of 2-grams hit = 1852  (95.41%)
Number of 1-grams hit = 89  (4.59%)
3 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article146.text
Will force inclusive back-off from OOVs.
Perplexity = 131.85, Entropy = 7.04 bits
Computation based on 238 words.
Number of 2-grams hit = 227  (95.38%)
Number of 1-grams hit = 11  (4.62%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article147.text
Will force inclusive back-off from OOVs.
Perplexity = 178.86, Entropy = 7.48 bits
Computation based on 657 words.
Number of 2-grams hit = 623  (94.82%)
Number of 1-grams hit = 34  (5.18%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article148.text
Will force inclusive back-off from OOVs.
Perplexity = 169.92, Entropy = 7.41 bits
Computation based on 1733 words.
Number of 2-grams hit = 1633  (94.23%)
Number of 1-grams hit = 100  (5.77%)
6 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article149.text
Will force inclusive back-off from OOVs.
Perplexity = 172.00, Entropy = 7.43 bits
Computation based on 557 words.
Number of 2-grams hit = 527  (94.61%)
Number of 1-grams hit = 30  (5.39%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article150.text
Will force inclusive back-off from OOVs.
Perplexity = 138.49, Entropy = 7.11 bits
Computation based on 638 words.
Number of 2-grams hit = 609  (95.45%)
Number of 1-grams hit = 29  (4.55%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article151.text
Will force inclusive back-off from OOVs.
Perplexity = 119.22, Entropy = 6.90 bits
Computation based on 173 words.
Number of 2-grams hit = 169  (97.69%)
Number of 1-grams hit = 4  (2.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article152.text
Will force inclusive back-off from OOVs.
Perplexity = 132.68, Entropy = 7.05 bits
Computation based on 1817 words.
Number of 2-grams hit = 1748  (96.20%)
Number of 1-grams hit = 69  (3.80%)
2 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article153.text
Will force inclusive back-off from OOVs.
Perplexity = 164.74, Entropy = 7.36 bits
Computation based on 588 words.
Number of 2-grams hit = 557  (94.73%)
Number of 1-grams hit = 31  (5.27%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article154.text
Will force inclusive back-off from OOVs.
Perplexity = 126.53, Entropy = 6.98 bits
Computation based on 1059 words.
Number of 2-grams hit = 1023  (96.60%)
Number of 1-grams hit = 36  (3.40%)
5 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article155.text
Will force inclusive back-off from OOVs.
Perplexity = 137.44, Entropy = 7.10 bits
Computation based on 639 words.
Number of 2-grams hit = 613  (95.93%)
Number of 1-grams hit = 26  (4.07%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article156.text
Will force inclusive back-off from OOVs.
Perplexity = 71.17, Entropy = 6.15 bits
Computation based on 1373 words.
Number of 2-grams hit = 1353  (98.54%)
Number of 1-grams hit = 20  (1.46%)
5 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article157.text
Will force inclusive back-off from OOVs.
Perplexity = 226.72, Entropy = 7.82 bits
Computation based on 404 words.
Number of 2-grams hit = 381  (94.31%)
Number of 1-grams hit = 23  (5.69%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article158.text
Will force inclusive back-off from OOVs.
Perplexity = 165.85, Entropy = 7.37 bits
Computation based on 574 words.
Number of 2-grams hit = 551  (95.99%)
Number of 1-grams hit = 23  (4.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article159.text
Will force inclusive back-off from OOVs.
Perplexity = 76.32, Entropy = 6.25 bits
Computation based on 1205 words.
Number of 2-grams hit = 1174  (97.43%)
Number of 1-grams hit = 31  (2.57%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article160.text
Will force inclusive back-off from OOVs.
Perplexity = 208.88, Entropy = 7.71 bits
Computation based on 534 words.
Number of 2-grams hit = 500  (93.63%)
Number of 1-grams hit = 34  (6.37%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article161.text
Will force inclusive back-off from OOVs.
Perplexity = 318.62, Entropy = 8.32 bits
Computation based on 189 words.
Number of 2-grams hit = 175  (92.59%)
Number of 1-grams hit = 14  (7.41%)
10 OOVs (5.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article162.text
Will force inclusive back-off from OOVs.
Perplexity = 140.84, Entropy = 7.14 bits
Computation based on 1148 words.
Number of 2-grams hit = 1109  (96.60%)
Number of 1-grams hit = 39  (3.40%)
6 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article163.text
Will force inclusive back-off from OOVs.
Perplexity = 161.39, Entropy = 7.33 bits
Computation based on 701 words.
Number of 2-grams hit = 661  (94.29%)
Number of 1-grams hit = 40  (5.71%)
4 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article164.text
Will force inclusive back-off from OOVs.
Perplexity = 118.57, Entropy = 6.89 bits
Computation based on 1767 words.
Number of 2-grams hit = 1705  (96.49%)
Number of 1-grams hit = 62  (3.51%)
10 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article165.text
Will force inclusive back-off from OOVs.
Perplexity = 237.69, Entropy = 7.89 bits
Computation based on 546 words.
Number of 2-grams hit = 515  (94.32%)
Number of 1-grams hit = 31  (5.68%)
11 OOVs (1.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article166.text
Will force inclusive back-off from OOVs.
Perplexity = 117.23, Entropy = 6.87 bits
Computation based on 517 words.
Number of 2-grams hit = 495  (95.74%)
Number of 1-grams hit = 22  (4.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article167.text
Will force inclusive back-off from OOVs.
Perplexity = 135.23, Entropy = 7.08 bits
Computation based on 1170 words.
Number of 2-grams hit = 1125  (96.15%)
Number of 1-grams hit = 45  (3.85%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article168.text
Will force inclusive back-off from OOVs.
Perplexity = 140.43, Entropy = 7.13 bits
Computation based on 524 words.
Number of 2-grams hit = 495  (94.47%)
Number of 1-grams hit = 29  (5.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article169.text
Will force inclusive back-off from OOVs.
Perplexity = 136.28, Entropy = 7.09 bits
Computation based on 499 words.
Number of 2-grams hit = 477  (95.59%)
Number of 1-grams hit = 22  (4.41%)
5 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article170.text
Will force inclusive back-off from OOVs.
Perplexity = 246.31, Entropy = 7.94 bits
Computation based on 1128 words.
Number of 2-grams hit = 1057  (93.71%)
Number of 1-grams hit = 71  (6.29%)
22 OOVs (1.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article171.text
Will force inclusive back-off from OOVs.
Perplexity = 339.95, Entropy = 8.41 bits
Computation based on 605 words.
Number of 2-grams hit = 549  (90.74%)
Number of 1-grams hit = 56  (9.26%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article172.text
Will force inclusive back-off from OOVs.
Perplexity = 150.40, Entropy = 7.23 bits
Computation based on 681 words.
Number of 2-grams hit = 653  (95.89%)
Number of 1-grams hit = 28  (4.11%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article173.text
Will force inclusive back-off from OOVs.
Perplexity = 220.90, Entropy = 7.79 bits
Computation based on 1009 words.
Number of 2-grams hit = 942  (93.36%)
Number of 1-grams hit = 67  (6.64%)
12 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article174.text
Will force inclusive back-off from OOVs.
Perplexity = 127.15, Entropy = 6.99 bits
Computation based on 446 words.
Number of 2-grams hit = 428  (95.96%)
Number of 1-grams hit = 18  (4.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article175.text
Will force inclusive back-off from OOVs.
Perplexity = 138.68, Entropy = 7.12 bits
Computation based on 458 words.
Number of 2-grams hit = 439  (95.85%)
Number of 1-grams hit = 19  (4.15%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article176.text
Will force inclusive back-off from OOVs.
Perplexity = 168.23, Entropy = 7.39 bits
Computation based on 743 words.
Number of 2-grams hit = 716  (96.37%)
Number of 1-grams hit = 27  (3.63%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article177.text
Will force inclusive back-off from OOVs.
Perplexity = 176.55, Entropy = 7.46 bits
Computation based on 511 words.
Number of 2-grams hit = 485  (94.91%)
Number of 1-grams hit = 26  (5.09%)
7 OOVs (1.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article178.text
Will force inclusive back-off from OOVs.
Perplexity = 182.22, Entropy = 7.51 bits
Computation based on 168 words.
Number of 2-grams hit = 156  (92.86%)
Number of 1-grams hit = 12  (7.14%)
1 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article179.text
Will force inclusive back-off from OOVs.
Perplexity = 188.43, Entropy = 7.56 bits
Computation based on 449 words.
Number of 2-grams hit = 419  (93.32%)
Number of 1-grams hit = 30  (6.68%)
5 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article180.text
Will force inclusive back-off from OOVs.
Perplexity = 142.00, Entropy = 7.15 bits
Computation based on 427 words.
Number of 2-grams hit = 406  (95.08%)
Number of 1-grams hit = 21  (4.92%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article181.text
Will force inclusive back-off from OOVs.
Perplexity = 99.02, Entropy = 6.63 bits
Computation based on 849 words.
Number of 2-grams hit = 822  (96.82%)
Number of 1-grams hit = 27  (3.18%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article182.text
Will force inclusive back-off from OOVs.
Perplexity = 109.85, Entropy = 6.78 bits
Computation based on 366 words.
Number of 2-grams hit = 354  (96.72%)
Number of 1-grams hit = 12  (3.28%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article183.text
Will force inclusive back-off from OOVs.
Perplexity = 113.61, Entropy = 6.83 bits
Computation based on 458 words.
Number of 2-grams hit = 436  (95.20%)
Number of 1-grams hit = 22  (4.80%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article184.text
Will force inclusive back-off from OOVs.
Perplexity = 148.70, Entropy = 7.22 bits
Computation based on 5472 words.
Number of 2-grams hit = 5211  (95.23%)
Number of 1-grams hit = 261  (4.77%)
20 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article185.text
Will force inclusive back-off from OOVs.
Perplexity = 127.82, Entropy = 7.00 bits
Computation based on 975 words.
Number of 2-grams hit = 937  (96.10%)
Number of 1-grams hit = 38  (3.90%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article186.text
Will force inclusive back-off from OOVs.
Perplexity = 142.04, Entropy = 7.15 bits
Computation based on 487 words.
Number of 2-grams hit = 469  (96.30%)
Number of 1-grams hit = 18  (3.70%)
3 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article187.text
Will force inclusive back-off from OOVs.
Perplexity = 149.52, Entropy = 7.22 bits
Computation based on 6010 words.
Number of 2-grams hit = 5739  (95.49%)
Number of 1-grams hit = 271  (4.51%)
17 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article188.text
Will force inclusive back-off from OOVs.
Perplexity = 141.82, Entropy = 7.15 bits
Computation based on 684 words.
Number of 2-grams hit = 663  (96.93%)
Number of 1-grams hit = 21  (3.07%)
7 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article189.text
Will force inclusive back-off from OOVs.
Perplexity = 192.23, Entropy = 7.59 bits
Computation based on 521 words.
Number of 2-grams hit = 489  (93.86%)
Number of 1-grams hit = 32  (6.14%)
15 OOVs (2.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article190.text
Will force inclusive back-off from OOVs.
Perplexity = 217.24, Entropy = 7.76 bits
Computation based on 390 words.
Number of 2-grams hit = 369  (94.62%)
Number of 1-grams hit = 21  (5.38%)
6 OOVs (1.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article191.text
Will force inclusive back-off from OOVs.
Perplexity = 144.44, Entropy = 7.17 bits
Computation based on 5442 words.
Number of 2-grams hit = 5195  (95.46%)
Number of 1-grams hit = 247  (4.54%)
26 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article192.text
Will force inclusive back-off from OOVs.
Perplexity = 161.51, Entropy = 7.34 bits
Computation based on 767 words.
Number of 2-grams hit = 746  (97.26%)
Number of 1-grams hit = 21  (2.74%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article193.text
Will force inclusive back-off from OOVs.
Perplexity = 120.19, Entropy = 6.91 bits
Computation based on 161 words.
Number of 2-grams hit = 154  (95.65%)
Number of 1-grams hit = 7  (4.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article194.text
Will force inclusive back-off from OOVs.
Perplexity = 134.38, Entropy = 7.07 bits
Computation based on 604 words.
Number of 2-grams hit = 576  (95.36%)
Number of 1-grams hit = 28  (4.64%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article195.text
Will force inclusive back-off from OOVs.
Perplexity = 134.20, Entropy = 7.07 bits
Computation based on 792 words.
Number of 2-grams hit = 757  (95.58%)
Number of 1-grams hit = 35  (4.42%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article196.text
Will force inclusive back-off from OOVs.
Perplexity = 139.89, Entropy = 7.13 bits
Computation based on 685 words.
Number of 2-grams hit = 665  (97.08%)
Number of 1-grams hit = 20  (2.92%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article197.text
Will force inclusive back-off from OOVs.
Perplexity = 125.80, Entropy = 6.97 bits
Computation based on 223 words.
Number of 2-grams hit = 210  (94.17%)
Number of 1-grams hit = 13  (5.83%)
3 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article198.text
Will force inclusive back-off from OOVs.
Perplexity = 178.83, Entropy = 7.48 bits
Computation based on 718 words.
Number of 2-grams hit = 681  (94.85%)
Number of 1-grams hit = 37  (5.15%)
3 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article199.text
Will force inclusive back-off from OOVs.
Perplexity = 115.65, Entropy = 6.85 bits
Computation based on 4477 words.
Number of 2-grams hit = 4333  (96.78%)
Number of 1-grams hit = 144  (3.22%)
16 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article200.text
Will force inclusive back-off from OOVs.
Perplexity = 128.74, Entropy = 7.01 bits
Computation based on 638 words.
Number of 2-grams hit = 610  (95.61%)
Number of 1-grams hit = 28  (4.39%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article201.text
Will force inclusive back-off from OOVs.
Perplexity = 210.75, Entropy = 7.72 bits
Computation based on 463 words.
Number of 2-grams hit = 434  (93.74%)
Number of 1-grams hit = 29  (6.26%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article202.text
Will force inclusive back-off from OOVs.
Perplexity = 145.24, Entropy = 7.18 bits
Computation based on 1170 words.
Number of 2-grams hit = 1128  (96.41%)
Number of 1-grams hit = 42  (3.59%)
5 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article203.text
Will force inclusive back-off from OOVs.
Perplexity = 96.29, Entropy = 6.59 bits
Computation based on 4808 words.
Number of 2-grams hit = 4703  (97.82%)
Number of 1-grams hit = 105  (2.18%)
22 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article204.text
Will force inclusive back-off from OOVs.
Perplexity = 106.83, Entropy = 6.74 bits
Computation based on 5390 words.
Number of 2-grams hit = 5241  (97.24%)
Number of 1-grams hit = 149  (2.76%)
26 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article205.text
Will force inclusive back-off from OOVs.
Perplexity = 132.21, Entropy = 7.05 bits
Computation based on 553 words.
Number of 2-grams hit = 538  (97.29%)
Number of 1-grams hit = 15  (2.71%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article206.text
Will force inclusive back-off from OOVs.
Perplexity = 173.99, Entropy = 7.44 bits
Computation based on 585 words.
Number of 2-grams hit = 562  (96.07%)
Number of 1-grams hit = 23  (3.93%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article207.text
Will force inclusive back-off from OOVs.
Perplexity = 168.54, Entropy = 7.40 bits
Computation based on 722 words.
Number of 2-grams hit = 687  (95.15%)
Number of 1-grams hit = 35  (4.85%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article208.text
Will force inclusive back-off from OOVs.
Perplexity = 167.25, Entropy = 7.39 bits
Computation based on 886 words.
Number of 2-grams hit = 858  (96.84%)
Number of 1-grams hit = 28  (3.16%)
8 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article209.text
Will force inclusive back-off from OOVs.
Perplexity = 90.14, Entropy = 6.49 bits
Computation based on 269 words.
Number of 2-grams hit = 261  (97.03%)
Number of 1-grams hit = 8  (2.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article210.text
Will force inclusive back-off from OOVs.
Perplexity = 175.97, Entropy = 7.46 bits
Computation based on 811 words.
Number of 2-grams hit = 772  (95.19%)
Number of 1-grams hit = 39  (4.81%)
26 OOVs (3.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article211.text
Will force inclusive back-off from OOVs.
Perplexity = 217.25, Entropy = 7.76 bits
Computation based on 462 words.
Number of 2-grams hit = 428  (92.64%)
Number of 1-grams hit = 34  (7.36%)
11 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article212.text
Will force inclusive back-off from OOVs.
Perplexity = 126.69, Entropy = 6.99 bits
Computation based on 416 words.
Number of 2-grams hit = 403  (96.88%)
Number of 1-grams hit = 13  (3.12%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article213.text
Will force inclusive back-off from OOVs.
Perplexity = 121.17, Entropy = 6.92 bits
Computation based on 630 words.
Number of 2-grams hit = 605  (96.03%)
Number of 1-grams hit = 25  (3.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article214.text
Will force inclusive back-off from OOVs.
Perplexity = 183.57, Entropy = 7.52 bits
Computation based on 444 words.
Number of 2-grams hit = 421  (94.82%)
Number of 1-grams hit = 23  (5.18%)
20 OOVs (4.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article215.text
Will force inclusive back-off from OOVs.
Perplexity = 348.89, Entropy = 8.45 bits
Computation based on 685 words.
Number of 2-grams hit = 620  (90.51%)
Number of 1-grams hit = 65  (9.49%)
11 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article216.text
Will force inclusive back-off from OOVs.
Perplexity = 288.82, Entropy = 8.17 bits
Computation based on 481 words.
Number of 2-grams hit = 445  (92.52%)
Number of 1-grams hit = 36  (7.48%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article217.text
Will force inclusive back-off from OOVs.
Perplexity = 125.50, Entropy = 6.97 bits
Computation based on 302 words.
Number of 2-grams hit = 292  (96.69%)
Number of 1-grams hit = 10  (3.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article218.text
Will force inclusive back-off from OOVs.
Perplexity = 94.23, Entropy = 6.56 bits
Computation based on 718 words.
Number of 2-grams hit = 706  (98.33%)
Number of 1-grams hit = 12  (1.67%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article219.text
Will force inclusive back-off from OOVs.
Perplexity = 223.30, Entropy = 7.80 bits
Computation based on 406 words.
Number of 2-grams hit = 384  (94.58%)
Number of 1-grams hit = 22  (5.42%)
5 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article220.text
Will force inclusive back-off from OOVs.
Perplexity = 88.53, Entropy = 6.47 bits
Computation based on 4293 words.
Number of 2-grams hit = 4212  (98.11%)
Number of 1-grams hit = 81  (1.89%)
27 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article221.text
Will force inclusive back-off from OOVs.
Perplexity = 155.15, Entropy = 7.28 bits
Computation based on 1174 words.
Number of 2-grams hit = 1137  (96.85%)
Number of 1-grams hit = 37  (3.15%)
8 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article222.text
Will force inclusive back-off from OOVs.
Perplexity = 153.41, Entropy = 7.26 bits
Computation based on 4454 words.
Number of 2-grams hit = 4233  (95.04%)
Number of 1-grams hit = 221  (4.96%)
17 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article223.text
Will force inclusive back-off from OOVs.
Perplexity = 158.81, Entropy = 7.31 bits
Computation based on 825 words.
Number of 2-grams hit = 778  (94.30%)
Number of 1-grams hit = 47  (5.70%)
9 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article224.text
Will force inclusive back-off from OOVs.
Perplexity = 136.70, Entropy = 7.09 bits
Computation based on 380 words.
Number of 2-grams hit = 364  (95.79%)
Number of 1-grams hit = 16  (4.21%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article225.text
Will force inclusive back-off from OOVs.
Perplexity = 256.95, Entropy = 8.01 bits
Computation based on 724 words.
Number of 2-grams hit = 666  (91.99%)
Number of 1-grams hit = 58  (8.01%)
6 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article226.text
Will force inclusive back-off from OOVs.
Perplexity = 165.71, Entropy = 7.37 bits
Computation based on 1224 words.
Number of 2-grams hit = 1174  (95.92%)
Number of 1-grams hit = 50  (4.08%)
12 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article227.text
Will force inclusive back-off from OOVs.
Perplexity = 151.64, Entropy = 7.24 bits
Computation based on 936 words.
Number of 2-grams hit = 897  (95.83%)
Number of 1-grams hit = 39  (4.17%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article228.text
Will force inclusive back-off from OOVs.
Perplexity = 183.38, Entropy = 7.52 bits
Computation based on 783 words.
Number of 2-grams hit = 748  (95.53%)
Number of 1-grams hit = 35  (4.47%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article229.text
Will force inclusive back-off from OOVs.
Perplexity = 153.75, Entropy = 7.26 bits
Computation based on 471 words.
Number of 2-grams hit = 456  (96.82%)
Number of 1-grams hit = 15  (3.18%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article230.text
Will force inclusive back-off from OOVs.
Perplexity = 93.85, Entropy = 6.55 bits
Computation based on 301 words.
Number of 2-grams hit = 291  (96.68%)
Number of 1-grams hit = 10  (3.32%)
5 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article231.text
Will force inclusive back-off from OOVs.
Perplexity = 138.65, Entropy = 7.12 bits
Computation based on 801 words.
Number of 2-grams hit = 772  (96.38%)
Number of 1-grams hit = 29  (3.62%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article232.text
Will force inclusive back-off from OOVs.
Perplexity = 142.66, Entropy = 7.16 bits
Computation based on 363 words.
Number of 2-grams hit = 348  (95.87%)
Number of 1-grams hit = 15  (4.13%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article233.text
Will force inclusive back-off from OOVs.
Perplexity = 128.87, Entropy = 7.01 bits
Computation based on 668 words.
Number of 2-grams hit = 650  (97.31%)
Number of 1-grams hit = 18  (2.69%)
5 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article234.text
Will force inclusive back-off from OOVs.
Perplexity = 143.76, Entropy = 7.17 bits
Computation based on 1331 words.
Number of 2-grams hit = 1272  (95.57%)
Number of 1-grams hit = 59  (4.43%)
7 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article235.text
Will force inclusive back-off from OOVs.
Perplexity = 146.21, Entropy = 7.19 bits
Computation based on 349 words.
Number of 2-grams hit = 334  (95.70%)
Number of 1-grams hit = 15  (4.30%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article236.text
Will force inclusive back-off from OOVs.
Perplexity = 303.28, Entropy = 8.24 bits
Computation based on 735 words.
Number of 2-grams hit = 665  (90.48%)
Number of 1-grams hit = 70  (9.52%)
18 OOVs (2.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article237.text
Will force inclusive back-off from OOVs.
Perplexity = 190.69, Entropy = 7.58 bits
Computation based on 389 words.
Number of 2-grams hit = 368  (94.60%)
Number of 1-grams hit = 21  (5.40%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article238.text
Will force inclusive back-off from OOVs.
Perplexity = 340.86, Entropy = 8.41 bits
Computation based on 300 words.
Number of 2-grams hit = 267  (89.00%)
Number of 1-grams hit = 33  (11.00%)
13 OOVs (4.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article239.text
Will force inclusive back-off from OOVs.
Perplexity = 129.88, Entropy = 7.02 bits
Computation based on 471 words.
Number of 2-grams hit = 454  (96.39%)
Number of 1-grams hit = 17  (3.61%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article240.text
Will force inclusive back-off from OOVs.
Perplexity = 243.49, Entropy = 7.93 bits
Computation based on 312 words.
Number of 2-grams hit = 280  (89.74%)
Number of 1-grams hit = 32  (10.26%)
4 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article241.text
Will force inclusive back-off from OOVs.
Perplexity = 233.85, Entropy = 7.87 bits
Computation based on 301 words.
Number of 2-grams hit = 278  (92.36%)
Number of 1-grams hit = 23  (7.64%)
6 OOVs (1.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article242.text
Will force inclusive back-off from OOVs.
Perplexity = 116.09, Entropy = 6.86 bits
Computation based on 291 words.
Number of 2-grams hit = 280  (96.22%)
Number of 1-grams hit = 11  (3.78%)
3 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article243.text
Will force inclusive back-off from OOVs.
Perplexity = 159.45, Entropy = 7.32 bits
Computation based on 743 words.
Number of 2-grams hit = 710  (95.56%)
Number of 1-grams hit = 33  (4.44%)
6 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article244.text
Will force inclusive back-off from OOVs.
Perplexity = 169.01, Entropy = 7.40 bits
Computation based on 424 words.
Number of 2-grams hit = 399  (94.10%)
Number of 1-grams hit = 25  (5.90%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article245.text
Will force inclusive back-off from OOVs.
Perplexity = 105.83, Entropy = 6.73 bits
Computation based on 5029 words.
Number of 2-grams hit = 4873  (96.90%)
Number of 1-grams hit = 156  (3.10%)
19 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article246.text
Will force inclusive back-off from OOVs.
Perplexity = 133.96, Entropy = 7.07 bits
Computation based on 275 words.
Number of 2-grams hit = 260  (94.55%)
Number of 1-grams hit = 15  (5.45%)
2 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article247.text
Will force inclusive back-off from OOVs.
Perplexity = 111.65, Entropy = 6.80 bits
Computation based on 449 words.
Number of 2-grams hit = 431  (95.99%)
Number of 1-grams hit = 18  (4.01%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article248.text
Will force inclusive back-off from OOVs.
Perplexity = 124.98, Entropy = 6.97 bits
Computation based on 355 words.
Number of 2-grams hit = 344  (96.90%)
Number of 1-grams hit = 11  (3.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article249.text
Will force inclusive back-off from OOVs.
Perplexity = 104.43, Entropy = 6.71 bits
Computation based on 858 words.
Number of 2-grams hit = 838  (97.67%)
Number of 1-grams hit = 20  (2.33%)
5 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article250.text
Will force inclusive back-off from OOVs.
Perplexity = 111.56, Entropy = 6.80 bits
Computation based on 4834 words.
Number of 2-grams hit = 4703  (97.29%)
Number of 1-grams hit = 131  (2.71%)
30 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article251.text
Will force inclusive back-off from OOVs.
Perplexity = 132.85, Entropy = 7.05 bits
Computation based on 4971 words.
Number of 2-grams hit = 4784  (96.24%)
Number of 1-grams hit = 187  (3.76%)
15 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article252.text
Will force inclusive back-off from OOVs.
Perplexity = 120.90, Entropy = 6.92 bits
Computation based on 525 words.
Number of 2-grams hit = 505  (96.19%)
Number of 1-grams hit = 20  (3.81%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article253.text
Will force inclusive back-off from OOVs.
Perplexity = 145.96, Entropy = 7.19 bits
Computation based on 4945 words.
Number of 2-grams hit = 4729  (95.63%)
Number of 1-grams hit = 216  (4.37%)
13 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article254.text
Will force inclusive back-off from OOVs.
Perplexity = 290.84, Entropy = 8.18 bits
Computation based on 774 words.
Number of 2-grams hit = 716  (92.51%)
Number of 1-grams hit = 58  (7.49%)
9 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article255.text
Will force inclusive back-off from OOVs.
Perplexity = 161.16, Entropy = 7.33 bits
Computation based on 486 words.
Number of 2-grams hit = 462  (95.06%)
Number of 1-grams hit = 24  (4.94%)
7 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article256.text
Will force inclusive back-off from OOVs.
Perplexity = 157.55, Entropy = 7.30 bits
Computation based on 783 words.
Number of 2-grams hit = 749  (95.66%)
Number of 1-grams hit = 34  (4.34%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article257.text
Will force inclusive back-off from OOVs.
Perplexity = 83.52, Entropy = 6.38 bits
Computation based on 5750 words.
Number of 2-grams hit = 5655  (98.35%)
Number of 1-grams hit = 95  (1.65%)
31 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article258.text
Will force inclusive back-off from OOVs.
Perplexity = 165.52, Entropy = 7.37 bits
Computation based on 409 words.
Number of 2-grams hit = 388  (94.87%)
Number of 1-grams hit = 21  (5.13%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article259.text
Will force inclusive back-off from OOVs.
Perplexity = 205.33, Entropy = 7.68 bits
Computation based on 685 words.
Number of 2-grams hit = 639  (93.28%)
Number of 1-grams hit = 46  (6.72%)
16 OOVs (2.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article260.text
Will force inclusive back-off from OOVs.
Perplexity = 134.06, Entropy = 7.07 bits
Computation based on 6973 words.
Number of 2-grams hit = 6700  (96.08%)
Number of 1-grams hit = 273  (3.92%)
22 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article261.text
Will force inclusive back-off from OOVs.
Perplexity = 93.73, Entropy = 6.55 bits
Computation based on 645 words.
Number of 2-grams hit = 624  (96.74%)
Number of 1-grams hit = 21  (3.26%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article262.text
Will force inclusive back-off from OOVs.
Perplexity = 112.59, Entropy = 6.81 bits
Computation based on 661 words.
Number of 2-grams hit = 644  (97.43%)
Number of 1-grams hit = 17  (2.57%)
3 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article263.text
Will force inclusive back-off from OOVs.
Perplexity = 149.81, Entropy = 7.23 bits
Computation based on 375 words.
Number of 2-grams hit = 354  (94.40%)
Number of 1-grams hit = 21  (5.60%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article264.text
Will force inclusive back-off from OOVs.
Perplexity = 134.17, Entropy = 7.07 bits
Computation based on 798 words.
Number of 2-grams hit = 771  (96.62%)
Number of 1-grams hit = 27  (3.38%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article265.text
Will force inclusive back-off from OOVs.
Perplexity = 128.77, Entropy = 7.01 bits
Computation based on 569 words.
Number of 2-grams hit = 554  (97.36%)
Number of 1-grams hit = 15  (2.64%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article266.text
Will force inclusive back-off from OOVs.
Perplexity = 152.96, Entropy = 7.26 bits
Computation based on 435 words.
Number of 2-grams hit = 414  (95.17%)
Number of 1-grams hit = 21  (4.83%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article267.text
Will force inclusive back-off from OOVs.
Perplexity = 142.18, Entropy = 7.15 bits
Computation based on 6221 words.
Number of 2-grams hit = 5956  (95.74%)
Number of 1-grams hit = 265  (4.26%)
31 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article268.text
Will force inclusive back-off from OOVs.
Perplexity = 143.60, Entropy = 7.17 bits
Computation based on 5821 words.
Number of 2-grams hit = 5549  (95.33%)
Number of 1-grams hit = 272  (4.67%)
19 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article269.text
Will force inclusive back-off from OOVs.
Perplexity = 171.96, Entropy = 7.43 bits
Computation based on 418 words.
Number of 2-grams hit = 396  (94.74%)
Number of 1-grams hit = 22  (5.26%)
4 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article270.text
Will force inclusive back-off from OOVs.
Perplexity = 204.95, Entropy = 7.68 bits
Computation based on 375 words.
Number of 2-grams hit = 350  (93.33%)
Number of 1-grams hit = 25  (6.67%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article271.text
Will force inclusive back-off from OOVs.
Perplexity = 129.76, Entropy = 7.02 bits
Computation based on 7295 words.
Number of 2-grams hit = 7047  (96.60%)
Number of 1-grams hit = 248  (3.40%)
52 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article272.text
Will force inclusive back-off from OOVs.
Perplexity = 98.23, Entropy = 6.62 bits
Computation based on 5195 words.
Number of 2-grams hit = 5061  (97.42%)
Number of 1-grams hit = 134  (2.58%)
23 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article273.text
Will force inclusive back-off from OOVs.
Perplexity = 122.15, Entropy = 6.93 bits
Computation based on 638 words.
Number of 2-grams hit = 617  (96.71%)
Number of 1-grams hit = 21  (3.29%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article274.text
Will force inclusive back-off from OOVs.
Perplexity = 142.55, Entropy = 7.16 bits
Computation based on 651 words.
Number of 2-grams hit = 620  (95.24%)
Number of 1-grams hit = 31  (4.76%)
6 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article275.text
Will force inclusive back-off from OOVs.
Perplexity = 157.89, Entropy = 7.30 bits
Computation based on 1923 words.
Number of 2-grams hit = 1839  (95.63%)
Number of 1-grams hit = 84  (4.37%)
4 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article276.text
Will force inclusive back-off from OOVs.
Perplexity = 223.06, Entropy = 7.80 bits
Computation based on 455 words.
Number of 2-grams hit = 422  (92.75%)
Number of 1-grams hit = 33  (7.25%)
16 OOVs (3.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article277.text
Will force inclusive back-off from OOVs.
Perplexity = 145.09, Entropy = 7.18 bits
Computation based on 768 words.
Number of 2-grams hit = 735  (95.70%)
Number of 1-grams hit = 33  (4.30%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article278.text
Will force inclusive back-off from OOVs.
Perplexity = 166.06, Entropy = 7.38 bits
Computation based on 806 words.
Number of 2-grams hit = 777  (96.40%)
Number of 1-grams hit = 29  (3.60%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article279.text
Will force inclusive back-off from OOVs.
Perplexity = 107.46, Entropy = 6.75 bits
Computation based on 401 words.
Number of 2-grams hit = 383  (95.51%)
Number of 1-grams hit = 18  (4.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article280.text
Will force inclusive back-off from OOVs.
Perplexity = 139.70, Entropy = 7.13 bits
Computation based on 1708 words.
Number of 2-grams hit = 1637  (95.84%)
Number of 1-grams hit = 71  (4.16%)
2 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article281.text
Will force inclusive back-off from OOVs.
Perplexity = 169.60, Entropy = 7.41 bits
Computation based on 546 words.
Number of 2-grams hit = 529  (96.89%)
Number of 1-grams hit = 17  (3.11%)
20 OOVs (3.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article282.text
Will force inclusive back-off from OOVs.
Perplexity = 115.80, Entropy = 6.86 bits
Computation based on 708 words.
Number of 2-grams hit = 678  (95.76%)
Number of 1-grams hit = 30  (4.24%)
5 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article283.text
Will force inclusive back-off from OOVs.
Perplexity = 125.67, Entropy = 6.97 bits
Computation based on 1182 words.
Number of 2-grams hit = 1135  (96.02%)
Number of 1-grams hit = 47  (3.98%)
4 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article284.text
Will force inclusive back-off from OOVs.
Perplexity = 92.84, Entropy = 6.54 bits
Computation based on 1579 words.
Number of 2-grams hit = 1535  (97.21%)
Number of 1-grams hit = 44  (2.79%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article285.text
Will force inclusive back-off from OOVs.
Perplexity = 138.66, Entropy = 7.12 bits
Computation based on 1094 words.
Number of 2-grams hit = 1061  (96.98%)
Number of 1-grams hit = 33  (3.02%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article286.text
Will force inclusive back-off from OOVs.
Perplexity = 187.83, Entropy = 7.55 bits
Computation based on 442 words.
Number of 2-grams hit = 418  (94.57%)
Number of 1-grams hit = 24  (5.43%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article287.text
Will force inclusive back-off from OOVs.
Perplexity = 127.40, Entropy = 6.99 bits
Computation based on 1652 words.
Number of 2-grams hit = 1598  (96.73%)
Number of 1-grams hit = 54  (3.27%)
6 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article288.text
Will force inclusive back-off from OOVs.
Perplexity = 144.81, Entropy = 7.18 bits
Computation based on 582 words.
Number of 2-grams hit = 559  (96.05%)
Number of 1-grams hit = 23  (3.95%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article289.text
Will force inclusive back-off from OOVs.
Perplexity = 82.76, Entropy = 6.37 bits
Computation based on 704 words.
Number of 2-grams hit = 693  (98.44%)
Number of 1-grams hit = 11  (1.56%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article290.text
Will force inclusive back-off from OOVs.
Perplexity = 131.06, Entropy = 7.03 bits
Computation based on 1280 words.
Number of 2-grams hit = 1243  (97.11%)
Number of 1-grams hit = 37  (2.89%)
13 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article291.text
Will force inclusive back-off from OOVs.
Perplexity = 206.94, Entropy = 7.69 bits
Computation based on 507 words.
Number of 2-grams hit = 475  (93.69%)
Number of 1-grams hit = 32  (6.31%)
9 OOVs (1.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article292.text
Will force inclusive back-off from OOVs.
Perplexity = 104.55, Entropy = 6.71 bits
Computation based on 533 words.
Number of 2-grams hit = 517  (97.00%)
Number of 1-grams hit = 16  (3.00%)
5 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article293.text
Will force inclusive back-off from OOVs.
Perplexity = 200.46, Entropy = 7.65 bits
Computation based on 552 words.
Number of 2-grams hit = 517  (93.66%)
Number of 1-grams hit = 35  (6.34%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article294.text
Will force inclusive back-off from OOVs.
Perplexity = 263.41, Entropy = 8.04 bits
Computation based on 2100 words.
Number of 2-grams hit = 1954  (93.05%)
Number of 1-grams hit = 146  (6.95%)
20 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article295.text
Will force inclusive back-off from OOVs.
Perplexity = 206.21, Entropy = 7.69 bits
Computation based on 583 words.
Number of 2-grams hit = 552  (94.68%)
Number of 1-grams hit = 31  (5.32%)
10 OOVs (1.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article296.text
Will force inclusive back-off from OOVs.
Perplexity = 123.54, Entropy = 6.95 bits
Computation based on 1523 words.
Number of 2-grams hit = 1476  (96.91%)
Number of 1-grams hit = 47  (3.09%)
7 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article297.text
Will force inclusive back-off from OOVs.
Perplexity = 92.59, Entropy = 6.53 bits
Computation based on 492 words.
Number of 2-grams hit = 484  (98.37%)
Number of 1-grams hit = 8  (1.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article298.text
Will force inclusive back-off from OOVs.
Perplexity = 135.94, Entropy = 7.09 bits
Computation based on 3442 words.
Number of 2-grams hit = 3305  (96.02%)
Number of 1-grams hit = 137  (3.98%)
12 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article299.text
Will force inclusive back-off from OOVs.
Perplexity = 187.76, Entropy = 7.55 bits
Computation based on 2703 words.
Number of 2-grams hit = 2553  (94.45%)
Number of 1-grams hit = 150  (5.55%)
39 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article300.text
Will force inclusive back-off from OOVs.
Perplexity = 110.23, Entropy = 6.78 bits
Computation based on 2528 words.
Number of 2-grams hit = 2471  (97.75%)
Number of 1-grams hit = 57  (2.25%)
11 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article301.text
Will force inclusive back-off from OOVs.
Perplexity = 160.67, Entropy = 7.33 bits
Computation based on 2136 words.
Number of 2-grams hit = 2052  (96.07%)
Number of 1-grams hit = 84  (3.93%)
10 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article302.text
Will force inclusive back-off from OOVs.
Perplexity = 113.83, Entropy = 6.83 bits
Computation based on 622 words.
Number of 2-grams hit = 595  (95.66%)
Number of 1-grams hit = 27  (4.34%)
4 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article303.text
Will force inclusive back-off from OOVs.
Perplexity = 215.59, Entropy = 7.75 bits
Computation based on 852 words.
Number of 2-grams hit = 823  (96.60%)
Number of 1-grams hit = 29  (3.40%)
18 OOVs (2.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article304.text
Will force inclusive back-off from OOVs.
Perplexity = 301.97, Entropy = 8.24 bits
Computation based on 392 words.
Number of 2-grams hit = 361  (92.09%)
Number of 1-grams hit = 31  (7.91%)
12 OOVs (2.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article305.text
Will force inclusive back-off from OOVs.
Perplexity = 142.47, Entropy = 7.15 bits
Computation based on 303 words.
Number of 2-grams hit = 294  (97.03%)
Number of 1-grams hit = 9  (2.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article306.text
Will force inclusive back-off from OOVs.
Perplexity = 139.02, Entropy = 7.12 bits
Computation based on 589 words.
Number of 2-grams hit = 559  (94.91%)
Number of 1-grams hit = 30  (5.09%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article307.text
Will force inclusive back-off from OOVs.
Perplexity = 148.55, Entropy = 7.21 bits
Computation based on 511 words.
Number of 2-grams hit = 491  (96.09%)
Number of 1-grams hit = 20  (3.91%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article308.text
Will force inclusive back-off from OOVs.
Perplexity = 161.43, Entropy = 7.33 bits
Computation based on 392 words.
Number of 2-grams hit = 379  (96.68%)
Number of 1-grams hit = 13  (3.32%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article309.text
Will force inclusive back-off from OOVs.
Perplexity = 109.03, Entropy = 6.77 bits
Computation based on 241 words.
Number of 2-grams hit = 232  (96.27%)
Number of 1-grams hit = 9  (3.73%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article310.text
Will force inclusive back-off from OOVs.
Perplexity = 182.86, Entropy = 7.51 bits
Computation based on 480 words.
Number of 2-grams hit = 458  (95.42%)
Number of 1-grams hit = 22  (4.58%)
4 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article311.text
Will force inclusive back-off from OOVs.
Perplexity = 135.48, Entropy = 7.08 bits
Computation based on 465 words.
Number of 2-grams hit = 444  (95.48%)
Number of 1-grams hit = 21  (4.52%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article312.text
Will force inclusive back-off from OOVs.
Perplexity = 186.21, Entropy = 7.54 bits
Computation based on 594 words.
Number of 2-grams hit = 560  (94.28%)
Number of 1-grams hit = 34  (5.72%)
4 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article313.text
Will force inclusive back-off from OOVs.
Perplexity = 204.83, Entropy = 7.68 bits
Computation based on 466 words.
Number of 2-grams hit = 441  (94.64%)
Number of 1-grams hit = 25  (5.36%)
11 OOVs (2.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article314.text
Will force inclusive back-off from OOVs.
Perplexity = 107.30, Entropy = 6.75 bits
Computation based on 3281 words.
Number of 2-grams hit = 3190  (97.23%)
Number of 1-grams hit = 91  (2.77%)
15 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article315.text
Will force inclusive back-off from OOVs.
Perplexity = 177.13, Entropy = 7.47 bits
Computation based on 1078 words.
Number of 2-grams hit = 1021  (94.71%)
Number of 1-grams hit = 57  (5.29%)
14 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article316.text
Will force inclusive back-off from OOVs.
Perplexity = 136.83, Entropy = 7.10 bits
Computation based on 329 words.
Number of 2-grams hit = 315  (95.74%)
Number of 1-grams hit = 14  (4.26%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article317.text
Will force inclusive back-off from OOVs.
Perplexity = 146.56, Entropy = 7.20 bits
Computation based on 623 words.
Number of 2-grams hit = 596  (95.67%)
Number of 1-grams hit = 27  (4.33%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article318.text
Will force inclusive back-off from OOVs.
Perplexity = 136.35, Entropy = 7.09 bits
Computation based on 3565 words.
Number of 2-grams hit = 3425  (96.07%)
Number of 1-grams hit = 140  (3.93%)
9 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article319.text
Will force inclusive back-off from OOVs.
Perplexity = 339.02, Entropy = 8.41 bits
Computation based on 589 words.
Number of 2-grams hit = 539  (91.51%)
Number of 1-grams hit = 50  (8.49%)
10 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article320.text
Will force inclusive back-off from OOVs.
Perplexity = 176.50, Entropy = 7.46 bits
Computation based on 376 words.
Number of 2-grams hit = 360  (95.74%)
Number of 1-grams hit = 16  (4.26%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article321.text
Will force inclusive back-off from OOVs.
Perplexity = 153.41, Entropy = 7.26 bits
Computation based on 1074 words.
Number of 2-grams hit = 1021  (95.07%)
Number of 1-grams hit = 53  (4.93%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article322.text
Will force inclusive back-off from OOVs.
Perplexity = 122.96, Entropy = 6.94 bits
Computation based on 1666 words.
Number of 2-grams hit = 1606  (96.40%)
Number of 1-grams hit = 60  (3.60%)
15 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article323.text
Will force inclusive back-off from OOVs.
Perplexity = 317.83, Entropy = 8.31 bits
Computation based on 808 words.
Number of 2-grams hit = 748  (92.57%)
Number of 1-grams hit = 60  (7.43%)
12 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article324.text
Will force inclusive back-off from OOVs.
Perplexity = 133.46, Entropy = 7.06 bits
Computation based on 1642 words.
Number of 2-grams hit = 1571  (95.68%)
Number of 1-grams hit = 71  (4.32%)
15 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article325.text
Will force inclusive back-off from OOVs.
Perplexity = 236.84, Entropy = 7.89 bits
Computation based on 1108 words.
Number of 2-grams hit = 1039  (93.77%)
Number of 1-grams hit = 69  (6.23%)
18 OOVs (1.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article326.text
Will force inclusive back-off from OOVs.
Perplexity = 164.47, Entropy = 7.36 bits
Computation based on 606 words.
Number of 2-grams hit = 572  (94.39%)
Number of 1-grams hit = 34  (5.61%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article327.text
Will force inclusive back-off from OOVs.
Perplexity = 141.47, Entropy = 7.14 bits
Computation based on 1125 words.
Number of 2-grams hit = 1094  (97.24%)
Number of 1-grams hit = 31  (2.76%)
5 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article328.text
Will force inclusive back-off from OOVs.
Perplexity = 101.38, Entropy = 6.66 bits
Computation based on 250 words.
Number of 2-grams hit = 245  (98.00%)
Number of 1-grams hit = 5  (2.00%)
1 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article329.text
Will force inclusive back-off from OOVs.
Perplexity = 118.39, Entropy = 6.89 bits
Computation based on 413 words.
Number of 2-grams hit = 405  (98.06%)
Number of 1-grams hit = 8  (1.94%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article330.text
Will force inclusive back-off from OOVs.
Perplexity = 124.56, Entropy = 6.96 bits
Computation based on 1051 words.
Number of 2-grams hit = 1018  (96.86%)
Number of 1-grams hit = 33  (3.14%)
8 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article331.text
Will force inclusive back-off from OOVs.
Perplexity = 131.83, Entropy = 7.04 bits
Computation based on 1948 words.
Number of 2-grams hit = 1876  (96.30%)
Number of 1-grams hit = 72  (3.70%)
10 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article332.text
Will force inclusive back-off from OOVs.
Perplexity = 113.86, Entropy = 6.83 bits
Computation based on 575 words.
Number of 2-grams hit = 554  (96.35%)
Number of 1-grams hit = 21  (3.65%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article333.text
Will force inclusive back-off from OOVs.
Perplexity = 171.37, Entropy = 7.42 bits
Computation based on 2146 words.
Number of 2-grams hit = 2031  (94.64%)
Number of 1-grams hit = 115  (5.36%)
7 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article334.text
Will force inclusive back-off from OOVs.
Perplexity = 155.11, Entropy = 7.28 bits
Computation based on 961 words.
Number of 2-grams hit = 918  (95.53%)
Number of 1-grams hit = 43  (4.47%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article335.text
Will force inclusive back-off from OOVs.
Perplexity = 127.99, Entropy = 7.00 bits
Computation based on 1753 words.
Number of 2-grams hit = 1705  (97.26%)
Number of 1-grams hit = 48  (2.74%)
8 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article336.text
Will force inclusive back-off from OOVs.
Perplexity = 126.05, Entropy = 6.98 bits
Computation based on 1140 words.
Number of 2-grams hit = 1103  (96.75%)
Number of 1-grams hit = 37  (3.25%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article337.text
Will force inclusive back-off from OOVs.
Perplexity = 123.02, Entropy = 6.94 bits
Computation based on 399 words.
Number of 2-grams hit = 386  (96.74%)
Number of 1-grams hit = 13  (3.26%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article338.text
Will force inclusive back-off from OOVs.
Perplexity = 210.97, Entropy = 7.72 bits
Computation based on 3481 words.
Number of 2-grams hit = 3268  (93.88%)
Number of 1-grams hit = 213  (6.12%)
91 OOVs (2.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article339.text
Will force inclusive back-off from OOVs.
Perplexity = 137.59, Entropy = 7.10 bits
Computation based on 373 words.
Number of 2-grams hit = 362  (97.05%)
Number of 1-grams hit = 11  (2.95%)
3 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article340.text
Will force inclusive back-off from OOVs.
Perplexity = 148.27, Entropy = 7.21 bits
Computation based on 1153 words.
Number of 2-grams hit = 1107  (96.01%)
Number of 1-grams hit = 46  (3.99%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article341.text
Will force inclusive back-off from OOVs.
Perplexity = 120.62, Entropy = 6.91 bits
Computation based on 1370 words.
Number of 2-grams hit = 1331  (97.15%)
Number of 1-grams hit = 39  (2.85%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article342.text
Will force inclusive back-off from OOVs.
Perplexity = 97.52, Entropy = 6.61 bits
Computation based on 1442 words.
Number of 2-grams hit = 1393  (96.60%)
Number of 1-grams hit = 49  (3.40%)
9 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article343.text
Will force inclusive back-off from OOVs.
Perplexity = 105.44, Entropy = 6.72 bits
Computation based on 477 words.
Number of 2-grams hit = 468  (98.11%)
Number of 1-grams hit = 9  (1.89%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article344.text
Will force inclusive back-off from OOVs.
Perplexity = 130.00, Entropy = 7.02 bits
Computation based on 581 words.
Number of 2-grams hit = 564  (97.07%)
Number of 1-grams hit = 17  (2.93%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article345.text
Will force inclusive back-off from OOVs.
Perplexity = 144.72, Entropy = 7.18 bits
Computation based on 3153 words.
Number of 2-grams hit = 3017  (95.69%)
Number of 1-grams hit = 136  (4.31%)
11 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article346.text
Will force inclusive back-off from OOVs.
Perplexity = 127.57, Entropy = 7.00 bits
Computation based on 1581 words.
Number of 2-grams hit = 1525  (96.46%)
Number of 1-grams hit = 56  (3.54%)
4 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article347.text
Will force inclusive back-off from OOVs.
Perplexity = 159.19, Entropy = 7.31 bits
Computation based on 647 words.
Number of 2-grams hit = 622  (96.14%)
Number of 1-grams hit = 25  (3.86%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article348.text
Will force inclusive back-off from OOVs.
Perplexity = 141.61, Entropy = 7.15 bits
Computation based on 2087 words.
Number of 2-grams hit = 1987  (95.21%)
Number of 1-grams hit = 100  (4.79%)
11 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article349.text
Will force inclusive back-off from OOVs.
Perplexity = 118.06, Entropy = 6.88 bits
Computation based on 1922 words.
Number of 2-grams hit = 1871  (97.35%)
Number of 1-grams hit = 51  (2.65%)
19 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article350.text
Will force inclusive back-off from OOVs.
Perplexity = 169.50, Entropy = 7.41 bits
Computation based on 357 words.
Number of 2-grams hit = 337  (94.40%)
Number of 1-grams hit = 20  (5.60%)
4 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article351.text
Will force inclusive back-off from OOVs.
Perplexity = 123.82, Entropy = 6.95 bits
Computation based on 432 words.
Number of 2-grams hit = 416  (96.30%)
Number of 1-grams hit = 16  (3.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article352.text
Will force inclusive back-off from OOVs.
Perplexity = 207.69, Entropy = 7.70 bits
Computation based on 446 words.
Number of 2-grams hit = 411  (92.15%)
Number of 1-grams hit = 35  (7.85%)
5 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article353.text
Will force inclusive back-off from OOVs.
Perplexity = 126.52, Entropy = 6.98 bits
Computation based on 425 words.
Number of 2-grams hit = 408  (96.00%)
Number of 1-grams hit = 17  (4.00%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article354.text
Will force inclusive back-off from OOVs.
Perplexity = 158.50, Entropy = 7.31 bits
Computation based on 508 words.
Number of 2-grams hit = 475  (93.50%)
Number of 1-grams hit = 33  (6.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article355.text
Will force inclusive back-off from OOVs.
Perplexity = 187.73, Entropy = 7.55 bits
Computation based on 407 words.
Number of 2-grams hit = 382  (93.86%)
Number of 1-grams hit = 25  (6.14%)
6 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article356.text
Will force inclusive back-off from OOVs.
Perplexity = 216.88, Entropy = 7.76 bits
Computation based on 691 words.
Number of 2-grams hit = 656  (94.93%)
Number of 1-grams hit = 35  (5.07%)
18 OOVs (2.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article357.text
Will force inclusive back-off from OOVs.
Perplexity = 89.13, Entropy = 6.48 bits
Computation based on 747 words.
Number of 2-grams hit = 731  (97.86%)
Number of 1-grams hit = 16  (2.14%)
8 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article358.text
Will force inclusive back-off from OOVs.
Perplexity = 210.14, Entropy = 7.72 bits
Computation based on 1164 words.
Number of 2-grams hit = 1089  (93.56%)
Number of 1-grams hit = 75  (6.44%)
33 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article359.text
Will force inclusive back-off from OOVs.
Perplexity = 136.10, Entropy = 7.09 bits
Computation based on 1254 words.
Number of 2-grams hit = 1229  (98.01%)
Number of 1-grams hit = 25  (1.99%)
16 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article360.text
Will force inclusive back-off from OOVs.
Perplexity = 138.50, Entropy = 7.11 bits
Computation based on 710 words.
Number of 2-grams hit = 686  (96.62%)
Number of 1-grams hit = 24  (3.38%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article361.text
Will force inclusive back-off from OOVs.
Perplexity = 157.45, Entropy = 7.30 bits
Computation based on 1080 words.
Number of 2-grams hit = 1020  (94.44%)
Number of 1-grams hit = 60  (5.56%)
5 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article362.text
Will force inclusive back-off from OOVs.
Perplexity = 116.56, Entropy = 6.86 bits
Computation based on 1417 words.
Number of 2-grams hit = 1371  (96.75%)
Number of 1-grams hit = 46  (3.25%)
6 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article363.text
Will force inclusive back-off from OOVs.
Perplexity = 143.02, Entropy = 7.16 bits
Computation based on 1157 words.
Number of 2-grams hit = 1100  (95.07%)
Number of 1-grams hit = 57  (4.93%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article364.text
Will force inclusive back-off from OOVs.
Perplexity = 138.28, Entropy = 7.11 bits
Computation based on 326 words.
Number of 2-grams hit = 315  (96.63%)
Number of 1-grams hit = 11  (3.37%)
3 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article365.text
Will force inclusive back-off from OOVs.
Perplexity = 126.53, Entropy = 6.98 bits
Computation based on 405 words.
Number of 2-grams hit = 394  (97.28%)
Number of 1-grams hit = 11  (2.72%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article366.text
Will force inclusive back-off from OOVs.
Perplexity = 248.41, Entropy = 7.96 bits
Computation based on 345 words.
Number of 2-grams hit = 317  (91.88%)
Number of 1-grams hit = 28  (8.12%)
6 OOVs (1.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article367.text
Will force inclusive back-off from OOVs.
Perplexity = 88.92, Entropy = 6.47 bits
Computation based on 906 words.
Number of 2-grams hit = 876  (96.69%)
Number of 1-grams hit = 30  (3.31%)
5 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article368.text
Will force inclusive back-off from OOVs.
Perplexity = 224.56, Entropy = 7.81 bits
Computation based on 320 words.
Number of 2-grams hit = 299  (93.44%)
Number of 1-grams hit = 21  (6.56%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article369.text
Will force inclusive back-off from OOVs.
Perplexity = 110.90, Entropy = 6.79 bits
Computation based on 316 words.
Number of 2-grams hit = 304  (96.20%)
Number of 1-grams hit = 12  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article370.text
Will force inclusive back-off from OOVs.
Perplexity = 135.85, Entropy = 7.09 bits
Computation based on 1644 words.
Number of 2-grams hit = 1579  (96.05%)
Number of 1-grams hit = 65  (3.95%)
3 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article371.text
Will force inclusive back-off from OOVs.
Perplexity = 155.90, Entropy = 7.28 bits
Computation based on 262 words.
Number of 2-grams hit = 244  (93.13%)
Number of 1-grams hit = 18  (6.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article372.text
Will force inclusive back-off from OOVs.
Perplexity = 139.63, Entropy = 7.13 bits
Computation based on 405 words.
Number of 2-grams hit = 387  (95.56%)
Number of 1-grams hit = 18  (4.44%)
16 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article373.text
Will force inclusive back-off from OOVs.
Perplexity = 112.12, Entropy = 6.81 bits
Computation based on 505 words.
Number of 2-grams hit = 493  (97.62%)
Number of 1-grams hit = 12  (2.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article374.text
Will force inclusive back-off from OOVs.
Perplexity = 119.55, Entropy = 6.90 bits
Computation based on 1521 words.
Number of 2-grams hit = 1464  (96.25%)
Number of 1-grams hit = 57  (3.75%)
2 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article375.text
Will force inclusive back-off from OOVs.
Perplexity = 86.41, Entropy = 6.43 bits
Computation based on 1608 words.
Number of 2-grams hit = 1576  (98.01%)
Number of 1-grams hit = 32  (1.99%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article376.text
Will force inclusive back-off from OOVs.
Perplexity = 150.35, Entropy = 7.23 bits
Computation based on 560 words.
Number of 2-grams hit = 534  (95.36%)
Number of 1-grams hit = 26  (4.64%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article377.text
Will force inclusive back-off from OOVs.
Perplexity = 153.61, Entropy = 7.26 bits
Computation based on 738 words.
Number of 2-grams hit = 698  (94.58%)
Number of 1-grams hit = 40  (5.42%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article378.text
Will force inclusive back-off from OOVs.
Perplexity = 258.22, Entropy = 8.01 bits
Computation based on 609 words.
Number of 2-grams hit = 569  (93.43%)
Number of 1-grams hit = 40  (6.57%)
13 OOVs (2.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article379.text
Will force inclusive back-off from OOVs.
Perplexity = 126.97, Entropy = 6.99 bits
Computation based on 2558 words.
Number of 2-grams hit = 2472  (96.64%)
Number of 1-grams hit = 86  (3.36%)
10 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article380.text
Will force inclusive back-off from OOVs.
Perplexity = 165.99, Entropy = 7.37 bits
Computation based on 346 words.
Number of 2-grams hit = 329  (95.09%)
Number of 1-grams hit = 17  (4.91%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article381.text
Will force inclusive back-off from OOVs.
Perplexity = 112.53, Entropy = 6.81 bits
Computation based on 622 words.
Number of 2-grams hit = 611  (98.23%)
Number of 1-grams hit = 11  (1.77%)
6 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article382.text
Will force inclusive back-off from OOVs.
Perplexity = 120.35, Entropy = 6.91 bits
Computation based on 1302 words.
Number of 2-grams hit = 1256  (96.47%)
Number of 1-grams hit = 46  (3.53%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article383.text
Will force inclusive back-off from OOVs.
Perplexity = 146.62, Entropy = 7.20 bits
Computation based on 1031 words.
Number of 2-grams hit = 977  (94.76%)
Number of 1-grams hit = 54  (5.24%)
8 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article384.text
Will force inclusive back-off from OOVs.
Perplexity = 159.61, Entropy = 7.32 bits
Computation based on 1005 words.
Number of 2-grams hit = 959  (95.42%)
Number of 1-grams hit = 46  (4.58%)
3 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article385.text
Will force inclusive back-off from OOVs.
Perplexity = 124.74, Entropy = 6.96 bits
Computation based on 2592 words.
Number of 2-grams hit = 2573  (99.27%)
Number of 1-grams hit = 19  (0.73%)
14 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article386.text
Will force inclusive back-off from OOVs.
Perplexity = 148.42, Entropy = 7.21 bits
Computation based on 405 words.
Number of 2-grams hit = 391  (96.54%)
Number of 1-grams hit = 14  (3.46%)
12 OOVs (2.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article387.text
Will force inclusive back-off from OOVs.
Perplexity = 135.20, Entropy = 7.08 bits
Computation based on 1197 words.
Number of 2-grams hit = 1150  (96.07%)
Number of 1-grams hit = 47  (3.93%)
18 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article388.text
Will force inclusive back-off from OOVs.
Perplexity = 138.93, Entropy = 7.12 bits
Computation based on 4328 words.
Number of 2-grams hit = 4133  (95.49%)
Number of 1-grams hit = 195  (4.51%)
11 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article389.text
Will force inclusive back-off from OOVs.
Perplexity = 180.02, Entropy = 7.49 bits
Computation based on 1099 words.
Number of 2-grams hit = 1042  (94.81%)
Number of 1-grams hit = 57  (5.19%)
22 OOVs (1.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article390.text
Will force inclusive back-off from OOVs.
Perplexity = 150.11, Entropy = 7.23 bits
Computation based on 1177 words.
Number of 2-grams hit = 1134  (96.35%)
Number of 1-grams hit = 43  (3.65%)
23 OOVs (1.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article391.text
Will force inclusive back-off from OOVs.
Perplexity = 118.97, Entropy = 6.89 bits
Computation based on 3505 words.
Number of 2-grams hit = 3384  (96.55%)
Number of 1-grams hit = 121  (3.45%)
36 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article392.text
Will force inclusive back-off from OOVs.
Perplexity = 97.66, Entropy = 6.61 bits
Computation based on 231 words.
Number of 2-grams hit = 226  (97.84%)
Number of 1-grams hit = 5  (2.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article393.text
Will force inclusive back-off from OOVs.
Perplexity = 235.35, Entropy = 7.88 bits
Computation based on 481 words.
Number of 2-grams hit = 441  (91.68%)
Number of 1-grams hit = 40  (8.32%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article394.text
Will force inclusive back-off from OOVs.
Perplexity = 206.07, Entropy = 7.69 bits
Computation based on 390 words.
Number of 2-grams hit = 361  (92.56%)
Number of 1-grams hit = 29  (7.44%)
5 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article395.text
Will force inclusive back-off from OOVs.
Perplexity = 115.67, Entropy = 6.85 bits
Computation based on 304 words.
Number of 2-grams hit = 298  (98.03%)
Number of 1-grams hit = 6  (1.97%)
3 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article396.text
Will force inclusive back-off from OOVs.
Perplexity = 82.12, Entropy = 6.36 bits
Computation based on 3049 words.
Number of 2-grams hit = 2991  (98.10%)
Number of 1-grams hit = 58  (1.90%)
3 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article397.text
Will force inclusive back-off from OOVs.
Perplexity = 150.23, Entropy = 7.23 bits
Computation based on 3657 words.
Number of 2-grams hit = 3476  (95.05%)
Number of 1-grams hit = 181  (4.95%)
16 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article398.text
Will force inclusive back-off from OOVs.
Perplexity = 145.17, Entropy = 7.18 bits
Computation based on 815 words.
Number of 2-grams hit = 783  (96.07%)
Number of 1-grams hit = 32  (3.93%)
5 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article399.text
Will force inclusive back-off from OOVs.
Perplexity = 143.87, Entropy = 7.17 bits
Computation based on 3657 words.
Number of 2-grams hit = 3511  (96.01%)
Number of 1-grams hit = 146  (3.99%)
15 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article400.text
Will force inclusive back-off from OOVs.
Perplexity = 141.46, Entropy = 7.14 bits
Computation based on 582 words.
Number of 2-grams hit = 563  (96.74%)
Number of 1-grams hit = 19  (3.26%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article401.text
Will force inclusive back-off from OOVs.
Perplexity = 198.61, Entropy = 7.63 bits
Computation based on 652 words.
Number of 2-grams hit = 614  (94.17%)
Number of 1-grams hit = 38  (5.83%)
11 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article402.text
Will force inclusive back-off from OOVs.
Perplexity = 145.23, Entropy = 7.18 bits
Computation based on 3662 words.
Number of 2-grams hit = 3526  (96.29%)
Number of 1-grams hit = 136  (3.71%)
8 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article403.text
Will force inclusive back-off from OOVs.
Perplexity = 162.52, Entropy = 7.34 bits
Computation based on 621 words.
Number of 2-grams hit = 597  (96.14%)
Number of 1-grams hit = 24  (3.86%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article404.text
Will force inclusive back-off from OOVs.
Perplexity = 184.72, Entropy = 7.53 bits
Computation based on 213 words.
Number of 2-grams hit = 198  (92.96%)
Number of 1-grams hit = 15  (7.04%)
1 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article405.text
Will force inclusive back-off from OOVs.
Perplexity = 234.80, Entropy = 7.88 bits
Computation based on 159 words.
Number of 2-grams hit = 147  (92.45%)
Number of 1-grams hit = 12  (7.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article406.text
Will force inclusive back-off from OOVs.
Perplexity = 182.47, Entropy = 7.51 bits
Computation based on 982 words.
Number of 2-grams hit = 932  (94.91%)
Number of 1-grams hit = 50  (5.09%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article407.text
Will force inclusive back-off from OOVs.
Perplexity = 140.24, Entropy = 7.13 bits
Computation based on 696 words.
Number of 2-grams hit = 663  (95.26%)
Number of 1-grams hit = 33  (4.74%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article408.text
Will force inclusive back-off from OOVs.
Perplexity = 155.30, Entropy = 7.28 bits
Computation based on 438 words.
Number of 2-grams hit = 418  (95.43%)
Number of 1-grams hit = 20  (4.57%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article409.text
Will force inclusive back-off from OOVs.
Perplexity = 140.91, Entropy = 7.14 bits
Computation based on 541 words.
Number of 2-grams hit = 518  (95.75%)
Number of 1-grams hit = 23  (4.25%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article410.text
Will force inclusive back-off from OOVs.
Perplexity = 88.85, Entropy = 6.47 bits
Computation based on 235 words.
Number of 2-grams hit = 229  (97.45%)
Number of 1-grams hit = 6  (2.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article411.text
Will force inclusive back-off from OOVs.
Perplexity = 150.01, Entropy = 7.23 bits
Computation based on 436 words.
Number of 2-grams hit = 417  (95.64%)
Number of 1-grams hit = 19  (4.36%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article412.text
Will force inclusive back-off from OOVs.
Perplexity = 96.45, Entropy = 6.59 bits
Computation based on 4559 words.
Number of 2-grams hit = 4481  (98.29%)
Number of 1-grams hit = 78  (1.71%)
9 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article413.text
Will force inclusive back-off from OOVs.
Perplexity = 136.49, Entropy = 7.09 bits
Computation based on 3853 words.
Number of 2-grams hit = 3693  (95.85%)
Number of 1-grams hit = 160  (4.15%)
14 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article414.text
Will force inclusive back-off from OOVs.
Perplexity = 131.62, Entropy = 7.04 bits
Computation based on 399 words.
Number of 2-grams hit = 381  (95.49%)
Number of 1-grams hit = 18  (4.51%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article415.text
Will force inclusive back-off from OOVs.
Perplexity = 217.35, Entropy = 7.76 bits
Computation based on 475 words.
Number of 2-grams hit = 445  (93.68%)
Number of 1-grams hit = 30  (6.32%)
5 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article416.text
Will force inclusive back-off from OOVs.
Perplexity = 191.12, Entropy = 7.58 bits
Computation based on 585 words.
Number of 2-grams hit = 560  (95.73%)
Number of 1-grams hit = 25  (4.27%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article417.text
Will force inclusive back-off from OOVs.
Perplexity = 111.25, Entropy = 6.80 bits
Computation based on 850 words.
Number of 2-grams hit = 835  (98.24%)
Number of 1-grams hit = 15  (1.76%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article418.text
Will force inclusive back-off from OOVs.
Perplexity = 190.62, Entropy = 7.57 bits
Computation based on 807 words.
Number of 2-grams hit = 760  (94.18%)
Number of 1-grams hit = 47  (5.82%)
5 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article419.text
Will force inclusive back-off from OOVs.
Perplexity = 191.65, Entropy = 7.58 bits
Computation based on 549 words.
Number of 2-grams hit = 525  (95.63%)
Number of 1-grams hit = 24  (4.37%)
5 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article420.text
Will force inclusive back-off from OOVs.
Perplexity = 145.63, Entropy = 7.19 bits
Computation based on 610 words.
Number of 2-grams hit = 587  (96.23%)
Number of 1-grams hit = 23  (3.77%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article421.text
Will force inclusive back-off from OOVs.
Perplexity = 111.39, Entropy = 6.80 bits
Computation based on 819 words.
Number of 2-grams hit = 795  (97.07%)
Number of 1-grams hit = 24  (2.93%)
2 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article422.text
Will force inclusive back-off from OOVs.
Perplexity = 153.51, Entropy = 7.26 bits
Computation based on 551 words.
Number of 2-grams hit = 517  (93.83%)
Number of 1-grams hit = 34  (6.17%)
8 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article423.text
Will force inclusive back-off from OOVs.
Perplexity = 257.14, Entropy = 8.01 bits
Computation based on 599 words.
Number of 2-grams hit = 559  (93.32%)
Number of 1-grams hit = 40  (6.68%)
17 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article424.text
Will force inclusive back-off from OOVs.
Perplexity = 121.86, Entropy = 6.93 bits
Computation based on 524 words.
Number of 2-grams hit = 505  (96.37%)
Number of 1-grams hit = 19  (3.63%)
5 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article425.text
Will force inclusive back-off from OOVs.
Perplexity = 128.69, Entropy = 7.01 bits
Computation based on 355 words.
Number of 2-grams hit = 342  (96.34%)
Number of 1-grams hit = 13  (3.66%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article426.text
Will force inclusive back-off from OOVs.
Perplexity = 111.93, Entropy = 6.81 bits
Computation based on 484 words.
Number of 2-grams hit = 466  (96.28%)
Number of 1-grams hit = 18  (3.72%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article427.text
Will force inclusive back-off from OOVs.
Perplexity = 133.33, Entropy = 7.06 bits
Computation based on 787 words.
Number of 2-grams hit = 759  (96.44%)
Number of 1-grams hit = 28  (3.56%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article428.text
Will force inclusive back-off from OOVs.
Perplexity = 192.69, Entropy = 7.59 bits
Computation based on 626 words.
Number of 2-grams hit = 594  (94.89%)
Number of 1-grams hit = 32  (5.11%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article429.text
Will force inclusive back-off from OOVs.
Perplexity = 132.28, Entropy = 7.05 bits
Computation based on 769 words.
Number of 2-grams hit = 747  (97.14%)
Number of 1-grams hit = 22  (2.86%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article430.text
Will force inclusive back-off from OOVs.
Perplexity = 139.77, Entropy = 7.13 bits
Computation based on 639 words.
Number of 2-grams hit = 614  (96.09%)
Number of 1-grams hit = 25  (3.91%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article431.text
Will force inclusive back-off from OOVs.
Perplexity = 114.46, Entropy = 6.84 bits
Computation based on 569 words.
Number of 2-grams hit = 548  (96.31%)
Number of 1-grams hit = 21  (3.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article432.text
Will force inclusive back-off from OOVs.
Perplexity = 144.81, Entropy = 7.18 bits
Computation based on 351 words.
Number of 2-grams hit = 338  (96.30%)
Number of 1-grams hit = 13  (3.70%)
2 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article433.text
Will force inclusive back-off from OOVs.
Perplexity = 125.83, Entropy = 6.98 bits
Computation based on 967 words.
Number of 2-grams hit = 932  (96.38%)
Number of 1-grams hit = 35  (3.62%)
5 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article434.text
Will force inclusive back-off from OOVs.
Perplexity = 133.76, Entropy = 7.06 bits
Computation based on 381 words.
Number of 2-grams hit = 365  (95.80%)
Number of 1-grams hit = 16  (4.20%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article435.text
Will force inclusive back-off from OOVs.
Perplexity = 101.79, Entropy = 6.67 bits
Computation based on 711 words.
Number of 2-grams hit = 690  (97.05%)
Number of 1-grams hit = 21  (2.95%)
8 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article436.text
Will force inclusive back-off from OOVs.
Perplexity = 139.68, Entropy = 7.13 bits
Computation based on 531 words.
Number of 2-grams hit = 511  (96.23%)
Number of 1-grams hit = 20  (3.77%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article437.text
Will force inclusive back-off from OOVs.
Perplexity = 155.90, Entropy = 7.28 bits
Computation based on 1007 words.
Number of 2-grams hit = 957  (95.03%)
Number of 1-grams hit = 50  (4.97%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article438.text
Will force inclusive back-off from OOVs.
Perplexity = 142.87, Entropy = 7.16 bits
Computation based on 290 words.
Number of 2-grams hit = 273  (94.14%)
Number of 1-grams hit = 17  (5.86%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article439.text
Will force inclusive back-off from OOVs.
Perplexity = 225.52, Entropy = 7.82 bits
Computation based on 516 words.
Number of 2-grams hit = 483  (93.60%)
Number of 1-grams hit = 33  (6.40%)
8 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article440.text
Will force inclusive back-off from OOVs.
Perplexity = 121.52, Entropy = 6.93 bits
Computation based on 688 words.
Number of 2-grams hit = 666  (96.80%)
Number of 1-grams hit = 22  (3.20%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article441.text
Will force inclusive back-off from OOVs.
Perplexity = 204.82, Entropy = 7.68 bits
Computation based on 400 words.
Number of 2-grams hit = 378  (94.50%)
Number of 1-grams hit = 22  (5.50%)
13 OOVs (3.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article442.text
Will force inclusive back-off from OOVs.
Perplexity = 85.85, Entropy = 6.42 bits
Computation based on 10008 words.
Number of 2-grams hit = 9806  (97.98%)
Number of 1-grams hit = 202  (2.02%)
23 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article443.text
Will force inclusive back-off from OOVs.
Perplexity = 142.73, Entropy = 7.16 bits
Computation based on 11815 words.
Number of 2-grams hit = 11314  (95.76%)
Number of 1-grams hit = 501  (4.24%)
39 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article444.text
Will force inclusive back-off from OOVs.
Perplexity = 156.46, Entropy = 7.29 bits
Computation based on 643 words.
Number of 2-grams hit = 615  (95.65%)
Number of 1-grams hit = 28  (4.35%)
8 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article445.text
Will force inclusive back-off from OOVs.
Perplexity = 100.23, Entropy = 6.65 bits
Computation based on 501 words.
Number of 2-grams hit = 482  (96.21%)
Number of 1-grams hit = 19  (3.79%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article446.text
Will force inclusive back-off from OOVs.
Perplexity = 155.25, Entropy = 7.28 bits
Computation based on 894 words.
Number of 2-grams hit = 861  (96.31%)
Number of 1-grams hit = 33  (3.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article447.text
Will force inclusive back-off from OOVs.
Perplexity = 141.02, Entropy = 7.14 bits
Computation based on 722 words.
Number of 2-grams hit = 690  (95.57%)
Number of 1-grams hit = 32  (4.43%)
9 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article448.text
Will force inclusive back-off from OOVs.
Perplexity = 89.26, Entropy = 6.48 bits
Computation based on 323 words.
Number of 2-grams hit = 317  (98.14%)
Number of 1-grams hit = 6  (1.86%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article449.text
Will force inclusive back-off from OOVs.
Perplexity = 148.12, Entropy = 7.21 bits
Computation based on 483 words.
Number of 2-grams hit = 460  (95.24%)
Number of 1-grams hit = 23  (4.76%)
5 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article450.text
Will force inclusive back-off from OOVs.
Perplexity = 138.46, Entropy = 7.11 bits
Computation based on 356 words.
Number of 2-grams hit = 347  (97.47%)
Number of 1-grams hit = 9  (2.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article451.text
Will force inclusive back-off from OOVs.
Perplexity = 151.13, Entropy = 7.24 bits
Computation based on 571 words.
Number of 2-grams hit = 544  (95.27%)
Number of 1-grams hit = 27  (4.73%)
3 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article452.text
Will force inclusive back-off from OOVs.
Perplexity = 129.59, Entropy = 7.02 bits
Computation based on 237 words.
Number of 2-grams hit = 224  (94.51%)
Number of 1-grams hit = 13  (5.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article453.text
Will force inclusive back-off from OOVs.
Perplexity = 165.12, Entropy = 7.37 bits
Computation based on 625 words.
Number of 2-grams hit = 600  (96.00%)
Number of 1-grams hit = 25  (4.00%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article454.text
Will force inclusive back-off from OOVs.
Perplexity = 144.31, Entropy = 7.17 bits
Computation based on 544 words.
Number of 2-grams hit = 521  (95.77%)
Number of 1-grams hit = 23  (4.23%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article455.text
Will force inclusive back-off from OOVs.
Perplexity = 138.46, Entropy = 7.11 bits
Computation based on 379 words.
Number of 2-grams hit = 368  (97.10%)
Number of 1-grams hit = 11  (2.90%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article456.text
Will force inclusive back-off from OOVs.
Perplexity = 157.55, Entropy = 7.30 bits
Computation based on 656 words.
Number of 2-grams hit = 626  (95.43%)
Number of 1-grams hit = 30  (4.57%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article457.text
Will force inclusive back-off from OOVs.
Perplexity = 161.85, Entropy = 7.34 bits
Computation based on 337 words.
Number of 2-grams hit = 321  (95.25%)
Number of 1-grams hit = 16  (4.75%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article458.text
Will force inclusive back-off from OOVs.
Perplexity = 125.56, Entropy = 6.97 bits
Computation based on 276 words.
Number of 2-grams hit = 267  (96.74%)
Number of 1-grams hit = 9  (3.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article459.text
Will force inclusive back-off from OOVs.
Perplexity = 131.95, Entropy = 7.04 bits
Computation based on 700 words.
Number of 2-grams hit = 668  (95.43%)
Number of 1-grams hit = 32  (4.57%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article460.text
Will force inclusive back-off from OOVs.
Perplexity = 146.36, Entropy = 7.19 bits
Computation based on 326 words.
Number of 2-grams hit = 311  (95.40%)
Number of 1-grams hit = 15  (4.60%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article461.text
Will force inclusive back-off from OOVs.
Perplexity = 202.66, Entropy = 7.66 bits
Computation based on 419 words.
Number of 2-grams hit = 396  (94.51%)
Number of 1-grams hit = 23  (5.49%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article462.text
Will force inclusive back-off from OOVs.
Perplexity = 164.46, Entropy = 7.36 bits
Computation based on 1406 words.
Number of 2-grams hit = 1337  (95.09%)
Number of 1-grams hit = 69  (4.91%)
4 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article463.text
Will force inclusive back-off from OOVs.
Perplexity = 165.13, Entropy = 7.37 bits
Computation based on 543 words.
Number of 2-grams hit = 521  (95.95%)
Number of 1-grams hit = 22  (4.05%)
12 OOVs (2.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article464.text
Will force inclusive back-off from OOVs.
Perplexity = 188.40, Entropy = 7.56 bits
Computation based on 421 words.
Number of 2-grams hit = 400  (95.01%)
Number of 1-grams hit = 21  (4.99%)
19 OOVs (4.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article465.text
Will force inclusive back-off from OOVs.
Perplexity = 94.90, Entropy = 6.57 bits
Computation based on 1805 words.
Number of 2-grams hit = 1766  (97.84%)
Number of 1-grams hit = 39  (2.16%)
2 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article466.text
Will force inclusive back-off from OOVs.
Perplexity = 137.97, Entropy = 7.11 bits
Computation based on 204 words.
Number of 2-grams hit = 195  (95.59%)
Number of 1-grams hit = 9  (4.41%)
1 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article467.text
Will force inclusive back-off from OOVs.
Perplexity = 114.12, Entropy = 6.83 bits
Computation based on 1028 words.
Number of 2-grams hit = 992  (96.50%)
Number of 1-grams hit = 36  (3.50%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article468.text
Will force inclusive back-off from OOVs.
Perplexity = 147.51, Entropy = 7.20 bits
Computation based on 776 words.
Number of 2-grams hit = 738  (95.10%)
Number of 1-grams hit = 38  (4.90%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article469.text
Will force inclusive back-off from OOVs.
Perplexity = 153.54, Entropy = 7.26 bits
Computation based on 355 words.
Number of 2-grams hit = 334  (94.08%)
Number of 1-grams hit = 21  (5.92%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article470.text
Will force inclusive back-off from OOVs.
Perplexity = 297.50, Entropy = 8.22 bits
Computation based on 235 words.
Number of 2-grams hit = 215  (91.49%)
Number of 1-grams hit = 20  (8.51%)
5 OOVs (2.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article471.text
Will force inclusive back-off from OOVs.
Perplexity = 129.86, Entropy = 7.02 bits
Computation based on 1994 words.
Number of 2-grams hit = 1922  (96.39%)
Number of 1-grams hit = 72  (3.61%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article472.text
Will force inclusive back-off from OOVs.
Perplexity = 212.35, Entropy = 7.73 bits
Computation based on 930 words.
Number of 2-grams hit = 871  (93.66%)
Number of 1-grams hit = 59  (6.34%)
6 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article473.text
Will force inclusive back-off from OOVs.
Perplexity = 108.37, Entropy = 6.76 bits
Computation based on 219 words.
Number of 2-grams hit = 207  (94.52%)
Number of 1-grams hit = 12  (5.48%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article474.text
Will force inclusive back-off from OOVs.
Perplexity = 157.72, Entropy = 7.30 bits
Computation based on 775 words.
Number of 2-grams hit = 741  (95.61%)
Number of 1-grams hit = 34  (4.39%)
9 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article475.text
Will force inclusive back-off from OOVs.
Perplexity = 170.13, Entropy = 7.41 bits
Computation based on 546 words.
Number of 2-grams hit = 526  (96.34%)
Number of 1-grams hit = 20  (3.66%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article476.text
Will force inclusive back-off from OOVs.
Perplexity = 202.25, Entropy = 7.66 bits
Computation based on 582 words.
Number of 2-grams hit = 544  (93.47%)
Number of 1-grams hit = 38  (6.53%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article477.text
Will force inclusive back-off from OOVs.
Perplexity = 138.27, Entropy = 7.11 bits
Computation based on 444 words.
Number of 2-grams hit = 422  (95.05%)
Number of 1-grams hit = 22  (4.95%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article478.text
Will force inclusive back-off from OOVs.
Perplexity = 188.36, Entropy = 7.56 bits
Computation based on 507 words.
Number of 2-grams hit = 475  (93.69%)
Number of 1-grams hit = 32  (6.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article479.text
Will force inclusive back-off from OOVs.
Perplexity = 149.47, Entropy = 7.22 bits
Computation based on 421 words.
Number of 2-grams hit = 406  (96.44%)
Number of 1-grams hit = 15  (3.56%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article480.text
Will force inclusive back-off from OOVs.
Perplexity = 112.35, Entropy = 6.81 bits
Computation based on 684 words.
Number of 2-grams hit = 665  (97.22%)
Number of 1-grams hit = 19  (2.78%)
5 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article481.text
Will force inclusive back-off from OOVs.
Perplexity = 154.53, Entropy = 7.27 bits
Computation based on 349 words.
Number of 2-grams hit = 332  (95.13%)
Number of 1-grams hit = 17  (4.87%)
2 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article482.text
Will force inclusive back-off from OOVs.
Perplexity = 102.67, Entropy = 6.68 bits
Computation based on 494 words.
Number of 2-grams hit = 478  (96.76%)
Number of 1-grams hit = 16  (3.24%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article483.text
Will force inclusive back-off from OOVs.
Perplexity = 100.88, Entropy = 6.66 bits
Computation based on 1664 words.
Number of 2-grams hit = 1611  (96.81%)
Number of 1-grams hit = 53  (3.19%)
12 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article484.text
Will force inclusive back-off from OOVs.
Perplexity = 154.86, Entropy = 7.27 bits
Computation based on 1427 words.
Number of 2-grams hit = 1356  (95.02%)
Number of 1-grams hit = 71  (4.98%)
5 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article485.text
Will force inclusive back-off from OOVs.
Perplexity = 316.00, Entropy = 8.30 bits
Computation based on 372 words.
Number of 2-grams hit = 336  (90.32%)
Number of 1-grams hit = 36  (9.68%)
5 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article486.text
Will force inclusive back-off from OOVs.
Perplexity = 156.97, Entropy = 7.29 bits
Computation based on 331 words.
Number of 2-grams hit = 316  (95.47%)
Number of 1-grams hit = 15  (4.53%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article487.text
Will force inclusive back-off from OOVs.
Perplexity = 135.12, Entropy = 7.08 bits
Computation based on 429 words.
Number of 2-grams hit = 415  (96.74%)
Number of 1-grams hit = 14  (3.26%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article488.text
Will force inclusive back-off from OOVs.
Perplexity = 113.75, Entropy = 6.83 bits
Computation based on 710 words.
Number of 2-grams hit = 692  (97.46%)
Number of 1-grams hit = 18  (2.54%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article489.text
Will force inclusive back-off from OOVs.
Perplexity = 106.56, Entropy = 6.74 bits
Computation based on 537 words.
Number of 2-grams hit = 528  (98.32%)
Number of 1-grams hit = 9  (1.68%)
9 OOVs (1.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article490.text
Will force inclusive back-off from OOVs.
Perplexity = 215.34, Entropy = 7.75 bits
Computation based on 479 words.
Number of 2-grams hit = 448  (93.53%)
Number of 1-grams hit = 31  (6.47%)
14 OOVs (2.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article491.text
Will force inclusive back-off from OOVs.
Perplexity = 155.19, Entropy = 7.28 bits
Computation based on 1388 words.
Number of 2-grams hit = 1313  (94.60%)
Number of 1-grams hit = 75  (5.40%)
7 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article492.text
Will force inclusive back-off from OOVs.
Perplexity = 147.09, Entropy = 7.20 bits
Computation based on 433 words.
Number of 2-grams hit = 423  (97.69%)
Number of 1-grams hit = 10  (2.31%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article493.text
Will force inclusive back-off from OOVs.
Perplexity = 219.17, Entropy = 7.78 bits
Computation based on 759 words.
Number of 2-grams hit = 717  (94.47%)
Number of 1-grams hit = 42  (5.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article494.text
Will force inclusive back-off from OOVs.
Perplexity = 149.94, Entropy = 7.23 bits
Computation based on 485 words.
Number of 2-grams hit = 463  (95.46%)
Number of 1-grams hit = 22  (4.54%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article495.text
Will force inclusive back-off from OOVs.
Perplexity = 172.43, Entropy = 7.43 bits
Computation based on 426 words.
Number of 2-grams hit = 407  (95.54%)
Number of 1-grams hit = 19  (4.46%)
4 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article496.text
Will force inclusive back-off from OOVs.
Perplexity = 244.94, Entropy = 7.94 bits
Computation based on 546 words.
Number of 2-grams hit = 514  (94.14%)
Number of 1-grams hit = 32  (5.86%)
12 OOVs (2.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article497.text
Will force inclusive back-off from OOVs.
Perplexity = 144.45, Entropy = 7.17 bits
Computation based on 647 words.
Number of 2-grams hit = 609  (94.13%)
Number of 1-grams hit = 38  (5.87%)
5 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article498.text
Will force inclusive back-off from OOVs.
Perplexity = 279.18, Entropy = 8.13 bits
Computation based on 1659 words.
Number of 2-grams hit = 1514  (91.26%)
Number of 1-grams hit = 145  (8.74%)
40 OOVs (2.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article499.text
Will force inclusive back-off from OOVs.
Perplexity = 126.34, Entropy = 6.98 bits
Computation based on 495 words.
Number of 2-grams hit = 481  (97.17%)
Number of 1-grams hit = 14  (2.83%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article500.text
Will force inclusive back-off from OOVs.
Perplexity = 156.45, Entropy = 7.29 bits
Computation based on 457 words.
Number of 2-grams hit = 437  (95.62%)
Number of 1-grams hit = 20  (4.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article501.text
Will force inclusive back-off from OOVs.
Perplexity = 129.98, Entropy = 7.02 bits
Computation based on 403 words.
Number of 2-grams hit = 389  (96.53%)
Number of 1-grams hit = 14  (3.47%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article502.text
Will force inclusive back-off from OOVs.
Perplexity = 182.31, Entropy = 7.51 bits
Computation based on 505 words.
Number of 2-grams hit = 482  (95.45%)
Number of 1-grams hit = 23  (4.55%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article503.text
Will force inclusive back-off from OOVs.
Perplexity = 136.35, Entropy = 7.09 bits
Computation based on 1013 words.
Number of 2-grams hit = 970  (95.76%)
Number of 1-grams hit = 43  (4.24%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article504.text
Will force inclusive back-off from OOVs.
Perplexity = 144.64, Entropy = 7.18 bits
Computation based on 613 words.
Number of 2-grams hit = 592  (96.57%)
Number of 1-grams hit = 21  (3.43%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article505.text
Will force inclusive back-off from OOVs.
Perplexity = 191.72, Entropy = 7.58 bits
Computation based on 701 words.
Number of 2-grams hit = 661  (94.29%)
Number of 1-grams hit = 40  (5.71%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article506.text
Will force inclusive back-off from OOVs.
Perplexity = 205.35, Entropy = 7.68 bits
Computation based on 522 words.
Number of 2-grams hit = 476  (91.19%)
Number of 1-grams hit = 46  (8.81%)
17 OOVs (3.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article507.text
Will force inclusive back-off from OOVs.
Perplexity = 120.71, Entropy = 6.92 bits
Computation based on 1584 words.
Number of 2-grams hit = 1535  (96.91%)
Number of 1-grams hit = 49  (3.09%)
9 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article508.text
Will force inclusive back-off from OOVs.
Perplexity = 91.85, Entropy = 6.52 bits
Computation based on 655 words.
Number of 2-grams hit = 645  (98.47%)
Number of 1-grams hit = 10  (1.53%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article509.text
Will force inclusive back-off from OOVs.
Perplexity = 156.22, Entropy = 7.29 bits
Computation based on 353 words.
Number of 2-grams hit = 331  (93.77%)
Number of 1-grams hit = 22  (6.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article510.text
Will force inclusive back-off from OOVs.
Perplexity = 160.66, Entropy = 7.33 bits
Computation based on 968 words.
Number of 2-grams hit = 917  (94.73%)
Number of 1-grams hit = 51  (5.27%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article511.text
Will force inclusive back-off from OOVs.
Perplexity = 219.49, Entropy = 7.78 bits
Computation based on 623 words.
Number of 2-grams hit = 587  (94.22%)
Number of 1-grams hit = 36  (5.78%)
8 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article512.text
Will force inclusive back-off from OOVs.
Perplexity = 136.53, Entropy = 7.09 bits
Computation based on 1900 words.
Number of 2-grams hit = 1826  (96.11%)
Number of 1-grams hit = 74  (3.89%)
8 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article513.text
Will force inclusive back-off from OOVs.
Perplexity = 276.00, Entropy = 8.11 bits
Computation based on 414 words.
Number of 2-grams hit = 381  (92.03%)
Number of 1-grams hit = 33  (7.97%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article514.text
Will force inclusive back-off from OOVs.
Perplexity = 120.80, Entropy = 6.92 bits
Computation based on 397 words.
Number of 2-grams hit = 383  (96.47%)
Number of 1-grams hit = 14  (3.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article515.text
Will force inclusive back-off from OOVs.
Perplexity = 186.44, Entropy = 7.54 bits
Computation based on 972 words.
Number of 2-grams hit = 933  (95.99%)
Number of 1-grams hit = 39  (4.01%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article516.text
Will force inclusive back-off from OOVs.
Perplexity = 149.00, Entropy = 7.22 bits
Computation based on 412 words.
Number of 2-grams hit = 396  (96.12%)
Number of 1-grams hit = 16  (3.88%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article517.text
Will force inclusive back-off from OOVs.
Perplexity = 181.77, Entropy = 7.51 bits
Computation based on 1060 words.
Number of 2-grams hit = 1002  (94.53%)
Number of 1-grams hit = 58  (5.47%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article518.text
Will force inclusive back-off from OOVs.
Perplexity = 164.65, Entropy = 7.36 bits
Computation based on 584 words.
Number of 2-grams hit = 563  (96.40%)
Number of 1-grams hit = 21  (3.60%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article519.text
Will force inclusive back-off from OOVs.
Perplexity = 152.71, Entropy = 7.25 bits
Computation based on 525 words.
Number of 2-grams hit = 503  (95.81%)
Number of 1-grams hit = 22  (4.19%)
6 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article520.text
Will force inclusive back-off from OOVs.
Perplexity = 109.29, Entropy = 6.77 bits
Computation based on 610 words.
Number of 2-grams hit = 590  (96.72%)
Number of 1-grams hit = 20  (3.28%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article521.text
Will force inclusive back-off from OOVs.
Perplexity = 221.05, Entropy = 7.79 bits
Computation based on 511 words.
Number of 2-grams hit = 484  (94.72%)
Number of 1-grams hit = 27  (5.28%)
11 OOVs (2.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article522.text
Will force inclusive back-off from OOVs.
Perplexity = 130.90, Entropy = 7.03 bits
Computation based on 2319 words.
Number of 2-grams hit = 2252  (97.11%)
Number of 1-grams hit = 67  (2.89%)
11 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article523.text
Will force inclusive back-off from OOVs.
Perplexity = 136.16, Entropy = 7.09 bits
Computation based on 327 words.
Number of 2-grams hit = 311  (95.11%)
Number of 1-grams hit = 16  (4.89%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article524.text
Will force inclusive back-off from OOVs.
Perplexity = 171.67, Entropy = 7.42 bits
Computation based on 1383 words.
Number of 2-grams hit = 1316  (95.16%)
Number of 1-grams hit = 67  (4.84%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article525.text
Will force inclusive back-off from OOVs.
Perplexity = 149.72, Entropy = 7.23 bits
Computation based on 1967 words.
Number of 2-grams hit = 1878  (95.48%)
Number of 1-grams hit = 89  (4.52%)
10 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article526.text
Will force inclusive back-off from OOVs.
Perplexity = 138.54, Entropy = 7.11 bits
Computation based on 1900 words.
Number of 2-grams hit = 1818  (95.68%)
Number of 1-grams hit = 82  (4.32%)
9 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article527.text
Will force inclusive back-off from OOVs.
Perplexity = 154.99, Entropy = 7.28 bits
Computation based on 733 words.
Number of 2-grams hit = 704  (96.04%)
Number of 1-grams hit = 29  (3.96%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article528.text
Will force inclusive back-off from OOVs.
Perplexity = 142.01, Entropy = 7.15 bits
Computation based on 723 words.
Number of 2-grams hit = 691  (95.57%)
Number of 1-grams hit = 32  (4.43%)
4 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article529.text
Will force inclusive back-off from OOVs.
Perplexity = 196.61, Entropy = 7.62 bits
Computation based on 886 words.
Number of 2-grams hit = 832  (93.91%)
Number of 1-grams hit = 54  (6.09%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article530.text
Will force inclusive back-off from OOVs.
Perplexity = 144.86, Entropy = 7.18 bits
Computation based on 420 words.
Number of 2-grams hit = 406  (96.67%)
Number of 1-grams hit = 14  (3.33%)
6 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article531.text
Will force inclusive back-off from OOVs.
Perplexity = 157.84, Entropy = 7.30 bits
Computation based on 944 words.
Number of 2-grams hit = 898  (95.13%)
Number of 1-grams hit = 46  (4.87%)
4 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article532.text
Will force inclusive back-off from OOVs.
Perplexity = 121.69, Entropy = 6.93 bits
Computation based on 933 words.
Number of 2-grams hit = 898  (96.25%)
Number of 1-grams hit = 35  (3.75%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article533.text
Will force inclusive back-off from OOVs.
Perplexity = 115.46, Entropy = 6.85 bits
Computation based on 1137 words.
Number of 2-grams hit = 1109  (97.54%)
Number of 1-grams hit = 28  (2.46%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article534.text
Will force inclusive back-off from OOVs.
Perplexity = 131.05, Entropy = 7.03 bits
Computation based on 494 words.
Number of 2-grams hit = 476  (96.36%)
Number of 1-grams hit = 18  (3.64%)
8 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article535.text
Will force inclusive back-off from OOVs.
Perplexity = 276.34, Entropy = 8.11 bits
Computation based on 358 words.
Number of 2-grams hit = 332  (92.74%)
Number of 1-grams hit = 26  (7.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article536.text
Will force inclusive back-off from OOVs.
Perplexity = 132.52, Entropy = 7.05 bits
Computation based on 1511 words.
Number of 2-grams hit = 1465  (96.96%)
Number of 1-grams hit = 46  (3.04%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article537.text
Will force inclusive back-off from OOVs.
Perplexity = 107.24, Entropy = 6.74 bits
Computation based on 999 words.
Number of 2-grams hit = 960  (96.10%)
Number of 1-grams hit = 39  (3.90%)
4 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article538.text
Will force inclusive back-off from OOVs.
Perplexity = 145.31, Entropy = 7.18 bits
Computation based on 3432 words.
Number of 2-grams hit = 3277  (95.48%)
Number of 1-grams hit = 155  (4.52%)
9 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article539.text
Will force inclusive back-off from OOVs.
Perplexity = 211.34, Entropy = 7.72 bits
Computation based on 534 words.
Number of 2-grams hit = 492  (92.13%)
Number of 1-grams hit = 42  (7.87%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article540.text
Will force inclusive back-off from OOVs.
Perplexity = 119.52, Entropy = 6.90 bits
Computation based on 2193 words.
Number of 2-grams hit = 2125  (96.90%)
Number of 1-grams hit = 68  (3.10%)
5 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article541.text
Will force inclusive back-off from OOVs.
Perplexity = 150.64, Entropy = 7.23 bits
Computation based on 497 words.
Number of 2-grams hit = 473  (95.17%)
Number of 1-grams hit = 24  (4.83%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article542.text
Will force inclusive back-off from OOVs.
Perplexity = 96.22, Entropy = 6.59 bits
Computation based on 1590 words.
Number of 2-grams hit = 1554  (97.74%)
Number of 1-grams hit = 36  (2.26%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article543.text
Will force inclusive back-off from OOVs.
Perplexity = 194.80, Entropy = 7.61 bits
Computation based on 534 words.
Number of 2-grams hit = 502  (94.01%)
Number of 1-grams hit = 32  (5.99%)
7 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article544.text
Will force inclusive back-off from OOVs.
Perplexity = 163.42, Entropy = 7.35 bits
Computation based on 1280 words.
Number of 2-grams hit = 1218  (95.16%)
Number of 1-grams hit = 62  (4.84%)
7 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article545.text
Will force inclusive back-off from OOVs.
Perplexity = 124.73, Entropy = 6.96 bits
Computation based on 749 words.
Number of 2-grams hit = 722  (96.40%)
Number of 1-grams hit = 27  (3.60%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article546.text
Will force inclusive back-off from OOVs.
Perplexity = 159.65, Entropy = 7.32 bits
Computation based on 685 words.
Number of 2-grams hit = 668  (97.52%)
Number of 1-grams hit = 17  (2.48%)
7 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article547.text
Will force inclusive back-off from OOVs.
Perplexity = 241.32, Entropy = 7.91 bits
Computation based on 880 words.
Number of 2-grams hit = 826  (93.86%)
Number of 1-grams hit = 54  (6.14%)
15 OOVs (1.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article548.text
Will force inclusive back-off from OOVs.
Perplexity = 141.47, Entropy = 7.14 bits
Computation based on 623 words.
Number of 2-grams hit = 603  (96.79%)
Number of 1-grams hit = 20  (3.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article549.text
Will force inclusive back-off from OOVs.
Perplexity = 192.73, Entropy = 7.59 bits
Computation based on 358 words.
Number of 2-grams hit = 341  (95.25%)
Number of 1-grams hit = 17  (4.75%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article550.text
Will force inclusive back-off from OOVs.
Perplexity = 100.57, Entropy = 6.65 bits
Computation based on 185 words.
Number of 2-grams hit = 178  (96.22%)
Number of 1-grams hit = 7  (3.78%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article551.text
Will force inclusive back-off from OOVs.
Perplexity = 101.62, Entropy = 6.67 bits
Computation based on 918 words.
Number of 2-grams hit = 901  (98.15%)
Number of 1-grams hit = 17  (1.85%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article552.text
Will force inclusive back-off from OOVs.
Perplexity = 171.17, Entropy = 7.42 bits
Computation based on 1051 words.
Number of 2-grams hit = 1004  (95.53%)
Number of 1-grams hit = 47  (4.47%)
26 OOVs (2.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article553.text
Will force inclusive back-off from OOVs.
Perplexity = 96.64, Entropy = 6.59 bits
Computation based on 1239 words.
Number of 2-grams hit = 1212  (97.82%)
Number of 1-grams hit = 27  (2.18%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article554.text
Will force inclusive back-off from OOVs.
Perplexity = 140.64, Entropy = 7.14 bits
Computation based on 4043 words.
Number of 2-grams hit = 3886  (96.12%)
Number of 1-grams hit = 157  (3.88%)
32 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article555.text
Will force inclusive back-off from OOVs.
Perplexity = 175.87, Entropy = 7.46 bits
Computation based on 505 words.
Number of 2-grams hit = 481  (95.25%)
Number of 1-grams hit = 24  (4.75%)
3 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article556.text
Will force inclusive back-off from OOVs.
Perplexity = 245.64, Entropy = 7.94 bits
Computation based on 555 words.
Number of 2-grams hit = 512  (92.25%)
Number of 1-grams hit = 43  (7.75%)
12 OOVs (2.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article557.text
Will force inclusive back-off from OOVs.
Perplexity = 101.75, Entropy = 6.67 bits
Computation based on 382 words.
Number of 2-grams hit = 369  (96.60%)
Number of 1-grams hit = 13  (3.40%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article558.text
Will force inclusive back-off from OOVs.
Perplexity = 143.26, Entropy = 7.16 bits
Computation based on 1710 words.
Number of 2-grams hit = 1643  (96.08%)
Number of 1-grams hit = 67  (3.92%)
19 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article559.text
Will force inclusive back-off from OOVs.
Perplexity = 202.38, Entropy = 7.66 bits
Computation based on 629 words.
Number of 2-grams hit = 597  (94.91%)
Number of 1-grams hit = 32  (5.09%)
9 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article560.text
Will force inclusive back-off from OOVs.
Perplexity = 104.73, Entropy = 6.71 bits
Computation based on 547 words.
Number of 2-grams hit = 529  (96.71%)
Number of 1-grams hit = 18  (3.29%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article561.text
Will force inclusive back-off from OOVs.
Perplexity = 203.42, Entropy = 7.67 bits
Computation based on 330 words.
Number of 2-grams hit = 307  (93.03%)
Number of 1-grams hit = 23  (6.97%)
3 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article562.text
Will force inclusive back-off from OOVs.
Perplexity = 152.68, Entropy = 7.25 bits
Computation based on 312 words.
Number of 2-grams hit = 301  (96.47%)
Number of 1-grams hit = 11  (3.53%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article563.text
Will force inclusive back-off from OOVs.
Perplexity = 108.71, Entropy = 6.76 bits
Computation based on 492 words.
Number of 2-grams hit = 481  (97.76%)
Number of 1-grams hit = 11  (2.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article564.text
Will force inclusive back-off from OOVs.
Perplexity = 138.23, Entropy = 7.11 bits
Computation based on 1335 words.
Number of 2-grams hit = 1275  (95.51%)
Number of 1-grams hit = 60  (4.49%)
4 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article565.text
Will force inclusive back-off from OOVs.
Perplexity = 194.20, Entropy = 7.60 bits
Computation based on 1286 words.
Number of 2-grams hit = 1219  (94.79%)
Number of 1-grams hit = 67  (5.21%)
15 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article566.text
Will force inclusive back-off from OOVs.
Perplexity = 119.14, Entropy = 6.90 bits
Computation based on 4132 words.
Number of 2-grams hit = 3992  (96.61%)
Number of 1-grams hit = 140  (3.39%)
21 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article567.text
Will force inclusive back-off from OOVs.
Perplexity = 202.88, Entropy = 7.66 bits
Computation based on 547 words.
Number of 2-grams hit = 506  (92.50%)
Number of 1-grams hit = 41  (7.50%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article568.text
Will force inclusive back-off from OOVs.
Perplexity = 131.20, Entropy = 7.04 bits
Computation based on 329 words.
Number of 2-grams hit = 320  (97.26%)
Number of 1-grams hit = 9  (2.74%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article569.text
Will force inclusive back-off from OOVs.
Perplexity = 133.65, Entropy = 7.06 bits
Computation based on 4344 words.
Number of 2-grams hit = 4177  (96.16%)
Number of 1-grams hit = 167  (3.84%)
12 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article570.text
Will force inclusive back-off from OOVs.
Perplexity = 128.64, Entropy = 7.01 bits
Computation based on 815 words.
Number of 2-grams hit = 783  (96.07%)
Number of 1-grams hit = 32  (3.93%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article571.text
Will force inclusive back-off from OOVs.
Perplexity = 211.65, Entropy = 7.73 bits
Computation based on 534 words.
Number of 2-grams hit = 500  (93.63%)
Number of 1-grams hit = 34  (6.37%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article572.text
Will force inclusive back-off from OOVs.
Perplexity = 150.96, Entropy = 7.24 bits
Computation based on 570 words.
Number of 2-grams hit = 539  (94.56%)
Number of 1-grams hit = 31  (5.44%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article573.text
Will force inclusive back-off from OOVs.
Perplexity = 119.04, Entropy = 6.90 bits
Computation based on 251 words.
Number of 2-grams hit = 239  (95.22%)
Number of 1-grams hit = 12  (4.78%)
1 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article574.text
Will force inclusive back-off from OOVs.
Perplexity = 151.40, Entropy = 7.24 bits
Computation based on 1089 words.
Number of 2-grams hit = 1041  (95.59%)
Number of 1-grams hit = 48  (4.41%)
5 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article575.text
Will force inclusive back-off from OOVs.
Perplexity = 155.83, Entropy = 7.28 bits
Computation based on 595 words.
Number of 2-grams hit = 566  (95.13%)
Number of 1-grams hit = 29  (4.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article576.text
Will force inclusive back-off from OOVs.
Perplexity = 309.39, Entropy = 8.27 bits
Computation based on 426 words.
Number of 2-grams hit = 385  (90.38%)
Number of 1-grams hit = 41  (9.62%)
19 OOVs (4.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article577.text
Will force inclusive back-off from OOVs.
Perplexity = 112.41, Entropy = 6.81 bits
Computation based on 363 words.
Number of 2-grams hit = 349  (96.14%)
Number of 1-grams hit = 14  (3.86%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article578.text
Will force inclusive back-off from OOVs.
Perplexity = 121.97, Entropy = 6.93 bits
Computation based on 406 words.
Number of 2-grams hit = 387  (95.32%)
Number of 1-grams hit = 19  (4.68%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article579.text
Will force inclusive back-off from OOVs.
Perplexity = 163.94, Entropy = 7.36 bits
Computation based on 440 words.
Number of 2-grams hit = 414  (94.09%)
Number of 1-grams hit = 26  (5.91%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article580.text
Will force inclusive back-off from OOVs.
Perplexity = 151.12, Entropy = 7.24 bits
Computation based on 532 words.
Number of 2-grams hit = 514  (96.62%)
Number of 1-grams hit = 18  (3.38%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article581.text
Will force inclusive back-off from OOVs.
Perplexity = 192.21, Entropy = 7.59 bits
Computation based on 533 words.
Number of 2-grams hit = 499  (93.62%)
Number of 1-grams hit = 34  (6.38%)
8 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article582.text
Will force inclusive back-off from OOVs.
Perplexity = 224.78, Entropy = 7.81 bits
Computation based on 500 words.
Number of 2-grams hit = 473  (94.60%)
Number of 1-grams hit = 27  (5.40%)
9 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article583.text
Will force inclusive back-off from OOVs.
Perplexity = 159.87, Entropy = 7.32 bits
Computation based on 664 words.
Number of 2-grams hit = 630  (94.88%)
Number of 1-grams hit = 34  (5.12%)
11 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article584.text
Will force inclusive back-off from OOVs.
Perplexity = 134.31, Entropy = 7.07 bits
Computation based on 433 words.
Number of 2-grams hit = 415  (95.84%)
Number of 1-grams hit = 18  (4.16%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article585.text
Will force inclusive back-off from OOVs.
Perplexity = 119.98, Entropy = 6.91 bits
Computation based on 946 words.
Number of 2-grams hit = 919  (97.15%)
Number of 1-grams hit = 27  (2.85%)
8 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article586.text
Will force inclusive back-off from OOVs.
Perplexity = 110.76, Entropy = 6.79 bits
Computation based on 733 words.
Number of 2-grams hit = 709  (96.73%)
Number of 1-grams hit = 24  (3.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article587.text
Will force inclusive back-off from OOVs.
Perplexity = 133.11, Entropy = 7.06 bits
Computation based on 522 words.
Number of 2-grams hit = 504  (96.55%)
Number of 1-grams hit = 18  (3.45%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article588.text
Will force inclusive back-off from OOVs.
Perplexity = 139.46, Entropy = 7.12 bits
Computation based on 955 words.
Number of 2-grams hit = 911  (95.39%)
Number of 1-grams hit = 44  (4.61%)
5 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article589.text
Will force inclusive back-off from OOVs.
Perplexity = 123.42, Entropy = 6.95 bits
Computation based on 504 words.
Number of 2-grams hit = 488  (96.83%)
Number of 1-grams hit = 16  (3.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article590.text
Will force inclusive back-off from OOVs.
Perplexity = 107.61, Entropy = 6.75 bits
Computation based on 1387 words.
Number of 2-grams hit = 1352  (97.48%)
Number of 1-grams hit = 35  (2.52%)
6 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article591.text
Will force inclusive back-off from OOVs.
Perplexity = 117.23, Entropy = 6.87 bits
Computation based on 686 words.
Number of 2-grams hit = 661  (96.36%)
Number of 1-grams hit = 25  (3.64%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article592.text
Will force inclusive back-off from OOVs.
Perplexity = 171.19, Entropy = 7.42 bits
Computation based on 339 words.
Number of 2-grams hit = 321  (94.69%)
Number of 1-grams hit = 18  (5.31%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article593.text
Will force inclusive back-off from OOVs.
Perplexity = 94.05, Entropy = 6.56 bits
Computation based on 376 words.
Number of 2-grams hit = 368  (97.87%)
Number of 1-grams hit = 8  (2.13%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article594.text
Will force inclusive back-off from OOVs.
Perplexity = 112.62, Entropy = 6.82 bits
Computation based on 586 words.
Number of 2-grams hit = 571  (97.44%)
Number of 1-grams hit = 15  (2.56%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article595.text
Will force inclusive back-off from OOVs.
Perplexity = 137.16, Entropy = 7.10 bits
Computation based on 369 words.
Number of 2-grams hit = 358  (97.02%)
Number of 1-grams hit = 11  (2.98%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article596.text
Will force inclusive back-off from OOVs.
Perplexity = 158.54, Entropy = 7.31 bits
Computation based on 963 words.
Number of 2-grams hit = 912  (94.70%)
Number of 1-grams hit = 51  (5.30%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article597.text
Will force inclusive back-off from OOVs.
Perplexity = 186.06, Entropy = 7.54 bits
Computation based on 1027 words.
Number of 2-grams hit = 970  (94.45%)
Number of 1-grams hit = 57  (5.55%)
7 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article598.text
Will force inclusive back-off from OOVs.
Perplexity = 149.36, Entropy = 7.22 bits
Computation based on 296 words.
Number of 2-grams hit = 282  (95.27%)
Number of 1-grams hit = 14  (4.73%)
3 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article599.text
Will force inclusive back-off from OOVs.
Perplexity = 171.13, Entropy = 7.42 bits
Computation based on 802 words.
Number of 2-grams hit = 762  (95.01%)
Number of 1-grams hit = 40  (4.99%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article600.text
Will force inclusive back-off from OOVs.
Perplexity = 75.51, Entropy = 6.24 bits
Computation based on 4658 words.
Number of 2-grams hit = 4560  (97.90%)
Number of 1-grams hit = 98  (2.10%)
15 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article601.text
Will force inclusive back-off from OOVs.
Perplexity = 206.30, Entropy = 7.69 bits
Computation based on 1088 words.
Number of 2-grams hit = 1028  (94.49%)
Number of 1-grams hit = 60  (5.51%)
11 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article602.text
Will force inclusive back-off from OOVs.
Perplexity = 121.47, Entropy = 6.92 bits
Computation based on 590 words.
Number of 2-grams hit = 567  (96.10%)
Number of 1-grams hit = 23  (3.90%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article603.text
Will force inclusive back-off from OOVs.
Perplexity = 177.90, Entropy = 7.47 bits
Computation based on 584 words.
Number of 2-grams hit = 546  (93.49%)
Number of 1-grams hit = 38  (6.51%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article604.text
Will force inclusive back-off from OOVs.
Perplexity = 117.55, Entropy = 6.88 bits
Computation based on 560 words.
Number of 2-grams hit = 544  (97.14%)
Number of 1-grams hit = 16  (2.86%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article605.text
Will force inclusive back-off from OOVs.
Perplexity = 127.09, Entropy = 6.99 bits
Computation based on 3116 words.
Number of 2-grams hit = 3012  (96.66%)
Number of 1-grams hit = 104  (3.34%)
40 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article606.text
Will force inclusive back-off from OOVs.
Perplexity = 148.39, Entropy = 7.21 bits
Computation based on 900 words.
Number of 2-grams hit = 864  (96.00%)
Number of 1-grams hit = 36  (4.00%)
13 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article607.text
Will force inclusive back-off from OOVs.
Perplexity = 137.17, Entropy = 7.10 bits
Computation based on 1058 words.
Number of 2-grams hit = 1017  (96.12%)
Number of 1-grams hit = 41  (3.88%)
5 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article608.text
Will force inclusive back-off from OOVs.
Perplexity = 116.48, Entropy = 6.86 bits
Computation based on 617 words.
Number of 2-grams hit = 593  (96.11%)
Number of 1-grams hit = 24  (3.89%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article609.text
Will force inclusive back-off from OOVs.
Perplexity = 126.04, Entropy = 6.98 bits
Computation based on 4371 words.
Number of 2-grams hit = 4218  (96.50%)
Number of 1-grams hit = 153  (3.50%)
38 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article610.text
Will force inclusive back-off from OOVs.
Perplexity = 138.12, Entropy = 7.11 bits
Computation based on 5950 words.
Number of 2-grams hit = 5696  (95.73%)
Number of 1-grams hit = 254  (4.27%)
13 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article611.text
Will force inclusive back-off from OOVs.
Perplexity = 138.34, Entropy = 7.11 bits
Computation based on 776 words.
Number of 2-grams hit = 738  (95.10%)
Number of 1-grams hit = 38  (4.90%)
4 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article612.text
Will force inclusive back-off from OOVs.
Perplexity = 99.76, Entropy = 6.64 bits
Computation based on 1629 words.
Number of 2-grams hit = 1594  (97.85%)
Number of 1-grams hit = 35  (2.15%)
3 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article613.text
Will force inclusive back-off from OOVs.
Perplexity = 120.42, Entropy = 6.91 bits
Computation based on 4146 words.
Number of 2-grams hit = 4001  (96.50%)
Number of 1-grams hit = 145  (3.50%)
42 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article614.text
Will force inclusive back-off from OOVs.
Perplexity = 118.52, Entropy = 6.89 bits
Computation based on 1524 words.
Number of 2-grams hit = 1484  (97.38%)
Number of 1-grams hit = 40  (2.62%)
26 OOVs (1.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article615.text
Will force inclusive back-off from OOVs.
Perplexity = 117.85, Entropy = 6.88 bits
Computation based on 452 words.
Number of 2-grams hit = 434  (96.02%)
Number of 1-grams hit = 18  (3.98%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article616.text
Will force inclusive back-off from OOVs.
Perplexity = 139.52, Entropy = 7.12 bits
Computation based on 767 words.
Number of 2-grams hit = 732  (95.44%)
Number of 1-grams hit = 35  (4.56%)
5 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article617.text
Will force inclusive back-off from OOVs.
Perplexity = 134.61, Entropy = 7.07 bits
Computation based on 1319 words.
Number of 2-grams hit = 1283  (97.27%)
Number of 1-grams hit = 36  (2.73%)
5 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article618.text
Will force inclusive back-off from OOVs.
Perplexity = 149.54, Entropy = 7.22 bits
Computation based on 3769 words.
Number of 2-grams hit = 3598  (95.46%)
Number of 1-grams hit = 171  (4.54%)
8 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article619.text
Will force inclusive back-off from OOVs.
Perplexity = 116.16, Entropy = 6.86 bits
Computation based on 761 words.
Number of 2-grams hit = 735  (96.58%)
Number of 1-grams hit = 26  (3.42%)
6 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article620.text
Will force inclusive back-off from OOVs.
Perplexity = 146.43, Entropy = 7.19 bits
Computation based on 5314 words.
Number of 2-grams hit = 5063  (95.28%)
Number of 1-grams hit = 251  (4.72%)
15 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article621.text
Will force inclusive back-off from OOVs.
Perplexity = 150.14, Entropy = 7.23 bits
Computation based on 321 words.
Number of 2-grams hit = 300  (93.46%)
Number of 1-grams hit = 21  (6.54%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article622.text
Will force inclusive back-off from OOVs.
Perplexity = 170.60, Entropy = 7.41 bits
Computation based on 452 words.
Number of 2-grams hit = 424  (93.81%)
Number of 1-grams hit = 28  (6.19%)
7 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article623.text
Will force inclusive back-off from OOVs.
Perplexity = 137.61, Entropy = 7.10 bits
Computation based on 1306 words.
Number of 2-grams hit = 1250  (95.71%)
Number of 1-grams hit = 56  (4.29%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article624.text
Will force inclusive back-off from OOVs.
Perplexity = 150.87, Entropy = 7.24 bits
Computation based on 5146 words.
Number of 2-grams hit = 4918  (95.57%)
Number of 1-grams hit = 228  (4.43%)
20 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article625.text
Will force inclusive back-off from OOVs.
Perplexity = 152.56, Entropy = 7.25 bits
Computation based on 1088 words.
Number of 2-grams hit = 1036  (95.22%)
Number of 1-grams hit = 52  (4.78%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article626.text
Will force inclusive back-off from OOVs.
Perplexity = 154.53, Entropy = 7.27 bits
Computation based on 446 words.
Number of 2-grams hit = 419  (93.95%)
Number of 1-grams hit = 27  (6.05%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article627.text
Will force inclusive back-off from OOVs.
Perplexity = 327.23, Entropy = 8.35 bits
Computation based on 141 words.
Number of 2-grams hit = 128  (90.78%)
Number of 1-grams hit = 13  (9.22%)
8 OOVs (5.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article628.text
Will force inclusive back-off from OOVs.
Perplexity = 168.48, Entropy = 7.40 bits
Computation based on 1057 words.
Number of 2-grams hit = 1003  (94.89%)
Number of 1-grams hit = 54  (5.11%)
18 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article629.text
Will force inclusive back-off from OOVs.
Perplexity = 160.97, Entropy = 7.33 bits
Computation based on 1027 words.
Number of 2-grams hit = 974  (94.84%)
Number of 1-grams hit = 53  (5.16%)
10 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article630.text
Will force inclusive back-off from OOVs.
Perplexity = 225.47, Entropy = 7.82 bits
Computation based on 328 words.
Number of 2-grams hit = 302  (92.07%)
Number of 1-grams hit = 26  (7.93%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article631.text
Will force inclusive back-off from OOVs.
Perplexity = 238.17, Entropy = 7.90 bits
Computation based on 857 words.
Number of 2-grams hit = 792  (92.42%)
Number of 1-grams hit = 65  (7.58%)
19 OOVs (2.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article632.text
Will force inclusive back-off from OOVs.
Perplexity = 169.04, Entropy = 7.40 bits
Computation based on 408 words.
Number of 2-grams hit = 390  (95.59%)
Number of 1-grams hit = 18  (4.41%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article633.text
Will force inclusive back-off from OOVs.
Perplexity = 147.23, Entropy = 7.20 bits
Computation based on 529 words.
Number of 2-grams hit = 504  (95.27%)
Number of 1-grams hit = 25  (4.73%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article634.text
Will force inclusive back-off from OOVs.
Perplexity = 86.62, Entropy = 6.44 bits
Computation based on 560 words.
Number of 2-grams hit = 550  (98.21%)
Number of 1-grams hit = 10  (1.79%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article635.text
Will force inclusive back-off from OOVs.
Perplexity = 134.51, Entropy = 7.07 bits
Computation based on 151 words.
Number of 2-grams hit = 146  (96.69%)
Number of 1-grams hit = 5  (3.31%)
1 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article636.text
Will force inclusive back-off from OOVs.
Perplexity = 288.13, Entropy = 8.17 bits
Computation based on 1322 words.
Number of 2-grams hit = 1213  (91.75%)
Number of 1-grams hit = 109  (8.25%)
22 OOVs (1.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article637.text
Will force inclusive back-off from OOVs.
Perplexity = 148.65, Entropy = 7.22 bits
Computation based on 1034 words.
Number of 2-grams hit = 984  (95.16%)
Number of 1-grams hit = 50  (4.84%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article638.text
Will force inclusive back-off from OOVs.
Perplexity = 258.26, Entropy = 8.01 bits
Computation based on 526 words.
Number of 2-grams hit = 481  (91.44%)
Number of 1-grams hit = 45  (8.56%)
7 OOVs (1.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article639.text
Will force inclusive back-off from OOVs.
Perplexity = 189.45, Entropy = 7.57 bits
Computation based on 522 words.
Number of 2-grams hit = 488  (93.49%)
Number of 1-grams hit = 34  (6.51%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article640.text
Will force inclusive back-off from OOVs.
Perplexity = 107.12, Entropy = 6.74 bits
Computation based on 204 words.
Number of 2-grams hit = 195  (95.59%)
Number of 1-grams hit = 9  (4.41%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article641.text
Will force inclusive back-off from OOVs.
Perplexity = 187.95, Entropy = 7.55 bits
Computation based on 825 words.
Number of 2-grams hit = 782  (94.79%)
Number of 1-grams hit = 43  (5.21%)
5 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article642.text
Will force inclusive back-off from OOVs.
Perplexity = 278.49, Entropy = 8.12 bits
Computation based on 429 words.
Number of 2-grams hit = 396  (92.31%)
Number of 1-grams hit = 33  (7.69%)
15 OOVs (3.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article643.text
Will force inclusive back-off from OOVs.
Perplexity = 128.53, Entropy = 7.01 bits
Computation based on 488 words.
Number of 2-grams hit = 478  (97.95%)
Number of 1-grams hit = 10  (2.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article644.text
Will force inclusive back-off from OOVs.
Perplexity = 134.23, Entropy = 7.07 bits
Computation based on 1116 words.
Number of 2-grams hit = 1076  (96.42%)
Number of 1-grams hit = 40  (3.58%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article645.text
Will force inclusive back-off from OOVs.
Perplexity = 145.36, Entropy = 7.18 bits
Computation based on 440 words.
Number of 2-grams hit = 427  (97.05%)
Number of 1-grams hit = 13  (2.95%)
6 OOVs (1.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article646.text
Will force inclusive back-off from OOVs.
Perplexity = 144.90, Entropy = 7.18 bits
Computation based on 573 words.
Number of 2-grams hit = 549  (95.81%)
Number of 1-grams hit = 24  (4.19%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article647.text
Will force inclusive back-off from OOVs.
Perplexity = 183.98, Entropy = 7.52 bits
Computation based on 608 words.
Number of 2-grams hit = 583  (95.89%)
Number of 1-grams hit = 25  (4.11%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article648.text
Will force inclusive back-off from OOVs.
Perplexity = 131.53, Entropy = 7.04 bits
Computation based on 475 words.
Number of 2-grams hit = 458  (96.42%)
Number of 1-grams hit = 17  (3.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article649.text
Will force inclusive back-off from OOVs.
Perplexity = 150.04, Entropy = 7.23 bits
Computation based on 530 words.
Number of 2-grams hit = 502  (94.72%)
Number of 1-grams hit = 28  (5.28%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article650.text
Will force inclusive back-off from OOVs.
Perplexity = 192.01, Entropy = 7.59 bits
Computation based on 529 words.
Number of 2-grams hit = 501  (94.71%)
Number of 1-grams hit = 28  (5.29%)
5 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article651.text
Will force inclusive back-off from OOVs.
Perplexity = 125.86, Entropy = 6.98 bits
Computation based on 459 words.
Number of 2-grams hit = 442  (96.30%)
Number of 1-grams hit = 17  (3.70%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article652.text
Will force inclusive back-off from OOVs.
Perplexity = 84.05, Entropy = 6.39 bits
Computation based on 878 words.
Number of 2-grams hit = 864  (98.41%)
Number of 1-grams hit = 14  (1.59%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article653.text
Will force inclusive back-off from OOVs.
Perplexity = 121.15, Entropy = 6.92 bits
Computation based on 765 words.
Number of 2-grams hit = 735  (96.08%)
Number of 1-grams hit = 30  (3.92%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article654.text
Will force inclusive back-off from OOVs.
Perplexity = 149.24, Entropy = 7.22 bits
Computation based on 772 words.
Number of 2-grams hit = 734  (95.08%)
Number of 1-grams hit = 38  (4.92%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article655.text
Will force inclusive back-off from OOVs.
Perplexity = 161.58, Entropy = 7.34 bits
Computation based on 481 words.
Number of 2-grams hit = 450  (93.56%)
Number of 1-grams hit = 31  (6.44%)
6 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article656.text
Will force inclusive back-off from OOVs.
Perplexity = 269.53, Entropy = 8.07 bits
Computation based on 461 words.
Number of 2-grams hit = 419  (90.89%)
Number of 1-grams hit = 42  (9.11%)
5 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article657.text
Will force inclusive back-off from OOVs.
Perplexity = 147.78, Entropy = 7.21 bits
Computation based on 376 words.
Number of 2-grams hit = 350  (93.09%)
Number of 1-grams hit = 26  (6.91%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article658.text
Will force inclusive back-off from OOVs.
Perplexity = 117.22, Entropy = 6.87 bits
Computation based on 1004 words.
Number of 2-grams hit = 967  (96.31%)
Number of 1-grams hit = 37  (3.69%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article659.text
Will force inclusive back-off from OOVs.
Perplexity = 169.98, Entropy = 7.41 bits
Computation based on 964 words.
Number of 2-grams hit = 910  (94.40%)
Number of 1-grams hit = 54  (5.60%)
26 OOVs (2.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article660.text
Will force inclusive back-off from OOVs.
Perplexity = 205.74, Entropy = 7.68 bits
Computation based on 139 words.
Number of 2-grams hit = 125  (89.93%)
Number of 1-grams hit = 14  (10.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article661.text
Will force inclusive back-off from OOVs.
Perplexity = 148.78, Entropy = 7.22 bits
Computation based on 1244 words.
Number of 2-grams hit = 1177  (94.61%)
Number of 1-grams hit = 67  (5.39%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article662.text
Will force inclusive back-off from OOVs.
Perplexity = 462.77, Entropy = 8.85 bits
Computation based on 123 words.
Number of 2-grams hit = 108  (87.80%)
Number of 1-grams hit = 15  (12.20%)
3 OOVs (2.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article663.text
Will force inclusive back-off from OOVs.
Perplexity = 207.11, Entropy = 7.69 bits
Computation based on 1338 words.
Number of 2-grams hit = 1267  (94.69%)
Number of 1-grams hit = 71  (5.31%)
22 OOVs (1.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article664.text
Will force inclusive back-off from OOVs.
Perplexity = 115.10, Entropy = 6.85 bits
Computation based on 692 words.
Number of 2-grams hit = 671  (96.97%)
Number of 1-grams hit = 21  (3.03%)
7 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article665.text
Will force inclusive back-off from OOVs.
Perplexity = 117.53, Entropy = 6.88 bits
Computation based on 756 words.
Number of 2-grams hit = 727  (96.16%)
Number of 1-grams hit = 29  (3.84%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article666.text
Will force inclusive back-off from OOVs.
Perplexity = 171.92, Entropy = 7.43 bits
Computation based on 383 words.
Number of 2-grams hit = 363  (94.78%)
Number of 1-grams hit = 20  (5.22%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article667.text
Will force inclusive back-off from OOVs.
Perplexity = 80.71, Entropy = 6.33 bits
Computation based on 1292 words.
Number of 2-grams hit = 1277  (98.84%)
Number of 1-grams hit = 15  (1.16%)
9 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article668.text
Will force inclusive back-off from OOVs.
Perplexity = 104.56, Entropy = 6.71 bits
Computation based on 466 words.
Number of 2-grams hit = 456  (97.85%)
Number of 1-grams hit = 10  (2.15%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article669.text
Will force inclusive back-off from OOVs.
Perplexity = 144.95, Entropy = 7.18 bits
Computation based on 485 words.
Number of 2-grams hit = 460  (94.85%)
Number of 1-grams hit = 25  (5.15%)
4 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article670.text
Will force inclusive back-off from OOVs.
Perplexity = 111.10, Entropy = 6.80 bits
Computation based on 448 words.
Number of 2-grams hit = 434  (96.88%)
Number of 1-grams hit = 14  (3.12%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article671.text
Will force inclusive back-off from OOVs.
Perplexity = 146.13, Entropy = 7.19 bits
Computation based on 2031 words.
Number of 2-grams hit = 1938  (95.42%)
Number of 1-grams hit = 93  (4.58%)
5 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article672.text
Will force inclusive back-off from OOVs.
Perplexity = 186.01, Entropy = 7.54 bits
Computation based on 284 words.
Number of 2-grams hit = 266  (93.66%)
Number of 1-grams hit = 18  (6.34%)
4 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article673.text
Will force inclusive back-off from OOVs.
Perplexity = 114.01, Entropy = 6.83 bits
Computation based on 301 words.
Number of 2-grams hit = 289  (96.01%)
Number of 1-grams hit = 12  (3.99%)
3 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article674.text
Will force inclusive back-off from OOVs.
Perplexity = 84.68, Entropy = 6.40 bits
Computation based on 141 words.
Number of 2-grams hit = 137  (97.16%)
Number of 1-grams hit = 4  (2.84%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article675.text
Will force inclusive back-off from OOVs.
Perplexity = 138.91, Entropy = 7.12 bits
Computation based on 3859 words.
Number of 2-grams hit = 3685  (95.49%)
Number of 1-grams hit = 174  (4.51%)
15 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article676.text
Will force inclusive back-off from OOVs.
Perplexity = 162.95, Entropy = 7.35 bits
Computation based on 496 words.
Number of 2-grams hit = 473  (95.36%)
Number of 1-grams hit = 23  (4.64%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article677.text
Will force inclusive back-off from OOVs.
Perplexity = 153.37, Entropy = 7.26 bits
Computation based on 641 words.
Number of 2-grams hit = 613  (95.63%)
Number of 1-grams hit = 28  (4.37%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article678.text
Will force inclusive back-off from OOVs.
Perplexity = 139.33, Entropy = 7.12 bits
Computation based on 340 words.
Number of 2-grams hit = 326  (95.88%)
Number of 1-grams hit = 14  (4.12%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article679.text
Will force inclusive back-off from OOVs.
Perplexity = 142.84, Entropy = 7.16 bits
Computation based on 986 words.
Number of 2-grams hit = 943  (95.64%)
Number of 1-grams hit = 43  (4.36%)
3 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article680.text
Will force inclusive back-off from OOVs.
Perplexity = 104.48, Entropy = 6.71 bits
Computation based on 317 words.
Number of 2-grams hit = 301  (94.95%)
Number of 1-grams hit = 16  (5.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article681.text
Will force inclusive back-off from OOVs.
Perplexity = 116.39, Entropy = 6.86 bits
Computation based on 474 words.
Number of 2-grams hit = 457  (96.41%)
Number of 1-grams hit = 17  (3.59%)
4 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article682.text
Will force inclusive back-off from OOVs.
Perplexity = 135.86, Entropy = 7.09 bits
Computation based on 3680 words.
Number of 2-grams hit = 3539  (96.17%)
Number of 1-grams hit = 141  (3.83%)
10 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article683.text
Will force inclusive back-off from OOVs.
Perplexity = 204.60, Entropy = 7.68 bits
Computation based on 261 words.
Number of 2-grams hit = 249  (95.40%)
Number of 1-grams hit = 12  (4.60%)
5 OOVs (1.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article684.text
Will force inclusive back-off from OOVs.
Perplexity = 118.25, Entropy = 6.89 bits
Computation based on 4339 words.
Number of 2-grams hit = 4208  (96.98%)
Number of 1-grams hit = 131  (3.02%)
19 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article685.text
Will force inclusive back-off from OOVs.
Perplexity = 199.45, Entropy = 7.64 bits
Computation based on 455 words.
Number of 2-grams hit = 429  (94.29%)
Number of 1-grams hit = 26  (5.71%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article686.text
Will force inclusive back-off from OOVs.
Perplexity = 150.29, Entropy = 7.23 bits
Computation based on 2213 words.
Number of 2-grams hit = 2112  (95.44%)
Number of 1-grams hit = 101  (4.56%)
16 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article687.text
Will force inclusive back-off from OOVs.
Perplexity = 133.98, Entropy = 7.07 bits
Computation based on 512 words.
Number of 2-grams hit = 491  (95.90%)
Number of 1-grams hit = 21  (4.10%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article688.text
Will force inclusive back-off from OOVs.
Perplexity = 117.99, Entropy = 6.88 bits
Computation based on 303 words.
Number of 2-grams hit = 294  (97.03%)
Number of 1-grams hit = 9  (2.97%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article689.text
Will force inclusive back-off from OOVs.
Perplexity = 190.56, Entropy = 7.57 bits
Computation based on 508 words.
Number of 2-grams hit = 472  (92.91%)
Number of 1-grams hit = 36  (7.09%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article690.text
Will force inclusive back-off from OOVs.
Perplexity = 127.33, Entropy = 6.99 bits
Computation based on 754 words.
Number of 2-grams hit = 732  (97.08%)
Number of 1-grams hit = 22  (2.92%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article691.text
Will force inclusive back-off from OOVs.
Perplexity = 153.65, Entropy = 7.26 bits
Computation based on 872 words.
Number of 2-grams hit = 826  (94.72%)
Number of 1-grams hit = 46  (5.28%)
6 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article692.text
Will force inclusive back-off from OOVs.
Perplexity = 336.05, Entropy = 8.39 bits
Computation based on 427 words.
Number of 2-grams hit = 387  (90.63%)
Number of 1-grams hit = 40  (9.37%)
6 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article693.text
Will force inclusive back-off from OOVs.
Perplexity = 152.84, Entropy = 7.26 bits
Computation based on 922 words.
Number of 2-grams hit = 885  (95.99%)
Number of 1-grams hit = 37  (4.01%)
22 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article694.text
Will force inclusive back-off from OOVs.
Perplexity = 116.34, Entropy = 6.86 bits
Computation based on 626 words.
Number of 2-grams hit = 610  (97.44%)
Number of 1-grams hit = 16  (2.56%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article695.text
Will force inclusive back-off from OOVs.
Perplexity = 137.67, Entropy = 7.11 bits
Computation based on 849 words.
Number of 2-grams hit = 811  (95.52%)
Number of 1-grams hit = 38  (4.48%)
5 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article696.text
Will force inclusive back-off from OOVs.
Perplexity = 135.65, Entropy = 7.08 bits
Computation based on 496 words.
Number of 2-grams hit = 481  (96.98%)
Number of 1-grams hit = 15  (3.02%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article697.text
Will force inclusive back-off from OOVs.
Perplexity = 152.76, Entropy = 7.26 bits
Computation based on 942 words.
Number of 2-grams hit = 899  (95.44%)
Number of 1-grams hit = 43  (4.56%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article698.text
Will force inclusive back-off from OOVs.
Perplexity = 162.72, Entropy = 7.35 bits
Computation based on 572 words.
Number of 2-grams hit = 548  (95.80%)
Number of 1-grams hit = 24  (4.20%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article699.text
Will force inclusive back-off from OOVs.
Perplexity = 153.65, Entropy = 7.26 bits
Computation based on 352 words.
Number of 2-grams hit = 340  (96.59%)
Number of 1-grams hit = 12  (3.41%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article700.text
Will force inclusive back-off from OOVs.
Perplexity = 107.92, Entropy = 6.75 bits
Computation based on 3290 words.
Number of 2-grams hit = 3289  (99.97%)
Number of 1-grams hit = 1  (0.03%)
53 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article701.text
Will force inclusive back-off from OOVs.
Perplexity = 156.86, Entropy = 7.29 bits
Computation based on 807 words.
Number of 2-grams hit = 766  (94.92%)
Number of 1-grams hit = 41  (5.08%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article702.text
Will force inclusive back-off from OOVs.
Perplexity = 226.84, Entropy = 7.83 bits
Computation based on 1792 words.
Number of 2-grams hit = 1662  (92.75%)
Number of 1-grams hit = 130  (7.25%)
45 OOVs (2.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article703.text
Will force inclusive back-off from OOVs.
Perplexity = 235.11, Entropy = 7.88 bits
Computation based on 421 words.
Number of 2-grams hit = 392  (93.11%)
Number of 1-grams hit = 29  (6.89%)
5 OOVs (1.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article704.text
Will force inclusive back-off from OOVs.
Perplexity = 122.81, Entropy = 6.94 bits
Computation based on 311 words.
Number of 2-grams hit = 298  (95.82%)
Number of 1-grams hit = 13  (4.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article705.text
Will force inclusive back-off from OOVs.
Perplexity = 140.25, Entropy = 7.13 bits
Computation based on 1010 words.
Number of 2-grams hit = 957  (94.75%)
Number of 1-grams hit = 53  (5.25%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article706.text
Will force inclusive back-off from OOVs.
Perplexity = 153.52, Entropy = 7.26 bits
Computation based on 476 words.
Number of 2-grams hit = 449  (94.33%)
Number of 1-grams hit = 27  (5.67%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article707.text
Will force inclusive back-off from OOVs.
Perplexity = 247.26, Entropy = 7.95 bits
Computation based on 869 words.
Number of 2-grams hit = 810  (93.21%)
Number of 1-grams hit = 59  (6.79%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article708.text
Will force inclusive back-off from OOVs.
Perplexity = 151.49, Entropy = 7.24 bits
Computation based on 523 words.
Number of 2-grams hit = 501  (95.79%)
Number of 1-grams hit = 22  (4.21%)
5 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article709.text
Will force inclusive back-off from OOVs.
Perplexity = 187.85, Entropy = 7.55 bits
Computation based on 548 words.
Number of 2-grams hit = 521  (95.07%)
Number of 1-grams hit = 27  (4.93%)
10 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article710.text
Will force inclusive back-off from OOVs.
Perplexity = 173.57, Entropy = 7.44 bits
Computation based on 753 words.
Number of 2-grams hit = 719  (95.48%)
Number of 1-grams hit = 34  (4.52%)
21 OOVs (2.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article711.text
Will force inclusive back-off from OOVs.
Perplexity = 129.75, Entropy = 7.02 bits
Computation based on 959 words.
Number of 2-grams hit = 925  (96.45%)
Number of 1-grams hit = 34  (3.55%)
3 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article712.text
Will force inclusive back-off from OOVs.
Perplexity = 164.44, Entropy = 7.36 bits
Computation based on 365 words.
Number of 2-grams hit = 338  (92.60%)
Number of 1-grams hit = 27  (7.40%)
4 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article713.text
Will force inclusive back-off from OOVs.
Perplexity = 109.89, Entropy = 6.78 bits
Computation based on 887 words.
Number of 2-grams hit = 860  (96.96%)
Number of 1-grams hit = 27  (3.04%)
3 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article714.text
Will force inclusive back-off from OOVs.
Perplexity = 140.44, Entropy = 7.13 bits
Computation based on 364 words.
Number of 2-grams hit = 348  (95.60%)
Number of 1-grams hit = 16  (4.40%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article715.text
Will force inclusive back-off from OOVs.
Perplexity = 134.01, Entropy = 7.07 bits
Computation based on 486 words.
Number of 2-grams hit = 467  (96.09%)
Number of 1-grams hit = 19  (3.91%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article716.text
Will force inclusive back-off from OOVs.
Perplexity = 191.43, Entropy = 7.58 bits
Computation based on 452 words.
Number of 2-grams hit = 415  (91.81%)
Number of 1-grams hit = 37  (8.19%)
16 OOVs (3.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article717.text
Will force inclusive back-off from OOVs.
Perplexity = 170.47, Entropy = 7.41 bits
Computation based on 377 words.
Number of 2-grams hit = 357  (94.69%)
Number of 1-grams hit = 20  (5.31%)
4 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article718.text
Will force inclusive back-off from OOVs.
Perplexity = 146.31, Entropy = 7.19 bits
Computation based on 6958 words.
Number of 2-grams hit = 6642  (95.46%)
Number of 1-grams hit = 316  (4.54%)
27 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article719.text
Will force inclusive back-off from OOVs.
Perplexity = 120.97, Entropy = 6.92 bits
Computation based on 1162 words.
Number of 2-grams hit = 1119  (96.30%)
Number of 1-grams hit = 43  (3.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article720.text
Will force inclusive back-off from OOVs.
Perplexity = 138.83, Entropy = 7.12 bits
Computation based on 806 words.
Number of 2-grams hit = 780  (96.77%)
Number of 1-grams hit = 26  (3.23%)
8 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article721.text
Will force inclusive back-off from OOVs.
Perplexity = 183.51, Entropy = 7.52 bits
Computation based on 423 words.
Number of 2-grams hit = 407  (96.22%)
Number of 1-grams hit = 16  (3.78%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article722.text
Will force inclusive back-off from OOVs.
Perplexity = 114.19, Entropy = 6.84 bits
Computation based on 1012 words.
Number of 2-grams hit = 981  (96.94%)
Number of 1-grams hit = 31  (3.06%)
8 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article723.text
Will force inclusive back-off from OOVs.
Perplexity = 172.95, Entropy = 7.43 bits
Computation based on 427 words.
Number of 2-grams hit = 403  (94.38%)
Number of 1-grams hit = 24  (5.62%)
13 OOVs (2.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article724.text
Will force inclusive back-off from OOVs.
Perplexity = 244.24, Entropy = 7.93 bits
Computation based on 458 words.
Number of 2-grams hit = 426  (93.01%)
Number of 1-grams hit = 32  (6.99%)
4 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article725.text
Will force inclusive back-off from OOVs.
Perplexity = 169.09, Entropy = 7.40 bits
Computation based on 710 words.
Number of 2-grams hit = 668  (94.08%)
Number of 1-grams hit = 42  (5.92%)
12 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article726.text
Will force inclusive back-off from OOVs.
Perplexity = 100.42, Entropy = 6.65 bits
Computation based on 519 words.
Number of 2-grams hit = 507  (97.69%)
Number of 1-grams hit = 12  (2.31%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article727.text
Will force inclusive back-off from OOVs.
Perplexity = 224.28, Entropy = 7.81 bits
Computation based on 482 words.
Number of 2-grams hit = 455  (94.40%)
Number of 1-grams hit = 27  (5.60%)
11 OOVs (2.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article728.text
Will force inclusive back-off from OOVs.
Perplexity = 179.27, Entropy = 7.49 bits
Computation based on 416 words.
Number of 2-grams hit = 391  (93.99%)
Number of 1-grams hit = 25  (6.01%)
5 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article729.text
Will force inclusive back-off from OOVs.
Perplexity = 226.46, Entropy = 7.82 bits
Computation based on 437 words.
Number of 2-grams hit = 410  (93.82%)
Number of 1-grams hit = 27  (6.18%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article730.text
Will force inclusive back-off from OOVs.
Perplexity = 121.03, Entropy = 6.92 bits
Computation based on 8423 words.
Number of 2-grams hit = 8160  (96.88%)
Number of 1-grams hit = 263  (3.12%)
54 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article731.text
Will force inclusive back-off from OOVs.
Perplexity = 100.76, Entropy = 6.65 bits
Computation based on 1450 words.
Number of 2-grams hit = 1414  (97.52%)
Number of 1-grams hit = 36  (2.48%)
6 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article732.text
Will force inclusive back-off from OOVs.
Perplexity = 129.65, Entropy = 7.02 bits
Computation based on 702 words.
Number of 2-grams hit = 676  (96.30%)
Number of 1-grams hit = 26  (3.70%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article733.text
Will force inclusive back-off from OOVs.
Perplexity = 114.55, Entropy = 6.84 bits
Computation based on 559 words.
Number of 2-grams hit = 536  (95.89%)
Number of 1-grams hit = 23  (4.11%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article734.text
Will force inclusive back-off from OOVs.
Perplexity = 369.38, Entropy = 8.53 bits
Computation based on 414 words.
Number of 2-grams hit = 370  (89.37%)
Number of 1-grams hit = 44  (10.63%)
7 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article735.text
Will force inclusive back-off from OOVs.
Perplexity = 124.92, Entropy = 6.96 bits
Computation based on 1178 words.
Number of 2-grams hit = 1140  (96.77%)
Number of 1-grams hit = 38  (3.23%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article736.text
Will force inclusive back-off from OOVs.
Perplexity = 156.16, Entropy = 7.29 bits
Computation based on 667 words.
Number of 2-grams hit = 639  (95.80%)
Number of 1-grams hit = 28  (4.20%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article737.text
Will force inclusive back-off from OOVs.
Perplexity = 122.11, Entropy = 6.93 bits
Computation based on 633 words.
Number of 2-grams hit = 611  (96.52%)
Number of 1-grams hit = 22  (3.48%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article738.text
Will force inclusive back-off from OOVs.
Perplexity = 201.24, Entropy = 7.65 bits
Computation based on 1002 words.
Number of 2-grams hit = 959  (95.71%)
Number of 1-grams hit = 43  (4.29%)
9 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article739.text
Will force inclusive back-off from OOVs.
Perplexity = 138.44, Entropy = 7.11 bits
Computation based on 433 words.
Number of 2-grams hit = 413  (95.38%)
Number of 1-grams hit = 20  (4.62%)
7 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article740.text
Will force inclusive back-off from OOVs.
Perplexity = 97.70, Entropy = 6.61 bits
Computation based on 1093 words.
Number of 2-grams hit = 1064  (97.35%)
Number of 1-grams hit = 29  (2.65%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article741.text
Will force inclusive back-off from OOVs.
Perplexity = 158.23, Entropy = 7.31 bits
Computation based on 756 words.
Number of 2-grams hit = 725  (95.90%)
Number of 1-grams hit = 31  (4.10%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article742.text
Will force inclusive back-off from OOVs.
Perplexity = 231.73, Entropy = 7.86 bits
Computation based on 609 words.
Number of 2-grams hit = 572  (93.92%)
Number of 1-grams hit = 37  (6.08%)
22 OOVs (3.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article743.text
Will force inclusive back-off from OOVs.
Perplexity = 168.26, Entropy = 7.39 bits
Computation based on 1973 words.
Number of 2-grams hit = 1880  (95.29%)
Number of 1-grams hit = 93  (4.71%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article744.text
Will force inclusive back-off from OOVs.
Perplexity = 116.64, Entropy = 6.87 bits
Computation based on 525 words.
Number of 2-grams hit = 499  (95.05%)
Number of 1-grams hit = 26  (4.95%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article745.text
Will force inclusive back-off from OOVs.
Perplexity = 137.38, Entropy = 7.10 bits
Computation based on 687 words.
Number of 2-grams hit = 659  (95.92%)
Number of 1-grams hit = 28  (4.08%)
6 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article746.text
Will force inclusive back-off from OOVs.
Perplexity = 97.94, Entropy = 6.61 bits
Computation based on 1313 words.
Number of 2-grams hit = 1285  (97.87%)
Number of 1-grams hit = 28  (2.13%)
19 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article747.text
Will force inclusive back-off from OOVs.
Perplexity = 92.60, Entropy = 6.53 bits
Computation based on 518 words.
Number of 2-grams hit = 511  (98.65%)
Number of 1-grams hit = 7  (1.35%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article748.text
Will force inclusive back-off from OOVs.
Perplexity = 206.99, Entropy = 7.69 bits
Computation based on 500 words.
Number of 2-grams hit = 463  (92.60%)
Number of 1-grams hit = 37  (7.40%)
9 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article749.text
Will force inclusive back-off from OOVs.
Perplexity = 154.81, Entropy = 7.27 bits
Computation based on 550 words.
Number of 2-grams hit = 527  (95.82%)
Number of 1-grams hit = 23  (4.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article750.text
Will force inclusive back-off from OOVs.
Perplexity = 123.78, Entropy = 6.95 bits
Computation based on 467 words.
Number of 2-grams hit = 447  (95.72%)
Number of 1-grams hit = 20  (4.28%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article751.text
Will force inclusive back-off from OOVs.
Perplexity = 154.12, Entropy = 7.27 bits
Computation based on 2005 words.
Number of 2-grams hit = 1908  (95.16%)
Number of 1-grams hit = 97  (4.84%)
11 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article752.text
Will force inclusive back-off from OOVs.
Perplexity = 123.46, Entropy = 6.95 bits
Computation based on 427 words.
Number of 2-grams hit = 409  (95.78%)
Number of 1-grams hit = 18  (4.22%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article753.text
Will force inclusive back-off from OOVs.
Perplexity = 130.44, Entropy = 7.03 bits
Computation based on 531 words.
Number of 2-grams hit = 510  (96.05%)
Number of 1-grams hit = 21  (3.95%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article754.text
Will force inclusive back-off from OOVs.
Perplexity = 129.70, Entropy = 7.02 bits
Computation based on 1850 words.
Number of 2-grams hit = 1769  (95.62%)
Number of 1-grams hit = 81  (4.38%)
12 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article755.text
Will force inclusive back-off from OOVs.
Perplexity = 152.59, Entropy = 7.25 bits
Computation based on 448 words.
Number of 2-grams hit = 426  (95.09%)
Number of 1-grams hit = 22  (4.91%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article756.text
Will force inclusive back-off from OOVs.
Perplexity = 185.12, Entropy = 7.53 bits
Computation based on 709 words.
Number of 2-grams hit = 681  (96.05%)
Number of 1-grams hit = 28  (3.95%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article757.text
Will force inclusive back-off from OOVs.
Perplexity = 119.49, Entropy = 6.90 bits
Computation based on 506 words.
Number of 2-grams hit = 501  (99.01%)
Number of 1-grams hit = 5  (0.99%)
4 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article758.text
Will force inclusive back-off from OOVs.
Perplexity = 144.47, Entropy = 7.17 bits
Computation based on 2506 words.
Number of 2-grams hit = 2407  (96.05%)
Number of 1-grams hit = 99  (3.95%)
10 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article759.text
Will force inclusive back-off from OOVs.
Perplexity = 117.58, Entropy = 6.88 bits
Computation based on 3315 words.
Number of 2-grams hit = 3235  (97.59%)
Number of 1-grams hit = 80  (2.41%)
7 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article760.text
Will force inclusive back-off from OOVs.
Perplexity = 135.14, Entropy = 7.08 bits
Computation based on 550 words.
Number of 2-grams hit = 544  (98.91%)
Number of 1-grams hit = 6  (1.09%)
6 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article761.text
Will force inclusive back-off from OOVs.
Perplexity = 106.10, Entropy = 6.73 bits
Computation based on 629 words.
Number of 2-grams hit = 610  (96.98%)
Number of 1-grams hit = 19  (3.02%)
6 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article762.text
Will force inclusive back-off from OOVs.
Perplexity = 151.29, Entropy = 7.24 bits
Computation based on 790 words.
Number of 2-grams hit = 748  (94.68%)
Number of 1-grams hit = 42  (5.32%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article763.text
Will force inclusive back-off from OOVs.
Perplexity = 126.69, Entropy = 6.99 bits
Computation based on 722 words.
Number of 2-grams hit = 696  (96.40%)
Number of 1-grams hit = 26  (3.60%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article764.text
Will force inclusive back-off from OOVs.
Perplexity = 192.34, Entropy = 7.59 bits
Computation based on 534 words.
Number of 2-grams hit = 503  (94.19%)
Number of 1-grams hit = 31  (5.81%)
7 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article765.text
Will force inclusive back-off from OOVs.
Perplexity = 263.29, Entropy = 8.04 bits
Computation based on 828 words.
Number of 2-grams hit = 781  (94.32%)
Number of 1-grams hit = 47  (5.68%)
10 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article766.text
Will force inclusive back-off from OOVs.
Perplexity = 132.08, Entropy = 7.05 bits
Computation based on 263 words.
Number of 2-grams hit = 249  (94.68%)
Number of 1-grams hit = 14  (5.32%)
5 OOVs (1.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article767.text
Will force inclusive back-off from OOVs.
Perplexity = 152.12, Entropy = 7.25 bits
Computation based on 4386 words.
Number of 2-grams hit = 4249  (96.88%)
Number of 1-grams hit = 137  (3.12%)
33 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article768.text
Will force inclusive back-off from OOVs.
Perplexity = 114.66, Entropy = 6.84 bits
Computation based on 461 words.
Number of 2-grams hit = 442  (95.88%)
Number of 1-grams hit = 19  (4.12%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article769.text
Will force inclusive back-off from OOVs.
Perplexity = 83.68, Entropy = 6.39 bits
Computation based on 677 words.
Number of 2-grams hit = 670  (98.97%)
Number of 1-grams hit = 7  (1.03%)
4 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article770.text
Will force inclusive back-off from OOVs.
Perplexity = 158.81, Entropy = 7.31 bits
Computation based on 606 words.
Number of 2-grams hit = 574  (94.72%)
Number of 1-grams hit = 32  (5.28%)
4 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article771.text
Will force inclusive back-off from OOVs.
Perplexity = 139.67, Entropy = 7.13 bits
Computation based on 468 words.
Number of 2-grams hit = 454  (97.01%)
Number of 1-grams hit = 14  (2.99%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article772.text
Will force inclusive back-off from OOVs.
Perplexity = 137.52, Entropy = 7.10 bits
Computation based on 541 words.
Number of 2-grams hit = 517  (95.56%)
Number of 1-grams hit = 24  (4.44%)
6 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article773.text
Will force inclusive back-off from OOVs.
Perplexity = 135.06, Entropy = 7.08 bits
Computation based on 2984 words.
Number of 2-grams hit = 2870  (96.18%)
Number of 1-grams hit = 114  (3.82%)
14 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article774.text
Will force inclusive back-off from OOVs.
Perplexity = 113.30, Entropy = 6.82 bits
Computation based on 2009 words.
Number of 2-grams hit = 1961  (97.61%)
Number of 1-grams hit = 48  (2.39%)
13 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article775.text
Will force inclusive back-off from OOVs.
Perplexity = 119.39, Entropy = 6.90 bits
Computation based on 498 words.
Number of 2-grams hit = 484  (97.19%)
Number of 1-grams hit = 14  (2.81%)
7 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article776.text
Will force inclusive back-off from OOVs.
Perplexity = 108.39, Entropy = 6.76 bits
Computation based on 1575 words.
Number of 2-grams hit = 1532  (97.27%)
Number of 1-grams hit = 43  (2.73%)
9 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article777.text
Will force inclusive back-off from OOVs.
Perplexity = 104.43, Entropy = 6.71 bits
Computation based on 9319 words.
Number of 2-grams hit = 9046  (97.07%)
Number of 1-grams hit = 273  (2.93%)
59 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article778.text
Will force inclusive back-off from OOVs.
Perplexity = 135.48, Entropy = 7.08 bits
Computation based on 503 words.
Number of 2-grams hit = 479  (95.23%)
Number of 1-grams hit = 24  (4.77%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article779.text
Will force inclusive back-off from OOVs.
Perplexity = 141.48, Entropy = 7.14 bits
Computation based on 932 words.
Number of 2-grams hit = 898  (96.35%)
Number of 1-grams hit = 34  (3.65%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article780.text
Will force inclusive back-off from OOVs.
Perplexity = 110.27, Entropy = 6.78 bits
Computation based on 401 words.
Number of 2-grams hit = 386  (96.26%)
Number of 1-grams hit = 15  (3.74%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article781.text
Will force inclusive back-off from OOVs.
Perplexity = 139.69, Entropy = 7.13 bits
Computation based on 732 words.
Number of 2-grams hit = 707  (96.58%)
Number of 1-grams hit = 25  (3.42%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article782.text
Will force inclusive back-off from OOVs.
Perplexity = 128.38, Entropy = 7.00 bits
Computation based on 1362 words.
Number of 2-grams hit = 1312  (96.33%)
Number of 1-grams hit = 50  (3.67%)
1 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article783.text
Will force inclusive back-off from OOVs.
Perplexity = 134.96, Entropy = 7.08 bits
Computation based on 501 words.
Number of 2-grams hit = 483  (96.41%)
Number of 1-grams hit = 18  (3.59%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article784.text
Will force inclusive back-off from OOVs.
Perplexity = 121.25, Entropy = 6.92 bits
Computation based on 1721 words.
Number of 2-grams hit = 1652  (95.99%)
Number of 1-grams hit = 69  (4.01%)
8 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article785.text
Will force inclusive back-off from OOVs.
Perplexity = 123.93, Entropy = 6.95 bits
Computation based on 2613 words.
Number of 2-grams hit = 2526  (96.67%)
Number of 1-grams hit = 87  (3.33%)
30 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article786.text
Will force inclusive back-off from OOVs.
Perplexity = 104.01, Entropy = 6.70 bits
Computation based on 353 words.
Number of 2-grams hit = 346  (98.02%)
Number of 1-grams hit = 7  (1.98%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article787.text
Will force inclusive back-off from OOVs.
Perplexity = 238.92, Entropy = 7.90 bits
Computation based on 472 words.
Number of 2-grams hit = 446  (94.49%)
Number of 1-grams hit = 26  (5.51%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article788.text
Will force inclusive back-off from OOVs.
Perplexity = 73.09, Entropy = 6.19 bits
Computation based on 1453 words.
Number of 2-grams hit = 1439  (99.04%)
Number of 1-grams hit = 14  (0.96%)
3 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article789.text
Will force inclusive back-off from OOVs.
Perplexity = 141.55, Entropy = 7.15 bits
Computation based on 11445 words.
Number of 2-grams hit = 10947  (95.65%)
Number of 1-grams hit = 498  (4.35%)
47 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article790.text
Will force inclusive back-off from OOVs.
Perplexity = 150.16, Entropy = 7.23 bits
Computation based on 935 words.
Number of 2-grams hit = 890  (95.19%)
Number of 1-grams hit = 45  (4.81%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article791.text
Will force inclusive back-off from OOVs.
Perplexity = 136.65, Entropy = 7.09 bits
Computation based on 696 words.
Number of 2-grams hit = 668  (95.98%)
Number of 1-grams hit = 28  (4.02%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article792.text
Will force inclusive back-off from OOVs.
Perplexity = 140.24, Entropy = 7.13 bits
Computation based on 2871 words.
Number of 2-grams hit = 2737  (95.33%)
Number of 1-grams hit = 134  (4.67%)
13 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article793.text
Will force inclusive back-off from OOVs.
Perplexity = 107.28, Entropy = 6.75 bits
Computation based on 180 words.
Number of 2-grams hit = 172  (95.56%)
Number of 1-grams hit = 8  (4.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article794.text
Will force inclusive back-off from OOVs.
Perplexity = 232.29, Entropy = 7.86 bits
Computation based on 500 words.
Number of 2-grams hit = 471  (94.20%)
Number of 1-grams hit = 29  (5.80%)
4 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article795.text
Will force inclusive back-off from OOVs.
Perplexity = 258.02, Entropy = 8.01 bits
Computation based on 426 words.
Number of 2-grams hit = 391  (91.78%)
Number of 1-grams hit = 35  (8.22%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article796.text
Will force inclusive back-off from OOVs.
Perplexity = 154.30, Entropy = 7.27 bits
Computation based on 1490 words.
Number of 2-grams hit = 1416  (95.03%)
Number of 1-grams hit = 74  (4.97%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article797.text
Will force inclusive back-off from OOVs.
Perplexity = 92.71, Entropy = 6.53 bits
Computation based on 375 words.
Number of 2-grams hit = 366  (97.60%)
Number of 1-grams hit = 9  (2.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article798.text
Will force inclusive back-off from OOVs.
Perplexity = 139.69, Entropy = 7.13 bits
Computation based on 458 words.
Number of 2-grams hit = 443  (96.72%)
Number of 1-grams hit = 15  (3.28%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article799.text
Will force inclusive back-off from OOVs.
Perplexity = 193.04, Entropy = 7.59 bits
Computation based on 741 words.
Number of 2-grams hit = 701  (94.60%)
Number of 1-grams hit = 40  (5.40%)
10 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article800.text
Will force inclusive back-off from OOVs.
Perplexity = 126.97, Entropy = 6.99 bits
Computation based on 431 words.
Number of 2-grams hit = 414  (96.06%)
Number of 1-grams hit = 17  (3.94%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article801.text
Will force inclusive back-off from OOVs.
Perplexity = 95.15, Entropy = 6.57 bits
Computation based on 3087 words.
Number of 2-grams hit = 3040  (98.48%)
Number of 1-grams hit = 47  (1.52%)
28 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article802.text
Will force inclusive back-off from OOVs.
Perplexity = 207.58, Entropy = 7.70 bits
Computation based on 2082 words.
Number of 2-grams hit = 1972  (94.72%)
Number of 1-grams hit = 110  (5.28%)
28 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article803.text
Will force inclusive back-off from OOVs.
Perplexity = 124.51, Entropy = 6.96 bits
Computation based on 4196 words.
Number of 2-grams hit = 4056  (96.66%)
Number of 1-grams hit = 140  (3.34%)
30 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article804.text
Will force inclusive back-off from OOVs.
Perplexity = 293.18, Entropy = 8.20 bits
Computation based on 1431 words.
Number of 2-grams hit = 1307  (91.33%)
Number of 1-grams hit = 124  (8.67%)
37 OOVs (2.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article805.text
Will force inclusive back-off from OOVs.
Perplexity = 148.26, Entropy = 7.21 bits
Computation based on 919 words.
Number of 2-grams hit = 882  (95.97%)
Number of 1-grams hit = 37  (4.03%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article806.text
Will force inclusive back-off from OOVs.
Perplexity = 129.62, Entropy = 7.02 bits
Computation based on 3287 words.
Number of 2-grams hit = 3180  (96.74%)
Number of 1-grams hit = 107  (3.26%)
16 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article807.text
Will force inclusive back-off from OOVs.
Perplexity = 109.03, Entropy = 6.77 bits
Computation based on 708 words.
Number of 2-grams hit = 690  (97.46%)
Number of 1-grams hit = 18  (2.54%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article808.text
Will force inclusive back-off from OOVs.
Perplexity = 156.23, Entropy = 7.29 bits
Computation based on 1682 words.
Number of 2-grams hit = 1597  (94.95%)
Number of 1-grams hit = 85  (5.05%)
12 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article809.text
Will force inclusive back-off from OOVs.
Perplexity = 145.32, Entropy = 7.18 bits
Computation based on 2017 words.
Number of 2-grams hit = 1936  (95.98%)
Number of 1-grams hit = 81  (4.02%)
9 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article810.text
Will force inclusive back-off from OOVs.
Perplexity = 126.49, Entropy = 6.98 bits
Computation based on 869 words.
Number of 2-grams hit = 844  (97.12%)
Number of 1-grams hit = 25  (2.88%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article811.text
Will force inclusive back-off from OOVs.
Perplexity = 141.74, Entropy = 7.15 bits
Computation based on 4497 words.
Number of 2-grams hit = 4305  (95.73%)
Number of 1-grams hit = 192  (4.27%)
14 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article812.text
Will force inclusive back-off from OOVs.
Perplexity = 113.09, Entropy = 6.82 bits
Computation based on 3353 words.
Number of 2-grams hit = 3263  (97.32%)
Number of 1-grams hit = 90  (2.68%)
16 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article813.text
Will force inclusive back-off from OOVs.
Perplexity = 220.40, Entropy = 7.78 bits
Computation based on 1357 words.
Number of 2-grams hit = 1269  (93.52%)
Number of 1-grams hit = 88  (6.48%)
23 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article814.text
Will force inclusive back-off from OOVs.
Perplexity = 106.29, Entropy = 6.73 bits
Computation based on 851 words.
Number of 2-grams hit = 832  (97.77%)
Number of 1-grams hit = 19  (2.23%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article815.text
Will force inclusive back-off from OOVs.
Perplexity = 134.98, Entropy = 7.08 bits
Computation based on 1264 words.
Number of 2-grams hit = 1209  (95.65%)
Number of 1-grams hit = 55  (4.35%)
10 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article816.text
Will force inclusive back-off from OOVs.
Perplexity = 100.90, Entropy = 6.66 bits
Computation based on 1331 words.
Number of 2-grams hit = 1294  (97.22%)
Number of 1-grams hit = 37  (2.78%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article817.text
Will force inclusive back-off from OOVs.
Perplexity = 149.52, Entropy = 7.22 bits
Computation based on 533 words.
Number of 2-grams hit = 508  (95.31%)
Number of 1-grams hit = 25  (4.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article818.text
Will force inclusive back-off from OOVs.
Perplexity = 234.64, Entropy = 7.87 bits
Computation based on 532 words.
Number of 2-grams hit = 485  (91.17%)
Number of 1-grams hit = 47  (8.83%)
21 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article819.text
Will force inclusive back-off from OOVs.
Perplexity = 150.88, Entropy = 7.24 bits
Computation based on 1803 words.
Number of 2-grams hit = 1715  (95.12%)
Number of 1-grams hit = 88  (4.88%)
6 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article820.text
Will force inclusive back-off from OOVs.
Perplexity = 167.35, Entropy = 7.39 bits
Computation based on 816 words.
Number of 2-grams hit = 781  (95.71%)
Number of 1-grams hit = 35  (4.29%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article821.text
Will force inclusive back-off from OOVs.
Perplexity = 99.00, Entropy = 6.63 bits
Computation based on 1100 words.
Number of 2-grams hit = 1077  (97.91%)
Number of 1-grams hit = 23  (2.09%)
4 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article822.text
Will force inclusive back-off from OOVs.
Perplexity = 147.09, Entropy = 7.20 bits
Computation based on 2420 words.
Number of 2-grams hit = 2318  (95.79%)
Number of 1-grams hit = 102  (4.21%)
4 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article823.text
Will force inclusive back-off from OOVs.
Perplexity = 141.13, Entropy = 7.14 bits
Computation based on 1307 words.
Number of 2-grams hit = 1256  (96.10%)
Number of 1-grams hit = 51  (3.90%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article824.text
Will force inclusive back-off from OOVs.
Perplexity = 235.29, Entropy = 7.88 bits
Computation based on 486 words.
Number of 2-grams hit = 454  (93.42%)
Number of 1-grams hit = 32  (6.58%)
16 OOVs (3.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article825.text
Will force inclusive back-off from OOVs.
Perplexity = 124.40, Entropy = 6.96 bits
Computation based on 571 words.
Number of 2-grams hit = 550  (96.32%)
Number of 1-grams hit = 21  (3.68%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article826.text
Will force inclusive back-off from OOVs.
Perplexity = 112.30, Entropy = 6.81 bits
Computation based on 889 words.
Number of 2-grams hit = 862  (96.96%)
Number of 1-grams hit = 27  (3.04%)
4 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article827.text
Will force inclusive back-off from OOVs.
Perplexity = 179.99, Entropy = 7.49 bits
Computation based on 627 words.
Number of 2-grams hit = 590  (94.10%)
Number of 1-grams hit = 37  (5.90%)
21 OOVs (3.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article828.text
Will force inclusive back-off from OOVs.
Perplexity = 129.44, Entropy = 7.02 bits
Computation based on 11850 words.
Number of 2-grams hit = 11409  (96.28%)
Number of 1-grams hit = 441  (3.72%)
147 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article829.text
Will force inclusive back-off from OOVs.
Perplexity = 278.17, Entropy = 8.12 bits
Computation based on 531 words.
Number of 2-grams hit = 492  (92.66%)
Number of 1-grams hit = 39  (7.34%)
9 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article830.text
Will force inclusive back-off from OOVs.
Perplexity = 211.93, Entropy = 7.73 bits
Computation based on 2276 words.
Number of 2-grams hit = 2103  (92.40%)
Number of 1-grams hit = 173  (7.60%)
43 OOVs (1.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article831.text
Will force inclusive back-off from OOVs.
Perplexity = 164.95, Entropy = 7.37 bits
Computation based on 1025 words.
Number of 2-grams hit = 966  (94.24%)
Number of 1-grams hit = 59  (5.76%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article832.text
Will force inclusive back-off from OOVs.
Perplexity = 131.31, Entropy = 7.04 bits
Computation based on 419 words.
Number of 2-grams hit = 401  (95.70%)
Number of 1-grams hit = 18  (4.30%)
3 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article833.text
Will force inclusive back-off from OOVs.
Perplexity = 150.96, Entropy = 7.24 bits
Computation based on 826 words.
Number of 2-grams hit = 789  (95.52%)
Number of 1-grams hit = 37  (4.48%)
3 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article834.text
Will force inclusive back-off from OOVs.
Perplexity = 125.30, Entropy = 6.97 bits
Computation based on 881 words.
Number of 2-grams hit = 853  (96.82%)
Number of 1-grams hit = 28  (3.18%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article835.text
Will force inclusive back-off from OOVs.
Perplexity = 149.76, Entropy = 7.23 bits
Computation based on 460 words.
Number of 2-grams hit = 441  (95.87%)
Number of 1-grams hit = 19  (4.13%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article836.text
Will force inclusive back-off from OOVs.
Perplexity = 131.81, Entropy = 7.04 bits
Computation based on 483 words.
Number of 2-grams hit = 465  (96.27%)
Number of 1-grams hit = 18  (3.73%)
8 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article837.text
Will force inclusive back-off from OOVs.
Perplexity = 253.90, Entropy = 7.99 bits
Computation based on 845 words.
Number of 2-grams hit = 781  (92.43%)
Number of 1-grams hit = 64  (7.57%)
7 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article838.text
Will force inclusive back-off from OOVs.
Perplexity = 179.09, Entropy = 7.48 bits
Computation based on 2236 words.
Number of 2-grams hit = 2126  (95.08%)
Number of 1-grams hit = 110  (4.92%)
35 OOVs (1.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article839.text
Will force inclusive back-off from OOVs.
Perplexity = 151.77, Entropy = 7.25 bits
Computation based on 461 words.
Number of 2-grams hit = 445  (96.53%)
Number of 1-grams hit = 16  (3.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article840.text
Will force inclusive back-off from OOVs.
Perplexity = 152.56, Entropy = 7.25 bits
Computation based on 521 words.
Number of 2-grams hit = 497  (95.39%)
Number of 1-grams hit = 24  (4.61%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article841.text
Will force inclusive back-off from OOVs.
Perplexity = 136.92, Entropy = 7.10 bits
Computation based on 916 words.
Number of 2-grams hit = 875  (95.52%)
Number of 1-grams hit = 41  (4.48%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article842.text
Will force inclusive back-off from OOVs.
Perplexity = 162.02, Entropy = 7.34 bits
Computation based on 473 words.
Number of 2-grams hit = 453  (95.77%)
Number of 1-grams hit = 20  (4.23%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article843.text
Will force inclusive back-off from OOVs.
Perplexity = 118.99, Entropy = 6.89 bits
Computation based on 1310 words.
Number of 2-grams hit = 1263  (96.41%)
Number of 1-grams hit = 47  (3.59%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article844.text
Will force inclusive back-off from OOVs.
Perplexity = 135.86, Entropy = 7.09 bits
Computation based on 15758 words.
Number of 2-grams hit = 15117  (95.93%)
Number of 1-grams hit = 641  (4.07%)
53 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article845.text
Will force inclusive back-off from OOVs.
Perplexity = 133.12, Entropy = 7.06 bits
Computation based on 601 words.
Number of 2-grams hit = 575  (95.67%)
Number of 1-grams hit = 26  (4.33%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article846.text
Will force inclusive back-off from OOVs.
Perplexity = 162.00, Entropy = 7.34 bits
Computation based on 556 words.
Number of 2-grams hit = 526  (94.60%)
Number of 1-grams hit = 30  (5.40%)
8 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article847.text
Will force inclusive back-off from OOVs.
Perplexity = 108.41, Entropy = 6.76 bits
Computation based on 1613 words.
Number of 2-grams hit = 1562  (96.84%)
Number of 1-grams hit = 51  (3.16%)
8 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article848.text
Will force inclusive back-off from OOVs.
Perplexity = 103.46, Entropy = 6.69 bits
Computation based on 366 words.
Number of 2-grams hit = 357  (97.54%)
Number of 1-grams hit = 9  (2.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article849.text
Will force inclusive back-off from OOVs.
Perplexity = 142.06, Entropy = 7.15 bits
Computation based on 1550 words.
Number of 2-grams hit = 1484  (95.74%)
Number of 1-grams hit = 66  (4.26%)
5 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article850.text
Will force inclusive back-off from OOVs.
Perplexity = 130.11, Entropy = 7.02 bits
Computation based on 443 words.
Number of 2-grams hit = 429  (96.84%)
Number of 1-grams hit = 14  (3.16%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article851.text
Will force inclusive back-off from OOVs.
Perplexity = 160.40, Entropy = 7.33 bits
Computation based on 558 words.
Number of 2-grams hit = 529  (94.80%)
Number of 1-grams hit = 29  (5.20%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article852.text
Will force inclusive back-off from OOVs.
Perplexity = 139.19, Entropy = 7.12 bits
Computation based on 622 words.
Number of 2-grams hit = 598  (96.14%)
Number of 1-grams hit = 24  (3.86%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article853.text
Will force inclusive back-off from OOVs.
Perplexity = 133.15, Entropy = 7.06 bits
Computation based on 1476 words.
Number of 2-grams hit = 1404  (95.12%)
Number of 1-grams hit = 72  (4.88%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article854.text
Will force inclusive back-off from OOVs.
Perplexity = 154.62, Entropy = 7.27 bits
Computation based on 2651 words.
Number of 2-grams hit = 2538  (95.74%)
Number of 1-grams hit = 113  (4.26%)
18 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article855.text
Will force inclusive back-off from OOVs.
Perplexity = 87.51, Entropy = 6.45 bits
Computation based on 423 words.
Number of 2-grams hit = 413  (97.64%)
Number of 1-grams hit = 10  (2.36%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article856.text
Will force inclusive back-off from OOVs.
Perplexity = 145.54, Entropy = 7.19 bits
Computation based on 824 words.
Number of 2-grams hit = 779  (94.54%)
Number of 1-grams hit = 45  (5.46%)
9 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article857.text
Will force inclusive back-off from OOVs.
Perplexity = 195.50, Entropy = 7.61 bits
Computation based on 354 words.
Number of 2-grams hit = 333  (94.07%)
Number of 1-grams hit = 21  (5.93%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article858.text
Will force inclusive back-off from OOVs.
Perplexity = 94.04, Entropy = 6.56 bits
Computation based on 237 words.
Number of 2-grams hit = 233  (98.31%)
Number of 1-grams hit = 4  (1.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article859.text
Will force inclusive back-off from OOVs.
Perplexity = 215.69, Entropy = 7.75 bits
Computation based on 1095 words.
Number of 2-grams hit = 1040  (94.98%)
Number of 1-grams hit = 55  (5.02%)
16 OOVs (1.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article860.text
Will force inclusive back-off from OOVs.
Perplexity = 132.71, Entropy = 7.05 bits
Computation based on 450 words.
Number of 2-grams hit = 436  (96.89%)
Number of 1-grams hit = 14  (3.11%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article861.text
Will force inclusive back-off from OOVs.
Perplexity = 134.96, Entropy = 7.08 bits
Computation based on 393 words.
Number of 2-grams hit = 373  (94.91%)
Number of 1-grams hit = 20  (5.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article862.text
Will force inclusive back-off from OOVs.
Perplexity = 106.55, Entropy = 6.74 bits
Computation based on 452 words.
Number of 2-grams hit = 435  (96.24%)
Number of 1-grams hit = 17  (3.76%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article863.text
Will force inclusive back-off from OOVs.
Perplexity = 136.23, Entropy = 7.09 bits
Computation based on 662 words.
Number of 2-grams hit = 635  (95.92%)
Number of 1-grams hit = 27  (4.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article864.text
Will force inclusive back-off from OOVs.
Perplexity = 329.42, Entropy = 8.36 bits
Computation based on 442 words.
Number of 2-grams hit = 409  (92.53%)
Number of 1-grams hit = 33  (7.47%)
7 OOVs (1.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article865.text
Will force inclusive back-off from OOVs.
Perplexity = 78.35, Entropy = 6.29 bits
Computation based on 6140 words.
Number of 2-grams hit = 6000  (97.72%)
Number of 1-grams hit = 140  (2.28%)
25 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article866.text
Will force inclusive back-off from OOVs.
Perplexity = 227.03, Entropy = 7.83 bits
Computation based on 615 words.
Number of 2-grams hit = 565  (91.87%)
Number of 1-grams hit = 50  (8.13%)
12 OOVs (1.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article867.text
Will force inclusive back-off from OOVs.
Perplexity = 138.56, Entropy = 7.11 bits
Computation based on 341 words.
Number of 2-grams hit = 323  (94.72%)
Number of 1-grams hit = 18  (5.28%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article868.text
Will force inclusive back-off from OOVs.
Perplexity = 128.05, Entropy = 7.00 bits
Computation based on 1652 words.
Number of 2-grams hit = 1585  (95.94%)
Number of 1-grams hit = 67  (4.06%)
5 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article869.text
Will force inclusive back-off from OOVs.
Perplexity = 130.38, Entropy = 7.03 bits
Computation based on 3106 words.
Number of 2-grams hit = 3005  (96.75%)
Number of 1-grams hit = 101  (3.25%)
10 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article870.text
Will force inclusive back-off from OOVs.
Perplexity = 163.80, Entropy = 7.36 bits
Computation based on 768 words.
Number of 2-grams hit = 731  (95.18%)
Number of 1-grams hit = 37  (4.82%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article871.text
Will force inclusive back-off from OOVs.
Perplexity = 150.65, Entropy = 7.24 bits
Computation based on 412 words.
Number of 2-grams hit = 401  (97.33%)
Number of 1-grams hit = 11  (2.67%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article872.text
Will force inclusive back-off from OOVs.
Perplexity = 164.83, Entropy = 7.36 bits
Computation based on 294 words.
Number of 2-grams hit = 285  (96.94%)
Number of 1-grams hit = 9  (3.06%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article873.text
Will force inclusive back-off from OOVs.
Perplexity = 159.15, Entropy = 7.31 bits
Computation based on 930 words.
Number of 2-grams hit = 880  (94.62%)
Number of 1-grams hit = 50  (5.38%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article874.text
Will force inclusive back-off from OOVs.
Perplexity = 144.33, Entropy = 7.17 bits
Computation based on 429 words.
Number of 2-grams hit = 414  (96.50%)
Number of 1-grams hit = 15  (3.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article875.text
Will force inclusive back-off from OOVs.
Perplexity = 192.58, Entropy = 7.59 bits
Computation based on 786 words.
Number of 2-grams hit = 736  (93.64%)
Number of 1-grams hit = 50  (6.36%)
7 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article876.text
Will force inclusive back-off from OOVs.
Perplexity = 144.20, Entropy = 7.17 bits
Computation based on 9198 words.
Number of 2-grams hit = 8794  (95.61%)
Number of 1-grams hit = 404  (4.39%)
33 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article877.text
Will force inclusive back-off from OOVs.
Perplexity = 110.96, Entropy = 6.79 bits
Computation based on 458 words.
Number of 2-grams hit = 438  (95.63%)
Number of 1-grams hit = 20  (4.37%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article878.text
Will force inclusive back-off from OOVs.
Perplexity = 129.11, Entropy = 7.01 bits
Computation based on 333 words.
Number of 2-grams hit = 320  (96.10%)
Number of 1-grams hit = 13  (3.90%)
5 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article879.text
Will force inclusive back-off from OOVs.
Perplexity = 149.41, Entropy = 7.22 bits
Computation based on 539 words.
Number of 2-grams hit = 517  (95.92%)
Number of 1-grams hit = 22  (4.08%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article880.text
Will force inclusive back-off from OOVs.
Perplexity = 129.55, Entropy = 7.02 bits
Computation based on 459 words.
Number of 2-grams hit = 444  (96.73%)
Number of 1-grams hit = 15  (3.27%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article881.text
Will force inclusive back-off from OOVs.
Perplexity = 201.81, Entropy = 7.66 bits
Computation based on 345 words.
Number of 2-grams hit = 329  (95.36%)
Number of 1-grams hit = 16  (4.64%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article882.text
Will force inclusive back-off from OOVs.
Perplexity = 120.95, Entropy = 6.92 bits
Computation based on 591 words.
Number of 2-grams hit = 571  (96.62%)
Number of 1-grams hit = 20  (3.38%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article883.text
Will force inclusive back-off from OOVs.
Perplexity = 111.43, Entropy = 6.80 bits
Computation based on 591 words.
Number of 2-grams hit = 574  (97.12%)
Number of 1-grams hit = 17  (2.88%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article884.text
Will force inclusive back-off from OOVs.
Perplexity = 130.69, Entropy = 7.03 bits
Computation based on 727 words.
Number of 2-grams hit = 704  (96.84%)
Number of 1-grams hit = 23  (3.16%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article885.text
Will force inclusive back-off from OOVs.
Perplexity = 95.63, Entropy = 6.58 bits
Computation based on 482 words.
Number of 2-grams hit = 472  (97.93%)
Number of 1-grams hit = 10  (2.07%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article886.text
Will force inclusive back-off from OOVs.
Perplexity = 148.84, Entropy = 7.22 bits
Computation based on 473 words.
Number of 2-grams hit = 455  (96.19%)
Number of 1-grams hit = 18  (3.81%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article887.text
Will force inclusive back-off from OOVs.
Perplexity = 121.29, Entropy = 6.92 bits
Computation based on 481 words.
Number of 2-grams hit = 472  (98.13%)
Number of 1-grams hit = 9  (1.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article888.text
Will force inclusive back-off from OOVs.
Perplexity = 161.95, Entropy = 7.34 bits
Computation based on 1207 words.
Number of 2-grams hit = 1144  (94.78%)
Number of 1-grams hit = 63  (5.22%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article889.text
Will force inclusive back-off from OOVs.
Perplexity = 144.86, Entropy = 7.18 bits
Computation based on 4303 words.
Number of 2-grams hit = 4108  (95.47%)
Number of 1-grams hit = 195  (4.53%)
10 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article890.text
Will force inclusive back-off from OOVs.
Perplexity = 154.94, Entropy = 7.28 bits
Computation based on 1038 words.
Number of 2-grams hit = 981  (94.51%)
Number of 1-grams hit = 57  (5.49%)
13 OOVs (1.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article891.text
Will force inclusive back-off from OOVs.
Perplexity = 135.78, Entropy = 7.09 bits
Computation based on 804 words.
Number of 2-grams hit = 767  (95.40%)
Number of 1-grams hit = 37  (4.60%)
4 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article892.text
Will force inclusive back-off from OOVs.
Perplexity = 97.20, Entropy = 6.60 bits
Computation based on 3156 words.
Number of 2-grams hit = 3080  (97.59%)
Number of 1-grams hit = 76  (2.41%)
7 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article893.text
Will force inclusive back-off from OOVs.
Perplexity = 121.17, Entropy = 6.92 bits
Computation based on 625 words.
Number of 2-grams hit = 601  (96.16%)
Number of 1-grams hit = 24  (3.84%)
8 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article894.text
Will force inclusive back-off from OOVs.
Perplexity = 181.56, Entropy = 7.50 bits
Computation based on 743 words.
Number of 2-grams hit = 707  (95.15%)
Number of 1-grams hit = 36  (4.85%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article895.text
Will force inclusive back-off from OOVs.
Perplexity = 139.18, Entropy = 7.12 bits
Computation based on 169 words.
Number of 2-grams hit = 162  (95.86%)
Number of 1-grams hit = 7  (4.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article896.text
Will force inclusive back-off from OOVs.
Perplexity = 152.44, Entropy = 7.25 bits
Computation based on 413 words.
Number of 2-grams hit = 387  (93.70%)
Number of 1-grams hit = 26  (6.30%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article897.text
Will force inclusive back-off from OOVs.
Perplexity = 113.00, Entropy = 6.82 bits
Computation based on 696 words.
Number of 2-grams hit = 677  (97.27%)
Number of 1-grams hit = 19  (2.73%)
4 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article898.text
Will force inclusive back-off from OOVs.
Perplexity = 133.77, Entropy = 7.06 bits
Computation based on 1592 words.
Number of 2-grams hit = 1526  (95.85%)
Number of 1-grams hit = 66  (4.15%)
10 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article899.text
Will force inclusive back-off from OOVs.
Perplexity = 249.25, Entropy = 7.96 bits
Computation based on 199 words.
Number of 2-grams hit = 183  (91.96%)
Number of 1-grams hit = 16  (8.04%)
5 OOVs (2.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article900.text
Will force inclusive back-off from OOVs.
Perplexity = 159.10, Entropy = 7.31 bits
Computation based on 1046 words.
Number of 2-grams hit = 994  (95.03%)
Number of 1-grams hit = 52  (4.97%)
5 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article901.text
Will force inclusive back-off from OOVs.
Perplexity = 191.32, Entropy = 7.58 bits
Computation based on 388 words.
Number of 2-grams hit = 362  (93.30%)
Number of 1-grams hit = 26  (6.70%)
11 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article902.text
Will force inclusive back-off from OOVs.
Perplexity = 130.71, Entropy = 7.03 bits
Computation based on 1995 words.
Number of 2-grams hit = 1933  (96.89%)
Number of 1-grams hit = 62  (3.11%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article903.text
Will force inclusive back-off from OOVs.
Perplexity = 147.20, Entropy = 7.20 bits
Computation based on 1184 words.
Number of 2-grams hit = 1137  (96.03%)
Number of 1-grams hit = 47  (3.97%)
5 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article904.text
Will force inclusive back-off from OOVs.
Perplexity = 108.65, Entropy = 6.76 bits
Computation based on 623 words.
Number of 2-grams hit = 612  (98.23%)
Number of 1-grams hit = 11  (1.77%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article905.text
Will force inclusive back-off from OOVs.
Perplexity = 228.88, Entropy = 7.84 bits
Computation based on 354 words.
Number of 2-grams hit = 329  (92.94%)
Number of 1-grams hit = 25  (7.06%)
3 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article906.text
Will force inclusive back-off from OOVs.
Perplexity = 104.76, Entropy = 6.71 bits
Computation based on 429 words.
Number of 2-grams hit = 409  (95.34%)
Number of 1-grams hit = 20  (4.66%)
7 OOVs (1.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article907.text
Will force inclusive back-off from OOVs.
Perplexity = 126.53, Entropy = 6.98 bits
Computation based on 445 words.
Number of 2-grams hit = 429  (96.40%)
Number of 1-grams hit = 16  (3.60%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article908.text
Will force inclusive back-off from OOVs.
Perplexity = 142.93, Entropy = 7.16 bits
Computation based on 524 words.
Number of 2-grams hit = 504  (96.18%)
Number of 1-grams hit = 20  (3.82%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article909.text
Will force inclusive back-off from OOVs.
Perplexity = 222.14, Entropy = 7.80 bits
Computation based on 413 words.
Number of 2-grams hit = 383  (92.74%)
Number of 1-grams hit = 30  (7.26%)
4 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article910.text
Will force inclusive back-off from OOVs.
Perplexity = 127.35, Entropy = 6.99 bits
Computation based on 667 words.
Number of 2-grams hit = 638  (95.65%)
Number of 1-grams hit = 29  (4.35%)
4 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article911.text
Will force inclusive back-off from OOVs.
Perplexity = 129.79, Entropy = 7.02 bits
Computation based on 635 words.
Number of 2-grams hit = 612  (96.38%)
Number of 1-grams hit = 23  (3.62%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article912.text
Will force inclusive back-off from OOVs.
Perplexity = 95.35, Entropy = 6.58 bits
Computation based on 675 words.
Number of 2-grams hit = 654  (96.89%)
Number of 1-grams hit = 21  (3.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article913.text
Will force inclusive back-off from OOVs.
Perplexity = 108.56, Entropy = 6.76 bits
Computation based on 670 words.
Number of 2-grams hit = 646  (96.42%)
Number of 1-grams hit = 24  (3.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article914.text
Will force inclusive back-off from OOVs.
Perplexity = 157.45, Entropy = 7.30 bits
Computation based on 529 words.
Number of 2-grams hit = 503  (95.09%)
Number of 1-grams hit = 26  (4.91%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article915.text
Will force inclusive back-off from OOVs.
Perplexity = 147.56, Entropy = 7.21 bits
Computation based on 1206 words.
Number of 2-grams hit = 1158  (96.02%)
Number of 1-grams hit = 48  (3.98%)
5 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article916.text
Will force inclusive back-off from OOVs.
Perplexity = 185.94, Entropy = 7.54 bits
Computation based on 939 words.
Number of 2-grams hit = 886  (94.36%)
Number of 1-grams hit = 53  (5.64%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article917.text
Will force inclusive back-off from OOVs.
Perplexity = 116.42, Entropy = 6.86 bits
Computation based on 1076 words.
Number of 2-grams hit = 1039  (96.56%)
Number of 1-grams hit = 37  (3.44%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article918.text
Will force inclusive back-off from OOVs.
Perplexity = 148.20, Entropy = 7.21 bits
Computation based on 482 words.
Number of 2-grams hit = 454  (94.19%)
Number of 1-grams hit = 28  (5.81%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article919.text
Will force inclusive back-off from OOVs.
Perplexity = 137.96, Entropy = 7.11 bits
Computation based on 3300 words.
Number of 2-grams hit = 3167  (95.97%)
Number of 1-grams hit = 133  (4.03%)
15 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article920.text
Will force inclusive back-off from OOVs.
Perplexity = 135.68, Entropy = 7.08 bits
Computation based on 915 words.
Number of 2-grams hit = 873  (95.41%)
Number of 1-grams hit = 42  (4.59%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article921.text
Will force inclusive back-off from OOVs.
Perplexity = 158.96, Entropy = 7.31 bits
Computation based on 795 words.
Number of 2-grams hit = 763  (95.97%)
Number of 1-grams hit = 32  (4.03%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article922.text
Will force inclusive back-off from OOVs.
Perplexity = 120.56, Entropy = 6.91 bits
Computation based on 474 words.
Number of 2-grams hit = 451  (95.15%)
Number of 1-grams hit = 23  (4.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article923.text
Will force inclusive back-off from OOVs.
Perplexity = 132.27, Entropy = 7.05 bits
Computation based on 519 words.
Number of 2-grams hit = 504  (97.11%)
Number of 1-grams hit = 15  (2.89%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article924.text
Will force inclusive back-off from OOVs.
Perplexity = 93.95, Entropy = 6.55 bits
Computation based on 341 words.
Number of 2-grams hit = 330  (96.77%)
Number of 1-grams hit = 11  (3.23%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article925.text
Will force inclusive back-off from OOVs.
Perplexity = 103.64, Entropy = 6.70 bits
Computation based on 3813 words.
Number of 2-grams hit = 3721  (97.59%)
Number of 1-grams hit = 92  (2.41%)
21 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article926.text
Will force inclusive back-off from OOVs.
Perplexity = 103.84, Entropy = 6.70 bits
Computation based on 1337 words.
Number of 2-grams hit = 1305  (97.61%)
Number of 1-grams hit = 32  (2.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article927.text
Will force inclusive back-off from OOVs.
Perplexity = 206.38, Entropy = 7.69 bits
Computation based on 403 words.
Number of 2-grams hit = 373  (92.56%)
Number of 1-grams hit = 30  (7.44%)
4 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article928.text
Will force inclusive back-off from OOVs.
Perplexity = 140.90, Entropy = 7.14 bits
Computation based on 483 words.
Number of 2-grams hit = 469  (97.10%)
Number of 1-grams hit = 14  (2.90%)
6 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article929.text
Will force inclusive back-off from OOVs.
Perplexity = 183.80, Entropy = 7.52 bits
Computation based on 502 words.
Number of 2-grams hit = 483  (96.22%)
Number of 1-grams hit = 19  (3.78%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article930.text
Will force inclusive back-off from OOVs.
Perplexity = 215.51, Entropy = 7.75 bits
Computation based on 417 words.
Number of 2-grams hit = 397  (95.20%)
Number of 1-grams hit = 20  (4.80%)
17 OOVs (3.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article931.text
Will force inclusive back-off from OOVs.
Perplexity = 145.20, Entropy = 7.18 bits
Computation based on 595 words.
Number of 2-grams hit = 574  (96.47%)
Number of 1-grams hit = 21  (3.53%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article932.text
Will force inclusive back-off from OOVs.
Perplexity = 162.99, Entropy = 7.35 bits
Computation based on 211 words.
Number of 2-grams hit = 200  (94.79%)
Number of 1-grams hit = 11  (5.21%)
1 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article933.text
Will force inclusive back-off from OOVs.
Perplexity = 123.57, Entropy = 6.95 bits
Computation based on 410 words.
Number of 2-grams hit = 392  (95.61%)
Number of 1-grams hit = 18  (4.39%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article934.text
Will force inclusive back-off from OOVs.
Perplexity = 193.28, Entropy = 7.59 bits
Computation based on 384 words.
Number of 2-grams hit = 359  (93.49%)
Number of 1-grams hit = 25  (6.51%)
7 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article935.text
Will force inclusive back-off from OOVs.
Perplexity = 129.80, Entropy = 7.02 bits
Computation based on 1153 words.
Number of 2-grams hit = 1106  (95.92%)
Number of 1-grams hit = 47  (4.08%)
3 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article936.text
Will force inclusive back-off from OOVs.
Perplexity = 212.98, Entropy = 7.73 bits
Computation based on 435 words.
Number of 2-grams hit = 399  (91.72%)
Number of 1-grams hit = 36  (8.28%)
6 OOVs (1.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article937.text
Will force inclusive back-off from OOVs.
Perplexity = 218.46, Entropy = 7.77 bits
Computation based on 228 words.
Number of 2-grams hit = 213  (93.42%)
Number of 1-grams hit = 15  (6.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article938.text
Will force inclusive back-off from OOVs.
Perplexity = 148.62, Entropy = 7.22 bits
Computation based on 1295 words.
Number of 2-grams hit = 1232  (95.14%)
Number of 1-grams hit = 63  (4.86%)
5 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article939.text
Will force inclusive back-off from OOVs.
Perplexity = 132.15, Entropy = 7.05 bits
Computation based on 476 words.
Number of 2-grams hit = 459  (96.43%)
Number of 1-grams hit = 17  (3.57%)
5 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article940.text
Will force inclusive back-off from OOVs.
Perplexity = 135.00, Entropy = 7.08 bits
Computation based on 1898 words.
Number of 2-grams hit = 1816  (95.68%)
Number of 1-grams hit = 82  (4.32%)
3 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article941.text
Will force inclusive back-off from OOVs.
Perplexity = 145.38, Entropy = 7.18 bits
Computation based on 1216 words.
Number of 2-grams hit = 1165  (95.81%)
Number of 1-grams hit = 51  (4.19%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article942.text
Will force inclusive back-off from OOVs.
Perplexity = 129.18, Entropy = 7.01 bits
Computation based on 524 words.
Number of 2-grams hit = 501  (95.61%)
Number of 1-grams hit = 23  (4.39%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article943.text
Will force inclusive back-off from OOVs.
Perplexity = 128.17, Entropy = 7.00 bits
Computation based on 1395 words.
Number of 2-grams hit = 1349  (96.70%)
Number of 1-grams hit = 46  (3.30%)
10 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article944.text
Will force inclusive back-off from OOVs.
Perplexity = 128.64, Entropy = 7.01 bits
Computation based on 436 words.
Number of 2-grams hit = 422  (96.79%)
Number of 1-grams hit = 14  (3.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article945.text
Will force inclusive back-off from OOVs.
Perplexity = 128.57, Entropy = 7.01 bits
Computation based on 3522 words.
Number of 2-grams hit = 3430  (97.39%)
Number of 1-grams hit = 92  (2.61%)
22 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article946.text
Will force inclusive back-off from OOVs.
Perplexity = 197.40, Entropy = 7.63 bits
Computation based on 551 words.
Number of 2-grams hit = 512  (92.92%)
Number of 1-grams hit = 39  (7.08%)
15 OOVs (2.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article947.text
Will force inclusive back-off from OOVs.
Perplexity = 181.04, Entropy = 7.50 bits
Computation based on 422 words.
Number of 2-grams hit = 402  (95.26%)
Number of 1-grams hit = 20  (4.74%)
7 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article948.text
Will force inclusive back-off from OOVs.
Perplexity = 145.73, Entropy = 7.19 bits
Computation based on 581 words.
Number of 2-grams hit = 556  (95.70%)
Number of 1-grams hit = 25  (4.30%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article949.text
Will force inclusive back-off from OOVs.
Perplexity = 155.26, Entropy = 7.28 bits
Computation based on 1701 words.
Number of 2-grams hit = 1623  (95.41%)
Number of 1-grams hit = 78  (4.59%)
9 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article950.text
Will force inclusive back-off from OOVs.
Perplexity = 309.62, Entropy = 8.27 bits
Computation based on 500 words.
Number of 2-grams hit = 456  (91.20%)
Number of 1-grams hit = 44  (8.80%)
7 OOVs (1.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article951.text
Will force inclusive back-off from OOVs.
Perplexity = 101.97, Entropy = 6.67 bits
Computation based on 564 words.
Number of 2-grams hit = 551  (97.70%)
Number of 1-grams hit = 13  (2.30%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article952.text
Will force inclusive back-off from OOVs.
Perplexity = 157.94, Entropy = 7.30 bits
Computation based on 1283 words.
Number of 2-grams hit = 1221  (95.17%)
Number of 1-grams hit = 62  (4.83%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article953.text
Will force inclusive back-off from OOVs.
Perplexity = 134.28, Entropy = 7.07 bits
Computation based on 2117 words.
Number of 2-grams hit = 2054  (97.02%)
Number of 1-grams hit = 63  (2.98%)
18 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article954.text
Will force inclusive back-off from OOVs.
Perplexity = 127.83, Entropy = 7.00 bits
Computation based on 730 words.
Number of 2-grams hit = 701  (96.03%)
Number of 1-grams hit = 29  (3.97%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article955.text
Will force inclusive back-off from OOVs.
Perplexity = 94.46, Entropy = 6.56 bits
Computation based on 1475 words.
Number of 2-grams hit = 1449  (98.24%)
Number of 1-grams hit = 26  (1.76%)
2 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article956.text
Will force inclusive back-off from OOVs.
Perplexity = 260.79, Entropy = 8.03 bits
Computation based on 1026 words.
Number of 2-grams hit = 947  (92.30%)
Number of 1-grams hit = 79  (7.70%)
8 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article957.text
Will force inclusive back-off from OOVs.
Perplexity = 238.07, Entropy = 7.90 bits
Computation based on 302 words.
Number of 2-grams hit = 276  (91.39%)
Number of 1-grams hit = 26  (8.61%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article958.text
Will force inclusive back-off from OOVs.
Perplexity = 151.40, Entropy = 7.24 bits
Computation based on 940 words.
Number of 2-grams hit = 898  (95.53%)
Number of 1-grams hit = 42  (4.47%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article959.text
Will force inclusive back-off from OOVs.
Perplexity = 134.69, Entropy = 7.07 bits
Computation based on 201 words.
Number of 2-grams hit = 190  (94.53%)
Number of 1-grams hit = 11  (5.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article960.text
Will force inclusive back-off from OOVs.
Perplexity = 124.25, Entropy = 6.96 bits
Computation based on 605 words.
Number of 2-grams hit = 586  (96.86%)
Number of 1-grams hit = 19  (3.14%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article961.text
Will force inclusive back-off from OOVs.
Perplexity = 173.75, Entropy = 7.44 bits
Computation based on 475 words.
Number of 2-grams hit = 456  (96.00%)
Number of 1-grams hit = 19  (4.00%)
17 OOVs (3.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article962.text
Will force inclusive back-off from OOVs.
Perplexity = 105.31, Entropy = 6.72 bits
Computation based on 2235 words.
Number of 2-grams hit = 2170  (97.09%)
Number of 1-grams hit = 65  (2.91%)
1 OOVs (0.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article963.text
Will force inclusive back-off from OOVs.
Perplexity = 123.12, Entropy = 6.94 bits
Computation based on 2012 words.
Number of 2-grams hit = 1937  (96.27%)
Number of 1-grams hit = 75  (3.73%)
5 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article964.text
Will force inclusive back-off from OOVs.
Perplexity = 112.59, Entropy = 6.81 bits
Computation based on 1010 words.
Number of 2-grams hit = 985  (97.52%)
Number of 1-grams hit = 25  (2.48%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article965.text
Will force inclusive back-off from OOVs.
Perplexity = 163.84, Entropy = 7.36 bits
Computation based on 643 words.
Number of 2-grams hit = 603  (93.78%)
Number of 1-grams hit = 40  (6.22%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article966.text
Will force inclusive back-off from OOVs.
Perplexity = 171.35, Entropy = 7.42 bits
Computation based on 789 words.
Number of 2-grams hit = 745  (94.42%)
Number of 1-grams hit = 44  (5.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article967.text
Will force inclusive back-off from OOVs.
Perplexity = 112.93, Entropy = 6.82 bits
Computation based on 388 words.
Number of 2-grams hit = 375  (96.65%)
Number of 1-grams hit = 13  (3.35%)
3 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article968.text
Will force inclusive back-off from OOVs.
Perplexity = 92.08, Entropy = 6.52 bits
Computation based on 1057 words.
Number of 2-grams hit = 1032  (97.63%)
Number of 1-grams hit = 25  (2.37%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article969.text
Will force inclusive back-off from OOVs.
Perplexity = 165.27, Entropy = 7.37 bits
Computation based on 719 words.
Number of 2-grams hit = 685  (95.27%)
Number of 1-grams hit = 34  (4.73%)
7 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article970.text
Will force inclusive back-off from OOVs.
Perplexity = 136.88, Entropy = 7.10 bits
Computation based on 739 words.
Number of 2-grams hit = 705  (95.40%)
Number of 1-grams hit = 34  (4.60%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article971.text
Will force inclusive back-off from OOVs.
Perplexity = 142.30, Entropy = 7.15 bits
Computation based on 468 words.
Number of 2-grams hit = 446  (95.30%)
Number of 1-grams hit = 22  (4.70%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article972.text
Will force inclusive back-off from OOVs.
Perplexity = 139.50, Entropy = 7.12 bits
Computation based on 1855 words.
Number of 2-grams hit = 1777  (95.80%)
Number of 1-grams hit = 78  (4.20%)
7 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article973.text
Will force inclusive back-off from OOVs.
Perplexity = 112.28, Entropy = 6.81 bits
Computation based on 605 words.
Number of 2-grams hit = 594  (98.18%)
Number of 1-grams hit = 11  (1.82%)
6 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article974.text
Will force inclusive back-off from OOVs.
Perplexity = 167.54, Entropy = 7.39 bits
Computation based on 942 words.
Number of 2-grams hit = 893  (94.80%)
Number of 1-grams hit = 49  (5.20%)
9 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article975.text
Will force inclusive back-off from OOVs.
Perplexity = 133.15, Entropy = 7.06 bits
Computation based on 353 words.
Number of 2-grams hit = 335  (94.90%)
Number of 1-grams hit = 18  (5.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article976.text
Will force inclusive back-off from OOVs.
Perplexity = 245.14, Entropy = 7.94 bits
Computation based on 347 words.
Number of 2-grams hit = 326  (93.95%)
Number of 1-grams hit = 21  (6.05%)
13 OOVs (3.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article977.text
Will force inclusive back-off from OOVs.
Perplexity = 113.04, Entropy = 6.82 bits
Computation based on 857 words.
Number of 2-grams hit = 829  (96.73%)
Number of 1-grams hit = 28  (3.27%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article978.text
Will force inclusive back-off from OOVs.
Perplexity = 144.82, Entropy = 7.18 bits
Computation based on 464 words.
Number of 2-grams hit = 443  (95.47%)
Number of 1-grams hit = 21  (4.53%)
10 OOVs (2.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article979.text
Will force inclusive back-off from OOVs.
Perplexity = 99.01, Entropy = 6.63 bits
Computation based on 1554 words.
Number of 2-grams hit = 1507  (96.98%)
Number of 1-grams hit = 47  (3.02%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article980.text
Will force inclusive back-off from OOVs.
Perplexity = 160.14, Entropy = 7.32 bits
Computation based on 598 words.
Number of 2-grams hit = 566  (94.65%)
Number of 1-grams hit = 32  (5.35%)
13 OOVs (2.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article981.text
Will force inclusive back-off from OOVs.
Perplexity = 173.87, Entropy = 7.44 bits
Computation based on 345 words.
Number of 2-grams hit = 328  (95.07%)
Number of 1-grams hit = 17  (4.93%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article982.text
Will force inclusive back-off from OOVs.
Perplexity = 122.87, Entropy = 6.94 bits
Computation based on 825 words.
Number of 2-grams hit = 800  (96.97%)
Number of 1-grams hit = 25  (3.03%)
12 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article983.text
Will force inclusive back-off from OOVs.
Perplexity = 146.87, Entropy = 7.20 bits
Computation based on 468 words.
Number of 2-grams hit = 441  (94.23%)
Number of 1-grams hit = 27  (5.77%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article984.text
Will force inclusive back-off from OOVs.
Perplexity = 108.78, Entropy = 6.77 bits
Computation based on 458 words.
Number of 2-grams hit = 448  (97.82%)
Number of 1-grams hit = 10  (2.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article985.text
Will force inclusive back-off from OOVs.
Perplexity = 158.76, Entropy = 7.31 bits
Computation based on 1233 words.
Number of 2-grams hit = 1184  (96.03%)
Number of 1-grams hit = 49  (3.97%)
13 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article986.text
Will force inclusive back-off from OOVs.
Perplexity = 132.68, Entropy = 7.05 bits
Computation based on 269 words.
Number of 2-grams hit = 261  (97.03%)
Number of 1-grams hit = 8  (2.97%)
2 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article987.text
Will force inclusive back-off from OOVs.
Perplexity = 92.92, Entropy = 6.54 bits
Computation based on 1737 words.
Number of 2-grams hit = 1712  (98.56%)
Number of 1-grams hit = 25  (1.44%)
7 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article988.text
Will force inclusive back-off from OOVs.
Perplexity = 140.00, Entropy = 7.13 bits
Computation based on 1241 words.
Number of 2-grams hit = 1185  (95.49%)
Number of 1-grams hit = 56  (4.51%)
8 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article989.text
Will force inclusive back-off from OOVs.
Perplexity = 106.25, Entropy = 6.73 bits
Computation based on 2270 words.
Number of 2-grams hit = 2212  (97.44%)
Number of 1-grams hit = 58  (2.56%)
9 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article990.text
Will force inclusive back-off from OOVs.
Perplexity = 96.58, Entropy = 6.59 bits
Computation based on 653 words.
Number of 2-grams hit = 637  (97.55%)
Number of 1-grams hit = 16  (2.45%)
7 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article991.text
Will force inclusive back-off from OOVs.
Perplexity = 126.41, Entropy = 6.98 bits
Computation based on 1540 words.
Number of 2-grams hit = 1487  (96.56%)
Number of 1-grams hit = 53  (3.44%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article992.text
Will force inclusive back-off from OOVs.
Perplexity = 192.65, Entropy = 7.59 bits
Computation based on 510 words.
Number of 2-grams hit = 481  (94.31%)
Number of 1-grams hit = 29  (5.69%)
25 OOVs (4.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article993.text
Will force inclusive back-off from OOVs.
Perplexity = 141.97, Entropy = 7.15 bits
Computation based on 2457 words.
Number of 2-grams hit = 2352  (95.73%)
Number of 1-grams hit = 105  (4.27%)
9 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article994.text
Will force inclusive back-off from OOVs.
Perplexity = 124.17, Entropy = 6.96 bits
Computation based on 477 words.
Number of 2-grams hit = 457  (95.81%)
Number of 1-grams hit = 20  (4.19%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article995.text
Will force inclusive back-off from OOVs.
Perplexity = 209.76, Entropy = 7.71 bits
Computation based on 7311 words.
Number of 2-grams hit = 6928  (94.76%)
Number of 1-grams hit = 383  (5.24%)
246 OOVs (3.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article996.text
Will force inclusive back-off from OOVs.
Perplexity = 166.66, Entropy = 7.38 bits
Computation based on 580 words.
Number of 2-grams hit = 552  (95.17%)
Number of 1-grams hit = 28  (4.83%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article997.text
Will force inclusive back-off from OOVs.
Perplexity = 144.56, Entropy = 7.18 bits
Computation based on 5318 words.
Number of 2-grams hit = 5084  (95.60%)
Number of 1-grams hit = 234  (4.40%)
13 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article998.text
Will force inclusive back-off from OOVs.
Perplexity = 118.27, Entropy = 6.89 bits
Computation based on 523 words.
Number of 2-grams hit = 506  (96.75%)
Number of 1-grams hit = 17  (3.25%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article999.text
Will force inclusive back-off from OOVs.
Perplexity = 93.44, Entropy = 6.55 bits
Computation based on 468 words.
Number of 2-grams hit = 456  (97.44%)
Number of 1-grams hit = 12  (2.56%)
4 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : 