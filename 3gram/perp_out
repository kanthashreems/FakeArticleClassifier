evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article0.text
Perplexity = 17.71, Entropy = 4.15 bits
Computation based on 1250 words.
Number of 3-grams hit = 1248  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
12 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article1.text
Perplexity = 19.30, Entropy = 4.27 bits
Computation based on 1503 words.
Number of 3-grams hit = 1501  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
2 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article2.text
Perplexity = 17.05, Entropy = 4.09 bits
Computation based on 534 words.
Number of 3-grams hit = 532  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article3.text
Perplexity = 16.53, Entropy = 4.05 bits
Computation based on 622 words.
Number of 3-grams hit = 620  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article4.text
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 394 words.
Number of 3-grams hit = 392  (99.49%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
4 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article5.text
Perplexity = 15.65, Entropy = 3.97 bits
Computation based on 882 words.
Number of 3-grams hit = 880  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
8 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article6.text
Perplexity = 14.52, Entropy = 3.86 bits
Computation based on 285 words.
Number of 3-grams hit = 283  (99.30%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
2 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article7.text
Perplexity = 16.30, Entropy = 4.03 bits
Computation based on 609 words.
Number of 3-grams hit = 607  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article8.text
Perplexity = 13.29, Entropy = 3.73 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article9.text
Perplexity = 17.54, Entropy = 4.13 bits
Computation based on 312 words.
Number of 3-grams hit = 310  (99.36%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
2 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article10.text
Perplexity = 15.57, Entropy = 3.96 bits
Computation based on 302 words.
Number of 3-grams hit = 300  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article11.text
Perplexity = 14.99, Entropy = 3.91 bits
Computation based on 303 words.
Number of 3-grams hit = 301  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article12.text
Perplexity = 17.88, Entropy = 4.16 bits
Computation based on 298 words.
Number of 3-grams hit = 296  (99.33%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
2 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article13.text
Perplexity = 14.31, Entropy = 3.84 bits
Computation based on 462 words.
Number of 3-grams hit = 460  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article14.text
Perplexity = 15.75, Entropy = 3.98 bits
Computation based on 474 words.
Number of 3-grams hit = 472  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article15.text
Perplexity = 16.51, Entropy = 4.04 bits
Computation based on 495 words.
Number of 3-grams hit = 493  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article16.text
Perplexity = 14.40, Entropy = 3.85 bits
Computation based on 364 words.
Number of 3-grams hit = 362  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
3 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article17.text
Perplexity = 15.23, Entropy = 3.93 bits
Computation based on 664 words.
Number of 3-grams hit = 662  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
9 OOVs (1.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article18.text
Perplexity = 18.59, Entropy = 4.22 bits
Computation based on 410 words.
Number of 3-grams hit = 408  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article19.text
Perplexity = 14.96, Entropy = 3.90 bits
Computation based on 526 words.
Number of 3-grams hit = 524  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article20.text
Perplexity = 15.17, Entropy = 3.92 bits
Computation based on 348 words.
Number of 3-grams hit = 346  (99.43%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
4 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article21.text
Perplexity = 15.37, Entropy = 3.94 bits
Computation based on 462 words.
Number of 3-grams hit = 460  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article22.text
Perplexity = 15.68, Entropy = 3.97 bits
Computation based on 361 words.
Number of 3-grams hit = 359  (99.45%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article23.text
Perplexity = 15.67, Entropy = 3.97 bits
Computation based on 353 words.
Number of 3-grams hit = 351  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
4 OOVs (1.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article24.text
Perplexity = 15.08, Entropy = 3.91 bits
Computation based on 563 words.
Number of 3-grams hit = 561  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article25.text
Perplexity = 15.26, Entropy = 3.93 bits
Computation based on 380 words.
Number of 3-grams hit = 378  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article26.text
Perplexity = 15.25, Entropy = 3.93 bits
Computation based on 1529 words.
Number of 3-grams hit = 1527  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article27.text
Perplexity = 16.57, Entropy = 4.05 bits
Computation based on 392 words.
Number of 3-grams hit = 390  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article28.text
Perplexity = 16.32, Entropy = 4.03 bits
Computation based on 1390 words.
Number of 3-grams hit = 1388  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
9 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article29.text
Perplexity = 17.21, Entropy = 4.11 bits
Computation based on 316 words.
Number of 3-grams hit = 314  (99.37%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
3 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article30.text
Perplexity = 16.20, Entropy = 4.02 bits
Computation based on 474 words.
Number of 3-grams hit = 472  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
4 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article31.text
Perplexity = 16.32, Entropy = 4.03 bits
Computation based on 436 words.
Number of 3-grams hit = 434  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
5 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article32.text
Perplexity = 15.60, Entropy = 3.96 bits
Computation based on 473 words.
Number of 3-grams hit = 471  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
4 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article33.text
Perplexity = 16.70, Entropy = 4.06 bits
Computation based on 443 words.
Number of 3-grams hit = 441  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article34.text
Perplexity = 18.13, Entropy = 4.18 bits
Computation based on 301 words.
Number of 3-grams hit = 300  (99.67%)
Number of 2-grams hit = 0  (0.00%)
Number of 1-grams hit = 1  (0.33%)
4 OOVs (1.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article35.text
Perplexity = 14.47, Entropy = 3.86 bits
Computation based on 378 words.
Number of 3-grams hit = 376  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article36.text
Perplexity = 18.23, Entropy = 4.19 bits
Computation based on 1009 words.
Number of 3-grams hit = 1007  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article37.text
Perplexity = 14.44, Entropy = 3.85 bits
Computation based on 567 words.
Number of 3-grams hit = 565  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article38.text
Perplexity = 14.63, Entropy = 3.87 bits
Computation based on 411 words.
Number of 3-grams hit = 409  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article39.text
Perplexity = 19.09, Entropy = 4.25 bits
Computation based on 2654 words.
Number of 3-grams hit = 2652  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
6 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article40.text
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 403 words.
Number of 3-grams hit = 401  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
3 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article41.text
Perplexity = 16.51, Entropy = 4.04 bits
Computation based on 970 words.
Number of 3-grams hit = 968  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article42.text
Perplexity = 17.99, Entropy = 4.17 bits
Computation based on 547 words.
Number of 3-grams hit = 545  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article43.text
Perplexity = 17.08, Entropy = 4.09 bits
Computation based on 450 words.
Number of 3-grams hit = 448  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article44.text
Perplexity = 18.48, Entropy = 4.21 bits
Computation based on 493 words.
Number of 3-grams hit = 491  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article45.text
Perplexity = 15.39, Entropy = 3.94 bits
Computation based on 462 words.
Number of 3-grams hit = 460  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article46.text
Perplexity = 13.20, Entropy = 3.72 bits
Computation based on 402 words.
Number of 3-grams hit = 400  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article47.text
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 2672 words.
Number of 3-grams hit = 2670  (99.93%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
12 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article48.text
Perplexity = 15.38, Entropy = 3.94 bits
Computation based on 369 words.
Number of 3-grams hit = 367  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
4 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article49.text
Perplexity = 18.29, Entropy = 4.19 bits
Computation based on 363 words.
Number of 3-grams hit = 361  (99.45%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article50.text
Perplexity = 18.29, Entropy = 4.19 bits
Computation based on 524 words.
Number of 3-grams hit = 522  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article51.text
Perplexity = 15.48, Entropy = 3.95 bits
Computation based on 1120 words.
Number of 3-grams hit = 1118  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article52.text
Perplexity = 18.84, Entropy = 4.24 bits
Computation based on 360 words.
Number of 3-grams hit = 358  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article53.text
Perplexity = 15.16, Entropy = 3.92 bits
Computation based on 477 words.
Number of 3-grams hit = 475  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
4 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article54.text
Perplexity = 16.81, Entropy = 4.07 bits
Computation based on 1075 words.
Number of 3-grams hit = 1073  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
9 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article55.text
Perplexity = 19.57, Entropy = 4.29 bits
Computation based on 531 words.
Number of 3-grams hit = 529  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article56.text
Perplexity = 18.37, Entropy = 4.20 bits
Computation based on 903 words.
Number of 3-grams hit = 901  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article57.text
Perplexity = 18.82, Entropy = 4.23 bits
Computation based on 1539 words.
Number of 3-grams hit = 1537  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article58.text
Perplexity = 15.90, Entropy = 3.99 bits
Computation based on 381 words.
Number of 3-grams hit = 379  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article59.text
Perplexity = 18.72, Entropy = 4.23 bits
Computation based on 205 words.
Number of 3-grams hit = 203  (99.02%)
Number of 2-grams hit = 1  (0.49%)
Number of 1-grams hit = 1  (0.49%)
3 OOVs (1.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article60.text
Perplexity = 17.17, Entropy = 4.10 bits
Computation based on 333 words.
Number of 3-grams hit = 331  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article61.text
Perplexity = 16.13, Entropy = 4.01 bits
Computation based on 485 words.
Number of 3-grams hit = 483  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
6 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article62.text
Perplexity = 14.19, Entropy = 3.83 bits
Computation based on 1290 words.
Number of 3-grams hit = 1288  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article63.text
Perplexity = 16.79, Entropy = 4.07 bits
Computation based on 990 words.
Number of 3-grams hit = 988  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article64.text
Perplexity = 16.33, Entropy = 4.03 bits
Computation based on 4209 words.
Number of 3-grams hit = 4207  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
7 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article65.text
Perplexity = 16.33, Entropy = 4.03 bits
Computation based on 626 words.
Number of 3-grams hit = 624  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article66.text
Perplexity = 18.35, Entropy = 4.20 bits
Computation based on 477 words.
Number of 3-grams hit = 475  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article67.text
Perplexity = 13.72, Entropy = 3.78 bits
Computation based on 451 words.
Number of 3-grams hit = 449  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article68.text
Perplexity = 17.77, Entropy = 4.15 bits
Computation based on 1116 words.
Number of 3-grams hit = 1114  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article69.text
Perplexity = 12.55, Entropy = 3.65 bits
Computation based on 380 words.
Number of 3-grams hit = 378  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
3 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article70.text
Perplexity = 18.56, Entropy = 4.21 bits
Computation based on 959 words.
Number of 3-grams hit = 957  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article71.text
Perplexity = 18.37, Entropy = 4.20 bits
Computation based on 548 words.
Number of 3-grams hit = 546  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article72.text
Perplexity = 17.22, Entropy = 4.11 bits
Computation based on 1298 words.
Number of 3-grams hit = 1296  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
10 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article73.text
Perplexity = 18.25, Entropy = 4.19 bits
Computation based on 513 words.
Number of 3-grams hit = 511  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article74.text
Perplexity = 16.14, Entropy = 4.01 bits
Computation based on 817 words.
Number of 3-grams hit = 815  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
6 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article75.text
Perplexity = 15.66, Entropy = 3.97 bits
Computation based on 1194 words.
Number of 3-grams hit = 1192  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article76.text
Perplexity = 18.60, Entropy = 4.22 bits
Computation based on 1376 words.
Number of 3-grams hit = 1374  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
6 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article77.text
Perplexity = 16.58, Entropy = 4.05 bits
Computation based on 282 words.
Number of 3-grams hit = 280  (99.29%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
6 OOVs (2.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article78.text
Perplexity = 17.21, Entropy = 4.11 bits
Computation based on 1107 words.
Number of 3-grams hit = 1105  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
11 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article79.text
Perplexity = 15.22, Entropy = 3.93 bits
Computation based on 1061 words.
Number of 3-grams hit = 1059  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
12 OOVs (1.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article80.text
Perplexity = 18.94, Entropy = 4.24 bits
Computation based on 368 words.
Number of 3-grams hit = 366  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article81.text
Perplexity = 17.50, Entropy = 4.13 bits
Computation based on 4025 words.
Number of 3-grams hit = 4023  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
25 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article82.text
Perplexity = 16.11, Entropy = 4.01 bits
Computation based on 471 words.
Number of 3-grams hit = 469  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article83.text
Perplexity = 18.25, Entropy = 4.19 bits
Computation based on 429 words.
Number of 3-grams hit = 427  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article84.text
Perplexity = 16.30, Entropy = 4.03 bits
Computation based on 398 words.
Number of 3-grams hit = 396  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article85.text
Perplexity = 16.49, Entropy = 4.04 bits
Computation based on 1210 words.
Number of 3-grams hit = 1208  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article86.text
Perplexity = 19.74, Entropy = 4.30 bits
Computation based on 289 words.
Number of 3-grams hit = 287  (99.31%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article87.text
Perplexity = 13.41, Entropy = 3.75 bits
Computation based on 624 words.
Number of 3-grams hit = 622  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
8 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article88.text
Perplexity = 19.48, Entropy = 4.28 bits
Computation based on 1496 words.
Number of 3-grams hit = 1494  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article89.text
Perplexity = 14.90, Entropy = 3.90 bits
Computation based on 390 words.
Number of 3-grams hit = 388  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article90.text
Perplexity = 16.82, Entropy = 4.07 bits
Computation based on 517 words.
Number of 3-grams hit = 515  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article91.text
Perplexity = 18.04, Entropy = 4.17 bits
Computation based on 339 words.
Number of 3-grams hit = 337  (99.41%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
3 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article92.text
Perplexity = 17.35, Entropy = 4.12 bits
Computation based on 1023 words.
Number of 3-grams hit = 1021  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article93.text
Perplexity = 16.21, Entropy = 4.02 bits
Computation based on 683 words.
Number of 3-grams hit = 681  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article94.text
Perplexity = 17.72, Entropy = 4.15 bits
Computation based on 1281 words.
Number of 3-grams hit = 1279  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article95.text
Perplexity = 17.62, Entropy = 4.14 bits
Computation based on 498 words.
Number of 3-grams hit = 496  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article96.text
Perplexity = 16.03, Entropy = 4.00 bits
Computation based on 395 words.
Number of 3-grams hit = 393  (99.49%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article97.text
Perplexity = 18.27, Entropy = 4.19 bits
Computation based on 552 words.
Number of 3-grams hit = 550  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article98.text
Perplexity = 15.91, Entropy = 3.99 bits
Computation based on 366 words.
Number of 3-grams hit = 364  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article99.text
Perplexity = 19.47, Entropy = 4.28 bits
Computation based on 452 words.
Number of 3-grams hit = 450  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
4 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article100.text
Perplexity = 17.14, Entropy = 4.10 bits
Computation based on 469 words.
Number of 3-grams hit = 467  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article101.text
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 505 words.
Number of 3-grams hit = 503  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article102.text
Perplexity = 13.89, Entropy = 3.80 bits
Computation based on 415 words.
Number of 3-grams hit = 413  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article103.text
Perplexity = 18.17, Entropy = 4.18 bits
Computation based on 497 words.
Number of 3-grams hit = 495  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article104.text
Perplexity = 19.28, Entropy = 4.27 bits
Computation based on 6356 words.
Number of 3-grams hit = 6354  (99.97%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
3 OOVs (0.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article105.text
Perplexity = 16.67, Entropy = 4.06 bits
Computation based on 448 words.
Number of 3-grams hit = 446  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article106.text
Perplexity = 13.94, Entropy = 3.80 bits
Computation based on 801 words.
Number of 3-grams hit = 799  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article107.text
Perplexity = 13.08, Entropy = 3.71 bits
Computation based on 483 words.
Number of 3-grams hit = 481  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article108.text
Perplexity = 18.42, Entropy = 4.20 bits
Computation based on 528 words.
Number of 3-grams hit = 526  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article109.text
Perplexity = 15.76, Entropy = 3.98 bits
Computation based on 1123 words.
Number of 3-grams hit = 1121  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article110.text
Perplexity = 16.17, Entropy = 4.02 bits
Computation based on 907 words.
Number of 3-grams hit = 905  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article111.text
Perplexity = 17.54, Entropy = 4.13 bits
Computation based on 1233 words.
Number of 3-grams hit = 1231  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article112.text
Perplexity = 15.69, Entropy = 3.97 bits
Computation based on 328 words.
Number of 3-grams hit = 326  (99.39%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article113.text
Perplexity = 20.71, Entropy = 4.37 bits
Computation based on 5823 words.
Number of 3-grams hit = 5821  (99.97%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
8 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article114.text
Perplexity = 18.55, Entropy = 4.21 bits
Computation based on 496 words.
Number of 3-grams hit = 494  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
4 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article115.text
Perplexity = 17.23, Entropy = 4.11 bits
Computation based on 643 words.
Number of 3-grams hit = 641  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article116.text
Perplexity = 18.18, Entropy = 4.18 bits
Computation based on 1538 words.
Number of 3-grams hit = 1536  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
1 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article117.text
Perplexity = 14.79, Entropy = 3.89 bits
Computation based on 512 words.
Number of 3-grams hit = 510  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
5 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article118.text
Perplexity = 18.11, Entropy = 4.18 bits
Computation based on 608 words.
Number of 3-grams hit = 606  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
7 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article119.text
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 387 words.
Number of 3-grams hit = 385  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
3 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article120.text
Perplexity = 16.86, Entropy = 4.08 bits
Computation based on 553 words.
Number of 3-grams hit = 551  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
6 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article121.text
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 7114 words.
Number of 3-grams hit = 7112  (99.97%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
47 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article122.text
Perplexity = 21.48, Entropy = 4.42 bits
Computation based on 323 words.
Number of 3-grams hit = 321  (99.38%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article123.text
Perplexity = 15.29, Entropy = 3.93 bits
Computation based on 524 words.
Number of 3-grams hit = 522  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article124.text
Perplexity = 18.05, Entropy = 4.17 bits
Computation based on 797 words.
Number of 3-grams hit = 795  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article125.text
Perplexity = 16.77, Entropy = 4.07 bits
Computation based on 481 words.
Number of 3-grams hit = 479  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article126.text
Perplexity = 17.46, Entropy = 4.13 bits
Computation based on 537 words.
Number of 3-grams hit = 535  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article127.text
Perplexity = 17.74, Entropy = 4.15 bits
Computation based on 918 words.
Number of 3-grams hit = 916  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
8 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article128.text
Perplexity = 13.54, Entropy = 3.76 bits
Computation based on 429 words.
Number of 3-grams hit = 427  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article129.text
Perplexity = 15.33, Entropy = 3.94 bits
Computation based on 468 words.
Number of 3-grams hit = 466  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article130.text
Perplexity = 15.86, Entropy = 3.99 bits
Computation based on 1521 words.
Number of 3-grams hit = 1519  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
12 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article131.text
Perplexity = 16.46, Entropy = 4.04 bits
Computation based on 6777 words.
Number of 3-grams hit = 6775  (99.97%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
39 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article132.text
Perplexity = 16.67, Entropy = 4.06 bits
Computation based on 1116 words.
Number of 3-grams hit = 1114  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
7 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article133.text
Perplexity = 21.46, Entropy = 4.42 bits
Computation based on 238 words.
Number of 3-grams hit = 236  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
5 OOVs (2.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article134.text
Perplexity = 16.59, Entropy = 4.05 bits
Computation based on 500 words.
Number of 3-grams hit = 498  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article135.text
Perplexity = 14.65, Entropy = 3.87 bits
Computation based on 489 words.
Number of 3-grams hit = 487  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article136.text
Perplexity = 18.63, Entropy = 4.22 bits
Computation based on 520 words.
Number of 3-grams hit = 518  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article137.text
Perplexity = 19.32, Entropy = 4.27 bits
Computation based on 332 words.
Number of 3-grams hit = 330  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article138.text
Perplexity = 16.91, Entropy = 4.08 bits
Computation based on 306 words.
Number of 3-grams hit = 304  (99.35%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article139.text
Perplexity = 18.00, Entropy = 4.17 bits
Computation based on 273 words.
Number of 3-grams hit = 271  (99.27%)
Number of 2-grams hit = 1  (0.37%)
Number of 1-grams hit = 1  (0.37%)
2 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article140.text
Perplexity = 13.18, Entropy = 3.72 bits
Computation based on 374 words.
Number of 3-grams hit = 372  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article141.text
Perplexity = 15.11, Entropy = 3.92 bits
Computation based on 432 words.
Number of 3-grams hit = 430  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article142.text
Perplexity = 17.24, Entropy = 4.11 bits
Computation based on 697 words.
Number of 3-grams hit = 695  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article143.text
Perplexity = 14.62, Entropy = 3.87 bits
Computation based on 380 words.
Number of 3-grams hit = 378  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article144.text
Perplexity = 11.53, Entropy = 3.53 bits
Computation based on 506 words.
Number of 3-grams hit = 504  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article145.text
Perplexity = 17.11, Entropy = 4.10 bits
Computation based on 1932 words.
Number of 3-grams hit = 1930  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
12 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article146.text
Perplexity = 17.30, Entropy = 4.11 bits
Computation based on 237 words.
Number of 3-grams hit = 235  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
1 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article147.text
Perplexity = 17.97, Entropy = 4.17 bits
Computation based on 650 words.
Number of 3-grams hit = 648  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
9 OOVs (1.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article148.text
Perplexity = 15.56, Entropy = 3.96 bits
Computation based on 1726 words.
Number of 3-grams hit = 1724  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
13 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article149.text
Perplexity = 15.78, Entropy = 3.98 bits
Computation based on 558 words.
Number of 3-grams hit = 556  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article150.text
Perplexity = 15.51, Entropy = 3.96 bits
Computation based on 636 words.
Number of 3-grams hit = 634  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article151.text
Perplexity = 21.42, Entropy = 4.42 bits
Computation based on 173 words.
Number of 3-grams hit = 171  (98.84%)
Number of 2-grams hit = 1  (0.58%)
Number of 1-grams hit = 1  (0.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article152.text
Perplexity = 16.79, Entropy = 4.07 bits
Computation based on 1799 words.
Number of 3-grams hit = 1797  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
20 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article153.text
Perplexity = 16.59, Entropy = 4.05 bits
Computation based on 583 words.
Number of 3-grams hit = 581  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
7 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article154.text
Perplexity = 17.05, Entropy = 4.09 bits
Computation based on 1053 words.
Number of 3-grams hit = 1051  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
11 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article155.text
Perplexity = 16.54, Entropy = 4.05 bits
Computation based on 641 words.
Number of 3-grams hit = 639  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article156.text
Perplexity = 19.66, Entropy = 4.30 bits
Computation based on 1376 words.
Number of 3-grams hit = 1374  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article157.text
Perplexity = 15.41, Entropy = 3.95 bits
Computation based on 405 words.
Number of 3-grams hit = 403  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article158.text
Perplexity = 15.73, Entropy = 3.98 bits
Computation based on 571 words.
Number of 3-grams hit = 569  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article159.text
Perplexity = 19.09, Entropy = 4.25 bits
Computation based on 1207 words.
Number of 3-grams hit = 1205  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article160.text
Perplexity = 13.72, Entropy = 3.78 bits
Computation based on 538 words.
Number of 3-grams hit = 536  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article161.text
Perplexity = 12.78, Entropy = 3.68 bits
Computation based on 198 words.
Number of 3-grams hit = 196  (98.99%)
Number of 2-grams hit = 1  (0.51%)
Number of 1-grams hit = 1  (0.51%)
1 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article162.text
Perplexity = 16.89, Entropy = 4.08 bits
Computation based on 1145 words.
Number of 3-grams hit = 1143  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
9 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article163.text
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 702 words.
Number of 3-grams hit = 700  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article164.text
Perplexity = 18.01, Entropy = 4.17 bits
Computation based on 1772 words.
Number of 3-grams hit = 1770  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article165.text
Perplexity = 13.46, Entropy = 3.75 bits
Computation based on 555 words.
Number of 3-grams hit = 553  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article166.text
Perplexity = 17.82, Entropy = 4.16 bits
Computation based on 516 words.
Number of 3-grams hit = 514  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article167.text
Perplexity = 17.87, Entropy = 4.16 bits
Computation based on 1169 words.
Number of 3-grams hit = 1167  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
7 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article168.text
Perplexity = 16.80, Entropy = 4.07 bits
Computation based on 521 words.
Number of 3-grams hit = 519  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article169.text
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article170.text
Perplexity = 13.74, Entropy = 3.78 bits
Computation based on 1145 words.
Number of 3-grams hit = 1143  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
5 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article171.text
Perplexity = 12.90, Entropy = 3.69 bits
Computation based on 605 words.
Number of 3-grams hit = 603  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article172.text
Perplexity = 16.23, Entropy = 4.02 bits
Computation based on 680 words.
Number of 3-grams hit = 678  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article173.text
Perplexity = 14.22, Entropy = 3.83 bits
Computation based on 1018 words.
Number of 3-grams hit = 1016  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
3 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article174.text
Perplexity = 18.25, Entropy = 4.19 bits
Computation based on 445 words.
Number of 3-grams hit = 443  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article175.text
Perplexity = 17.24, Entropy = 4.11 bits
Computation based on 456 words.
Number of 3-grams hit = 454  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article176.text
Perplexity = 16.25, Entropy = 4.02 bits
Computation based on 743 words.
Number of 3-grams hit = 741  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article177.text
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 516 words.
Number of 3-grams hit = 514  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article178.text
Perplexity = 16.11, Entropy = 4.01 bits
Computation based on 167 words.
Number of 3-grams hit = 165  (98.80%)
Number of 2-grams hit = 1  (0.60%)
Number of 1-grams hit = 1  (0.60%)
2 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article179.text
Perplexity = 16.25, Entropy = 4.02 bits
Computation based on 453 words.
Number of 3-grams hit = 451  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article180.text
Perplexity = 18.24, Entropy = 4.19 bits
Computation based on 428 words.
Number of 3-grams hit = 426  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article181.text
Perplexity = 20.26, Entropy = 4.34 bits
Computation based on 851 words.
Number of 3-grams hit = 849  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article182.text
Perplexity = 18.40, Entropy = 4.20 bits
Computation based on 365 words.
Number of 3-grams hit = 363  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article183.text
Perplexity = 16.81, Entropy = 4.07 bits
Computation based on 459 words.
Number of 3-grams hit = 457  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article184.text
Perplexity = 16.21, Entropy = 4.02 bits
Computation based on 5455 words.
Number of 3-grams hit = 5453  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
37 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article185.text
Perplexity = 16.96, Entropy = 4.08 bits
Computation based on 971 words.
Number of 3-grams hit = 969  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article186.text
Perplexity = 15.01, Entropy = 3.91 bits
Computation based on 489 words.
Number of 3-grams hit = 487  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article187.text
Perplexity = 16.65, Entropy = 4.06 bits
Computation based on 5986 words.
Number of 3-grams hit = 5984  (99.97%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
41 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article188.text
Perplexity = 15.08, Entropy = 3.91 bits
Computation based on 691 words.
Number of 3-grams hit = 689  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article189.text
Perplexity = 15.98, Entropy = 4.00 bits
Computation based on 534 words.
Number of 3-grams hit = 532  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article190.text
Perplexity = 14.28, Entropy = 3.84 bits
Computation based on 393 words.
Number of 3-grams hit = 391  (99.49%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
3 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article191.text
Perplexity = 17.53, Entropy = 4.13 bits
Computation based on 5426 words.
Number of 3-grams hit = 5424  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
42 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article192.text
Perplexity = 16.21, Entropy = 4.02 bits
Computation based on 770 words.
Number of 3-grams hit = 768  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article193.text
Perplexity = 15.20, Entropy = 3.93 bits
Computation based on 161 words.
Number of 3-grams hit = 159  (98.76%)
Number of 2-grams hit = 1  (0.62%)
Number of 1-grams hit = 1  (0.62%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article194.text
Perplexity = 17.11, Entropy = 4.10 bits
Computation based on 601 words.
Number of 3-grams hit = 599  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article195.text
Perplexity = 17.05, Entropy = 4.09 bits
Computation based on 789 words.
Number of 3-grams hit = 787  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
6 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article196.text
Perplexity = 20.74, Entropy = 4.37 bits
Computation based on 686 words.
Number of 3-grams hit = 684  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article197.text
Perplexity = 18.64, Entropy = 4.22 bits
Computation based on 226 words.
Number of 3-grams hit = 224  (99.12%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article198.text
Perplexity = 16.24, Entropy = 4.02 bits
Computation based on 715 words.
Number of 3-grams hit = 713  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
6 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article199.text
Perplexity = 18.49, Entropy = 4.21 bits
Computation based on 4482 words.
Number of 3-grams hit = 4480  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
11 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article200.text
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 637 words.
Number of 3-grams hit = 635  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article201.text
Perplexity = 14.84, Entropy = 3.89 bits
Computation based on 463 words.
Number of 3-grams hit = 461  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article202.text
Perplexity = 16.15, Entropy = 4.01 bits
Computation based on 1169 words.
Number of 3-grams hit = 1167  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article203.text
Perplexity = 18.67, Entropy = 4.22 bits
Computation based on 4825 words.
Number of 3-grams hit = 4823  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
5 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article204.text
Perplexity = 19.89, Entropy = 4.31 bits
Computation based on 5410 words.
Number of 3-grams hit = 5408  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
6 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article205.text
Perplexity = 18.00, Entropy = 4.17 bits
Computation based on 554 words.
Number of 3-grams hit = 552  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article206.text
Perplexity = 14.82, Entropy = 3.89 bits
Computation based on 589 words.
Number of 3-grams hit = 587  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article207.text
Perplexity = 15.85, Entropy = 3.99 bits
Computation based on 718 words.
Number of 3-grams hit = 716  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
6 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article208.text
Perplexity = 15.67, Entropy = 3.97 bits
Computation based on 891 words.
Number of 3-grams hit = 889  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
3 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article209.text
Perplexity = 17.38, Entropy = 4.12 bits
Computation based on 269 words.
Number of 3-grams hit = 267  (99.26%)
Number of 2-grams hit = 1  (0.37%)
Number of 1-grams hit = 1  (0.37%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article210.text
Perplexity = 17.72, Entropy = 4.15 bits
Computation based on 832 words.
Number of 3-grams hit = 830  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
5 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article211.text
Perplexity = 16.88, Entropy = 4.08 bits
Computation based on 472 words.
Number of 3-grams hit = 470  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article212.text
Perplexity = 17.57, Entropy = 4.13 bits
Computation based on 414 words.
Number of 3-grams hit = 412  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article213.text
Perplexity = 18.40, Entropy = 4.20 bits
Computation based on 620 words.
Number of 3-grams hit = 618  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
10 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article214.text
Perplexity = 15.74, Entropy = 3.98 bits
Computation based on 463 words.
Number of 3-grams hit = 461  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article215.text
Perplexity = 12.65, Entropy = 3.66 bits
Computation based on 695 words.
Number of 3-grams hit = 693  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article216.text
Perplexity = 15.54, Entropy = 3.96 bits
Computation based on 482 words.
Number of 3-grams hit = 480  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
4 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article217.text
Perplexity = 22.83, Entropy = 4.51 bits
Computation based on 300 words.
Number of 3-grams hit = 298  (99.33%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article218.text
Perplexity = 19.16, Entropy = 4.26 bits
Computation based on 719 words.
Number of 3-grams hit = 717  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article219.text
Perplexity = 13.36, Entropy = 3.74 bits
Computation based on 409 words.
Number of 3-grams hit = 407  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article220.text
Perplexity = 20.44, Entropy = 4.35 bits
Computation based on 4316 words.
Number of 3-grams hit = 4314  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
4 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article221.text
Perplexity = 16.72, Entropy = 4.06 bits
Computation based on 1182 words.
Number of 3-grams hit = 1180  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article222.text
Perplexity = 16.02, Entropy = 4.00 bits
Computation based on 4439 words.
Number of 3-grams hit = 4437  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
32 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article223.text
Perplexity = 16.17, Entropy = 4.02 bits
Computation based on 831 words.
Number of 3-grams hit = 829  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article224.text
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 382 words.
Number of 3-grams hit = 380  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article225.text
Perplexity = 14.04, Entropy = 3.81 bits
Computation based on 726 words.
Number of 3-grams hit = 724  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
4 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article226.text
Perplexity = 15.75, Entropy = 3.98 bits
Computation based on 1233 words.
Number of 3-grams hit = 1231  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article227.text
Perplexity = 15.26, Entropy = 3.93 bits
Computation based on 934 words.
Number of 3-grams hit = 932  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
4 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article228.text
Perplexity = 15.84, Entropy = 3.99 bits
Computation based on 778 words.
Number of 3-grams hit = 776  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
8 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article229.text
Perplexity = 15.17, Entropy = 3.92 bits
Computation based on 476 words.
Number of 3-grams hit = 474  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article230.text
Perplexity = 18.18, Entropy = 4.18 bits
Computation based on 304 words.
Number of 3-grams hit = 302  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article231.text
Perplexity = 16.86, Entropy = 4.08 bits
Computation based on 800 words.
Number of 3-grams hit = 798  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article232.text
Perplexity = 14.95, Entropy = 3.90 bits
Computation based on 364 words.
Number of 3-grams hit = 362  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article233.text
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 670 words.
Number of 3-grams hit = 668  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article234.text
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 1331 words.
Number of 3-grams hit = 1329  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article235.text
Perplexity = 17.26, Entropy = 4.11 bits
Computation based on 349 words.
Number of 3-grams hit = 347  (99.43%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article236.text
Perplexity = 13.24, Entropy = 3.73 bits
Computation based on 750 words.
Number of 3-grams hit = 748  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article237.text
Perplexity = 15.25, Entropy = 3.93 bits
Computation based on 388 words.
Number of 3-grams hit = 386  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
3 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article238.text
Perplexity = 14.27, Entropy = 3.84 bits
Computation based on 311 words.
Number of 3-grams hit = 309  (99.36%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
2 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article239.text
Perplexity = 16.94, Entropy = 4.08 bits
Computation based on 474 words.
Number of 3-grams hit = 472  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article240.text
Perplexity = 14.96, Entropy = 3.90 bits
Computation based on 314 words.
Number of 3-grams hit = 312  (99.36%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
2 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article241.text
Perplexity = 13.94, Entropy = 3.80 bits
Computation based on 306 words.
Number of 3-grams hit = 304  (99.35%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article242.text
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 293 words.
Number of 3-grams hit = 291  (99.32%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article243.text
Perplexity = 17.11, Entropy = 4.10 bits
Computation based on 742 words.
Number of 3-grams hit = 740  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article244.text
Perplexity = 14.43, Entropy = 3.85 bits
Computation based on 423 words.
Number of 3-grams hit = 421  (99.53%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
4 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article245.text
Perplexity = 18.78, Entropy = 4.23 bits
Computation based on 5038 words.
Number of 3-grams hit = 5036  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
10 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article246.text
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 277 words.
Number of 3-grams hit = 275  (99.28%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article247.text
Perplexity = 18.14, Entropy = 4.18 bits
Computation based on 448 words.
Number of 3-grams hit = 446  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
4 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article248.text
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 354 words.
Number of 3-grams hit = 352  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article249.text
Perplexity = 19.65, Entropy = 4.30 bits
Computation based on 863 words.
Number of 3-grams hit = 861  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article250.text
Perplexity = 18.65, Entropy = 4.22 bits
Computation based on 4859 words.
Number of 3-grams hit = 4857  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
5 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article251.text
Perplexity = 16.96, Entropy = 4.08 bits
Computation based on 4942 words.
Number of 3-grams hit = 4940  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
44 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article252.text
Perplexity = 17.22, Entropy = 4.11 bits
Computation based on 519 words.
Number of 3-grams hit = 517  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
7 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article253.text
Perplexity = 16.40, Entropy = 4.04 bits
Computation based on 4923 words.
Number of 3-grams hit = 4921  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
35 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article254.text
Perplexity = 13.91, Entropy = 3.80 bits
Computation based on 779 words.
Number of 3-grams hit = 777  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article255.text
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 490 words.
Number of 3-grams hit = 488  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article256.text
Perplexity = 15.89, Entropy = 3.99 bits
Computation based on 779 words.
Number of 3-grams hit = 777  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article257.text
Perplexity = 19.72, Entropy = 4.30 bits
Computation based on 5776 words.
Number of 3-grams hit = 5774  (99.97%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
5 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article258.text
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 409 words.
Number of 3-grams hit = 407  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article259.text
Perplexity = 16.70, Entropy = 4.06 bits
Computation based on 694 words.
Number of 3-grams hit = 692  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
7 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article260.text
Perplexity = 16.92, Entropy = 4.08 bits
Computation based on 6951 words.
Number of 3-grams hit = 6949  (99.97%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
44 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article261.text
Perplexity = 16.90, Entropy = 4.08 bits
Computation based on 645 words.
Number of 3-grams hit = 643  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article262.text
Perplexity = 17.62, Entropy = 4.14 bits
Computation based on 662 words.
Number of 3-grams hit = 660  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article263.text
Perplexity = 17.40, Entropy = 4.12 bits
Computation based on 373 words.
Number of 3-grams hit = 371  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article264.text
Perplexity = 16.97, Entropy = 4.08 bits
Computation based on 799 words.
Number of 3-grams hit = 797  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article265.text
Perplexity = 13.87, Entropy = 3.79 bits
Computation based on 569 words.
Number of 3-grams hit = 567  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article266.text
Perplexity = 17.46, Entropy = 4.13 bits
Computation based on 434 words.
Number of 3-grams hit = 432  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article267.text
Perplexity = 17.00, Entropy = 4.09 bits
Computation based on 6212 words.
Number of 3-grams hit = 6210  (99.97%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
40 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article268.text
Perplexity = 17.14, Entropy = 4.10 bits
Computation based on 5792 words.
Number of 3-grams hit = 5790  (99.97%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
48 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article269.text
Perplexity = 16.61, Entropy = 4.05 bits
Computation based on 421 words.
Number of 3-grams hit = 419  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article270.text
Perplexity = 13.86, Entropy = 3.79 bits
Computation based on 375 words.
Number of 3-grams hit = 373  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article271.text
Perplexity = 19.28, Entropy = 4.27 bits
Computation based on 7334 words.
Number of 3-grams hit = 7332  (99.97%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
13 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article272.text
Perplexity = 19.00, Entropy = 4.25 bits
Computation based on 5210 words.
Number of 3-grams hit = 5208  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
8 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article273.text
Perplexity = 17.03, Entropy = 4.09 bits
Computation based on 637 words.
Number of 3-grams hit = 635  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article274.text
Perplexity = 17.63, Entropy = 4.14 bits
Computation based on 649 words.
Number of 3-grams hit = 647  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
8 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article275.text
Perplexity = 15.68, Entropy = 3.97 bits
Computation based on 1915 words.
Number of 3-grams hit = 1913  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
12 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article276.text
Perplexity = 16.02, Entropy = 4.00 bits
Computation based on 469 words.
Number of 3-grams hit = 467  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article277.text
Perplexity = 16.58, Entropy = 4.05 bits
Computation based on 766 words.
Number of 3-grams hit = 764  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article278.text
Perplexity = 13.95, Entropy = 3.80 bits
Computation based on 807 words.
Number of 3-grams hit = 805  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article279.text
Perplexity = 20.04, Entropy = 4.32 bits
Computation based on 400 words.
Number of 3-grams hit = 398  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article280.text
Perplexity = 17.27, Entropy = 4.11 bits
Computation based on 1700 words.
Number of 3-grams hit = 1698  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
10 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article281.text
Perplexity = 15.33, Entropy = 3.94 bits
Computation based on 566 words.
Number of 3-grams hit = 564  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article282.text
Perplexity = 18.12, Entropy = 4.18 bits
Computation based on 703 words.
Number of 3-grams hit = 701  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
10 OOVs (1.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article283.text
Perplexity = 17.78, Entropy = 4.15 bits
Computation based on 1179 words.
Number of 3-grams hit = 1177  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article284.text
Perplexity = 20.23, Entropy = 4.34 bits
Computation based on 1579 words.
Number of 3-grams hit = 1577  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article285.text
Perplexity = 18.23, Entropy = 4.19 bits
Computation based on 1097 words.
Number of 3-grams hit = 1095  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article286.text
Perplexity = 16.55, Entropy = 4.05 bits
Computation based on 444 words.
Number of 3-grams hit = 442  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article287.text
Perplexity = 16.25, Entropy = 4.02 bits
Computation based on 1658 words.
Number of 3-grams hit = 1656  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article288.text
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 580 words.
Number of 3-grams hit = 578  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article289.text
Perplexity = 21.48, Entropy = 4.42 bits
Computation based on 706 words.
Number of 3-grams hit = 704  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article290.text
Perplexity = 18.67, Entropy = 4.22 bits
Computation based on 1290 words.
Number of 3-grams hit = 1288  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article291.text
Perplexity = 16.08, Entropy = 4.01 bits
Computation based on 516 words.
Number of 3-grams hit = 514  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article292.text
Perplexity = 21.06, Entropy = 4.40 bits
Computation based on 537 words.
Number of 3-grams hit = 535  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article293.text
Perplexity = 15.04, Entropy = 3.91 bits
Computation based on 548 words.
Number of 3-grams hit = 546  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
7 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article294.text
Perplexity = 15.15, Entropy = 3.92 bits
Computation based on 2111 words.
Number of 3-grams hit = 2109  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
9 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article295.text
Perplexity = 14.57, Entropy = 3.87 bits
Computation based on 589 words.
Number of 3-grams hit = 587  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article296.text
Perplexity = 17.26, Entropy = 4.11 bits
Computation based on 1520 words.
Number of 3-grams hit = 1518  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
10 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article297.text
Perplexity = 19.54, Entropy = 4.29 bits
Computation based on 491 words.
Number of 3-grams hit = 489  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article298.text
Perplexity = 17.31, Entropy = 4.11 bits
Computation based on 3433 words.
Number of 3-grams hit = 3431  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
21 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article299.text
Perplexity = 17.45, Entropy = 4.13 bits
Computation based on 2733 words.
Number of 3-grams hit = 2731  (99.93%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
9 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article300.text
Perplexity = 18.73, Entropy = 4.23 bits
Computation based on 2533 words.
Number of 3-grams hit = 2531  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
6 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article301.text
Perplexity = 15.97, Entropy = 4.00 bits
Computation based on 2131 words.
Number of 3-grams hit = 2129  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
15 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article302.text
Perplexity = 18.29, Entropy = 4.19 bits
Computation based on 622 words.
Number of 3-grams hit = 620  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
4 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article303.text
Perplexity = 14.88, Entropy = 3.89 bits
Computation based on 866 words.
Number of 3-grams hit = 864  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article304.text
Perplexity = 13.80, Entropy = 3.79 bits
Computation based on 402 words.
Number of 3-grams hit = 400  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article305.text
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 300 words.
Number of 3-grams hit = 298  (99.33%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
3 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article306.text
Perplexity = 16.18, Entropy = 4.02 bits
Computation based on 586 words.
Number of 3-grams hit = 584  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article307.text
Perplexity = 15.44, Entropy = 3.95 bits
Computation based on 512 words.
Number of 3-grams hit = 510  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article308.text
Perplexity = 17.85, Entropy = 4.16 bits
Computation based on 393 words.
Number of 3-grams hit = 391  (99.49%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article309.text
Perplexity = 22.96, Entropy = 4.52 bits
Computation based on 238 words.
Number of 3-grams hit = 236  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
3 OOVs (1.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article310.text
Perplexity = 16.79, Entropy = 4.07 bits
Computation based on 483 words.
Number of 3-grams hit = 481  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article311.text
Perplexity = 18.06, Entropy = 4.18 bits
Computation based on 465 words.
Number of 3-grams hit = 463  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article312.text
Perplexity = 15.08, Entropy = 3.91 bits
Computation based on 591 words.
Number of 3-grams hit = 589  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
7 OOVs (1.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article313.text
Perplexity = 16.12, Entropy = 4.01 bits
Computation based on 476 words.
Number of 3-grams hit = 474  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article314.text
Perplexity = 19.71, Entropy = 4.30 bits
Computation based on 3293 words.
Number of 3-grams hit = 3291  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
3 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article315.text
Perplexity = 16.84, Entropy = 4.07 bits
Computation based on 1089 words.
Number of 3-grams hit = 1087  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article316.text
Perplexity = 15.90, Entropy = 3.99 bits
Computation based on 328 words.
Number of 3-grams hit = 326  (99.39%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
3 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article317.text
Perplexity = 16.16, Entropy = 4.01 bits
Computation based on 624 words.
Number of 3-grams hit = 622  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article318.text
Perplexity = 16.72, Entropy = 4.06 bits
Computation based on 3559 words.
Number of 3-grams hit = 3557  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
15 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article319.text
Perplexity = 13.39, Entropy = 3.74 bits
Computation based on 596 words.
Number of 3-grams hit = 594  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
3 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article320.text
Perplexity = 12.43, Entropy = 3.64 bits
Computation based on 375 words.
Number of 3-grams hit = 373  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article321.text
Perplexity = 18.16, Entropy = 4.18 bits
Computation based on 1066 words.
Number of 3-grams hit = 1064  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
10 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article322.text
Perplexity = 18.65, Entropy = 4.22 bits
Computation based on 1676 words.
Number of 3-grams hit = 1674  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article323.text
Perplexity = 14.66, Entropy = 3.87 bits
Computation based on 816 words.
Number of 3-grams hit = 814  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article324.text
Perplexity = 18.46, Entropy = 4.21 bits
Computation based on 1652 words.
Number of 3-grams hit = 1650  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article325.text
Perplexity = 14.87, Entropy = 3.89 bits
Computation based on 1122 words.
Number of 3-grams hit = 1120  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article326.text
Perplexity = 16.62, Entropy = 4.06 bits
Computation based on 603 words.
Number of 3-grams hit = 601  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
8 OOVs (1.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article327.text
Perplexity = 15.55, Entropy = 3.96 bits
Computation based on 1127 words.
Number of 3-grams hit = 1125  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article328.text
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 249 words.
Number of 3-grams hit = 247  (99.20%)
Number of 2-grams hit = 1  (0.40%)
Number of 1-grams hit = 1  (0.40%)
2 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article329.text
Perplexity = 16.54, Entropy = 4.05 bits
Computation based on 412 words.
Number of 3-grams hit = 410  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article330.text
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 1058 words.
Number of 3-grams hit = 1056  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article331.text
Perplexity = 17.82, Entropy = 4.16 bits
Computation based on 1950 words.
Number of 3-grams hit = 1948  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
8 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article332.text
Perplexity = 18.67, Entropy = 4.22 bits
Computation based on 572 words.
Number of 3-grams hit = 570  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article333.text
Perplexity = 15.58, Entropy = 3.96 bits
Computation based on 2140 words.
Number of 3-grams hit = 2138  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
13 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article334.text
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 958 words.
Number of 3-grams hit = 956  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
7 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article335.text
Perplexity = 17.45, Entropy = 4.12 bits
Computation based on 1757 words.
Number of 3-grams hit = 1755  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
4 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article336.text
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 1140 words.
Number of 3-grams hit = 1138  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article337.text
Perplexity = 19.04, Entropy = 4.25 bits
Computation based on 401 words.
Number of 3-grams hit = 399  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article338.text
Perplexity = 18.14, Entropy = 4.18 bits
Computation based on 3561 words.
Number of 3-grams hit = 3559  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
11 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article339.text
Perplexity = 17.59, Entropy = 4.14 bits
Computation based on 374 words.
Number of 3-grams hit = 372  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article340.text
Perplexity = 16.13, Entropy = 4.01 bits
Computation based on 1149 words.
Number of 3-grams hit = 1147  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
5 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article341.text
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 1370 words.
Number of 3-grams hit = 1368  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article342.text
Perplexity = 19.98, Entropy = 4.32 bits
Computation based on 1450 words.
Number of 3-grams hit = 1448  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
1 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article343.text
Perplexity = 18.42, Entropy = 4.20 bits
Computation based on 478 words.
Number of 3-grams hit = 476  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article344.text
Perplexity = 17.27, Entropy = 4.11 bits
Computation based on 578 words.
Number of 3-grams hit = 576  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
3 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article345.text
Perplexity = 16.40, Entropy = 4.04 bits
Computation based on 3148 words.
Number of 3-grams hit = 3146  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
16 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article346.text
Perplexity = 17.19, Entropy = 4.10 bits
Computation based on 1571 words.
Number of 3-grams hit = 1569  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
14 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article347.text
Perplexity = 15.78, Entropy = 3.98 bits
Computation based on 642 words.
Number of 3-grams hit = 640  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
7 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article348.text
Perplexity = 17.44, Entropy = 4.12 bits
Computation based on 2084 words.
Number of 3-grams hit = 2082  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
14 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article349.text
Perplexity = 18.96, Entropy = 4.25 bits
Computation based on 1935 words.
Number of 3-grams hit = 1933  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
6 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article350.text
Perplexity = 17.51, Entropy = 4.13 bits
Computation based on 361 words.
Number of 3-grams hit = 359  (99.45%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article351.text
Perplexity = 16.40, Entropy = 4.04 bits
Computation based on 429 words.
Number of 3-grams hit = 427  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article352.text
Perplexity = 15.26, Entropy = 3.93 bits
Computation based on 449 words.
Number of 3-grams hit = 447  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article353.text
Perplexity = 16.30, Entropy = 4.03 bits
Computation based on 425 words.
Number of 3-grams hit = 423  (99.53%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article354.text
Perplexity = 16.74, Entropy = 4.06 bits
Computation based on 506 words.
Number of 3-grams hit = 504  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article355.text
Perplexity = 14.18, Entropy = 3.83 bits
Computation based on 411 words.
Number of 3-grams hit = 409  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article356.text
Perplexity = 14.39, Entropy = 3.85 bits
Computation based on 708 words.
Number of 3-grams hit = 706  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article357.text
Perplexity = 20.59, Entropy = 4.36 bits
Computation based on 755 words.
Number of 3-grams hit = 753  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article358.text
Perplexity = 17.31, Entropy = 4.11 bits
Computation based on 1189 words.
Number of 3-grams hit = 1187  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
8 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article359.text
Perplexity = 16.37, Entropy = 4.03 bits
Computation based on 1266 words.
Number of 3-grams hit = 1264  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article360.text
Perplexity = 16.51, Entropy = 4.05 bits
Computation based on 710 words.
Number of 3-grams hit = 708  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article361.text
Perplexity = 16.31, Entropy = 4.03 bits
Computation based on 1073 words.
Number of 3-grams hit = 1071  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
12 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article362.text
Perplexity = 19.01, Entropy = 4.25 bits
Computation based on 1414 words.
Number of 3-grams hit = 1412  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
9 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article363.text
Perplexity = 17.04, Entropy = 4.09 bits
Computation based on 1150 words.
Number of 3-grams hit = 1148  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
7 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article364.text
Perplexity = 15.71, Entropy = 3.97 bits
Computation based on 328 words.
Number of 3-grams hit = 326  (99.39%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article365.text
Perplexity = 16.97, Entropy = 4.09 bits
Computation based on 401 words.
Number of 3-grams hit = 399  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
6 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article366.text
Perplexity = 16.20, Entropy = 4.02 bits
Computation based on 348 words.
Number of 3-grams hit = 346  (99.43%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
3 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article367.text
Perplexity = 19.61, Entropy = 4.29 bits
Computation based on 910 words.
Number of 3-grams hit = 908  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article368.text
Perplexity = 13.21, Entropy = 3.72 bits
Computation based on 322 words.
Number of 3-grams hit = 320  (99.38%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article369.text
Perplexity = 18.28, Entropy = 4.19 bits
Computation based on 313 words.
Number of 3-grams hit = 311  (99.36%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
3 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article370.text
Perplexity = 17.86, Entropy = 4.16 bits
Computation based on 1637 words.
Number of 3-grams hit = 1635  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
10 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article371.text
Perplexity = 18.98, Entropy = 4.25 bits
Computation based on 259 words.
Number of 3-grams hit = 257  (99.23%)
Number of 2-grams hit = 1  (0.39%)
Number of 1-grams hit = 1  (0.39%)
3 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article372.text
Perplexity = 18.01, Entropy = 4.17 bits
Computation based on 421 words.
Number of 3-grams hit = 419  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article373.text
Perplexity = 18.21, Entropy = 4.19 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article374.text
Perplexity = 17.09, Entropy = 4.10 bits
Computation based on 1509 words.
Number of 3-grams hit = 1507  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
14 OOVs (0.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article375.text
Perplexity = 20.37, Entropy = 4.35 bits
Computation based on 1610 words.
Number of 3-grams hit = 1608  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
1 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article376.text
Perplexity = 16.22, Entropy = 4.02 bits
Computation based on 561 words.
Number of 3-grams hit = 559  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article377.text
Perplexity = 16.39, Entropy = 4.03 bits
Computation based on 740 words.
Number of 3-grams hit = 738  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article378.text
Perplexity = 14.26, Entropy = 3.83 bits
Computation based on 621 words.
Number of 3-grams hit = 619  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article379.text
Perplexity = 17.28, Entropy = 4.11 bits
Computation based on 2555 words.
Number of 3-grams hit = 2553  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
13 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article380.text
Perplexity = 15.17, Entropy = 3.92 bits
Computation based on 343 words.
Number of 3-grams hit = 341  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
3 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article381.text
Perplexity = 16.82, Entropy = 4.07 bits
Computation based on 626 words.
Number of 3-grams hit = 624  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article382.text
Perplexity = 17.78, Entropy = 4.15 bits
Computation based on 1297 words.
Number of 3-grams hit = 1295  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article383.text
Perplexity = 16.63, Entropy = 4.06 bits
Computation based on 1035 words.
Number of 3-grams hit = 1033  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
4 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article384.text
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 1000 words.
Number of 3-grams hit = 998  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
8 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article385.text
Perplexity = 17.50, Entropy = 4.13 bits
Computation based on 2601 words.
Number of 3-grams hit = 2599  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
5 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article386.text
Perplexity = 16.12, Entropy = 4.01 bits
Computation based on 416 words.
Number of 3-grams hit = 414  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article387.text
Perplexity = 18.47, Entropy = 4.21 bits
Computation based on 1212 words.
Number of 3-grams hit = 1210  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article388.text
Perplexity = 16.84, Entropy = 4.07 bits
Computation based on 4315 words.
Number of 3-grams hit = 4313  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
24 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article389.text
Perplexity = 15.21, Entropy = 3.93 bits
Computation based on 1118 words.
Number of 3-grams hit = 1116  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article390.text
Perplexity = 17.65, Entropy = 4.14 bits
Computation based on 1198 words.
Number of 3-grams hit = 1196  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article391.text
Perplexity = 18.10, Entropy = 4.18 bits
Computation based on 3534 words.
Number of 3-grams hit = 3532  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
7 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article392.text
Perplexity = 20.25, Entropy = 4.34 bits
Computation based on 229 words.
Number of 3-grams hit = 227  (99.13%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
2 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article393.text
Perplexity = 14.38, Entropy = 3.85 bits
Computation based on 484 words.
Number of 3-grams hit = 482  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article394.text
Perplexity = 16.52, Entropy = 4.05 bits
Computation based on 391 words.
Number of 3-grams hit = 389  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
4 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article395.text
Perplexity = 17.19, Entropy = 4.10 bits
Computation based on 307 words.
Number of 3-grams hit = 305  (99.35%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article396.text
Perplexity = 19.32, Entropy = 4.27 bits
Computation based on 3050 words.
Number of 3-grams hit = 3048  (99.93%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
2 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article397.text
Perplexity = 16.19, Entropy = 4.02 bits
Computation based on 3643 words.
Number of 3-grams hit = 3641  (99.95%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
30 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article398.text
Perplexity = 17.22, Entropy = 4.11 bits
Computation based on 818 words.
Number of 3-grams hit = 816  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
2 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article399.text
Perplexity = 17.52, Entropy = 4.13 bits
Computation based on 3649 words.
Number of 3-grams hit = 3647  (99.95%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
23 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article400.text
Perplexity = 17.97, Entropy = 4.17 bits
Computation based on 580 words.
Number of 3-grams hit = 578  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article401.text
Perplexity = 14.62, Entropy = 3.87 bits
Computation based on 661 words.
Number of 3-grams hit = 659  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article402.text
Perplexity = 16.33, Entropy = 4.03 bits
Computation based on 3663 words.
Number of 3-grams hit = 3661  (99.95%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
7 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article403.text
Perplexity = 17.29, Entropy = 4.11 bits
Computation based on 619 words.
Number of 3-grams hit = 617  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article404.text
Perplexity = 15.14, Entropy = 3.92 bits
Computation based on 211 words.
Number of 3-grams hit = 209  (99.05%)
Number of 2-grams hit = 1  (0.47%)
Number of 1-grams hit = 1  (0.47%)
3 OOVs (1.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article405.text
Perplexity = 13.17, Entropy = 3.72 bits
Computation based on 159 words.
Number of 3-grams hit = 157  (98.74%)
Number of 2-grams hit = 1  (0.63%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article406.text
Perplexity = 16.16, Entropy = 4.01 bits
Computation based on 981 words.
Number of 3-grams hit = 979  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
5 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article407.text
Perplexity = 17.30, Entropy = 4.11 bits
Computation based on 694 words.
Number of 3-grams hit = 692  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article408.text
Perplexity = 14.43, Entropy = 3.85 bits
Computation based on 437 words.
Number of 3-grams hit = 435  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article409.text
Perplexity = 17.63, Entropy = 4.14 bits
Computation based on 540 words.
Number of 3-grams hit = 538  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article410.text
Perplexity = 20.52, Entropy = 4.36 bits
Computation based on 234 words.
Number of 3-grams hit = 232  (99.15%)
Number of 2-grams hit = 1  (0.43%)
Number of 1-grams hit = 1  (0.43%)
1 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article411.text
Perplexity = 14.25, Entropy = 3.83 bits
Computation based on 438 words.
Number of 3-grams hit = 436  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article412.text
Perplexity = 19.57, Entropy = 4.29 bits
Computation based on 4561 words.
Number of 3-grams hit = 4559  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
7 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article413.text
Perplexity = 16.70, Entropy = 4.06 bits
Computation based on 3839 words.
Number of 3-grams hit = 3837  (99.95%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
28 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article414.text
Perplexity = 16.97, Entropy = 4.09 bits
Computation based on 398 words.
Number of 3-grams hit = 396  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article415.text
Perplexity = 15.35, Entropy = 3.94 bits
Computation based on 477 words.
Number of 3-grams hit = 475  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article416.text
Perplexity = 15.03, Entropy = 3.91 bits
Computation based on 591 words.
Number of 3-grams hit = 589  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article417.text
Perplexity = 19.02, Entropy = 4.25 bits
Computation based on 853 words.
Number of 3-grams hit = 851  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article418.text
Perplexity = 12.88, Entropy = 3.69 bits
Computation based on 809 words.
Number of 3-grams hit = 807  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article419.text
Perplexity = 14.18, Entropy = 3.83 bits
Computation based on 551 words.
Number of 3-grams hit = 549  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article420.text
Perplexity = 17.17, Entropy = 4.10 bits
Computation based on 611 words.
Number of 3-grams hit = 609  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article421.text
Perplexity = 17.30, Entropy = 4.11 bits
Computation based on 815 words.
Number of 3-grams hit = 813  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
6 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article422.text
Perplexity = 16.23, Entropy = 4.02 bits
Computation based on 554 words.
Number of 3-grams hit = 552  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
5 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article423.text
Perplexity = 15.09, Entropy = 3.92 bits
Computation based on 612 words.
Number of 3-grams hit = 610  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
4 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article424.text
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 528 words.
Number of 3-grams hit = 526  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article425.text
Perplexity = 16.38, Entropy = 4.03 bits
Computation based on 356 words.
Number of 3-grams hit = 354  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article426.text
Perplexity = 17.36, Entropy = 4.12 bits
Computation based on 482 words.
Number of 3-grams hit = 480  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article427.text
Perplexity = 16.66, Entropy = 4.06 bits
Computation based on 789 words.
Number of 3-grams hit = 787  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article428.text
Perplexity = 15.85, Entropy = 3.99 bits
Computation based on 624 words.
Number of 3-grams hit = 622  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
6 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article429.text
Perplexity = 19.02, Entropy = 4.25 bits
Computation based on 769 words.
Number of 3-grams hit = 767  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article430.text
Perplexity = 15.96, Entropy = 4.00 bits
Computation based on 640 words.
Number of 3-grams hit = 638  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article431.text
Perplexity = 18.39, Entropy = 4.20 bits
Computation based on 566 words.
Number of 3-grams hit = 564  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article432.text
Perplexity = 14.83, Entropy = 3.89 bits
Computation based on 352 words.
Number of 3-grams hit = 350  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article433.text
Perplexity = 17.17, Entropy = 4.10 bits
Computation based on 967 words.
Number of 3-grams hit = 965  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
5 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article434.text
Perplexity = 14.44, Entropy = 3.85 bits
Computation based on 383 words.
Number of 3-grams hit = 381  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article435.text
Perplexity = 18.39, Entropy = 4.20 bits
Computation based on 718 words.
Number of 3-grams hit = 716  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article436.text
Perplexity = 16.01, Entropy = 4.00 bits
Computation based on 531 words.
Number of 3-grams hit = 529  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article437.text
Perplexity = 17.04, Entropy = 4.09 bits
Computation based on 1007 words.
Number of 3-grams hit = 1005  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article438.text
Perplexity = 16.51, Entropy = 4.05 bits
Computation based on 288 words.
Number of 3-grams hit = 286  (99.31%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
3 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article439.text
Perplexity = 15.86, Entropy = 3.99 bits
Computation based on 520 words.
Number of 3-grams hit = 518  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article440.text
Perplexity = 16.72, Entropy = 4.06 bits
Computation based on 687 words.
Number of 3-grams hit = 685  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article441.text
Perplexity = 15.57, Entropy = 3.96 bits
Computation based on 410 words.
Number of 3-grams hit = 408  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article442.text
Perplexity = 20.50, Entropy = 4.36 bits
Computation based on 10019 words.
Number of 3-grams hit = 10017  (99.98%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
12 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article443.text
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 11770 words.
Number of 3-grams hit = 11768  (99.98%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
84 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article444.text
Perplexity = 18.40, Entropy = 4.20 bits
Computation based on 650 words.
Number of 3-grams hit = 648  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article445.text
Perplexity = 16.96, Entropy = 4.08 bits
Computation based on 498 words.
Number of 3-grams hit = 496  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article446.text
Perplexity = 15.66, Entropy = 3.97 bits
Computation based on 887 words.
Number of 3-grams hit = 885  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article447.text
Perplexity = 16.93, Entropy = 4.08 bits
Computation based on 728 words.
Number of 3-grams hit = 726  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article448.text
Perplexity = 20.68, Entropy = 4.37 bits
Computation based on 323 words.
Number of 3-grams hit = 321  (99.38%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article449.text
Perplexity = 18.98, Entropy = 4.25 bits
Computation based on 483 words.
Number of 3-grams hit = 481  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article450.text
Perplexity = 15.15, Entropy = 3.92 bits
Computation based on 356 words.
Number of 3-grams hit = 354  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article451.text
Perplexity = 14.88, Entropy = 3.90 bits
Computation based on 574 words.
Number of 3-grams hit = 572  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article452.text
Perplexity = 18.56, Entropy = 4.21 bits
Computation based on 237 words.
Number of 3-grams hit = 235  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article453.text
Perplexity = 16.14, Entropy = 4.01 bits
Computation based on 625 words.
Number of 3-grams hit = 623  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article454.text
Perplexity = 16.02, Entropy = 4.00 bits
Computation based on 542 words.
Number of 3-grams hit = 540  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article455.text
Perplexity = 15.59, Entropy = 3.96 bits
Computation based on 380 words.
Number of 3-grams hit = 378  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article456.text
Perplexity = 16.35, Entropy = 4.03 bits
Computation based on 656 words.
Number of 3-grams hit = 654  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article457.text
Perplexity = 17.14, Entropy = 4.10 bits
Computation based on 333 words.
Number of 3-grams hit = 331  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
4 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article458.text
Perplexity = 18.14, Entropy = 4.18 bits
Computation based on 275 words.
Number of 3-grams hit = 273  (99.27%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
1 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article459.text
Perplexity = 17.23, Entropy = 4.11 bits
Computation based on 696 words.
Number of 3-grams hit = 694  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
5 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article460.text
Perplexity = 17.57, Entropy = 4.14 bits
Computation based on 322 words.
Number of 3-grams hit = 320  (99.38%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
5 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article461.text
Perplexity = 12.49, Entropy = 3.64 bits
Computation based on 419 words.
Number of 3-grams hit = 417  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article462.text
Perplexity = 15.72, Entropy = 3.97 bits
Computation based on 1399 words.
Number of 3-grams hit = 1397  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
11 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article463.text
Perplexity = 14.78, Entropy = 3.89 bits
Computation based on 551 words.
Number of 3-grams hit = 549  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article464.text
Perplexity = 16.55, Entropy = 4.05 bits
Computation based on 438 words.
Number of 3-grams hit = 436  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article465.text
Perplexity = 19.17, Entropy = 4.26 bits
Computation based on 1804 words.
Number of 3-grams hit = 1802  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
3 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article466.text
Perplexity = 17.49, Entropy = 4.13 bits
Computation based on 204 words.
Number of 3-grams hit = 202  (99.02%)
Number of 2-grams hit = 1  (0.49%)
Number of 1-grams hit = 1  (0.49%)
1 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article467.text
Perplexity = 18.06, Entropy = 4.17 bits
Computation based on 1022 words.
Number of 3-grams hit = 1020  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
8 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article468.text
Perplexity = 16.48, Entropy = 4.04 bits
Computation based on 772 words.
Number of 3-grams hit = 770  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
6 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article469.text
Perplexity = 17.07, Entropy = 4.09 bits
Computation based on 355 words.
Number of 3-grams hit = 353  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article470.text
Perplexity = 13.64, Entropy = 3.77 bits
Computation based on 239 words.
Number of 3-grams hit = 237  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
1 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article471.text
Perplexity = 18.34, Entropy = 4.20 bits
Computation based on 1996 words.
Number of 3-grams hit = 1994  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
8 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article472.text
Perplexity = 16.49, Entropy = 4.04 bits
Computation based on 930 words.
Number of 3-grams hit = 928  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
6 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article473.text
Perplexity = 18.69, Entropy = 4.22 bits
Computation based on 217 words.
Number of 3-grams hit = 215  (99.08%)
Number of 2-grams hit = 1  (0.46%)
Number of 1-grams hit = 1  (0.46%)
2 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article474.text
Perplexity = 17.18, Entropy = 4.10 bits
Computation based on 783 words.
Number of 3-grams hit = 781  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article475.text
Perplexity = 15.38, Entropy = 3.94 bits
Computation based on 540 words.
Number of 3-grams hit = 538  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
8 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article476.text
Perplexity = 15.08, Entropy = 3.91 bits
Computation based on 580 words.
Number of 3-grams hit = 578  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article477.text
Perplexity = 17.05, Entropy = 4.09 bits
Computation based on 445 words.
Number of 3-grams hit = 443  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article478.text
Perplexity = 16.10, Entropy = 4.01 bits
Computation based on 504 words.
Number of 3-grams hit = 502  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article479.text
Perplexity = 16.81, Entropy = 4.07 bits
Computation based on 420 words.
Number of 3-grams hit = 418  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article480.text
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 687 words.
Number of 3-grams hit = 685  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article481.text
Perplexity = 17.07, Entropy = 4.09 bits
Computation based on 351 words.
Number of 3-grams hit = 349  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article482.text
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 491 words.
Number of 3-grams hit = 489  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
4 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article483.text
Perplexity = 19.41, Entropy = 4.28 bits
Computation based on 1672 words.
Number of 3-grams hit = 1670  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
4 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article484.text
Perplexity = 15.72, Entropy = 3.97 bits
Computation based on 1418 words.
Number of 3-grams hit = 1416  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
14 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article485.text
Perplexity = 12.94, Entropy = 3.69 bits
Computation based on 376 words.
Number of 3-grams hit = 374  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article486.text
Perplexity = 18.61, Entropy = 4.22 bits
Computation based on 332 words.
Number of 3-grams hit = 330  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article487.text
Perplexity = 17.43, Entropy = 4.12 bits
Computation based on 431 words.
Number of 3-grams hit = 429  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article488.text
Perplexity = 18.66, Entropy = 4.22 bits
Computation based on 706 words.
Number of 3-grams hit = 704  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
5 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article489.text
Perplexity = 18.80, Entropy = 4.23 bits
Computation based on 545 words.
Number of 3-grams hit = 543  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article490.text
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 491 words.
Number of 3-grams hit = 489  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article491.text
Perplexity = 15.74, Entropy = 3.98 bits
Computation based on 1386 words.
Number of 3-grams hit = 1384  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
9 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article492.text
Perplexity = 15.18, Entropy = 3.92 bits
Computation based on 434 words.
Number of 3-grams hit = 432  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article493.text
Perplexity = 14.58, Entropy = 3.87 bits
Computation based on 756 words.
Number of 3-grams hit = 754  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article494.text
Perplexity = 16.09, Entropy = 4.01 bits
Computation based on 481 words.
Number of 3-grams hit = 479  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article495.text
Perplexity = 15.22, Entropy = 3.93 bits
Computation based on 430 words.
Number of 3-grams hit = 428  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article496.text
Perplexity = 14.66, Entropy = 3.87 bits
Computation based on 555 words.
Number of 3-grams hit = 553  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article497.text
Perplexity = 17.61, Entropy = 4.14 bits
Computation based on 648 words.
Number of 3-grams hit = 646  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
4 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article498.text
Perplexity = 17.51, Entropy = 4.13 bits
Computation based on 1692 words.
Number of 3-grams hit = 1690  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
7 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article499.text
Perplexity = 18.91, Entropy = 4.24 bits
Computation based on 494 words.
Number of 3-grams hit = 492  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article500.text
Perplexity = 15.05, Entropy = 3.91 bits
Computation based on 454 words.
Number of 3-grams hit = 452  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article501.text
Perplexity = 15.68, Entropy = 3.97 bits
Computation based on 403 words.
Number of 3-grams hit = 401  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article502.text
Perplexity = 15.72, Entropy = 3.97 bits
Computation based on 505 words.
Number of 3-grams hit = 503  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article503.text
Perplexity = 17.25, Entropy = 4.11 bits
Computation based on 1011 words.
Number of 3-grams hit = 1009  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
7 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article504.text
Perplexity = 15.10, Entropy = 3.92 bits
Computation based on 612 words.
Number of 3-grams hit = 610  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article505.text
Perplexity = 14.45, Entropy = 3.85 bits
Computation based on 697 words.
Number of 3-grams hit = 695  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
5 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article506.text
Perplexity = 17.43, Entropy = 4.12 bits
Computation based on 535 words.
Number of 3-grams hit = 533  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article507.text
Perplexity = 16.24, Entropy = 4.02 bits
Computation based on 1588 words.
Number of 3-grams hit = 1586  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article508.text
Perplexity = 17.14, Entropy = 4.10 bits
Computation based on 656 words.
Number of 3-grams hit = 654  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article509.text
Perplexity = 15.34, Entropy = 3.94 bits
Computation based on 350 words.
Number of 3-grams hit = 348  (99.43%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
3 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article510.text
Perplexity = 16.29, Entropy = 4.03 bits
Computation based on 971 words.
Number of 3-grams hit = 969  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article511.text
Perplexity = 14.30, Entropy = 3.84 bits
Computation based on 629 words.
Number of 3-grams hit = 627  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article512.text
Perplexity = 17.35, Entropy = 4.12 bits
Computation based on 1888 words.
Number of 3-grams hit = 1886  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
20 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article513.text
Perplexity = 12.60, Entropy = 3.65 bits
Computation based on 416 words.
Number of 3-grams hit = 414  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article514.text
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 396 words.
Number of 3-grams hit = 394  (99.49%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article515.text
Perplexity = 14.76, Entropy = 3.88 bits
Computation based on 971 words.
Number of 3-grams hit = 969  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
3 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article516.text
Perplexity = 17.31, Entropy = 4.11 bits
Computation based on 410 words.
Number of 3-grams hit = 408  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
4 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article517.text
Perplexity = 16.14, Entropy = 4.01 bits
Computation based on 1054 words.
Number of 3-grams hit = 1052  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
8 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article518.text
Perplexity = 15.73, Entropy = 3.98 bits
Computation based on 587 words.
Number of 3-grams hit = 585  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article519.text
Perplexity = 16.21, Entropy = 4.02 bits
Computation based on 529 words.
Number of 3-grams hit = 527  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article520.text
Perplexity = 17.92, Entropy = 4.16 bits
Computation based on 609 words.
Number of 3-grams hit = 607  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article521.text
Perplexity = 15.56, Entropy = 3.96 bits
Computation based on 518 words.
Number of 3-grams hit = 516  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article522.text
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 2324 words.
Number of 3-grams hit = 2322  (99.91%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
6 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article523.text
Perplexity = 17.22, Entropy = 4.11 bits
Computation based on 327 words.
Number of 3-grams hit = 325  (99.39%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article524.text
Perplexity = 14.86, Entropy = 3.89 bits
Computation based on 1376 words.
Number of 3-grams hit = 1374  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
11 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article525.text
Perplexity = 17.04, Entropy = 4.09 bits
Computation based on 1962 words.
Number of 3-grams hit = 1960  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
15 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article526.text
Perplexity = 18.13, Entropy = 4.18 bits
Computation based on 1890 words.
Number of 3-grams hit = 1888  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
19 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article527.text
Perplexity = 15.86, Entropy = 3.99 bits
Computation based on 727 words.
Number of 3-grams hit = 725  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
9 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article528.text
Perplexity = 17.06, Entropy = 4.09 bits
Computation based on 723 words.
Number of 3-grams hit = 721  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
4 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article529.text
Perplexity = 14.84, Entropy = 3.89 bits
Computation based on 881 words.
Number of 3-grams hit = 879  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article530.text
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 426 words.
Number of 3-grams hit = 424  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article531.text
Perplexity = 16.61, Entropy = 4.05 bits
Computation based on 943 words.
Number of 3-grams hit = 941  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article532.text
Perplexity = 18.61, Entropy = 4.22 bits
Computation based on 927 words.
Number of 3-grams hit = 925  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
9 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article533.text
Perplexity = 17.21, Entropy = 4.11 bits
Computation based on 1134 words.
Number of 3-grams hit = 1132  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article534.text
Perplexity = 14.98, Entropy = 3.90 bits
Computation based on 501 words.
Number of 3-grams hit = 499  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article535.text
Perplexity = 13.96, Entropy = 3.80 bits
Computation based on 357 words.
Number of 3-grams hit = 355  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article536.text
Perplexity = 17.28, Entropy = 4.11 bits
Computation based on 1510 words.
Number of 3-grams hit = 1508  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
5 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article537.text
Perplexity = 20.06, Entropy = 4.33 bits
Computation based on 996 words.
Number of 3-grams hit = 994  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
7 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article538.text
Perplexity = 16.54, Entropy = 4.05 bits
Computation based on 3417 words.
Number of 3-grams hit = 3415  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
24 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article539.text
Perplexity = 16.43, Entropy = 4.04 bits
Computation based on 534 words.
Number of 3-grams hit = 532  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article540.text
Perplexity = 18.11, Entropy = 4.18 bits
Computation based on 2197 words.
Number of 3-grams hit = 2195  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
1 OOVs (0.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article541.text
Perplexity = 17.14, Entropy = 4.10 bits
Computation based on 491 words.
Number of 3-grams hit = 489  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
7 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article542.text
Perplexity = 19.44, Entropy = 4.28 bits
Computation based on 1590 words.
Number of 3-grams hit = 1588  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article543.text
Perplexity = 16.07, Entropy = 4.01 bits
Computation based on 539 words.
Number of 3-grams hit = 537  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article544.text
Perplexity = 15.58, Entropy = 3.96 bits
Computation based on 1274 words.
Number of 3-grams hit = 1272  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
13 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article545.text
Perplexity = 16.95, Entropy = 4.08 bits
Computation based on 748 words.
Number of 3-grams hit = 746  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article546.text
Perplexity = 16.55, Entropy = 4.05 bits
Computation based on 691 words.
Number of 3-grams hit = 689  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article547.text
Perplexity = 16.13, Entropy = 4.01 bits
Computation based on 891 words.
Number of 3-grams hit = 889  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
4 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article548.text
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 618 words.
Number of 3-grams hit = 616  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
5 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article549.text
Perplexity = 15.41, Entropy = 3.95 bits
Computation based on 357 words.
Number of 3-grams hit = 355  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article550.text
Perplexity = 21.47, Entropy = 4.42 bits
Computation based on 184 words.
Number of 3-grams hit = 182  (98.91%)
Number of 2-grams hit = 1  (0.54%)
Number of 1-grams hit = 1  (0.54%)
1 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article551.text
Perplexity = 19.63, Entropy = 4.29 bits
Computation based on 922 words.
Number of 3-grams hit = 920  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article552.text
Perplexity = 15.86, Entropy = 3.99 bits
Computation based on 1075 words.
Number of 3-grams hit = 1073  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article553.text
Perplexity = 21.07, Entropy = 4.40 bits
Computation based on 1239 words.
Number of 3-grams hit = 1237  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article554.text
Perplexity = 17.61, Entropy = 4.14 bits
Computation based on 4066 words.
Number of 3-grams hit = 4064  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
9 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article555.text
Perplexity = 16.85, Entropy = 4.07 bits
Computation based on 507 words.
Number of 3-grams hit = 505  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article556.text
Perplexity = 15.02, Entropy = 3.91 bits
Computation based on 567 words.
Number of 3-grams hit = 565  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article557.text
Perplexity = 17.87, Entropy = 4.16 bits
Computation based on 380 words.
Number of 3-grams hit = 378  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
3 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article558.text
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 1728 words.
Number of 3-grams hit = 1726  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
1 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article559.text
Perplexity = 14.48, Entropy = 3.86 bits
Computation based on 635 words.
Number of 3-grams hit = 633  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article560.text
Perplexity = 17.80, Entropy = 4.15 bits
Computation based on 548 words.
Number of 3-grams hit = 546  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article561.text
Perplexity = 14.74, Entropy = 3.88 bits
Computation based on 331 words.
Number of 3-grams hit = 329  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article562.text
Perplexity = 13.30, Entropy = 3.73 bits
Computation based on 311 words.
Number of 3-grams hit = 309  (99.36%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
2 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article563.text
Perplexity = 18.43, Entropy = 4.20 bits
Computation based on 491 words.
Number of 3-grams hit = 489  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article564.text
Perplexity = 16.53, Entropy = 4.05 bits
Computation based on 1328 words.
Number of 3-grams hit = 1326  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
11 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article565.text
Perplexity = 14.99, Entropy = 3.91 bits
Computation based on 1298 words.
Number of 3-grams hit = 1296  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article566.text
Perplexity = 16.98, Entropy = 4.09 bits
Computation based on 4140 words.
Number of 3-grams hit = 4138  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
13 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article567.text
Perplexity = 15.36, Entropy = 3.94 bits
Computation based on 547 words.
Number of 3-grams hit = 545  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article568.text
Perplexity = 15.69, Entropy = 3.97 bits
Computation based on 329 words.
Number of 3-grams hit = 327  (99.39%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article569.text
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 4332 words.
Number of 3-grams hit = 4330  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
24 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article570.text
Perplexity = 18.94, Entropy = 4.24 bits
Computation based on 816 words.
Number of 3-grams hit = 814  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
2 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article571.text
Perplexity = 13.52, Entropy = 3.76 bits
Computation based on 540 words.
Number of 3-grams hit = 538  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article572.text
Perplexity = 16.62, Entropy = 4.06 bits
Computation based on 563 words.
Number of 3-grams hit = 561  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
8 OOVs (1.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article573.text
Perplexity = 18.84, Entropy = 4.24 bits
Computation based on 249 words.
Number of 3-grams hit = 247  (99.20%)
Number of 2-grams hit = 1  (0.40%)
Number of 1-grams hit = 1  (0.40%)
3 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article574.text
Perplexity = 16.08, Entropy = 4.01 bits
Computation based on 1091 words.
Number of 3-grams hit = 1089  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article575.text
Perplexity = 16.07, Entropy = 4.01 bits
Computation based on 594 words.
Number of 3-grams hit = 592  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article576.text
Perplexity = 17.82, Entropy = 4.16 bits
Computation based on 441 words.
Number of 3-grams hit = 439  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
4 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article577.text
Perplexity = 18.00, Entropy = 4.17 bits
Computation based on 362 words.
Number of 3-grams hit = 360  (99.45%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article578.text
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 405 words.
Number of 3-grams hit = 403  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article579.text
Perplexity = 15.63, Entropy = 3.97 bits
Computation based on 438 words.
Number of 3-grams hit = 436  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
4 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article580.text
Perplexity = 19.06, Entropy = 4.25 bits
Computation based on 535 words.
Number of 3-grams hit = 533  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article581.text
Perplexity = 16.99, Entropy = 4.09 bits
Computation based on 538 words.
Number of 3-grams hit = 536  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article582.text
Perplexity = 14.62, Entropy = 3.87 bits
Computation based on 508 words.
Number of 3-grams hit = 506  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article583.text
Perplexity = 15.90, Entropy = 3.99 bits
Computation based on 673 words.
Number of 3-grams hit = 671  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article584.text
Perplexity = 16.36, Entropy = 4.03 bits
Computation based on 430 words.
Number of 3-grams hit = 428  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
4 OOVs (0.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article585.text
Perplexity = 18.19, Entropy = 4.18 bits
Computation based on 952 words.
Number of 3-grams hit = 950  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article586.text
Perplexity = 19.53, Entropy = 4.29 bits
Computation based on 733 words.
Number of 3-grams hit = 731  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article587.text
Perplexity = 16.80, Entropy = 4.07 bits
Computation based on 519 words.
Number of 3-grams hit = 517  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article588.text
Perplexity = 17.58, Entropy = 4.14 bits
Computation based on 954 words.
Number of 3-grams hit = 952  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article589.text
Perplexity = 17.74, Entropy = 4.15 bits
Computation based on 502 words.
Number of 3-grams hit = 500  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article590.text
Perplexity = 17.02, Entropy = 4.09 bits
Computation based on 1390 words.
Number of 3-grams hit = 1388  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article591.text
Perplexity = 18.93, Entropy = 4.24 bits
Computation based on 681 words.
Number of 3-grams hit = 679  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
6 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article592.text
Perplexity = 14.50, Entropy = 3.86 bits
Computation based on 340 words.
Number of 3-grams hit = 338  (99.41%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article593.text
Perplexity = 18.89, Entropy = 4.24 bits
Computation based on 377 words.
Number of 3-grams hit = 375  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article594.text
Perplexity = 18.51, Entropy = 4.21 bits
Computation based on 586 words.
Number of 3-grams hit = 584  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article595.text
Perplexity = 15.82, Entropy = 3.98 bits
Computation based on 371 words.
Number of 3-grams hit = 369  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article596.text
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 959 words.
Number of 3-grams hit = 957  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article597.text
Perplexity = 15.00, Entropy = 3.91 bits
Computation based on 1032 words.
Number of 3-grams hit = 1030  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article598.text
Perplexity = 14.86, Entropy = 3.89 bits
Computation based on 297 words.
Number of 3-grams hit = 295  (99.33%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
2 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article599.text
Perplexity = 13.97, Entropy = 3.80 bits
Computation based on 803 words.
Number of 3-grams hit = 801  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article600.text
Perplexity = 19.60, Entropy = 4.29 bits
Computation based on 4669 words.
Number of 3-grams hit = 4667  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
4 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article601.text
Perplexity = 13.54, Entropy = 3.76 bits
Computation based on 1097 words.
Number of 3-grams hit = 1095  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article602.text
Perplexity = 16.66, Entropy = 4.06 bits
Computation based on 592 words.
Number of 3-grams hit = 590  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article603.text
Perplexity = 15.83, Entropy = 3.98 bits
Computation based on 582 words.
Number of 3-grams hit = 580  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article604.text
Perplexity = 16.52, Entropy = 4.05 bits
Computation based on 561 words.
Number of 3-grams hit = 559  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article605.text
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 3152 words.
Number of 3-grams hit = 3150  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
4 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article606.text
Perplexity = 16.61, Entropy = 4.05 bits
Computation based on 910 words.
Number of 3-grams hit = 908  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article607.text
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 1055 words.
Number of 3-grams hit = 1053  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
8 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article608.text
Perplexity = 17.33, Entropy = 4.12 bits
Computation based on 615 words.
Number of 3-grams hit = 613  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article609.text
Perplexity = 17.80, Entropy = 4.15 bits
Computation based on 4403 words.
Number of 3-grams hit = 4401  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
6 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article610.text
Perplexity = 17.29, Entropy = 4.11 bits
Computation based on 5927 words.
Number of 3-grams hit = 5925  (99.97%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
36 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article611.text
Perplexity = 18.28, Entropy = 4.19 bits
Computation based on 776 words.
Number of 3-grams hit = 774  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article612.text
Perplexity = 17.98, Entropy = 4.17 bits
Computation based on 1625 words.
Number of 3-grams hit = 1623  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
7 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article613.text
Perplexity = 17.63, Entropy = 4.14 bits
Computation based on 4183 words.
Number of 3-grams hit = 4181  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
5 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article614.text
Perplexity = 17.74, Entropy = 4.15 bits
Computation based on 1548 words.
Number of 3-grams hit = 1546  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
2 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article615.text
Perplexity = 16.02, Entropy = 4.00 bits
Computation based on 452 words.
Number of 3-grams hit = 450  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article616.text
Perplexity = 18.01, Entropy = 4.17 bits
Computation based on 771 words.
Number of 3-grams hit = 769  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article617.text
Perplexity = 16.43, Entropy = 4.04 bits
Computation based on 1323 words.
Number of 3-grams hit = 1321  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
1 OOVs (0.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article618.text
Perplexity = 15.99, Entropy = 4.00 bits
Computation based on 3755 words.
Number of 3-grams hit = 3753  (99.95%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
22 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article619.text
Perplexity = 17.50, Entropy = 4.13 bits
Computation based on 764 words.
Number of 3-grams hit = 762  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article620.text
Perplexity = 16.14, Entropy = 4.01 bits
Computation based on 5287 words.
Number of 3-grams hit = 5285  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
42 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article621.text
Perplexity = 14.45, Entropy = 3.85 bits
Computation based on 321 words.
Number of 3-grams hit = 319  (99.38%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article622.text
Perplexity = 17.77, Entropy = 4.15 bits
Computation based on 458 words.
Number of 3-grams hit = 456  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article623.text
Perplexity = 17.76, Entropy = 4.15 bits
Computation based on 1300 words.
Number of 3-grams hit = 1298  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
8 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article624.text
Perplexity = 16.07, Entropy = 4.01 bits
Computation based on 5132 words.
Number of 3-grams hit = 5130  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
34 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article625.text
Perplexity = 16.06, Entropy = 4.01 bits
Computation based on 1088 words.
Number of 3-grams hit = 1086  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article626.text
Perplexity = 15.52, Entropy = 3.96 bits
Computation based on 441 words.
Number of 3-grams hit = 439  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
6 OOVs (1.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article627.text
Perplexity = 14.92, Entropy = 3.90 bits
Computation based on 148 words.
Number of 3-grams hit = 146  (98.65%)
Number of 2-grams hit = 1  (0.68%)
Number of 1-grams hit = 1  (0.68%)
1 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article628.text
Perplexity = 16.46, Entropy = 4.04 bits
Computation based on 1073 words.
Number of 3-grams hit = 1071  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article629.text
Perplexity = 16.26, Entropy = 4.02 bits
Computation based on 1028 words.
Number of 3-grams hit = 1026  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
9 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article630.text
Perplexity = 14.25, Entropy = 3.83 bits
Computation based on 326 words.
Number of 3-grams hit = 324  (99.39%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
3 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article631.text
Perplexity = 14.49, Entropy = 3.86 bits
Computation based on 872 words.
Number of 3-grams hit = 870  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article632.text
Perplexity = 16.23, Entropy = 4.02 bits
Computation based on 408 words.
Number of 3-grams hit = 406  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article633.text
Perplexity = 15.80, Entropy = 3.98 bits
Computation based on 529 words.
Number of 3-grams hit = 527  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article634.text
Perplexity = 18.80, Entropy = 4.23 bits
Computation based on 563 words.
Number of 3-grams hit = 561  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article635.text
Perplexity = 18.57, Entropy = 4.21 bits
Computation based on 149 words.
Number of 3-grams hit = 147  (98.66%)
Number of 2-grams hit = 1  (0.67%)
Number of 1-grams hit = 1  (0.67%)
3 OOVs (1.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article636.text
Perplexity = 14.75, Entropy = 3.88 bits
Computation based on 1341 words.
Number of 3-grams hit = 1339  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article637.text
Perplexity = 15.94, Entropy = 3.99 bits
Computation based on 1030 words.
Number of 3-grams hit = 1028  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
5 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article638.text
Perplexity = 14.40, Entropy = 3.85 bits
Computation based on 527 words.
Number of 3-grams hit = 525  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article639.text
Perplexity = 14.25, Entropy = 3.83 bits
Computation based on 524 words.
Number of 3-grams hit = 522  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article640.text
Perplexity = 16.23, Entropy = 4.02 bits
Computation based on 201 words.
Number of 3-grams hit = 199  (99.00%)
Number of 2-grams hit = 1  (0.50%)
Number of 1-grams hit = 1  (0.50%)
3 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article641.text
Perplexity = 16.12, Entropy = 4.01 bits
Computation based on 825 words.
Number of 3-grams hit = 823  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
5 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article642.text
Perplexity = 13.35, Entropy = 3.74 bits
Computation based on 441 words.
Number of 3-grams hit = 439  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article643.text
Perplexity = 18.48, Entropy = 4.21 bits
Computation based on 486 words.
Number of 3-grams hit = 484  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article644.text
Perplexity = 18.05, Entropy = 4.17 bits
Computation based on 1114 words.
Number of 3-grams hit = 1112  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
7 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article645.text
Perplexity = 15.70, Entropy = 3.97 bits
Computation based on 444 words.
Number of 3-grams hit = 442  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article646.text
Perplexity = 16.20, Entropy = 4.02 bits
Computation based on 574 words.
Number of 3-grams hit = 572  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article647.text
Perplexity = 16.14, Entropy = 4.01 bits
Computation based on 604 words.
Number of 3-grams hit = 602  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
7 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article648.text
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 472 words.
Number of 3-grams hit = 470  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article649.text
Perplexity = 17.57, Entropy = 4.13 bits
Computation based on 526 words.
Number of 3-grams hit = 524  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article650.text
Perplexity = 14.91, Entropy = 3.90 bits
Computation based on 534 words.
Number of 3-grams hit = 532  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article651.text
Perplexity = 16.79, Entropy = 4.07 bits
Computation based on 461 words.
Number of 3-grams hit = 459  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article652.text
Perplexity = 18.11, Entropy = 4.18 bits
Computation based on 880 words.
Number of 3-grams hit = 878  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article653.text
Perplexity = 18.70, Entropy = 4.22 bits
Computation based on 762 words.
Number of 3-grams hit = 760  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article654.text
Perplexity = 16.73, Entropy = 4.06 bits
Computation based on 766 words.
Number of 3-grams hit = 764  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article655.text
Perplexity = 16.90, Entropy = 4.08 bits
Computation based on 486 words.
Number of 3-grams hit = 484  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article656.text
Perplexity = 13.14, Entropy = 3.72 bits
Computation based on 463 words.
Number of 3-grams hit = 461  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article657.text
Perplexity = 16.58, Entropy = 4.05 bits
Computation based on 371 words.
Number of 3-grams hit = 369  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
6 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article658.text
Perplexity = 17.39, Entropy = 4.12 bits
Computation based on 1000 words.
Number of 3-grams hit = 998  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article659.text
Perplexity = 17.71, Entropy = 4.15 bits
Computation based on 989 words.
Number of 3-grams hit = 987  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article660.text
Perplexity = 15.84, Entropy = 3.99 bits
Computation based on 136 words.
Number of 3-grams hit = 134  (98.53%)
Number of 2-grams hit = 1  (0.74%)
Number of 1-grams hit = 1  (0.74%)
3 OOVs (2.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article661.text
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 1237 words.
Number of 3-grams hit = 1235  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article662.text
Perplexity = 16.55, Entropy = 4.05 bits
Computation based on 123 words.
Number of 3-grams hit = 121  (98.37%)
Number of 2-grams hit = 1  (0.81%)
Number of 1-grams hit = 1  (0.81%)
3 OOVs (2.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article663.text
Perplexity = 15.96, Entropy = 4.00 bits
Computation based on 1358 words.
Number of 3-grams hit = 1356  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article664.text
Perplexity = 18.37, Entropy = 4.20 bits
Computation based on 698 words.
Number of 3-grams hit = 696  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article665.text
Perplexity = 17.31, Entropy = 4.11 bits
Computation based on 753 words.
Number of 3-grams hit = 751  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article666.text
Perplexity = 16.63, Entropy = 4.06 bits
Computation based on 381 words.
Number of 3-grams hit = 379  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
4 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article667.text
Perplexity = 21.66, Entropy = 4.44 bits
Computation based on 1300 words.
Number of 3-grams hit = 1298  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
1 OOVs (0.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article668.text
Perplexity = 20.00, Entropy = 4.32 bits
Computation based on 467 words.
Number of 3-grams hit = 465  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article669.text
Perplexity = 16.55, Entropy = 4.05 bits
Computation based on 486 words.
Number of 3-grams hit = 484  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article670.text
Perplexity = 15.19, Entropy = 3.93 bits
Computation based on 446 words.
Number of 3-grams hit = 444  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article671.text
Perplexity = 16.82, Entropy = 4.07 bits
Computation based on 2024 words.
Number of 3-grams hit = 2022  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
12 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article672.text
Perplexity = 18.02, Entropy = 4.17 bits
Computation based on 282 words.
Number of 3-grams hit = 280  (99.29%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
6 OOVs (2.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article673.text
Perplexity = 19.72, Entropy = 4.30 bits
Computation based on 302 words.
Number of 3-grams hit = 300  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article674.text
Perplexity = 18.61, Entropy = 4.22 bits
Computation based on 141 words.
Number of 3-grams hit = 139  (98.58%)
Number of 2-grams hit = 1  (0.71%)
Number of 1-grams hit = 1  (0.71%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article675.text
Perplexity = 17.04, Entropy = 4.09 bits
Computation based on 3857 words.
Number of 3-grams hit = 3855  (99.95%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
17 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article676.text
Perplexity = 16.89, Entropy = 4.08 bits
Computation based on 495 words.
Number of 3-grams hit = 493  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article677.text
Perplexity = 16.10, Entropy = 4.01 bits
Computation based on 641 words.
Number of 3-grams hit = 639  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article678.text
Perplexity = 17.28, Entropy = 4.11 bits
Computation based on 340 words.
Number of 3-grams hit = 338  (99.41%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article679.text
Perplexity = 17.98, Entropy = 4.17 bits
Computation based on 978 words.
Number of 3-grams hit = 976  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
11 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article680.text
Perplexity = 18.03, Entropy = 4.17 bits
Computation based on 316 words.
Number of 3-grams hit = 314  (99.37%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article681.text
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 478 words.
Number of 3-grams hit = 476  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article682.text
Perplexity = 16.92, Entropy = 4.08 bits
Computation based on 3663 words.
Number of 3-grams hit = 3661  (99.95%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
27 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article683.text
Perplexity = 14.62, Entropy = 3.87 bits
Computation based on 266 words.
Number of 3-grams hit = 264  (99.25%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article684.text
Perplexity = 17.80, Entropy = 4.15 bits
Computation based on 4351 words.
Number of 3-grams hit = 4349  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
7 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article685.text
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 456 words.
Number of 3-grams hit = 454  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article686.text
Perplexity = 16.24, Entropy = 4.02 bits
Computation based on 2213 words.
Number of 3-grams hit = 2211  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
16 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article687.text
Perplexity = 15.83, Entropy = 3.99 bits
Computation based on 511 words.
Number of 3-grams hit = 509  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article688.text
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 304 words.
Number of 3-grams hit = 302  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article689.text
Perplexity = 15.25, Entropy = 3.93 bits
Computation based on 507 words.
Number of 3-grams hit = 505  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article690.text
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 749 words.
Number of 3-grams hit = 747  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
6 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article691.text
Perplexity = 16.06, Entropy = 4.01 bits
Computation based on 871 words.
Number of 3-grams hit = 869  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article692.text
Perplexity = 14.35, Entropy = 3.84 bits
Computation based on 430 words.
Number of 3-grams hit = 428  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article693.text
Perplexity = 17.03, Entropy = 4.09 bits
Computation based on 943 words.
Number of 3-grams hit = 941  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article694.text
Perplexity = 18.31, Entropy = 4.19 bits
Computation based on 630 words.
Number of 3-grams hit = 628  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article695.text
Perplexity = 17.49, Entropy = 4.13 bits
Computation based on 843 words.
Number of 3-grams hit = 841  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
11 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article696.text
Perplexity = 15.22, Entropy = 3.93 bits
Computation based on 499 words.
Number of 3-grams hit = 497  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article697.text
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 936 words.
Number of 3-grams hit = 934  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
9 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article698.text
Perplexity = 14.83, Entropy = 3.89 bits
Computation based on 570 words.
Number of 3-grams hit = 568  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article699.text
Perplexity = 16.16, Entropy = 4.01 bits
Computation based on 352 words.
Number of 3-grams hit = 350  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article700.text
Perplexity = 18.59, Entropy = 4.22 bits
Computation based on 3334 words.
Number of 3-grams hit = 3332  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
9 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article701.text
Perplexity = 15.99, Entropy = 4.00 bits
Computation based on 806 words.
Number of 3-grams hit = 804  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article702.text
Perplexity = 16.38, Entropy = 4.03 bits
Computation based on 1828 words.
Number of 3-grams hit = 1826  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
9 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article703.text
Perplexity = 14.91, Entropy = 3.90 bits
Computation based on 425 words.
Number of 3-grams hit = 423  (99.53%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article704.text
Perplexity = 20.39, Entropy = 4.35 bits
Computation based on 307 words.
Number of 3-grams hit = 305  (99.35%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
4 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article705.text
Perplexity = 17.31, Entropy = 4.11 bits
Computation based on 1009 words.
Number of 3-grams hit = 1007  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article706.text
Perplexity = 16.35, Entropy = 4.03 bits
Computation based on 475 words.
Number of 3-grams hit = 473  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article707.text
Perplexity = 14.34, Entropy = 3.84 bits
Computation based on 865 words.
Number of 3-grams hit = 863  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
6 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article708.text
Perplexity = 16.60, Entropy = 4.05 bits
Computation based on 526 words.
Number of 3-grams hit = 524  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article709.text
Perplexity = 18.67, Entropy = 4.22 bits
Computation based on 554 words.
Number of 3-grams hit = 552  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article710.text
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 771 words.
Number of 3-grams hit = 769  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article711.text
Perplexity = 15.60, Entropy = 3.96 bits
Computation based on 962 words.
Number of 3-grams hit = 960  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article712.text
Perplexity = 16.52, Entropy = 4.05 bits
Computation based on 366 words.
Number of 3-grams hit = 364  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
3 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article713.text
Perplexity = 18.01, Entropy = 4.17 bits
Computation based on 888 words.
Number of 3-grams hit = 886  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article714.text
Perplexity = 16.22, Entropy = 4.02 bits
Computation based on 365 words.
Number of 3-grams hit = 363  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article715.text
Perplexity = 16.66, Entropy = 4.06 bits
Computation based on 482 words.
Number of 3-grams hit = 480  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article716.text
Perplexity = 16.13, Entropy = 4.01 bits
Computation based on 466 words.
Number of 3-grams hit = 464  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article717.text
Perplexity = 14.76, Entropy = 3.88 bits
Computation based on 381 words.
Number of 3-grams hit = 379  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article718.text
Perplexity = 16.57, Entropy = 4.05 bits
Computation based on 6935 words.
Number of 3-grams hit = 6933  (99.97%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
50 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article719.text
Perplexity = 18.15, Entropy = 4.18 bits
Computation based on 1153 words.
Number of 3-grams hit = 1151  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
9 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article720.text
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 814 words.
Number of 3-grams hit = 812  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article721.text
Perplexity = 16.99, Entropy = 4.09 bits
Computation based on 423 words.
Number of 3-grams hit = 421  (99.53%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article722.text
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 1018 words.
Number of 3-grams hit = 1016  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article723.text
Perplexity = 14.00, Entropy = 3.81 bits
Computation based on 440 words.
Number of 3-grams hit = 438  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article724.text
Perplexity = 13.74, Entropy = 3.78 bits
Computation based on 460 words.
Number of 3-grams hit = 458  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article725.text
Perplexity = 16.50, Entropy = 4.04 bits
Computation based on 719 words.
Number of 3-grams hit = 717  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article726.text
Perplexity = 18.78, Entropy = 4.23 bits
Computation based on 521 words.
Number of 3-grams hit = 519  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article727.text
Perplexity = 13.98, Entropy = 3.81 bits
Computation based on 493 words.
Number of 3-grams hit = 491  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article728.text
Perplexity = 16.69, Entropy = 4.06 bits
Computation based on 421 words.
Number of 3-grams hit = 419  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article729.text
Perplexity = 14.10, Entropy = 3.82 bits
Computation based on 437 words.
Number of 3-grams hit = 435  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article730.text
Perplexity = 19.40, Entropy = 4.28 bits
Computation based on 8470 words.
Number of 3-grams hit = 8468  (99.98%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
7 OOVs (0.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article731.text
Perplexity = 18.54, Entropy = 4.21 bits
Computation based on 1455 words.
Number of 3-grams hit = 1453  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
1 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article732.text
Perplexity = 17.70, Entropy = 4.15 bits
Computation based on 698 words.
Number of 3-grams hit = 696  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
7 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article733.text
Perplexity = 18.49, Entropy = 4.21 bits
Computation based on 561 words.
Number of 3-grams hit = 559  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article734.text
Perplexity = 13.03, Entropy = 3.70 bits
Computation based on 417 words.
Number of 3-grams hit = 415  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
4 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article735.text
Perplexity = 17.63, Entropy = 4.14 bits
Computation based on 1178 words.
Number of 3-grams hit = 1176  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article736.text
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 666 words.
Number of 3-grams hit = 664  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article737.text
Perplexity = 17.49, Entropy = 4.13 bits
Computation based on 631 words.
Number of 3-grams hit = 629  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article738.text
Perplexity = 16.36, Entropy = 4.03 bits
Computation based on 1009 words.
Number of 3-grams hit = 1007  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article739.text
Perplexity = 18.15, Entropy = 4.18 bits
Computation based on 440 words.
Number of 3-grams hit = 438  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article740.text
Perplexity = 19.09, Entropy = 4.25 bits
Computation based on 1094 words.
Number of 3-grams hit = 1092  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article741.text
Perplexity = 15.74, Entropy = 3.98 bits
Computation based on 755 words.
Number of 3-grams hit = 753  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article742.text
Perplexity = 16.70, Entropy = 4.06 bits
Computation based on 627 words.
Number of 3-grams hit = 625  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article743.text
Perplexity = 16.22, Entropy = 4.02 bits
Computation based on 1974 words.
Number of 3-grams hit = 1972  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
9 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article744.text
Perplexity = 16.19, Entropy = 4.02 bits
Computation based on 525 words.
Number of 3-grams hit = 523  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article745.text
Perplexity = 16.54, Entropy = 4.05 bits
Computation based on 692 words.
Number of 3-grams hit = 690  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article746.text
Perplexity = 21.10, Entropy = 4.40 bits
Computation based on 1328 words.
Number of 3-grams hit = 1326  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
4 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article747.text
Perplexity = 16.89, Entropy = 4.08 bits
Computation based on 520 words.
Number of 3-grams hit = 518  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article748.text
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 507 words.
Number of 3-grams hit = 505  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article749.text
Perplexity = 14.26, Entropy = 3.83 bits
Computation based on 551 words.
Number of 3-grams hit = 549  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article750.text
Perplexity = 17.95, Entropy = 4.17 bits
Computation based on 467 words.
Number of 3-grams hit = 465  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article751.text
Perplexity = 15.72, Entropy = 3.97 bits
Computation based on 2010 words.
Number of 3-grams hit = 2008  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
6 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article752.text
Perplexity = 18.18, Entropy = 4.18 bits
Computation based on 429 words.
Number of 3-grams hit = 427  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article753.text
Perplexity = 15.44, Entropy = 3.95 bits
Computation based on 530 words.
Number of 3-grams hit = 528  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article754.text
Perplexity = 18.12, Entropy = 4.18 bits
Computation based on 1848 words.
Number of 3-grams hit = 1846  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
14 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article755.text
Perplexity = 16.62, Entropy = 4.05 bits
Computation based on 445 words.
Number of 3-grams hit = 443  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
6 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article756.text
Perplexity = 15.17, Entropy = 3.92 bits
Computation based on 703 words.
Number of 3-grams hit = 701  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
7 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article757.text
Perplexity = 16.95, Entropy = 4.08 bits
Computation based on 508 words.
Number of 3-grams hit = 506  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article758.text
Perplexity = 17.08, Entropy = 4.09 bits
Computation based on 2493 words.
Number of 3-grams hit = 2491  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
23 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article759.text
Perplexity = 17.83, Entropy = 4.16 bits
Computation based on 3319 words.
Number of 3-grams hit = 3317  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
3 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article760.text
Perplexity = 16.85, Entropy = 4.07 bits
Computation based on 553 words.
Number of 3-grams hit = 551  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article761.text
Perplexity = 17.39, Entropy = 4.12 bits
Computation based on 634 words.
Number of 3-grams hit = 632  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article762.text
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 782 words.
Number of 3-grams hit = 780  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
10 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article763.text
Perplexity = 17.91, Entropy = 4.16 bits
Computation based on 722 words.
Number of 3-grams hit = 720  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article764.text
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 539 words.
Number of 3-grams hit = 537  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article765.text
Perplexity = 13.41, Entropy = 3.75 bits
Computation based on 834 words.
Number of 3-grams hit = 832  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article766.text
Perplexity = 17.73, Entropy = 4.15 bits
Computation based on 267 words.
Number of 3-grams hit = 265  (99.25%)
Number of 2-grams hit = 1  (0.37%)
Number of 1-grams hit = 1  (0.37%)
1 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article767.text
Perplexity = 16.36, Entropy = 4.03 bits
Computation based on 4411 words.
Number of 3-grams hit = 4409  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
8 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article768.text
Perplexity = 17.01, Entropy = 4.09 bits
Computation based on 458 words.
Number of 3-grams hit = 456  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
4 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article769.text
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 680 words.
Number of 3-grams hit = 678  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article770.text
Perplexity = 16.79, Entropy = 4.07 bits
Computation based on 605 words.
Number of 3-grams hit = 603  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article771.text
Perplexity = 16.81, Entropy = 4.07 bits
Computation based on 463 words.
Number of 3-grams hit = 461  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
6 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article772.text
Perplexity = 15.56, Entropy = 3.96 bits
Computation based on 544 words.
Number of 3-grams hit = 542  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article773.text
Perplexity = 16.70, Entropy = 4.06 bits
Computation based on 2976 words.
Number of 3-grams hit = 2974  (99.93%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
22 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article774.text
Perplexity = 19.35, Entropy = 4.27 bits
Computation based on 2020 words.
Number of 3-grams hit = 2018  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
2 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article775.text
Perplexity = 19.19, Entropy = 4.26 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article776.text
Perplexity = 18.37, Entropy = 4.20 bits
Computation based on 1580 words.
Number of 3-grams hit = 1578  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
4 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article777.text
Perplexity = 18.75, Entropy = 4.23 bits
Computation based on 9364 words.
Number of 3-grams hit = 9362  (99.98%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
14 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article778.text
Perplexity = 17.62, Entropy = 4.14 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article779.text
Perplexity = 18.77, Entropy = 4.23 bits
Computation based on 933 words.
Number of 3-grams hit = 931  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article780.text
Perplexity = 19.07, Entropy = 4.25 bits
Computation based on 401 words.
Number of 3-grams hit = 399  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article781.text
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 735 words.
Number of 3-grams hit = 733  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article782.text
Perplexity = 17.74, Entropy = 4.15 bits
Computation based on 1356 words.
Number of 3-grams hit = 1354  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
7 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article783.text
Perplexity = 16.37, Entropy = 4.03 bits
Computation based on 502 words.
Number of 3-grams hit = 500  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article784.text
Perplexity = 18.22, Entropy = 4.19 bits
Computation based on 1724 words.
Number of 3-grams hit = 1722  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article785.text
Perplexity = 18.32, Entropy = 4.20 bits
Computation based on 2638 words.
Number of 3-grams hit = 2636  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
5 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article786.text
Perplexity = 18.29, Entropy = 4.19 bits
Computation based on 354 words.
Number of 3-grams hit = 352  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article787.text
Perplexity = 13.80, Entropy = 3.79 bits
Computation based on 477 words.
Number of 3-grams hit = 475  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article788.text
Perplexity = 20.83, Entropy = 4.38 bits
Computation based on 1451 words.
Number of 3-grams hit = 1449  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
5 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article789.text
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 11419 words.
Number of 3-grams hit = 11417  (99.98%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
73 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article790.text
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 932 words.
Number of 3-grams hit = 930  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
8 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article791.text
Perplexity = 16.37, Entropy = 4.03 bits
Computation based on 692 words.
Number of 3-grams hit = 690  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
6 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article792.text
Perplexity = 16.59, Entropy = 4.05 bits
Computation based on 2866 words.
Number of 3-grams hit = 2864  (99.93%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
18 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article793.text
Perplexity = 18.12, Entropy = 4.18 bits
Computation based on 180 words.
Number of 3-grams hit = 178  (98.89%)
Number of 2-grams hit = 1  (0.56%)
Number of 1-grams hit = 1  (0.56%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article794.text
Perplexity = 13.05, Entropy = 3.71 bits
Computation based on 504 words.
Number of 3-grams hit = 502  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article795.text
Perplexity = 14.90, Entropy = 3.90 bits
Computation based on 421 words.
Number of 3-grams hit = 419  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
7 OOVs (1.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article796.text
Perplexity = 16.90, Entropy = 4.08 bits
Computation based on 1490 words.
Number of 3-grams hit = 1488  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article797.text
Perplexity = 20.10, Entropy = 4.33 bits
Computation based on 374 words.
Number of 3-grams hit = 372  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article798.text
Perplexity = 16.57, Entropy = 4.05 bits
Computation based on 459 words.
Number of 3-grams hit = 457  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article799.text
Perplexity = 15.87, Entropy = 3.99 bits
Computation based on 751 words.
Number of 3-grams hit = 749  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article800.text
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 427 words.
Number of 3-grams hit = 425  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
5 OOVs (1.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article801.text
Perplexity = 20.16, Entropy = 4.33 bits
Computation based on 3112 words.
Number of 3-grams hit = 3110  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
3 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article802.text
Perplexity = 13.78, Entropy = 3.78 bits
Computation based on 2109 words.
Number of 3-grams hit = 2107  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
1 OOVs (0.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article803.text
Perplexity = 18.27, Entropy = 4.19 bits
Computation based on 4223 words.
Number of 3-grams hit = 4221  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
3 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article804.text
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 1462 words.
Number of 3-grams hit = 1460  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
6 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article805.text
Perplexity = 16.43, Entropy = 4.04 bits
Computation based on 919 words.
Number of 3-grams hit = 917  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article806.text
Perplexity = 17.20, Entropy = 4.10 bits
Computation based on 3283 words.
Number of 3-grams hit = 3281  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
20 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article807.text
Perplexity = 17.53, Entropy = 4.13 bits
Computation based on 709 words.
Number of 3-grams hit = 707  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article808.text
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 1680 words.
Number of 3-grams hit = 1678  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
14 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article809.text
Perplexity = 18.51, Entropy = 4.21 bits
Computation based on 2024 words.
Number of 3-grams hit = 2022  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
2 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article810.text
Perplexity = 16.82, Entropy = 4.07 bits
Computation based on 871 words.
Number of 3-grams hit = 869  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article811.text
Perplexity = 17.01, Entropy = 4.09 bits
Computation based on 4475 words.
Number of 3-grams hit = 4473  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
36 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article812.text
Perplexity = 19.16, Entropy = 4.26 bits
Computation based on 3368 words.
Number of 3-grams hit = 3366  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
1 OOVs (0.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article813.text
Perplexity = 16.25, Entropy = 4.02 bits
Computation based on 1377 words.
Number of 3-grams hit = 1375  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article814.text
Perplexity = 16.39, Entropy = 4.03 bits
Computation based on 850 words.
Number of 3-grams hit = 848  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article815.text
Perplexity = 16.12, Entropy = 4.01 bits
Computation based on 1263 words.
Number of 3-grams hit = 1261  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
11 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article816.text
Perplexity = 19.02, Entropy = 4.25 bits
Computation based on 1332 words.
Number of 3-grams hit = 1330  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
1 OOVs (0.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article817.text
Perplexity = 16.94, Entropy = 4.08 bits
Computation based on 528 words.
Number of 3-grams hit = 526  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
5 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article818.text
Perplexity = 17.71, Entropy = 4.15 bits
Computation based on 549 words.
Number of 3-grams hit = 547  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article819.text
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 1791 words.
Number of 3-grams hit = 1789  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
18 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article820.text
Perplexity = 14.82, Entropy = 3.89 bits
Computation based on 814 words.
Number of 3-grams hit = 812  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
6 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article821.text
Perplexity = 20.26, Entropy = 4.34 bits
Computation based on 1104 words.
Number of 3-grams hit = 1102  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article822.text
Perplexity = 16.48, Entropy = 4.04 bits
Computation based on 2402 words.
Number of 3-grams hit = 2400  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
22 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article823.text
Perplexity = 16.29, Entropy = 4.03 bits
Computation based on 1301 words.
Number of 3-grams hit = 1299  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article824.text
Perplexity = 15.39, Entropy = 3.94 bits
Computation based on 501 words.
Number of 3-grams hit = 499  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article825.text
Perplexity = 17.31, Entropy = 4.11 bits
Computation based on 567 words.
Number of 3-grams hit = 565  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
6 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article826.text
Perplexity = 19.30, Entropy = 4.27 bits
Computation based on 892 words.
Number of 3-grams hit = 890  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article827.text
Perplexity = 18.06, Entropy = 4.17 bits
Computation based on 642 words.
Number of 3-grams hit = 640  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
6 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article828.text
Perplexity = 19.81, Entropy = 4.31 bits
Computation based on 11962 words.
Number of 3-grams hit = 11960  (99.98%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
35 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article829.text
Perplexity = 13.21, Entropy = 3.72 bits
Computation based on 536 words.
Number of 3-grams hit = 534  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article830.text
Perplexity = 17.37, Entropy = 4.12 bits
Computation based on 2314 words.
Number of 3-grams hit = 2312  (99.91%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
5 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article831.text
Perplexity = 15.93, Entropy = 3.99 bits
Computation based on 1020 words.
Number of 3-grams hit = 1018  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
10 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article832.text
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 419 words.
Number of 3-grams hit = 417  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article833.text
Perplexity = 16.32, Entropy = 4.03 bits
Computation based on 825 words.
Number of 3-grams hit = 823  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article834.text
Perplexity = 17.73, Entropy = 4.15 bits
Computation based on 877 words.
Number of 3-grams hit = 875  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
5 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article835.text
Perplexity = 17.24, Entropy = 4.11 bits
Computation based on 458 words.
Number of 3-grams hit = 456  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article836.text
Perplexity = 16.01, Entropy = 4.00 bits
Computation based on 490 words.
Number of 3-grams hit = 488  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article837.text
Perplexity = 13.12, Entropy = 3.71 bits
Computation based on 848 words.
Number of 3-grams hit = 846  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article838.text
Perplexity = 16.60, Entropy = 4.05 bits
Computation based on 2259 words.
Number of 3-grams hit = 2257  (99.91%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
12 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article839.text
Perplexity = 17.15, Entropy = 4.10 bits
Computation based on 460 words.
Number of 3-grams hit = 458  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article840.text
Perplexity = 16.31, Entropy = 4.03 bits
Computation based on 519 words.
Number of 3-grams hit = 517  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article841.text
Perplexity = 17.33, Entropy = 4.12 bits
Computation based on 912 words.
Number of 3-grams hit = 910  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article842.text
Perplexity = 17.33, Entropy = 4.12 bits
Computation based on 475 words.
Number of 3-grams hit = 473  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article843.text
Perplexity = 18.03, Entropy = 4.17 bits
Computation based on 1305 words.
Number of 3-grams hit = 1303  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article844.text
Perplexity = 17.19, Entropy = 4.10 bits
Computation based on 15704 words.
Number of 3-grams hit = 15702  (99.99%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
107 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article845.text
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 602 words.
Number of 3-grams hit = 600  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article846.text
Perplexity = 17.87, Entropy = 4.16 bits
Computation based on 563 words.
Number of 3-grams hit = 561  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article847.text
Perplexity = 18.57, Entropy = 4.21 bits
Computation based on 1611 words.
Number of 3-grams hit = 1609  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
10 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article848.text
Perplexity = 16.63, Entropy = 4.06 bits
Computation based on 366 words.
Number of 3-grams hit = 364  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article849.text
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 1555 words.
Number of 3-grams hit = 1553  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article850.text
Perplexity = 17.96, Entropy = 4.17 bits
Computation based on 444 words.
Number of 3-grams hit = 442  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article851.text
Perplexity = 16.60, Entropy = 4.05 bits
Computation based on 552 words.
Number of 3-grams hit = 550  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
7 OOVs (1.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article852.text
Perplexity = 17.53, Entropy = 4.13 bits
Computation based on 624 words.
Number of 3-grams hit = 622  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article853.text
Perplexity = 16.87, Entropy = 4.08 bits
Computation based on 1479 words.
Number of 3-grams hit = 1477  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
7 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article854.text
Perplexity = 17.38, Entropy = 4.12 bits
Computation based on 2661 words.
Number of 3-grams hit = 2659  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
8 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article855.text
Perplexity = 21.91, Entropy = 4.45 bits
Computation based on 419 words.
Number of 3-grams hit = 417  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
5 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article856.text
Perplexity = 16.51, Entropy = 4.05 bits
Computation based on 825 words.
Number of 3-grams hit = 823  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
8 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article857.text
Perplexity = 13.51, Entropy = 3.76 bits
Computation based on 353 words.
Number of 3-grams hit = 351  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article858.text
Perplexity = 20.93, Entropy = 4.39 bits
Computation based on 237 words.
Number of 3-grams hit = 235  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article859.text
Perplexity = 14.40, Entropy = 3.85 bits
Computation based on 1106 words.
Number of 3-grams hit = 1104  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article860.text
Perplexity = 17.85, Entropy = 4.16 bits
Computation based on 451 words.
Number of 3-grams hit = 449  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article861.text
Perplexity = 14.90, Entropy = 3.90 bits
Computation based on 392 words.
Number of 3-grams hit = 390  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article862.text
Perplexity = 22.21, Entropy = 4.47 bits
Computation based on 451 words.
Number of 3-grams hit = 449  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article863.text
Perplexity = 16.97, Entropy = 4.08 bits
Computation based on 658 words.
Number of 3-grams hit = 656  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
4 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article864.text
Perplexity = 12.76, Entropy = 3.67 bits
Computation based on 447 words.
Number of 3-grams hit = 445  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article865.text
Perplexity = 21.05, Entropy = 4.40 bits
Computation based on 6161 words.
Number of 3-grams hit = 6159  (99.97%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
4 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article866.text
Perplexity = 15.58, Entropy = 3.96 bits
Computation based on 625 words.
Number of 3-grams hit = 623  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article867.text
Perplexity = 17.23, Entropy = 4.11 bits
Computation based on 341 words.
Number of 3-grams hit = 339  (99.41%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article868.text
Perplexity = 17.65, Entropy = 4.14 bits
Computation based on 1649 words.
Number of 3-grams hit = 1647  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
8 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article869.text
Perplexity = 17.17, Entropy = 4.10 bits
Computation based on 3099 words.
Number of 3-grams hit = 3097  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
17 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article870.text
Perplexity = 14.39, Entropy = 3.85 bits
Computation based on 768 words.
Number of 3-grams hit = 766  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article871.text
Perplexity = 17.08, Entropy = 4.09 bits
Computation based on 411 words.
Number of 3-grams hit = 409  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article872.text
Perplexity = 15.95, Entropy = 4.00 bits
Computation based on 295 words.
Number of 3-grams hit = 293  (99.32%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article873.text
Perplexity = 15.95, Entropy = 4.00 bits
Computation based on 930 words.
Number of 3-grams hit = 928  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article874.text
Perplexity = 16.30, Entropy = 4.03 bits
Computation based on 426 words.
Number of 3-grams hit = 424  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article875.text
Perplexity = 14.13, Entropy = 3.82 bits
Computation based on 786 words.
Number of 3-grams hit = 784  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article876.text
Perplexity = 17.09, Entropy = 4.10 bits
Computation based on 9154 words.
Number of 3-grams hit = 9152  (99.98%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
77 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article877.text
Perplexity = 18.12, Entropy = 4.18 bits
Computation based on 455 words.
Number of 3-grams hit = 453  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
4 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article878.text
Perplexity = 16.22, Entropy = 4.02 bits
Computation based on 336 words.
Number of 3-grams hit = 334  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
2 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article879.text
Perplexity = 17.92, Entropy = 4.16 bits
Computation based on 539 words.
Number of 3-grams hit = 537  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article880.text
Perplexity = 19.63, Entropy = 4.30 bits
Computation based on 460 words.
Number of 3-grams hit = 458  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article881.text
Perplexity = 14.86, Entropy = 3.89 bits
Computation based on 342 words.
Number of 3-grams hit = 340  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
4 OOVs (1.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article882.text
Perplexity = 18.38, Entropy = 4.20 bits
Computation based on 589 words.
Number of 3-grams hit = 587  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article883.text
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 590 words.
Number of 3-grams hit = 588  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article884.text
Perplexity = 15.13, Entropy = 3.92 bits
Computation based on 729 words.
Number of 3-grams hit = 727  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article885.text
Perplexity = 19.67, Entropy = 4.30 bits
Computation based on 485 words.
Number of 3-grams hit = 483  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article886.text
Perplexity = 17.83, Entropy = 4.16 bits
Computation based on 470 words.
Number of 3-grams hit = 468  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article887.text
Perplexity = 16.21, Entropy = 4.02 bits
Computation based on 480 words.
Number of 3-grams hit = 478  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article888.text
Perplexity = 15.54, Entropy = 3.96 bits
Computation based on 1202 words.
Number of 3-grams hit = 1200  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article889.text
Perplexity = 16.47, Entropy = 4.04 bits
Computation based on 4280 words.
Number of 3-grams hit = 4278  (99.95%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
33 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article890.text
Perplexity = 13.76, Entropy = 3.78 bits
Computation based on 1047 words.
Number of 3-grams hit = 1045  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
4 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article891.text
Perplexity = 17.67, Entropy = 4.14 bits
Computation based on 801 words.
Number of 3-grams hit = 799  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
7 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article892.text
Perplexity = 18.87, Entropy = 4.24 bits
Computation based on 3160 words.
Number of 3-grams hit = 3158  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
3 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article893.text
Perplexity = 17.75, Entropy = 4.15 bits
Computation based on 628 words.
Number of 3-grams hit = 626  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
5 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article894.text
Perplexity = 16.69, Entropy = 4.06 bits
Computation based on 739 words.
Number of 3-grams hit = 737  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
8 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article895.text
Perplexity = 17.94, Entropy = 4.17 bits
Computation based on 167 words.
Number of 3-grams hit = 165  (98.80%)
Number of 2-grams hit = 1  (0.60%)
Number of 1-grams hit = 1  (0.60%)
2 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article896.text
Perplexity = 18.93, Entropy = 4.24 bits
Computation based on 410 words.
Number of 3-grams hit = 408  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
5 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article897.text
Perplexity = 17.94, Entropy = 4.17 bits
Computation based on 698 words.
Number of 3-grams hit = 696  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article898.text
Perplexity = 18.35, Entropy = 4.20 bits
Computation based on 1591 words.
Number of 3-grams hit = 1589  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
11 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article899.text
Perplexity = 14.75, Entropy = 3.88 bits
Computation based on 202 words.
Number of 3-grams hit = 200  (99.01%)
Number of 2-grams hit = 1  (0.50%)
Number of 1-grams hit = 1  (0.50%)
2 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article900.text
Perplexity = 15.44, Entropy = 3.95 bits
Computation based on 1046 words.
Number of 3-grams hit = 1044  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
5 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article901.text
Perplexity = 15.42, Entropy = 3.95 bits
Computation based on 399 words.
Number of 3-grams hit = 397  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article902.text
Perplexity = 17.99, Entropy = 4.17 bits
Computation based on 2002 words.
Number of 3-grams hit = 2000  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
3 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article903.text
Perplexity = 16.75, Entropy = 4.07 bits
Computation based on 1186 words.
Number of 3-grams hit = 1184  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article904.text
Perplexity = 17.18, Entropy = 4.10 bits
Computation based on 622 words.
Number of 3-grams hit = 620  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article905.text
Perplexity = 17.55, Entropy = 4.13 bits
Computation based on 348 words.
Number of 3-grams hit = 346  (99.43%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
9 OOVs (2.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article906.text
Perplexity = 20.54, Entropy = 4.36 bits
Computation based on 436 words.
Number of 3-grams hit = 434  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article907.text
Perplexity = 17.66, Entropy = 4.14 bits
Computation based on 445 words.
Number of 3-grams hit = 443  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article908.text
Perplexity = 14.29, Entropy = 3.84 bits
Computation based on 525 words.
Number of 3-grams hit = 523  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article909.text
Perplexity = 16.22, Entropy = 4.02 bits
Computation based on 412 words.
Number of 3-grams hit = 410  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
5 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article910.text
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 668 words.
Number of 3-grams hit = 666  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article911.text
Perplexity = 17.93, Entropy = 4.16 bits
Computation based on 634 words.
Number of 3-grams hit = 632  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article912.text
Perplexity = 19.24, Entropy = 4.27 bits
Computation based on 672 words.
Number of 3-grams hit = 670  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article913.text
Perplexity = 17.46, Entropy = 4.13 bits
Computation based on 666 words.
Number of 3-grams hit = 664  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
4 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article914.text
Perplexity = 14.56, Entropy = 3.86 bits
Computation based on 530 words.
Number of 3-grams hit = 528  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article915.text
Perplexity = 17.18, Entropy = 4.10 bits
Computation based on 1203 words.
Number of 3-grams hit = 1201  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
8 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article916.text
Perplexity = 14.51, Entropy = 3.86 bits
Computation based on 940 words.
Number of 3-grams hit = 938  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article917.text
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 1081 words.
Number of 3-grams hit = 1079  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article918.text
Perplexity = 16.96, Entropy = 4.08 bits
Computation based on 483 words.
Number of 3-grams hit = 481  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article919.text
Perplexity = 17.39, Entropy = 4.12 bits
Computation based on 3288 words.
Number of 3-grams hit = 3286  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
27 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article920.text
Perplexity = 17.09, Entropy = 4.10 bits
Computation based on 915 words.
Number of 3-grams hit = 913  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article921.text
Perplexity = 15.68, Entropy = 3.97 bits
Computation based on 795 words.
Number of 3-grams hit = 793  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article922.text
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 469 words.
Number of 3-grams hit = 467  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article923.text
Perplexity = 17.86, Entropy = 4.16 bits
Computation based on 517 words.
Number of 3-grams hit = 515  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article924.text
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 341 words.
Number of 3-grams hit = 339  (99.41%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article925.text
Perplexity = 18.91, Entropy = 4.24 bits
Computation based on 3826 words.
Number of 3-grams hit = 3824  (99.95%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
8 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article926.text
Perplexity = 17.97, Entropy = 4.17 bits
Computation based on 1334 words.
Number of 3-grams hit = 1332  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article927.text
Perplexity = 17.77, Entropy = 4.15 bits
Computation based on 404 words.
Number of 3-grams hit = 402  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
3 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article928.text
Perplexity = 15.85, Entropy = 3.99 bits
Computation based on 487 words.
Number of 3-grams hit = 485  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article929.text
Perplexity = 15.02, Entropy = 3.91 bits
Computation based on 499 words.
Number of 3-grams hit = 497  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
5 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article930.text
Perplexity = 15.45, Entropy = 3.95 bits
Computation based on 434 words.
Number of 3-grams hit = 432  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article931.text
Perplexity = 15.90, Entropy = 3.99 bits
Computation based on 596 words.
Number of 3-grams hit = 594  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article932.text
Perplexity = 14.19, Entropy = 3.83 bits
Computation based on 209 words.
Number of 3-grams hit = 207  (99.04%)
Number of 2-grams hit = 1  (0.48%)
Number of 1-grams hit = 1  (0.48%)
3 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article933.text
Perplexity = 15.25, Entropy = 3.93 bits
Computation based on 411 words.
Number of 3-grams hit = 409  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article934.text
Perplexity = 15.28, Entropy = 3.93 bits
Computation based on 389 words.
Number of 3-grams hit = 387  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article935.text
Perplexity = 17.57, Entropy = 4.13 bits
Computation based on 1145 words.
Number of 3-grams hit = 1143  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
11 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article936.text
Perplexity = 15.47, Entropy = 3.95 bits
Computation based on 439 words.
Number of 3-grams hit = 437  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article937.text
Perplexity = 14.57, Entropy = 3.86 bits
Computation based on 226 words.
Number of 3-grams hit = 224  (99.12%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
2 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article938.text
Perplexity = 16.27, Entropy = 4.02 bits
Computation based on 1292 words.
Number of 3-grams hit = 1290  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
8 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article939.text
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 481 words.
Number of 3-grams hit = 479  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article940.text
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 1885 words.
Number of 3-grams hit = 1883  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
16 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article941.text
Perplexity = 15.61, Entropy = 3.96 bits
Computation based on 1216 words.
Number of 3-grams hit = 1214  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article942.text
Perplexity = 16.61, Entropy = 4.05 bits
Computation based on 526 words.
Number of 3-grams hit = 524  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article943.text
Perplexity = 18.49, Entropy = 4.21 bits
Computation based on 1399 words.
Number of 3-grams hit = 1397  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
6 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article944.text
Perplexity = 17.29, Entropy = 4.11 bits
Computation based on 435 words.
Number of 3-grams hit = 433  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article945.text
Perplexity = 16.97, Entropy = 4.09 bits
Computation based on 3539 words.
Number of 3-grams hit = 3537  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
5 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article946.text
Perplexity = 17.27, Entropy = 4.11 bits
Computation based on 560 words.
Number of 3-grams hit = 558  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
6 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article947.text
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 428 words.
Number of 3-grams hit = 426  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article948.text
Perplexity = 17.88, Entropy = 4.16 bits
Computation based on 577 words.
Number of 3-grams hit = 575  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article949.text
Perplexity = 16.65, Entropy = 4.06 bits
Computation based on 1691 words.
Number of 3-grams hit = 1689  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
19 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article950.text
Perplexity = 13.89, Entropy = 3.80 bits
Computation based on 501 words.
Number of 3-grams hit = 499  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
6 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article951.text
Perplexity = 17.76, Entropy = 4.15 bits
Computation based on 566 words.
Number of 3-grams hit = 564  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article952.text
Perplexity = 15.85, Entropy = 3.99 bits
Computation based on 1278 words.
Number of 3-grams hit = 1276  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article953.text
Perplexity = 17.81, Entropy = 4.15 bits
Computation based on 2131 words.
Number of 3-grams hit = 2129  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
4 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article954.text
Perplexity = 19.96, Entropy = 4.32 bits
Computation based on 732 words.
Number of 3-grams hit = 730  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article955.text
Perplexity = 19.25, Entropy = 4.27 bits
Computation based on 1475 words.
Number of 3-grams hit = 1473  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
2 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article956.text
Perplexity = 12.72, Entropy = 3.67 bits
Computation based on 1031 words.
Number of 3-grams hit = 1029  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
3 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article957.text
Perplexity = 13.71, Entropy = 3.78 bits
Computation based on 304 words.
Number of 3-grams hit = 302  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article958.text
Perplexity = 17.49, Entropy = 4.13 bits
Computation based on 935 words.
Number of 3-grams hit = 933  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
8 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article959.text
Perplexity = 17.73, Entropy = 4.15 bits
Computation based on 198 words.
Number of 3-grams hit = 196  (98.99%)
Number of 2-grams hit = 1  (0.51%)
Number of 1-grams hit = 1  (0.51%)
3 OOVs (1.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article960.text
Perplexity = 17.71, Entropy = 4.15 bits
Computation based on 606 words.
Number of 3-grams hit = 604  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article961.text
Perplexity = 16.33, Entropy = 4.03 bits
Computation based on 492 words.
Number of 3-grams hit = 490  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article962.text
Perplexity = 19.43, Entropy = 4.28 bits
Computation based on 2235 words.
Number of 3-grams hit = 2233  (99.91%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
1 OOVs (0.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article963.text
Perplexity = 17.75, Entropy = 4.15 bits
Computation based on 2007 words.
Number of 3-grams hit = 2005  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article964.text
Perplexity = 18.15, Entropy = 4.18 bits
Computation based on 1006 words.
Number of 3-grams hit = 1004  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article965.text
Perplexity = 16.10, Entropy = 4.01 bits
Computation based on 646 words.
Number of 3-grams hit = 644  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article966.text
Perplexity = 16.53, Entropy = 4.05 bits
Computation based on 782 words.
Number of 3-grams hit = 780  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article967.text
Perplexity = 19.29, Entropy = 4.27 bits
Computation based on 391 words.
Number of 3-grams hit = 389  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article968.text
Perplexity = 18.60, Entropy = 4.22 bits
Computation based on 1058 words.
Number of 3-grams hit = 1056  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article969.text
Perplexity = 13.79, Entropy = 3.79 bits
Computation based on 725 words.
Number of 3-grams hit = 723  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article970.text
Perplexity = 17.00, Entropy = 4.09 bits
Computation based on 740 words.
Number of 3-grams hit = 738  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article971.text
Perplexity = 18.06, Entropy = 4.17 bits
Computation based on 466 words.
Number of 3-grams hit = 464  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article972.text
Perplexity = 17.23, Entropy = 4.11 bits
Computation based on 1851 words.
Number of 3-grams hit = 1849  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
11 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article973.text
Perplexity = 19.44, Entropy = 4.28 bits
Computation based on 609 words.
Number of 3-grams hit = 607  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article974.text
Perplexity = 16.29, Entropy = 4.03 bits
Computation based on 944 words.
Number of 3-grams hit = 942  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article975.text
Perplexity = 16.94, Entropy = 4.08 bits
Computation based on 352 words.
Number of 3-grams hit = 350  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article976.text
Perplexity = 13.98, Entropy = 3.81 bits
Computation based on 360 words.
Number of 3-grams hit = 358  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article977.text
Perplexity = 18.36, Entropy = 4.20 bits
Computation based on 861 words.
Number of 3-grams hit = 859  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article978.text
Perplexity = 17.58, Entropy = 4.14 bits
Computation based on 472 words.
Number of 3-grams hit = 470  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article979.text
Perplexity = 18.31, Entropy = 4.19 bits
Computation based on 1552 words.
Number of 3-grams hit = 1550  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article980.text
Perplexity = 15.88, Entropy = 3.99 bits
Computation based on 609 words.
Number of 3-grams hit = 607  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article981.text
Perplexity = 14.24, Entropy = 3.83 bits
Computation based on 346 words.
Number of 3-grams hit = 344  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article982.text
Perplexity = 14.89, Entropy = 3.90 bits
Computation based on 836 words.
Number of 3-grams hit = 834  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article983.text
Perplexity = 17.05, Entropy = 4.09 bits
Computation based on 466 words.
Number of 3-grams hit = 464  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article984.text
Perplexity = 16.68, Entropy = 4.06 bits
Computation based on 457 words.
Number of 3-grams hit = 455  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article985.text
Perplexity = 16.20, Entropy = 4.02 bits
Computation based on 1243 words.
Number of 3-grams hit = 1241  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article986.text
Perplexity = 16.05, Entropy = 4.00 bits
Computation based on 268 words.
Number of 3-grams hit = 266  (99.25%)
Number of 2-grams hit = 1  (0.37%)
Number of 1-grams hit = 1  (0.37%)
3 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article987.text
Perplexity = 18.18, Entropy = 4.18 bits
Computation based on 1743 words.
Number of 3-grams hit = 1741  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
1 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article988.text
Perplexity = 18.86, Entropy = 4.24 bits
Computation based on 1240 words.
Number of 3-grams hit = 1238  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article989.text
Perplexity = 18.54, Entropy = 4.21 bits
Computation based on 2276 words.
Number of 3-grams hit = 2274  (99.91%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
3 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article990.text
Perplexity = 16.06, Entropy = 4.01 bits
Computation based on 659 words.
Number of 3-grams hit = 657  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article991.text
Perplexity = 16.70, Entropy = 4.06 bits
Computation based on 1536 words.
Number of 3-grams hit = 1534  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
7 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article992.text
Perplexity = 16.82, Entropy = 4.07 bits
Computation based on 532 words.
Number of 3-grams hit = 530  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article993.text
Perplexity = 16.77, Entropy = 4.07 bits
Computation based on 2446 words.
Number of 3-grams hit = 2444  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
20 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article994.text
Perplexity = 17.94, Entropy = 4.17 bits
Computation based on 473 words.
Number of 3-grams hit = 471  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
6 OOVs (1.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article995.text
Perplexity = 18.05, Entropy = 4.17 bits
Computation based on 7534 words.
Number of 3-grams hit = 7532  (99.97%)
Number of 2-grams hit = 1  (0.01%)
Number of 1-grams hit = 1  (0.01%)
23 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article996.text
Perplexity = 15.13, Entropy = 3.92 bits
Computation based on 574 words.
Number of 3-grams hit = 572  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
6 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article997.text
Perplexity = 16.36, Entropy = 4.03 bits
Computation based on 5292 words.
Number of 3-grams hit = 5290  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
39 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article998.text
Perplexity = 17.98, Entropy = 4.17 bits
Computation based on 518 words.
Number of 3-grams hit = 516  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article999.text
Perplexity = 19.37, Entropy = 4.28 bits
Computation based on 471 words.
Number of 3-grams hit = 469  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : 