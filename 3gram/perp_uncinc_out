evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article0.text
Will force inclusive back-off from OOVs.
Perplexity = 18.17, Entropy = 4.18 bits
Computation based on 1250 words.
Number of 3-grams hit = 1238  (99.04%)
Number of 2-grams hit = 11  (0.88%)
Number of 1-grams hit = 1  (0.08%)
12 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article1.text
Will force inclusive back-off from OOVs.
Perplexity = 19.36, Entropy = 4.27 bits
Computation based on 1503 words.
Number of 3-grams hit = 1499  (99.73%)
Number of 2-grams hit = 3  (0.20%)
Number of 1-grams hit = 1  (0.07%)
2 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article2.text
Will force inclusive back-off from OOVs.
Perplexity = 17.94, Entropy = 4.16 bits
Computation based on 534 words.
Number of 3-grams hit = 526  (98.50%)
Number of 2-grams hit = 7  (1.31%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article3.text
Will force inclusive back-off from OOVs.
Perplexity = 16.63, Entropy = 4.06 bits
Computation based on 622 words.
Number of 3-grams hit = 617  (99.20%)
Number of 2-grams hit = 4  (0.64%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article4.text
Will force inclusive back-off from OOVs.
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 394 words.
Number of 3-grams hit = 388  (98.48%)
Number of 2-grams hit = 5  (1.27%)
Number of 1-grams hit = 1  (0.25%)
4 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article5.text
Will force inclusive back-off from OOVs.
Perplexity = 16.30, Entropy = 4.03 bits
Computation based on 882 words.
Number of 3-grams hit = 873  (98.98%)
Number of 2-grams hit = 8  (0.91%)
Number of 1-grams hit = 1  (0.11%)
8 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article6.text
Will force inclusive back-off from OOVs.
Perplexity = 14.72, Entropy = 3.88 bits
Computation based on 285 words.
Number of 3-grams hit = 281  (98.60%)
Number of 2-grams hit = 3  (1.05%)
Number of 1-grams hit = 1  (0.35%)
2 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article7.text
Will force inclusive back-off from OOVs.
Perplexity = 16.61, Entropy = 4.05 bits
Computation based on 609 words.
Number of 3-grams hit = 605  (99.34%)
Number of 2-grams hit = 3  (0.49%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article8.text
Will force inclusive back-off from OOVs.
Perplexity = 13.34, Entropy = 3.74 bits
Computation based on 503 words.
Number of 3-grams hit = 500  (99.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article9.text
Will force inclusive back-off from OOVs.
Perplexity = 17.65, Entropy = 4.14 bits
Computation based on 312 words.
Number of 3-grams hit = 308  (98.72%)
Number of 2-grams hit = 3  (0.96%)
Number of 1-grams hit = 1  (0.32%)
2 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article10.text
Will force inclusive back-off from OOVs.
Perplexity = 15.89, Entropy = 3.99 bits
Computation based on 302 words.
Number of 3-grams hit = 298  (98.68%)
Number of 2-grams hit = 3  (0.99%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article11.text
Will force inclusive back-off from OOVs.
Perplexity = 15.17, Entropy = 3.92 bits
Computation based on 303 words.
Number of 3-grams hit = 300  (99.01%)
Number of 2-grams hit = 2  (0.66%)
Number of 1-grams hit = 1  (0.33%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article12.text
Will force inclusive back-off from OOVs.
Perplexity = 17.76, Entropy = 4.15 bits
Computation based on 298 words.
Number of 3-grams hit = 294  (98.66%)
Number of 2-grams hit = 3  (1.01%)
Number of 1-grams hit = 1  (0.34%)
2 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article13.text
Will force inclusive back-off from OOVs.
Perplexity = 14.52, Entropy = 3.86 bits
Computation based on 462 words.
Number of 3-grams hit = 458  (99.13%)
Number of 2-grams hit = 3  (0.65%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article14.text
Will force inclusive back-off from OOVs.
Perplexity = 15.96, Entropy = 4.00 bits
Computation based on 474 words.
Number of 3-grams hit = 471  (99.37%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article15.text
Will force inclusive back-off from OOVs.
Perplexity = 16.87, Entropy = 4.08 bits
Computation based on 495 words.
Number of 3-grams hit = 491  (99.19%)
Number of 2-grams hit = 3  (0.61%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article16.text
Will force inclusive back-off from OOVs.
Perplexity = 14.74, Entropy = 3.88 bits
Computation based on 364 words.
Number of 3-grams hit = 359  (98.63%)
Number of 2-grams hit = 4  (1.10%)
Number of 1-grams hit = 1  (0.27%)
3 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article17.text
Will force inclusive back-off from OOVs.
Perplexity = 15.91, Entropy = 3.99 bits
Computation based on 664 words.
Number of 3-grams hit = 653  (98.34%)
Number of 2-grams hit = 10  (1.51%)
Number of 1-grams hit = 1  (0.15%)
9 OOVs (1.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article18.text
Will force inclusive back-off from OOVs.
Perplexity = 18.83, Entropy = 4.23 bits
Computation based on 410 words.
Number of 3-grams hit = 406  (99.02%)
Number of 2-grams hit = 3  (0.73%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article19.text
Will force inclusive back-off from OOVs.
Perplexity = 14.97, Entropy = 3.90 bits
Computation based on 526 words.
Number of 3-grams hit = 523  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article20.text
Will force inclusive back-off from OOVs.
Perplexity = 16.06, Entropy = 4.01 bits
Computation based on 348 words.
Number of 3-grams hit = 342  (98.28%)
Number of 2-grams hit = 5  (1.44%)
Number of 1-grams hit = 1  (0.29%)
4 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article21.text
Will force inclusive back-off from OOVs.
Perplexity = 15.60, Entropy = 3.96 bits
Computation based on 462 words.
Number of 3-grams hit = 457  (98.92%)
Number of 2-grams hit = 4  (0.87%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article22.text
Will force inclusive back-off from OOVs.
Perplexity = 15.68, Entropy = 3.97 bits
Computation based on 361 words.
Number of 3-grams hit = 359  (99.45%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article23.text
Will force inclusive back-off from OOVs.
Perplexity = 16.31, Entropy = 4.03 bits
Computation based on 353 words.
Number of 3-grams hit = 347  (98.30%)
Number of 2-grams hit = 5  (1.42%)
Number of 1-grams hit = 1  (0.28%)
4 OOVs (1.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article24.text
Will force inclusive back-off from OOVs.
Perplexity = 15.32, Entropy = 3.94 bits
Computation based on 563 words.
Number of 3-grams hit = 558  (99.11%)
Number of 2-grams hit = 4  (0.71%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article25.text
Will force inclusive back-off from OOVs.
Perplexity = 15.26, Entropy = 3.93 bits
Computation based on 380 words.
Number of 3-grams hit = 378  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article26.text
Will force inclusive back-off from OOVs.
Perplexity = 15.32, Entropy = 3.94 bits
Computation based on 1529 words.
Number of 3-grams hit = 1524  (99.67%)
Number of 2-grams hit = 4  (0.26%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article27.text
Will force inclusive back-off from OOVs.
Perplexity = 16.60, Entropy = 4.05 bits
Computation based on 392 words.
Number of 3-grams hit = 389  (99.23%)
Number of 2-grams hit = 2  (0.51%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article28.text
Will force inclusive back-off from OOVs.
Perplexity = 16.84, Entropy = 4.07 bits
Computation based on 1390 words.
Number of 3-grams hit = 1379  (99.21%)
Number of 2-grams hit = 10  (0.72%)
Number of 1-grams hit = 1  (0.07%)
9 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article29.text
Will force inclusive back-off from OOVs.
Perplexity = 17.34, Entropy = 4.12 bits
Computation based on 316 words.
Number of 3-grams hit = 311  (98.42%)
Number of 2-grams hit = 4  (1.27%)
Number of 1-grams hit = 1  (0.32%)
3 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article30.text
Will force inclusive back-off from OOVs.
Perplexity = 16.42, Entropy = 4.04 bits
Computation based on 474 words.
Number of 3-grams hit = 468  (98.73%)
Number of 2-grams hit = 5  (1.05%)
Number of 1-grams hit = 1  (0.21%)
4 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article31.text
Will force inclusive back-off from OOVs.
Perplexity = 17.00, Entropy = 4.09 bits
Computation based on 436 words.
Number of 3-grams hit = 429  (98.39%)
Number of 2-grams hit = 6  (1.38%)
Number of 1-grams hit = 1  (0.23%)
5 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article32.text
Will force inclusive back-off from OOVs.
Perplexity = 15.72, Entropy = 3.97 bits
Computation based on 473 words.
Number of 3-grams hit = 467  (98.73%)
Number of 2-grams hit = 5  (1.06%)
Number of 1-grams hit = 1  (0.21%)
4 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article33.text
Will force inclusive back-off from OOVs.
Perplexity = 16.75, Entropy = 4.07 bits
Computation based on 443 words.
Number of 3-grams hit = 440  (99.32%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article34.text
Will force inclusive back-off from OOVs.
Perplexity = 18.91, Entropy = 4.24 bits
Computation based on 301 words.
Number of 3-grams hit = 297  (98.67%)
Number of 2-grams hit = 3  (1.00%)
Number of 1-grams hit = 1  (0.33%)
4 OOVs (1.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article35.text
Will force inclusive back-off from OOVs.
Perplexity = 14.68, Entropy = 3.88 bits
Computation based on 378 words.
Number of 3-grams hit = 375  (99.21%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article36.text
Will force inclusive back-off from OOVs.
Perplexity = 18.40, Entropy = 4.20 bits
Computation based on 1009 words.
Number of 3-grams hit = 1005  (99.60%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article37.text
Will force inclusive back-off from OOVs.
Perplexity = 14.80, Entropy = 3.89 bits
Computation based on 567 words.
Number of 3-grams hit = 563  (99.29%)
Number of 2-grams hit = 3  (0.53%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article38.text
Will force inclusive back-off from OOVs.
Perplexity = 14.74, Entropy = 3.88 bits
Computation based on 411 words.
Number of 3-grams hit = 408  (99.27%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article39.text
Will force inclusive back-off from OOVs.
Perplexity = 19.18, Entropy = 4.26 bits
Computation based on 2654 words.
Number of 3-grams hit = 2646  (99.70%)
Number of 2-grams hit = 7  (0.26%)
Number of 1-grams hit = 1  (0.04%)
6 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article40.text
Will force inclusive back-off from OOVs.
Perplexity = 17.44, Entropy = 4.12 bits
Computation based on 403 words.
Number of 3-grams hit = 398  (98.76%)
Number of 2-grams hit = 4  (0.99%)
Number of 1-grams hit = 1  (0.25%)
3 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article41.text
Will force inclusive back-off from OOVs.
Perplexity = 16.63, Entropy = 4.06 bits
Computation based on 970 words.
Number of 3-grams hit = 967  (99.69%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.10%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article42.text
Will force inclusive back-off from OOVs.
Perplexity = 18.25, Entropy = 4.19 bits
Computation based on 547 words.
Number of 3-grams hit = 542  (99.09%)
Number of 2-grams hit = 4  (0.73%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article43.text
Will force inclusive back-off from OOVs.
Perplexity = 17.36, Entropy = 4.12 bits
Computation based on 450 words.
Number of 3-grams hit = 447  (99.33%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article44.text
Will force inclusive back-off from OOVs.
Perplexity = 18.60, Entropy = 4.22 bits
Computation based on 493 words.
Number of 3-grams hit = 490  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article45.text
Will force inclusive back-off from OOVs.
Perplexity = 15.97, Entropy = 4.00 bits
Computation based on 462 words.
Number of 3-grams hit = 457  (98.92%)
Number of 2-grams hit = 4  (0.87%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article46.text
Will force inclusive back-off from OOVs.
Perplexity = 13.27, Entropy = 3.73 bits
Computation based on 402 words.
Number of 3-grams hit = 399  (99.25%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article47.text
Will force inclusive back-off from OOVs.
Perplexity = 17.40, Entropy = 4.12 bits
Computation based on 2672 words.
Number of 3-grams hit = 2658  (99.48%)
Number of 2-grams hit = 13  (0.49%)
Number of 1-grams hit = 1  (0.04%)
12 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article48.text
Will force inclusive back-off from OOVs.
Perplexity = 15.88, Entropy = 3.99 bits
Computation based on 369 words.
Number of 3-grams hit = 363  (98.37%)
Number of 2-grams hit = 5  (1.36%)
Number of 1-grams hit = 1  (0.27%)
4 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article49.text
Will force inclusive back-off from OOVs.
Perplexity = 18.42, Entropy = 4.20 bits
Computation based on 363 words.
Number of 3-grams hit = 360  (99.17%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article50.text
Will force inclusive back-off from OOVs.
Perplexity = 18.29, Entropy = 4.19 bits
Computation based on 524 words.
Number of 3-grams hit = 522  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article51.text
Will force inclusive back-off from OOVs.
Perplexity = 15.47, Entropy = 3.95 bits
Computation based on 1120 words.
Number of 3-grams hit = 1117  (99.73%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article52.text
Will force inclusive back-off from OOVs.
Perplexity = 18.91, Entropy = 4.24 bits
Computation based on 360 words.
Number of 3-grams hit = 356  (98.89%)
Number of 2-grams hit = 3  (0.83%)
Number of 1-grams hit = 1  (0.28%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article53.text
Will force inclusive back-off from OOVs.
Perplexity = 15.70, Entropy = 3.97 bits
Computation based on 477 words.
Number of 3-grams hit = 471  (98.74%)
Number of 2-grams hit = 5  (1.05%)
Number of 1-grams hit = 1  (0.21%)
4 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article54.text
Will force inclusive back-off from OOVs.
Perplexity = 17.28, Entropy = 4.11 bits
Computation based on 1075 words.
Number of 3-grams hit = 1064  (98.98%)
Number of 2-grams hit = 10  (0.93%)
Number of 1-grams hit = 1  (0.09%)
9 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article55.text
Will force inclusive back-off from OOVs.
Perplexity = 19.74, Entropy = 4.30 bits
Computation based on 531 words.
Number of 3-grams hit = 527  (99.25%)
Number of 2-grams hit = 3  (0.56%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article56.text
Will force inclusive back-off from OOVs.
Perplexity = 18.35, Entropy = 4.20 bits
Computation based on 903 words.
Number of 3-grams hit = 899  (99.56%)
Number of 2-grams hit = 3  (0.33%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article57.text
Will force inclusive back-off from OOVs.
Perplexity = 18.99, Entropy = 4.25 bits
Computation based on 1539 words.
Number of 3-grams hit = 1533  (99.61%)
Number of 2-grams hit = 5  (0.32%)
Number of 1-grams hit = 1  (0.06%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article58.text
Will force inclusive back-off from OOVs.
Perplexity = 16.58, Entropy = 4.05 bits
Computation based on 381 words.
Number of 3-grams hit = 377  (98.95%)
Number of 2-grams hit = 3  (0.79%)
Number of 1-grams hit = 1  (0.26%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article59.text
Will force inclusive back-off from OOVs.
Perplexity = 19.36, Entropy = 4.27 bits
Computation based on 205 words.
Number of 3-grams hit = 201  (98.05%)
Number of 2-grams hit = 3  (1.46%)
Number of 1-grams hit = 1  (0.49%)
3 OOVs (1.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article60.text
Will force inclusive back-off from OOVs.
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 333 words.
Number of 3-grams hit = 330  (99.10%)
Number of 2-grams hit = 2  (0.60%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article61.text
Will force inclusive back-off from OOVs.
Perplexity = 16.66, Entropy = 4.06 bits
Computation based on 485 words.
Number of 3-grams hit = 477  (98.35%)
Number of 2-grams hit = 7  (1.44%)
Number of 1-grams hit = 1  (0.21%)
6 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article62.text
Will force inclusive back-off from OOVs.
Perplexity = 14.32, Entropy = 3.84 bits
Computation based on 1290 words.
Number of 3-grams hit = 1285  (99.61%)
Number of 2-grams hit = 4  (0.31%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article63.text
Will force inclusive back-off from OOVs.
Perplexity = 16.79, Entropy = 4.07 bits
Computation based on 990 words.
Number of 3-grams hit = 988  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article64.text
Will force inclusive back-off from OOVs.
Perplexity = 16.44, Entropy = 4.04 bits
Computation based on 4209 words.
Number of 3-grams hit = 4200  (99.79%)
Number of 2-grams hit = 8  (0.19%)
Number of 1-grams hit = 1  (0.02%)
7 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article65.text
Will force inclusive back-off from OOVs.
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 626 words.
Number of 3-grams hit = 621  (99.20%)
Number of 2-grams hit = 4  (0.64%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article66.text
Will force inclusive back-off from OOVs.
Perplexity = 18.35, Entropy = 4.20 bits
Computation based on 477 words.
Number of 3-grams hit = 475  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article67.text
Will force inclusive back-off from OOVs.
Perplexity = 13.99, Entropy = 3.81 bits
Computation based on 451 words.
Number of 3-grams hit = 446  (98.89%)
Number of 2-grams hit = 4  (0.89%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article68.text
Will force inclusive back-off from OOVs.
Perplexity = 18.09, Entropy = 4.18 bits
Computation based on 1116 words.
Number of 3-grams hit = 1110  (99.46%)
Number of 2-grams hit = 5  (0.45%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article69.text
Will force inclusive back-off from OOVs.
Perplexity = 12.64, Entropy = 3.66 bits
Computation based on 380 words.
Number of 3-grams hit = 375  (98.68%)
Number of 2-grams hit = 4  (1.05%)
Number of 1-grams hit = 1  (0.26%)
3 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article70.text
Will force inclusive back-off from OOVs.
Perplexity = 18.59, Entropy = 4.22 bits
Computation based on 959 words.
Number of 3-grams hit = 956  (99.69%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.10%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article71.text
Will force inclusive back-off from OOVs.
Perplexity = 18.43, Entropy = 4.20 bits
Computation based on 548 words.
Number of 3-grams hit = 544  (99.27%)
Number of 2-grams hit = 3  (0.55%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article72.text
Will force inclusive back-off from OOVs.
Perplexity = 17.88, Entropy = 4.16 bits
Computation based on 1298 words.
Number of 3-grams hit = 1287  (99.15%)
Number of 2-grams hit = 10  (0.77%)
Number of 1-grams hit = 1  (0.08%)
10 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article73.text
Will force inclusive back-off from OOVs.
Perplexity = 18.25, Entropy = 4.19 bits
Computation based on 513 words.
Number of 3-grams hit = 511  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article74.text
Will force inclusive back-off from OOVs.
Perplexity = 16.56, Entropy = 4.05 bits
Computation based on 817 words.
Number of 3-grams hit = 809  (99.02%)
Number of 2-grams hit = 7  (0.86%)
Number of 1-grams hit = 1  (0.12%)
6 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article75.text
Will force inclusive back-off from OOVs.
Perplexity = 15.66, Entropy = 3.97 bits
Computation based on 1194 words.
Number of 3-grams hit = 1192  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article76.text
Will force inclusive back-off from OOVs.
Perplexity = 18.94, Entropy = 4.24 bits
Computation based on 1376 words.
Number of 3-grams hit = 1368  (99.42%)
Number of 2-grams hit = 7  (0.51%)
Number of 1-grams hit = 1  (0.07%)
6 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article77.text
Will force inclusive back-off from OOVs.
Perplexity = 18.55, Entropy = 4.21 bits
Computation based on 282 words.
Number of 3-grams hit = 274  (97.16%)
Number of 2-grams hit = 7  (2.48%)
Number of 1-grams hit = 1  (0.35%)
6 OOVs (2.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article78.text
Will force inclusive back-off from OOVs.
Perplexity = 17.75, Entropy = 4.15 bits
Computation based on 1107 words.
Number of 3-grams hit = 1094  (98.83%)
Number of 2-grams hit = 12  (1.08%)
Number of 1-grams hit = 1  (0.09%)
11 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article79.text
Will force inclusive back-off from OOVs.
Perplexity = 16.06, Entropy = 4.01 bits
Computation based on 1061 words.
Number of 3-grams hit = 1048  (98.77%)
Number of 2-grams hit = 12  (1.13%)
Number of 1-grams hit = 1  (0.09%)
12 OOVs (1.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article80.text
Will force inclusive back-off from OOVs.
Perplexity = 18.93, Entropy = 4.24 bits
Computation based on 368 words.
Number of 3-grams hit = 364  (98.91%)
Number of 2-grams hit = 3  (0.82%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article81.text
Will force inclusive back-off from OOVs.
Perplexity = 17.90, Entropy = 4.16 bits
Computation based on 4025 words.
Number of 3-grams hit = 4000  (99.38%)
Number of 2-grams hit = 24  (0.60%)
Number of 1-grams hit = 1  (0.02%)
25 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article82.text
Will force inclusive back-off from OOVs.
Perplexity = 16.11, Entropy = 4.01 bits
Computation based on 471 words.
Number of 3-grams hit = 469  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article83.text
Will force inclusive back-off from OOVs.
Perplexity = 18.58, Entropy = 4.22 bits
Computation based on 429 words.
Number of 3-grams hit = 424  (98.83%)
Number of 2-grams hit = 4  (0.93%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article84.text
Will force inclusive back-off from OOVs.
Perplexity = 16.60, Entropy = 4.05 bits
Computation based on 398 words.
Number of 3-grams hit = 395  (99.25%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article85.text
Will force inclusive back-off from OOVs.
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 1210 words.
Number of 3-grams hit = 1199  (99.09%)
Number of 2-grams hit = 10  (0.83%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article86.text
Will force inclusive back-off from OOVs.
Perplexity = 19.74, Entropy = 4.30 bits
Computation based on 289 words.
Number of 3-grams hit = 287  (99.31%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article87.text
Will force inclusive back-off from OOVs.
Perplexity = 13.99, Entropy = 3.81 bits
Computation based on 624 words.
Number of 3-grams hit = 614  (98.40%)
Number of 2-grams hit = 9  (1.44%)
Number of 1-grams hit = 1  (0.16%)
8 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article88.text
Will force inclusive back-off from OOVs.
Perplexity = 19.54, Entropy = 4.29 bits
Computation based on 1496 words.
Number of 3-grams hit = 1491  (99.67%)
Number of 2-grams hit = 4  (0.27%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article89.text
Will force inclusive back-off from OOVs.
Perplexity = 14.87, Entropy = 3.89 bits
Computation based on 390 words.
Number of 3-grams hit = 387  (99.23%)
Number of 2-grams hit = 2  (0.51%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article90.text
Will force inclusive back-off from OOVs.
Perplexity = 16.95, Entropy = 4.08 bits
Computation based on 517 words.
Number of 3-grams hit = 512  (99.03%)
Number of 2-grams hit = 4  (0.77%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article91.text
Will force inclusive back-off from OOVs.
Perplexity = 18.81, Entropy = 4.23 bits
Computation based on 339 words.
Number of 3-grams hit = 334  (98.53%)
Number of 2-grams hit = 4  (1.18%)
Number of 1-grams hit = 1  (0.29%)
3 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article92.text
Will force inclusive back-off from OOVs.
Perplexity = 17.74, Entropy = 4.15 bits
Computation based on 1023 words.
Number of 3-grams hit = 1015  (99.22%)
Number of 2-grams hit = 7  (0.68%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article93.text
Will force inclusive back-off from OOVs.
Perplexity = 16.59, Entropy = 4.05 bits
Computation based on 683 words.
Number of 3-grams hit = 678  (99.27%)
Number of 2-grams hit = 4  (0.59%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article94.text
Will force inclusive back-off from OOVs.
Perplexity = 17.93, Entropy = 4.16 bits
Computation based on 1281 words.
Number of 3-grams hit = 1271  (99.22%)
Number of 2-grams hit = 9  (0.70%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article95.text
Will force inclusive back-off from OOVs.
Perplexity = 17.94, Entropy = 4.17 bits
Computation based on 498 words.
Number of 3-grams hit = 494  (99.20%)
Number of 2-grams hit = 3  (0.60%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article96.text
Will force inclusive back-off from OOVs.
Perplexity = 16.41, Entropy = 4.04 bits
Computation based on 395 words.
Number of 3-grams hit = 391  (98.99%)
Number of 2-grams hit = 3  (0.76%)
Number of 1-grams hit = 1  (0.25%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article97.text
Will force inclusive back-off from OOVs.
Perplexity = 18.26, Entropy = 4.19 bits
Computation based on 552 words.
Number of 3-grams hit = 549  (99.46%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article98.text
Will force inclusive back-off from OOVs.
Perplexity = 15.91, Entropy = 3.99 bits
Computation based on 366 words.
Number of 3-grams hit = 364  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article99.text
Will force inclusive back-off from OOVs.
Perplexity = 20.01, Entropy = 4.32 bits
Computation based on 452 words.
Number of 3-grams hit = 446  (98.67%)
Number of 2-grams hit = 5  (1.11%)
Number of 1-grams hit = 1  (0.22%)
4 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article100.text
Will force inclusive back-off from OOVs.
Perplexity = 17.61, Entropy = 4.14 bits
Computation based on 469 words.
Number of 3-grams hit = 462  (98.51%)
Number of 2-grams hit = 6  (1.28%)
Number of 1-grams hit = 1  (0.21%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article101.text
Will force inclusive back-off from OOVs.
Perplexity = 17.74, Entropy = 4.15 bits
Computation based on 505 words.
Number of 3-grams hit = 501  (99.21%)
Number of 2-grams hit = 3  (0.59%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article102.text
Will force inclusive back-off from OOVs.
Perplexity = 13.89, Entropy = 3.80 bits
Computation based on 415 words.
Number of 3-grams hit = 413  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article103.text
Will force inclusive back-off from OOVs.
Perplexity = 18.33, Entropy = 4.20 bits
Computation based on 497 words.
Number of 3-grams hit = 494  (99.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article104.text
Will force inclusive back-off from OOVs.
Perplexity = 19.30, Entropy = 4.27 bits
Computation based on 6356 words.
Number of 3-grams hit = 6351  (99.92%)
Number of 2-grams hit = 4  (0.06%)
Number of 1-grams hit = 1  (0.02%)
3 OOVs (0.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article105.text
Will force inclusive back-off from OOVs.
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 448 words.
Number of 3-grams hit = 445  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article106.text
Will force inclusive back-off from OOVs.
Perplexity = 13.97, Entropy = 3.80 bits
Computation based on 801 words.
Number of 3-grams hit = 798  (99.63%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article107.text
Will force inclusive back-off from OOVs.
Perplexity = 13.18, Entropy = 3.72 bits
Computation based on 483 words.
Number of 3-grams hit = 479  (99.17%)
Number of 2-grams hit = 3  (0.62%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article108.text
Will force inclusive back-off from OOVs.
Perplexity = 18.81, Entropy = 4.23 bits
Computation based on 528 words.
Number of 3-grams hit = 524  (99.24%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article109.text
Will force inclusive back-off from OOVs.
Perplexity = 15.94, Entropy = 3.99 bits
Computation based on 1123 words.
Number of 3-grams hit = 1117  (99.47%)
Number of 2-grams hit = 5  (0.45%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article110.text
Will force inclusive back-off from OOVs.
Perplexity = 16.26, Entropy = 4.02 bits
Computation based on 907 words.
Number of 3-grams hit = 903  (99.56%)
Number of 2-grams hit = 3  (0.33%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article111.text
Will force inclusive back-off from OOVs.
Perplexity = 17.54, Entropy = 4.13 bits
Computation based on 1233 words.
Number of 3-grams hit = 1231  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article112.text
Will force inclusive back-off from OOVs.
Perplexity = 15.69, Entropy = 3.97 bits
Computation based on 328 words.
Number of 3-grams hit = 326  (99.39%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article113.text
Will force inclusive back-off from OOVs.
Perplexity = 20.77, Entropy = 4.38 bits
Computation based on 5823 words.
Number of 3-grams hit = 5813  (99.83%)
Number of 2-grams hit = 9  (0.15%)
Number of 1-grams hit = 1  (0.02%)
8 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article114.text
Will force inclusive back-off from OOVs.
Perplexity = 18.85, Entropy = 4.24 bits
Computation based on 496 words.
Number of 3-grams hit = 490  (98.79%)
Number of 2-grams hit = 5  (1.01%)
Number of 1-grams hit = 1  (0.20%)
4 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article115.text
Will force inclusive back-off from OOVs.
Perplexity = 17.51, Entropy = 4.13 bits
Computation based on 643 words.
Number of 3-grams hit = 639  (99.38%)
Number of 2-grams hit = 3  (0.47%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article116.text
Will force inclusive back-off from OOVs.
Perplexity = 18.25, Entropy = 4.19 bits
Computation based on 1538 words.
Number of 3-grams hit = 1535  (99.80%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
1 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article117.text
Will force inclusive back-off from OOVs.
Perplexity = 15.56, Entropy = 3.96 bits
Computation based on 512 words.
Number of 3-grams hit = 505  (98.63%)
Number of 2-grams hit = 6  (1.17%)
Number of 1-grams hit = 1  (0.20%)
5 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article118.text
Will force inclusive back-off from OOVs.
Perplexity = 19.09, Entropy = 4.26 bits
Computation based on 608 words.
Number of 3-grams hit = 599  (98.52%)
Number of 2-grams hit = 8  (1.32%)
Number of 1-grams hit = 1  (0.16%)
7 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article119.text
Will force inclusive back-off from OOVs.
Perplexity = 17.27, Entropy = 4.11 bits
Computation based on 387 words.
Number of 3-grams hit = 382  (98.71%)
Number of 2-grams hit = 4  (1.03%)
Number of 1-grams hit = 1  (0.26%)
3 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article120.text
Will force inclusive back-off from OOVs.
Perplexity = 17.29, Entropy = 4.11 bits
Computation based on 553 words.
Number of 3-grams hit = 547  (98.92%)
Number of 2-grams hit = 5  (0.90%)
Number of 1-grams hit = 1  (0.18%)
6 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article121.text
Will force inclusive back-off from OOVs.
Perplexity = 17.19, Entropy = 4.10 bits
Computation based on 7114 words.
Number of 3-grams hit = 7068  (99.35%)
Number of 2-grams hit = 45  (0.63%)
Number of 1-grams hit = 1  (0.01%)
47 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article122.text
Will force inclusive back-off from OOVs.
Perplexity = 21.49, Entropy = 4.43 bits
Computation based on 323 words.
Number of 3-grams hit = 319  (98.76%)
Number of 2-grams hit = 3  (0.93%)
Number of 1-grams hit = 1  (0.31%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article123.text
Will force inclusive back-off from OOVs.
Perplexity = 15.44, Entropy = 3.95 bits
Computation based on 524 words.
Number of 3-grams hit = 519  (99.05%)
Number of 2-grams hit = 4  (0.76%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article124.text
Will force inclusive back-off from OOVs.
Perplexity = 18.09, Entropy = 4.18 bits
Computation based on 797 words.
Number of 3-grams hit = 792  (99.37%)
Number of 2-grams hit = 4  (0.50%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article125.text
Will force inclusive back-off from OOVs.
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 481 words.
Number of 3-grams hit = 476  (98.96%)
Number of 2-grams hit = 4  (0.83%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article126.text
Will force inclusive back-off from OOVs.
Perplexity = 18.02, Entropy = 4.17 bits
Computation based on 537 words.
Number of 3-grams hit = 531  (98.88%)
Number of 2-grams hit = 5  (0.93%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article127.text
Will force inclusive back-off from OOVs.
Perplexity = 18.14, Entropy = 4.18 bits
Computation based on 918 words.
Number of 3-grams hit = 909  (99.02%)
Number of 2-grams hit = 8  (0.87%)
Number of 1-grams hit = 1  (0.11%)
8 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article128.text
Will force inclusive back-off from OOVs.
Perplexity = 13.82, Entropy = 3.79 bits
Computation based on 429 words.
Number of 3-grams hit = 425  (99.07%)
Number of 2-grams hit = 3  (0.70%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article129.text
Will force inclusive back-off from OOVs.
Perplexity = 15.48, Entropy = 3.95 bits
Computation based on 468 words.
Number of 3-grams hit = 463  (98.93%)
Number of 2-grams hit = 4  (0.85%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article130.text
Will force inclusive back-off from OOVs.
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 1521 words.
Number of 3-grams hit = 1507  (99.08%)
Number of 2-grams hit = 13  (0.85%)
Number of 1-grams hit = 1  (0.07%)
12 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article131.text
Will force inclusive back-off from OOVs.
Perplexity = 16.80, Entropy = 4.07 bits
Computation based on 6777 words.
Number of 3-grams hit = 6739  (99.44%)
Number of 2-grams hit = 37  (0.55%)
Number of 1-grams hit = 1  (0.01%)
39 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article132.text
Will force inclusive back-off from OOVs.
Perplexity = 17.11, Entropy = 4.10 bits
Computation based on 1116 words.
Number of 3-grams hit = 1107  (99.19%)
Number of 2-grams hit = 8  (0.72%)
Number of 1-grams hit = 1  (0.09%)
7 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article133.text
Will force inclusive back-off from OOVs.
Perplexity = 22.81, Entropy = 4.51 bits
Computation based on 238 words.
Number of 3-grams hit = 231  (97.06%)
Number of 2-grams hit = 6  (2.52%)
Number of 1-grams hit = 1  (0.42%)
5 OOVs (2.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article134.text
Will force inclusive back-off from OOVs.
Perplexity = 16.96, Entropy = 4.08 bits
Computation based on 500 words.
Number of 3-grams hit = 496  (99.20%)
Number of 2-grams hit = 3  (0.60%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article135.text
Will force inclusive back-off from OOVs.
Perplexity = 14.75, Entropy = 3.88 bits
Computation based on 489 words.
Number of 3-grams hit = 485  (99.18%)
Number of 2-grams hit = 3  (0.61%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article136.text
Will force inclusive back-off from OOVs.
Perplexity = 19.26, Entropy = 4.27 bits
Computation based on 520 words.
Number of 3-grams hit = 512  (98.46%)
Number of 2-grams hit = 7  (1.35%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article137.text
Will force inclusive back-off from OOVs.
Perplexity = 19.28, Entropy = 4.27 bits
Computation based on 332 words.
Number of 3-grams hit = 329  (99.10%)
Number of 2-grams hit = 2  (0.60%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article138.text
Will force inclusive back-off from OOVs.
Perplexity = 17.15, Entropy = 4.10 bits
Computation based on 306 words.
Number of 3-grams hit = 302  (98.69%)
Number of 2-grams hit = 3  (0.98%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article139.text
Will force inclusive back-off from OOVs.
Perplexity = 18.74, Entropy = 4.23 bits
Computation based on 273 words.
Number of 3-grams hit = 269  (98.53%)
Number of 2-grams hit = 3  (1.10%)
Number of 1-grams hit = 1  (0.37%)
2 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article140.text
Will force inclusive back-off from OOVs.
Perplexity = 13.18, Entropy = 3.72 bits
Computation based on 374 words.
Number of 3-grams hit = 372  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article141.text
Will force inclusive back-off from OOVs.
Perplexity = 15.29, Entropy = 3.93 bits
Computation based on 432 words.
Number of 3-grams hit = 428  (99.07%)
Number of 2-grams hit = 3  (0.69%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article142.text
Will force inclusive back-off from OOVs.
Perplexity = 17.31, Entropy = 4.11 bits
Computation based on 697 words.
Number of 3-grams hit = 693  (99.43%)
Number of 2-grams hit = 3  (0.43%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article143.text
Will force inclusive back-off from OOVs.
Perplexity = 14.64, Entropy = 3.87 bits
Computation based on 380 words.
Number of 3-grams hit = 377  (99.21%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article144.text
Will force inclusive back-off from OOVs.
Perplexity = 11.63, Entropy = 3.54 bits
Computation based on 506 words.
Number of 3-grams hit = 503  (99.41%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article145.text
Will force inclusive back-off from OOVs.
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 1932 words.
Number of 3-grams hit = 1918  (99.28%)
Number of 2-grams hit = 13  (0.67%)
Number of 1-grams hit = 1  (0.05%)
12 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article146.text
Will force inclusive back-off from OOVs.
Perplexity = 17.33, Entropy = 4.12 bits
Computation based on 237 words.
Number of 3-grams hit = 234  (98.73%)
Number of 2-grams hit = 2  (0.84%)
Number of 1-grams hit = 1  (0.42%)
1 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article147.text
Will force inclusive back-off from OOVs.
Perplexity = 18.97, Entropy = 4.25 bits
Computation based on 650 words.
Number of 3-grams hit = 639  (98.31%)
Number of 2-grams hit = 10  (1.54%)
Number of 1-grams hit = 1  (0.15%)
9 OOVs (1.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article148.text
Will force inclusive back-off from OOVs.
Perplexity = 16.08, Entropy = 4.01 bits
Computation based on 1726 words.
Number of 3-grams hit = 1711  (99.13%)
Number of 2-grams hit = 14  (0.81%)
Number of 1-grams hit = 1  (0.06%)
13 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article149.text
Will force inclusive back-off from OOVs.
Perplexity = 15.85, Entropy = 3.99 bits
Computation based on 558 words.
Number of 3-grams hit = 554  (99.28%)
Number of 2-grams hit = 3  (0.54%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article150.text
Will force inclusive back-off from OOVs.
Perplexity = 15.97, Entropy = 4.00 bits
Computation based on 636 words.
Number of 3-grams hit = 630  (99.06%)
Number of 2-grams hit = 5  (0.79%)
Number of 1-grams hit = 1  (0.16%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article151.text
Will force inclusive back-off from OOVs.
Perplexity = 21.42, Entropy = 4.42 bits
Computation based on 173 words.
Number of 3-grams hit = 171  (98.84%)
Number of 2-grams hit = 1  (0.58%)
Number of 1-grams hit = 1  (0.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article152.text
Will force inclusive back-off from OOVs.
Perplexity = 17.46, Entropy = 4.13 bits
Computation based on 1799 words.
Number of 3-grams hit = 1778  (98.83%)
Number of 2-grams hit = 20  (1.11%)
Number of 1-grams hit = 1  (0.06%)
20 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article153.text
Will force inclusive back-off from OOVs.
Perplexity = 17.65, Entropy = 4.14 bits
Computation based on 583 words.
Number of 3-grams hit = 574  (98.46%)
Number of 2-grams hit = 8  (1.37%)
Number of 1-grams hit = 1  (0.17%)
7 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article154.text
Will force inclusive back-off from OOVs.
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 1053 words.
Number of 3-grams hit = 1041  (98.86%)
Number of 2-grams hit = 11  (1.04%)
Number of 1-grams hit = 1  (0.09%)
11 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article155.text
Will force inclusive back-off from OOVs.
Perplexity = 16.80, Entropy = 4.07 bits
Computation based on 641 words.
Number of 3-grams hit = 637  (99.38%)
Number of 2-grams hit = 3  (0.47%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article156.text
Will force inclusive back-off from OOVs.
Perplexity = 19.76, Entropy = 4.30 bits
Computation based on 1376 words.
Number of 3-grams hit = 1372  (99.71%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article157.text
Will force inclusive back-off from OOVs.
Perplexity = 15.41, Entropy = 3.95 bits
Computation based on 405 words.
Number of 3-grams hit = 403  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article158.text
Will force inclusive back-off from OOVs.
Perplexity = 15.99, Entropy = 4.00 bits
Computation based on 571 words.
Number of 3-grams hit = 566  (99.12%)
Number of 2-grams hit = 4  (0.70%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article159.text
Will force inclusive back-off from OOVs.
Perplexity = 19.09, Entropy = 4.25 bits
Computation based on 1207 words.
Number of 3-grams hit = 1205  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article160.text
Will force inclusive back-off from OOVs.
Perplexity = 13.80, Entropy = 3.79 bits
Computation based on 538 words.
Number of 3-grams hit = 534  (99.26%)
Number of 2-grams hit = 3  (0.56%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article161.text
Will force inclusive back-off from OOVs.
Perplexity = 12.74, Entropy = 3.67 bits
Computation based on 198 words.
Number of 3-grams hit = 195  (98.48%)
Number of 2-grams hit = 2  (1.01%)
Number of 1-grams hit = 1  (0.51%)
1 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article162.text
Will force inclusive back-off from OOVs.
Perplexity = 17.23, Entropy = 4.11 bits
Computation based on 1145 words.
Number of 3-grams hit = 1135  (99.13%)
Number of 2-grams hit = 9  (0.79%)
Number of 1-grams hit = 1  (0.09%)
9 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article163.text
Will force inclusive back-off from OOVs.
Perplexity = 17.67, Entropy = 4.14 bits
Computation based on 702 words.
Number of 3-grams hit = 697  (99.29%)
Number of 2-grams hit = 4  (0.57%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article164.text
Will force inclusive back-off from OOVs.
Perplexity = 18.16, Entropy = 4.18 bits
Computation based on 1772 words.
Number of 3-grams hit = 1765  (99.60%)
Number of 2-grams hit = 6  (0.34%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article165.text
Will force inclusive back-off from OOVs.
Perplexity = 13.64, Entropy = 3.77 bits
Computation based on 555 words.
Number of 3-grams hit = 551  (99.28%)
Number of 2-grams hit = 3  (0.54%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article166.text
Will force inclusive back-off from OOVs.
Perplexity = 17.91, Entropy = 4.16 bits
Computation based on 516 words.
Number of 3-grams hit = 513  (99.42%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article167.text
Will force inclusive back-off from OOVs.
Perplexity = 18.06, Entropy = 4.17 bits
Computation based on 1169 words.
Number of 3-grams hit = 1161  (99.32%)
Number of 2-grams hit = 7  (0.60%)
Number of 1-grams hit = 1  (0.09%)
7 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article168.text
Will force inclusive back-off from OOVs.
Perplexity = 17.37, Entropy = 4.12 bits
Computation based on 521 words.
Number of 3-grams hit = 516  (99.04%)
Number of 2-grams hit = 4  (0.77%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article169.text
Will force inclusive back-off from OOVs.
Perplexity = 16.77, Entropy = 4.07 bits
Computation based on 503 words.
Number of 3-grams hit = 500  (99.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article170.text
Will force inclusive back-off from OOVs.
Perplexity = 13.94, Entropy = 3.80 bits
Computation based on 1145 words.
Number of 3-grams hit = 1139  (99.48%)
Number of 2-grams hit = 5  (0.44%)
Number of 1-grams hit = 1  (0.09%)
5 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article171.text
Will force inclusive back-off from OOVs.
Perplexity = 12.98, Entropy = 3.70 bits
Computation based on 605 words.
Number of 3-grams hit = 598  (98.84%)
Number of 2-grams hit = 6  (0.99%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article172.text
Will force inclusive back-off from OOVs.
Perplexity = 16.53, Entropy = 4.05 bits
Computation based on 680 words.
Number of 3-grams hit = 675  (99.26%)
Number of 2-grams hit = 4  (0.59%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article173.text
Will force inclusive back-off from OOVs.
Perplexity = 14.28, Entropy = 3.84 bits
Computation based on 1018 words.
Number of 3-grams hit = 1013  (99.51%)
Number of 2-grams hit = 4  (0.39%)
Number of 1-grams hit = 1  (0.10%)
3 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article174.text
Will force inclusive back-off from OOVs.
Perplexity = 18.26, Entropy = 4.19 bits
Computation based on 445 words.
Number of 3-grams hit = 442  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article175.text
Will force inclusive back-off from OOVs.
Perplexity = 17.59, Entropy = 4.14 bits
Computation based on 456 words.
Number of 3-grams hit = 452  (99.12%)
Number of 2-grams hit = 3  (0.66%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article176.text
Will force inclusive back-off from OOVs.
Perplexity = 16.41, Entropy = 4.04 bits
Computation based on 743 words.
Number of 3-grams hit = 738  (99.33%)
Number of 2-grams hit = 4  (0.54%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article177.text
Will force inclusive back-off from OOVs.
Perplexity = 15.99, Entropy = 4.00 bits
Computation based on 516 words.
Number of 3-grams hit = 512  (99.22%)
Number of 2-grams hit = 3  (0.58%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article178.text
Will force inclusive back-off from OOVs.
Perplexity = 16.70, Entropy = 4.06 bits
Computation based on 167 words.
Number of 3-grams hit = 163  (97.60%)
Number of 2-grams hit = 3  (1.80%)
Number of 1-grams hit = 1  (0.60%)
2 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article179.text
Will force inclusive back-off from OOVs.
Perplexity = 16.33, Entropy = 4.03 bits
Computation based on 453 words.
Number of 3-grams hit = 450  (99.34%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article180.text
Will force inclusive back-off from OOVs.
Perplexity = 18.59, Entropy = 4.22 bits
Computation based on 428 words.
Number of 3-grams hit = 424  (99.07%)
Number of 2-grams hit = 3  (0.70%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article181.text
Will force inclusive back-off from OOVs.
Perplexity = 20.46, Entropy = 4.35 bits
Computation based on 851 words.
Number of 3-grams hit = 847  (99.53%)
Number of 2-grams hit = 3  (0.35%)
Number of 1-grams hit = 1  (0.12%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article182.text
Will force inclusive back-off from OOVs.
Perplexity = 18.50, Entropy = 4.21 bits
Computation based on 365 words.
Number of 3-grams hit = 362  (99.18%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article183.text
Will force inclusive back-off from OOVs.
Perplexity = 16.81, Entropy = 4.07 bits
Computation based on 459 words.
Number of 3-grams hit = 457  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article184.text
Will force inclusive back-off from OOVs.
Perplexity = 16.66, Entropy = 4.06 bits
Computation based on 5455 words.
Number of 3-grams hit = 5416  (99.29%)
Number of 2-grams hit = 38  (0.70%)
Number of 1-grams hit = 1  (0.02%)
37 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article185.text
Will force inclusive back-off from OOVs.
Perplexity = 17.19, Entropy = 4.10 bits
Computation based on 971 words.
Number of 3-grams hit = 965  (99.38%)
Number of 2-grams hit = 5  (0.51%)
Number of 1-grams hit = 1  (0.10%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article186.text
Will force inclusive back-off from OOVs.
Perplexity = 15.02, Entropy = 3.91 bits
Computation based on 489 words.
Number of 3-grams hit = 486  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article187.text
Will force inclusive back-off from OOVs.
Perplexity = 17.15, Entropy = 4.10 bits
Computation based on 5986 words.
Number of 3-grams hit = 5945  (99.32%)
Number of 2-grams hit = 40  (0.67%)
Number of 1-grams hit = 1  (0.02%)
41 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article188.text
Will force inclusive back-off from OOVs.
Perplexity = 15.08, Entropy = 3.91 bits
Computation based on 691 words.
Number of 3-grams hit = 689  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article189.text
Will force inclusive back-off from OOVs.
Perplexity = 16.14, Entropy = 4.01 bits
Computation based on 534 words.
Number of 3-grams hit = 530  (99.25%)
Number of 2-grams hit = 3  (0.56%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article190.text
Will force inclusive back-off from OOVs.
Perplexity = 14.68, Entropy = 3.88 bits
Computation based on 393 words.
Number of 3-grams hit = 388  (98.73%)
Number of 2-grams hit = 4  (1.02%)
Number of 1-grams hit = 1  (0.25%)
3 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article191.text
Will force inclusive back-off from OOVs.
Perplexity = 18.07, Entropy = 4.18 bits
Computation based on 5426 words.
Number of 3-grams hit = 5383  (99.21%)
Number of 2-grams hit = 42  (0.77%)
Number of 1-grams hit = 1  (0.02%)
42 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article192.text
Will force inclusive back-off from OOVs.
Perplexity = 16.34, Entropy = 4.03 bits
Computation based on 770 words.
Number of 3-grams hit = 767  (99.61%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article193.text
Will force inclusive back-off from OOVs.
Perplexity = 15.20, Entropy = 3.93 bits
Computation based on 161 words.
Number of 3-grams hit = 159  (98.76%)
Number of 2-grams hit = 1  (0.62%)
Number of 1-grams hit = 1  (0.62%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article194.text
Will force inclusive back-off from OOVs.
Perplexity = 17.64, Entropy = 4.14 bits
Computation based on 601 words.
Number of 3-grams hit = 595  (99.00%)
Number of 2-grams hit = 5  (0.83%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article195.text
Will force inclusive back-off from OOVs.
Perplexity = 17.64, Entropy = 4.14 bits
Computation based on 789 words.
Number of 3-grams hit = 781  (98.99%)
Number of 2-grams hit = 7  (0.89%)
Number of 1-grams hit = 1  (0.13%)
6 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article196.text
Will force inclusive back-off from OOVs.
Perplexity = 20.79, Entropy = 4.38 bits
Computation based on 686 words.
Number of 3-grams hit = 683  (99.56%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article197.text
Will force inclusive back-off from OOVs.
Perplexity = 18.64, Entropy = 4.22 bits
Computation based on 226 words.
Number of 3-grams hit = 224  (99.12%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article198.text
Will force inclusive back-off from OOVs.
Perplexity = 16.54, Entropy = 4.05 bits
Computation based on 715 words.
Number of 3-grams hit = 708  (99.02%)
Number of 2-grams hit = 6  (0.84%)
Number of 1-grams hit = 1  (0.14%)
6 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article199.text
Will force inclusive back-off from OOVs.
Perplexity = 18.67, Entropy = 4.22 bits
Computation based on 4482 words.
Number of 3-grams hit = 4469  (99.71%)
Number of 2-grams hit = 12  (0.27%)
Number of 1-grams hit = 1  (0.02%)
11 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article200.text
Will force inclusive back-off from OOVs.
Perplexity = 17.08, Entropy = 4.09 bits
Computation based on 637 words.
Number of 3-grams hit = 632  (99.22%)
Number of 2-grams hit = 4  (0.63%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article201.text
Will force inclusive back-off from OOVs.
Perplexity = 15.21, Entropy = 3.93 bits
Computation based on 463 words.
Number of 3-grams hit = 458  (98.92%)
Number of 2-grams hit = 4  (0.86%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article202.text
Will force inclusive back-off from OOVs.
Perplexity = 16.46, Entropy = 4.04 bits
Computation based on 1169 words.
Number of 3-grams hit = 1161  (99.32%)
Number of 2-grams hit = 7  (0.60%)
Number of 1-grams hit = 1  (0.09%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article203.text
Will force inclusive back-off from OOVs.
Perplexity = 18.74, Entropy = 4.23 bits
Computation based on 4825 words.
Number of 3-grams hit = 4818  (99.85%)
Number of 2-grams hit = 6  (0.12%)
Number of 1-grams hit = 1  (0.02%)
5 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article204.text
Will force inclusive back-off from OOVs.
Perplexity = 19.94, Entropy = 4.32 bits
Computation based on 5410 words.
Number of 3-grams hit = 5402  (99.85%)
Number of 2-grams hit = 7  (0.13%)
Number of 1-grams hit = 1  (0.02%)
6 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article205.text
Will force inclusive back-off from OOVs.
Perplexity = 18.25, Entropy = 4.19 bits
Computation based on 554 words.
Number of 3-grams hit = 549  (99.10%)
Number of 2-grams hit = 4  (0.72%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article206.text
Will force inclusive back-off from OOVs.
Perplexity = 15.01, Entropy = 3.91 bits
Computation based on 589 words.
Number of 3-grams hit = 585  (99.32%)
Number of 2-grams hit = 3  (0.51%)
Number of 1-grams hit = 1  (0.17%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article207.text
Will force inclusive back-off from OOVs.
Perplexity = 16.21, Entropy = 4.02 bits
Computation based on 718 words.
Number of 3-grams hit = 710  (98.89%)
Number of 2-grams hit = 7  (0.97%)
Number of 1-grams hit = 1  (0.14%)
6 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article208.text
Will force inclusive back-off from OOVs.
Perplexity = 15.79, Entropy = 3.98 bits
Computation based on 891 words.
Number of 3-grams hit = 886  (99.44%)
Number of 2-grams hit = 4  (0.45%)
Number of 1-grams hit = 1  (0.11%)
3 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article209.text
Will force inclusive back-off from OOVs.
Perplexity = 17.38, Entropy = 4.12 bits
Computation based on 269 words.
Number of 3-grams hit = 267  (99.26%)
Number of 2-grams hit = 1  (0.37%)
Number of 1-grams hit = 1  (0.37%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article210.text
Will force inclusive back-off from OOVs.
Perplexity = 17.86, Entropy = 4.16 bits
Computation based on 832 words.
Number of 3-grams hit = 825  (99.16%)
Number of 2-grams hit = 6  (0.72%)
Number of 1-grams hit = 1  (0.12%)
5 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article211.text
Will force inclusive back-off from OOVs.
Perplexity = 17.06, Entropy = 4.09 bits
Computation based on 472 words.
Number of 3-grams hit = 469  (99.36%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article212.text
Will force inclusive back-off from OOVs.
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 414 words.
Number of 3-grams hit = 409  (98.79%)
Number of 2-grams hit = 4  (0.97%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article213.text
Will force inclusive back-off from OOVs.
Perplexity = 19.23, Entropy = 4.27 bits
Computation based on 620 words.
Number of 3-grams hit = 608  (98.06%)
Number of 2-grams hit = 11  (1.77%)
Number of 1-grams hit = 1  (0.16%)
10 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article214.text
Will force inclusive back-off from OOVs.
Perplexity = 15.73, Entropy = 3.98 bits
Computation based on 463 words.
Number of 3-grams hit = 460  (99.35%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article215.text
Will force inclusive back-off from OOVs.
Perplexity = 12.69, Entropy = 3.67 bits
Computation based on 695 words.
Number of 3-grams hit = 692  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article216.text
Will force inclusive back-off from OOVs.
Perplexity = 15.82, Entropy = 3.98 bits
Computation based on 482 words.
Number of 3-grams hit = 476  (98.76%)
Number of 2-grams hit = 5  (1.04%)
Number of 1-grams hit = 1  (0.21%)
4 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article217.text
Will force inclusive back-off from OOVs.
Perplexity = 23.75, Entropy = 4.57 bits
Computation based on 300 words.
Number of 3-grams hit = 296  (98.67%)
Number of 2-grams hit = 3  (1.00%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article218.text
Will force inclusive back-off from OOVs.
Perplexity = 19.16, Entropy = 4.26 bits
Computation based on 719 words.
Number of 3-grams hit = 717  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article219.text
Will force inclusive back-off from OOVs.
Perplexity = 13.89, Entropy = 3.80 bits
Computation based on 409 words.
Number of 3-grams hit = 405  (99.02%)
Number of 2-grams hit = 3  (0.73%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article220.text
Will force inclusive back-off from OOVs.
Perplexity = 20.45, Entropy = 4.35 bits
Computation based on 4316 words.
Number of 3-grams hit = 4310  (99.86%)
Number of 2-grams hit = 5  (0.12%)
Number of 1-grams hit = 1  (0.02%)
4 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article221.text
Will force inclusive back-off from OOVs.
Perplexity = 16.72, Entropy = 4.06 bits
Computation based on 1182 words.
Number of 3-grams hit = 1180  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article222.text
Will force inclusive back-off from OOVs.
Perplexity = 16.48, Entropy = 4.04 bits
Computation based on 4439 words.
Number of 3-grams hit = 4407  (99.28%)
Number of 2-grams hit = 31  (0.70%)
Number of 1-grams hit = 1  (0.02%)
32 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article223.text
Will force inclusive back-off from OOVs.
Perplexity = 16.46, Entropy = 4.04 bits
Computation based on 831 words.
Number of 3-grams hit = 826  (99.40%)
Number of 2-grams hit = 4  (0.48%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article224.text
Will force inclusive back-off from OOVs.
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 382 words.
Number of 3-grams hit = 380  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article225.text
Will force inclusive back-off from OOVs.
Perplexity = 14.13, Entropy = 3.82 bits
Computation based on 726 words.
Number of 3-grams hit = 720  (99.17%)
Number of 2-grams hit = 5  (0.69%)
Number of 1-grams hit = 1  (0.14%)
4 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article226.text
Will force inclusive back-off from OOVs.
Perplexity = 15.80, Entropy = 3.98 bits
Computation based on 1233 words.
Number of 3-grams hit = 1228  (99.59%)
Number of 2-grams hit = 4  (0.32%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article227.text
Will force inclusive back-off from OOVs.
Perplexity = 15.40, Entropy = 3.95 bits
Computation based on 934 words.
Number of 3-grams hit = 928  (99.36%)
Number of 2-grams hit = 5  (0.54%)
Number of 1-grams hit = 1  (0.11%)
4 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article228.text
Will force inclusive back-off from OOVs.
Perplexity = 16.42, Entropy = 4.04 bits
Computation based on 778 words.
Number of 3-grams hit = 768  (98.71%)
Number of 2-grams hit = 9  (1.16%)
Number of 1-grams hit = 1  (0.13%)
8 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article229.text
Will force inclusive back-off from OOVs.
Perplexity = 15.17, Entropy = 3.92 bits
Computation based on 476 words.
Number of 3-grams hit = 474  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article230.text
Will force inclusive back-off from OOVs.
Perplexity = 18.76, Entropy = 4.23 bits
Computation based on 304 words.
Number of 3-grams hit = 300  (98.68%)
Number of 2-grams hit = 3  (0.99%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article231.text
Will force inclusive back-off from OOVs.
Perplexity = 16.98, Entropy = 4.09 bits
Computation based on 800 words.
Number of 3-grams hit = 795  (99.38%)
Number of 2-grams hit = 4  (0.50%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article232.text
Will force inclusive back-off from OOVs.
Perplexity = 15.23, Entropy = 3.93 bits
Computation based on 364 words.
Number of 3-grams hit = 361  (99.18%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article233.text
Will force inclusive back-off from OOVs.
Perplexity = 17.59, Entropy = 4.14 bits
Computation based on 670 words.
Number of 3-grams hit = 665  (99.25%)
Number of 2-grams hit = 4  (0.60%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article234.text
Will force inclusive back-off from OOVs.
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 1331 words.
Number of 3-grams hit = 1322  (99.32%)
Number of 2-grams hit = 8  (0.60%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article235.text
Will force inclusive back-off from OOVs.
Perplexity = 17.62, Entropy = 4.14 bits
Computation based on 349 words.
Number of 3-grams hit = 346  (99.14%)
Number of 2-grams hit = 2  (0.57%)
Number of 1-grams hit = 1  (0.29%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article236.text
Will force inclusive back-off from OOVs.
Perplexity = 13.49, Entropy = 3.75 bits
Computation based on 750 words.
Number of 3-grams hit = 745  (99.33%)
Number of 2-grams hit = 4  (0.53%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article237.text
Will force inclusive back-off from OOVs.
Perplexity = 15.37, Entropy = 3.94 bits
Computation based on 388 words.
Number of 3-grams hit = 383  (98.71%)
Number of 2-grams hit = 4  (1.03%)
Number of 1-grams hit = 1  (0.26%)
3 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article238.text
Will force inclusive back-off from OOVs.
Perplexity = 14.53, Entropy = 3.86 bits
Computation based on 311 words.
Number of 3-grams hit = 307  (98.71%)
Number of 2-grams hit = 3  (0.96%)
Number of 1-grams hit = 1  (0.32%)
2 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article239.text
Will force inclusive back-off from OOVs.
Perplexity = 17.15, Entropy = 4.10 bits
Computation based on 474 words.
Number of 3-grams hit = 469  (98.95%)
Number of 2-grams hit = 4  (0.84%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article240.text
Will force inclusive back-off from OOVs.
Perplexity = 15.51, Entropy = 3.96 bits
Computation based on 314 words.
Number of 3-grams hit = 310  (98.73%)
Number of 2-grams hit = 3  (0.96%)
Number of 1-grams hit = 1  (0.32%)
2 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article241.text
Will force inclusive back-off from OOVs.
Perplexity = 14.17, Entropy = 3.82 bits
Computation based on 306 words.
Number of 3-grams hit = 303  (99.02%)
Number of 2-grams hit = 2  (0.65%)
Number of 1-grams hit = 1  (0.33%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article242.text
Will force inclusive back-off from OOVs.
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 293 words.
Number of 3-grams hit = 290  (98.98%)
Number of 2-grams hit = 2  (0.68%)
Number of 1-grams hit = 1  (0.34%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article243.text
Will force inclusive back-off from OOVs.
Perplexity = 17.67, Entropy = 4.14 bits
Computation based on 742 words.
Number of 3-grams hit = 733  (98.79%)
Number of 2-grams hit = 8  (1.08%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article244.text
Will force inclusive back-off from OOVs.
Perplexity = 15.32, Entropy = 3.94 bits
Computation based on 423 words.
Number of 3-grams hit = 417  (98.58%)
Number of 2-grams hit = 5  (1.18%)
Number of 1-grams hit = 1  (0.24%)
4 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article245.text
Will force inclusive back-off from OOVs.
Perplexity = 18.88, Entropy = 4.24 bits
Computation based on 5038 words.
Number of 3-grams hit = 5026  (99.76%)
Number of 2-grams hit = 11  (0.22%)
Number of 1-grams hit = 1  (0.02%)
10 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article246.text
Will force inclusive back-off from OOVs.
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 277 words.
Number of 3-grams hit = 275  (99.28%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article247.text
Will force inclusive back-off from OOVs.
Perplexity = 18.95, Entropy = 4.24 bits
Computation based on 448 words.
Number of 3-grams hit = 442  (98.66%)
Number of 2-grams hit = 5  (1.12%)
Number of 1-grams hit = 1  (0.22%)
4 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article248.text
Will force inclusive back-off from OOVs.
Perplexity = 18.10, Entropy = 4.18 bits
Computation based on 354 words.
Number of 3-grams hit = 351  (99.15%)
Number of 2-grams hit = 2  (0.56%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article249.text
Will force inclusive back-off from OOVs.
Perplexity = 19.65, Entropy = 4.30 bits
Computation based on 863 words.
Number of 3-grams hit = 861  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article250.text
Will force inclusive back-off from OOVs.
Perplexity = 18.69, Entropy = 4.22 bits
Computation based on 4859 words.
Number of 3-grams hit = 4852  (99.86%)
Number of 2-grams hit = 6  (0.12%)
Number of 1-grams hit = 1  (0.02%)
5 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article251.text
Will force inclusive back-off from OOVs.
Perplexity = 17.54, Entropy = 4.13 bits
Computation based on 4942 words.
Number of 3-grams hit = 4897  (99.09%)
Number of 2-grams hit = 44  (0.89%)
Number of 1-grams hit = 1  (0.02%)
44 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article252.text
Will force inclusive back-off from OOVs.
Perplexity = 17.77, Entropy = 4.15 bits
Computation based on 519 words.
Number of 3-grams hit = 511  (98.46%)
Number of 2-grams hit = 7  (1.35%)
Number of 1-grams hit = 1  (0.19%)
7 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article253.text
Will force inclusive back-off from OOVs.
Perplexity = 16.77, Entropy = 4.07 bits
Computation based on 4923 words.
Number of 3-grams hit = 4886  (99.25%)
Number of 2-grams hit = 36  (0.73%)
Number of 1-grams hit = 1  (0.02%)
35 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article254.text
Will force inclusive back-off from OOVs.
Perplexity = 14.13, Entropy = 3.82 bits
Computation based on 779 words.
Number of 3-grams hit = 773  (99.23%)
Number of 2-grams hit = 5  (0.64%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article255.text
Will force inclusive back-off from OOVs.
Perplexity = 17.55, Entropy = 4.13 bits
Computation based on 490 words.
Number of 3-grams hit = 485  (98.98%)
Number of 2-grams hit = 4  (0.82%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article256.text
Will force inclusive back-off from OOVs.
Perplexity = 16.52, Entropy = 4.05 bits
Computation based on 779 words.
Number of 3-grams hit = 770  (98.84%)
Number of 2-grams hit = 8  (1.03%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article257.text
Will force inclusive back-off from OOVs.
Perplexity = 19.76, Entropy = 4.30 bits
Computation based on 5776 words.
Number of 3-grams hit = 5769  (99.88%)
Number of 2-grams hit = 6  (0.10%)
Number of 1-grams hit = 1  (0.02%)
5 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article258.text
Will force inclusive back-off from OOVs.
Perplexity = 17.87, Entropy = 4.16 bits
Computation based on 409 words.
Number of 3-grams hit = 405  (99.02%)
Number of 2-grams hit = 3  (0.73%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article259.text
Will force inclusive back-off from OOVs.
Perplexity = 17.11, Entropy = 4.10 bits
Computation based on 694 words.
Number of 3-grams hit = 685  (98.70%)
Number of 2-grams hit = 8  (1.15%)
Number of 1-grams hit = 1  (0.14%)
7 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article260.text
Will force inclusive back-off from OOVs.
Perplexity = 17.30, Entropy = 4.11 bits
Computation based on 6951 words.
Number of 3-grams hit = 6905  (99.34%)
Number of 2-grams hit = 45  (0.65%)
Number of 1-grams hit = 1  (0.01%)
44 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article261.text
Will force inclusive back-off from OOVs.
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 645 words.
Number of 3-grams hit = 641  (99.38%)
Number of 2-grams hit = 3  (0.47%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article262.text
Will force inclusive back-off from OOVs.
Perplexity = 17.85, Entropy = 4.16 bits
Computation based on 662 words.
Number of 3-grams hit = 658  (99.40%)
Number of 2-grams hit = 3  (0.45%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article263.text
Will force inclusive back-off from OOVs.
Perplexity = 17.61, Entropy = 4.14 bits
Computation based on 373 words.
Number of 3-grams hit = 369  (98.93%)
Number of 2-grams hit = 3  (0.80%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article264.text
Will force inclusive back-off from OOVs.
Perplexity = 17.03, Entropy = 4.09 bits
Computation based on 799 words.
Number of 3-grams hit = 796  (99.62%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article265.text
Will force inclusive back-off from OOVs.
Perplexity = 13.87, Entropy = 3.79 bits
Computation based on 569 words.
Number of 3-grams hit = 567  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article266.text
Will force inclusive back-off from OOVs.
Perplexity = 17.64, Entropy = 4.14 bits
Computation based on 434 words.
Number of 3-grams hit = 431  (99.31%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article267.text
Will force inclusive back-off from OOVs.
Perplexity = 17.39, Entropy = 4.12 bits
Computation based on 6212 words.
Number of 3-grams hit = 6172  (99.36%)
Number of 2-grams hit = 39  (0.63%)
Number of 1-grams hit = 1  (0.02%)
40 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article268.text
Will force inclusive back-off from OOVs.
Perplexity = 17.64, Entropy = 4.14 bits
Computation based on 5792 words.
Number of 3-grams hit = 5744  (99.17%)
Number of 2-grams hit = 47  (0.81%)
Number of 1-grams hit = 1  (0.02%)
48 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article269.text
Will force inclusive back-off from OOVs.
Perplexity = 16.87, Entropy = 4.08 bits
Computation based on 421 words.
Number of 3-grams hit = 418  (99.29%)
Number of 2-grams hit = 2  (0.48%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article270.text
Will force inclusive back-off from OOVs.
Perplexity = 13.92, Entropy = 3.80 bits
Computation based on 375 words.
Number of 3-grams hit = 372  (99.20%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article271.text
Will force inclusive back-off from OOVs.
Perplexity = 19.35, Entropy = 4.27 bits
Computation based on 7334 words.
Number of 3-grams hit = 7319  (99.80%)
Number of 2-grams hit = 14  (0.19%)
Number of 1-grams hit = 1  (0.01%)
13 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article272.text
Will force inclusive back-off from OOVs.
Perplexity = 19.05, Entropy = 4.25 bits
Computation based on 5210 words.
Number of 3-grams hit = 5200  (99.81%)
Number of 2-grams hit = 9  (0.17%)
Number of 1-grams hit = 1  (0.02%)
8 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article273.text
Will force inclusive back-off from OOVs.
Perplexity = 17.29, Entropy = 4.11 bits
Computation based on 637 words.
Number of 3-grams hit = 633  (99.37%)
Number of 2-grams hit = 3  (0.47%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article274.text
Will force inclusive back-off from OOVs.
Perplexity = 18.39, Entropy = 4.20 bits
Computation based on 649 words.
Number of 3-grams hit = 639  (98.46%)
Number of 2-grams hit = 9  (1.39%)
Number of 1-grams hit = 1  (0.15%)
8 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article275.text
Will force inclusive back-off from OOVs.
Perplexity = 16.04, Entropy = 4.00 bits
Computation based on 1915 words.
Number of 3-grams hit = 1902  (99.32%)
Number of 2-grams hit = 12  (0.63%)
Number of 1-grams hit = 1  (0.05%)
12 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article276.text
Will force inclusive back-off from OOVs.
Perplexity = 16.15, Entropy = 4.01 bits
Computation based on 469 words.
Number of 3-grams hit = 465  (99.15%)
Number of 2-grams hit = 3  (0.64%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article277.text
Will force inclusive back-off from OOVs.
Perplexity = 16.88, Entropy = 4.08 bits
Computation based on 766 words.
Number of 3-grams hit = 760  (99.22%)
Number of 2-grams hit = 5  (0.65%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article278.text
Will force inclusive back-off from OOVs.
Perplexity = 14.08, Entropy = 3.82 bits
Computation based on 807 words.
Number of 3-grams hit = 802  (99.38%)
Number of 2-grams hit = 4  (0.50%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article279.text
Will force inclusive back-off from OOVs.
Perplexity = 20.01, Entropy = 4.32 bits
Computation based on 400 words.
Number of 3-grams hit = 397  (99.25%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article280.text
Will force inclusive back-off from OOVs.
Perplexity = 17.53, Entropy = 4.13 bits
Computation based on 1700 words.
Number of 3-grams hit = 1690  (99.41%)
Number of 2-grams hit = 9  (0.53%)
Number of 1-grams hit = 1  (0.06%)
10 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article281.text
Will force inclusive back-off from OOVs.
Perplexity = 15.33, Entropy = 3.94 bits
Computation based on 566 words.
Number of 3-grams hit = 564  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article282.text
Will force inclusive back-off from OOVs.
Perplexity = 18.42, Entropy = 4.20 bits
Computation based on 703 words.
Number of 3-grams hit = 693  (98.58%)
Number of 2-grams hit = 9  (1.28%)
Number of 1-grams hit = 1  (0.14%)
10 OOVs (1.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article283.text
Will force inclusive back-off from OOVs.
Perplexity = 18.12, Entropy = 4.18 bits
Computation based on 1179 words.
Number of 3-grams hit = 1170  (99.24%)
Number of 2-grams hit = 8  (0.68%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article284.text
Will force inclusive back-off from OOVs.
Perplexity = 20.33, Entropy = 4.35 bits
Computation based on 1579 words.
Number of 3-grams hit = 1574  (99.68%)
Number of 2-grams hit = 4  (0.25%)
Number of 1-grams hit = 1  (0.06%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article285.text
Will force inclusive back-off from OOVs.
Perplexity = 18.37, Entropy = 4.20 bits
Computation based on 1097 words.
Number of 3-grams hit = 1093  (99.64%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article286.text
Will force inclusive back-off from OOVs.
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 444 words.
Number of 3-grams hit = 441  (99.32%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article287.text
Will force inclusive back-off from OOVs.
Perplexity = 16.25, Entropy = 4.02 bits
Computation based on 1658 words.
Number of 3-grams hit = 1656  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article288.text
Will force inclusive back-off from OOVs.
Perplexity = 17.71, Entropy = 4.15 bits
Computation based on 580 words.
Number of 3-grams hit = 576  (99.31%)
Number of 2-grams hit = 3  (0.52%)
Number of 1-grams hit = 1  (0.17%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article289.text
Will force inclusive back-off from OOVs.
Perplexity = 21.48, Entropy = 4.42 bits
Computation based on 706 words.
Number of 3-grams hit = 704  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article290.text
Will force inclusive back-off from OOVs.
Perplexity = 18.88, Entropy = 4.24 bits
Computation based on 1290 words.
Number of 3-grams hit = 1285  (99.61%)
Number of 2-grams hit = 4  (0.31%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article291.text
Will force inclusive back-off from OOVs.
Perplexity = 16.08, Entropy = 4.01 bits
Computation based on 516 words.
Number of 3-grams hit = 514  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article292.text
Will force inclusive back-off from OOVs.
Perplexity = 21.23, Entropy = 4.41 bits
Computation based on 537 words.
Number of 3-grams hit = 534  (99.44%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article293.text
Will force inclusive back-off from OOVs.
Perplexity = 15.85, Entropy = 3.99 bits
Computation based on 548 words.
Number of 3-grams hit = 539  (98.36%)
Number of 2-grams hit = 8  (1.46%)
Number of 1-grams hit = 1  (0.18%)
7 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article294.text
Will force inclusive back-off from OOVs.
Perplexity = 15.36, Entropy = 3.94 bits
Computation based on 2111 words.
Number of 3-grams hit = 2100  (99.48%)
Number of 2-grams hit = 10  (0.47%)
Number of 1-grams hit = 1  (0.05%)
9 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article295.text
Will force inclusive back-off from OOVs.
Perplexity = 14.92, Entropy = 3.90 bits
Computation based on 589 words.
Number of 3-grams hit = 583  (98.98%)
Number of 2-grams hit = 5  (0.85%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article296.text
Will force inclusive back-off from OOVs.
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 1520 words.
Number of 3-grams hit = 1508  (99.21%)
Number of 2-grams hit = 11  (0.72%)
Number of 1-grams hit = 1  (0.07%)
10 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article297.text
Will force inclusive back-off from OOVs.
Perplexity = 19.70, Entropy = 4.30 bits
Computation based on 491 words.
Number of 3-grams hit = 488  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article298.text
Will force inclusive back-off from OOVs.
Perplexity = 17.83, Entropy = 4.16 bits
Computation based on 3433 words.
Number of 3-grams hit = 3410  (99.33%)
Number of 2-grams hit = 22  (0.64%)
Number of 1-grams hit = 1  (0.03%)
21 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article299.text
Will force inclusive back-off from OOVs.
Perplexity = 17.57, Entropy = 4.13 bits
Computation based on 2733 words.
Number of 3-grams hit = 2722  (99.60%)
Number of 2-grams hit = 10  (0.37%)
Number of 1-grams hit = 1  (0.04%)
9 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article300.text
Will force inclusive back-off from OOVs.
Perplexity = 18.86, Entropy = 4.24 bits
Computation based on 2533 words.
Number of 3-grams hit = 2525  (99.68%)
Number of 2-grams hit = 7  (0.28%)
Number of 1-grams hit = 1  (0.04%)
6 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article301.text
Will force inclusive back-off from OOVs.
Perplexity = 16.31, Entropy = 4.03 bits
Computation based on 2131 words.
Number of 3-grams hit = 2114  (99.20%)
Number of 2-grams hit = 16  (0.75%)
Number of 1-grams hit = 1  (0.05%)
15 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article302.text
Will force inclusive back-off from OOVs.
Perplexity = 18.69, Entropy = 4.22 bits
Computation based on 622 words.
Number of 3-grams hit = 616  (99.04%)
Number of 2-grams hit = 5  (0.80%)
Number of 1-grams hit = 1  (0.16%)
4 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article303.text
Will force inclusive back-off from OOVs.
Perplexity = 15.05, Entropy = 3.91 bits
Computation based on 866 words.
Number of 3-grams hit = 860  (99.31%)
Number of 2-grams hit = 5  (0.58%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article304.text
Will force inclusive back-off from OOVs.
Perplexity = 13.97, Entropy = 3.80 bits
Computation based on 402 words.
Number of 3-grams hit = 398  (99.00%)
Number of 2-grams hit = 3  (0.75%)
Number of 1-grams hit = 1  (0.25%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article305.text
Will force inclusive back-off from OOVs.
Perplexity = 18.13, Entropy = 4.18 bits
Computation based on 300 words.
Number of 3-grams hit = 295  (98.33%)
Number of 2-grams hit = 4  (1.33%)
Number of 1-grams hit = 1  (0.33%)
3 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article306.text
Will force inclusive back-off from OOVs.
Perplexity = 17.15, Entropy = 4.10 bits
Computation based on 586 words.
Number of 3-grams hit = 579  (98.81%)
Number of 2-grams hit = 6  (1.02%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article307.text
Will force inclusive back-off from OOVs.
Perplexity = 15.44, Entropy = 3.95 bits
Computation based on 512 words.
Number of 3-grams hit = 510  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article308.text
Will force inclusive back-off from OOVs.
Perplexity = 18.08, Entropy = 4.18 bits
Computation based on 393 words.
Number of 3-grams hit = 390  (99.24%)
Number of 2-grams hit = 2  (0.51%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article309.text
Will force inclusive back-off from OOVs.
Perplexity = 23.98, Entropy = 4.58 bits
Computation based on 238 words.
Number of 3-grams hit = 234  (98.32%)
Number of 2-grams hit = 3  (1.26%)
Number of 1-grams hit = 1  (0.42%)
3 OOVs (1.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article310.text
Will force inclusive back-off from OOVs.
Perplexity = 17.07, Entropy = 4.09 bits
Computation based on 483 words.
Number of 3-grams hit = 480  (99.38%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article311.text
Will force inclusive back-off from OOVs.
Perplexity = 18.21, Entropy = 4.19 bits
Computation based on 465 words.
Number of 3-grams hit = 461  (99.14%)
Number of 2-grams hit = 3  (0.65%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article312.text
Will force inclusive back-off from OOVs.
Perplexity = 15.99, Entropy = 4.00 bits
Computation based on 591 words.
Number of 3-grams hit = 582  (98.48%)
Number of 2-grams hit = 8  (1.35%)
Number of 1-grams hit = 1  (0.17%)
7 OOVs (1.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article313.text
Will force inclusive back-off from OOVs.
Perplexity = 16.15, Entropy = 4.01 bits
Computation based on 476 words.
Number of 3-grams hit = 473  (99.37%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article314.text
Will force inclusive back-off from OOVs.
Perplexity = 19.76, Entropy = 4.30 bits
Computation based on 3293 words.
Number of 3-grams hit = 3288  (99.85%)
Number of 2-grams hit = 4  (0.12%)
Number of 1-grams hit = 1  (0.03%)
3 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article315.text
Will force inclusive back-off from OOVs.
Perplexity = 16.95, Entropy = 4.08 bits
Computation based on 1089 words.
Number of 3-grams hit = 1084  (99.54%)
Number of 2-grams hit = 4  (0.37%)
Number of 1-grams hit = 1  (0.09%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article316.text
Will force inclusive back-off from OOVs.
Perplexity = 16.29, Entropy = 4.03 bits
Computation based on 328 words.
Number of 3-grams hit = 323  (98.48%)
Number of 2-grams hit = 4  (1.22%)
Number of 1-grams hit = 1  (0.30%)
3 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article317.text
Will force inclusive back-off from OOVs.
Perplexity = 16.40, Entropy = 4.04 bits
Computation based on 624 words.
Number of 3-grams hit = 620  (99.36%)
Number of 2-grams hit = 3  (0.48%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article318.text
Will force inclusive back-off from OOVs.
Perplexity = 16.97, Entropy = 4.09 bits
Computation based on 3559 words.
Number of 3-grams hit = 3542  (99.52%)
Number of 2-grams hit = 16  (0.45%)
Number of 1-grams hit = 1  (0.03%)
15 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article319.text
Will force inclusive back-off from OOVs.
Perplexity = 13.62, Entropy = 3.77 bits
Computation based on 596 words.
Number of 3-grams hit = 591  (99.16%)
Number of 2-grams hit = 4  (0.67%)
Number of 1-grams hit = 1  (0.17%)
3 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article320.text
Will force inclusive back-off from OOVs.
Perplexity = 12.83, Entropy = 3.68 bits
Computation based on 375 words.
Number of 3-grams hit = 371  (98.93%)
Number of 2-grams hit = 3  (0.80%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article321.text
Will force inclusive back-off from OOVs.
Perplexity = 18.85, Entropy = 4.24 bits
Computation based on 1066 words.
Number of 3-grams hit = 1055  (98.97%)
Number of 2-grams hit = 10  (0.94%)
Number of 1-grams hit = 1  (0.09%)
10 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article322.text
Will force inclusive back-off from OOVs.
Perplexity = 18.82, Entropy = 4.23 bits
Computation based on 1676 words.
Number of 3-grams hit = 1669  (99.58%)
Number of 2-grams hit = 6  (0.36%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article323.text
Will force inclusive back-off from OOVs.
Perplexity = 14.79, Entropy = 3.89 bits
Computation based on 816 words.
Number of 3-grams hit = 810  (99.26%)
Number of 2-grams hit = 5  (0.61%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article324.text
Will force inclusive back-off from OOVs.
Perplexity = 18.61, Entropy = 4.22 bits
Computation based on 1652 words.
Number of 3-grams hit = 1645  (99.58%)
Number of 2-grams hit = 6  (0.36%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article325.text
Will force inclusive back-off from OOVs.
Perplexity = 14.98, Entropy = 3.90 bits
Computation based on 1122 words.
Number of 3-grams hit = 1116  (99.47%)
Number of 2-grams hit = 5  (0.45%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article326.text
Will force inclusive back-off from OOVs.
Perplexity = 17.71, Entropy = 4.15 bits
Computation based on 603 words.
Number of 3-grams hit = 593  (98.34%)
Number of 2-grams hit = 9  (1.49%)
Number of 1-grams hit = 1  (0.17%)
8 OOVs (1.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article327.text
Will force inclusive back-off from OOVs.
Perplexity = 15.64, Entropy = 3.97 bits
Computation based on 1127 words.
Number of 3-grams hit = 1122  (99.56%)
Number of 2-grams hit = 4  (0.35%)
Number of 1-grams hit = 1  (0.09%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article328.text
Will force inclusive back-off from OOVs.
Perplexity = 18.08, Entropy = 4.18 bits
Computation based on 249 words.
Number of 3-grams hit = 245  (98.39%)
Number of 2-grams hit = 3  (1.20%)
Number of 1-grams hit = 1  (0.40%)
2 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article329.text
Will force inclusive back-off from OOVs.
Perplexity = 16.73, Entropy = 4.06 bits
Computation based on 412 words.
Number of 3-grams hit = 409  (99.27%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article330.text
Will force inclusive back-off from OOVs.
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 1058 words.
Number of 3-grams hit = 1055  (99.72%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article331.text
Will force inclusive back-off from OOVs.
Perplexity = 18.17, Entropy = 4.18 bits
Computation based on 1950 words.
Number of 3-grams hit = 1940  (99.49%)
Number of 2-grams hit = 9  (0.46%)
Number of 1-grams hit = 1  (0.05%)
8 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article332.text
Will force inclusive back-off from OOVs.
Perplexity = 19.43, Entropy = 4.28 bits
Computation based on 572 words.
Number of 3-grams hit = 566  (98.95%)
Number of 2-grams hit = 5  (0.87%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article333.text
Will force inclusive back-off from OOVs.
Perplexity = 15.95, Entropy = 4.00 bits
Computation based on 2140 words.
Number of 3-grams hit = 2125  (99.30%)
Number of 2-grams hit = 14  (0.65%)
Number of 1-grams hit = 1  (0.05%)
13 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article334.text
Will force inclusive back-off from OOVs.
Perplexity = 18.05, Entropy = 4.17 bits
Computation based on 958 words.
Number of 3-grams hit = 949  (99.06%)
Number of 2-grams hit = 8  (0.84%)
Number of 1-grams hit = 1  (0.10%)
7 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article335.text
Will force inclusive back-off from OOVs.
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 1757 words.
Number of 3-grams hit = 1751  (99.66%)
Number of 2-grams hit = 5  (0.28%)
Number of 1-grams hit = 1  (0.06%)
4 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article336.text
Will force inclusive back-off from OOVs.
Perplexity = 17.39, Entropy = 4.12 bits
Computation based on 1140 words.
Number of 3-grams hit = 1134  (99.47%)
Number of 2-grams hit = 5  (0.44%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article337.text
Will force inclusive back-off from OOVs.
Perplexity = 19.04, Entropy = 4.25 bits
Computation based on 401 words.
Number of 3-grams hit = 399  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article338.text
Will force inclusive back-off from OOVs.
Perplexity = 18.30, Entropy = 4.19 bits
Computation based on 3561 words.
Number of 3-grams hit = 3548  (99.63%)
Number of 2-grams hit = 12  (0.34%)
Number of 1-grams hit = 1  (0.03%)
11 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article339.text
Will force inclusive back-off from OOVs.
Perplexity = 17.80, Entropy = 4.15 bits
Computation based on 374 words.
Number of 3-grams hit = 370  (98.93%)
Number of 2-grams hit = 3  (0.80%)
Number of 1-grams hit = 1  (0.27%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article340.text
Will force inclusive back-off from OOVs.
Perplexity = 16.55, Entropy = 4.05 bits
Computation based on 1149 words.
Number of 3-grams hit = 1142  (99.39%)
Number of 2-grams hit = 6  (0.52%)
Number of 1-grams hit = 1  (0.09%)
5 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article341.text
Will force inclusive back-off from OOVs.
Perplexity = 17.29, Entropy = 4.11 bits
Computation based on 1370 words.
Number of 3-grams hit = 1364  (99.56%)
Number of 2-grams hit = 5  (0.36%)
Number of 1-grams hit = 1  (0.07%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article342.text
Will force inclusive back-off from OOVs.
Perplexity = 20.01, Entropy = 4.32 bits
Computation based on 1450 words.
Number of 3-grams hit = 1447  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
1 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article343.text
Will force inclusive back-off from OOVs.
Perplexity = 18.57, Entropy = 4.21 bits
Computation based on 478 words.
Number of 3-grams hit = 475  (99.37%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article344.text
Will force inclusive back-off from OOVs.
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 578 words.
Number of 3-grams hit = 573  (99.13%)
Number of 2-grams hit = 4  (0.69%)
Number of 1-grams hit = 1  (0.17%)
3 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article345.text
Will force inclusive back-off from OOVs.
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 3148 words.
Number of 3-grams hit = 3131  (99.46%)
Number of 2-grams hit = 16  (0.51%)
Number of 1-grams hit = 1  (0.03%)
16 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article346.text
Will force inclusive back-off from OOVs.
Perplexity = 17.99, Entropy = 4.17 bits
Computation based on 1571 words.
Number of 3-grams hit = 1555  (98.98%)
Number of 2-grams hit = 15  (0.95%)
Number of 1-grams hit = 1  (0.06%)
14 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article347.text
Will force inclusive back-off from OOVs.
Perplexity = 16.59, Entropy = 4.05 bits
Computation based on 642 words.
Number of 3-grams hit = 633  (98.60%)
Number of 2-grams hit = 8  (1.25%)
Number of 1-grams hit = 1  (0.16%)
7 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article348.text
Will force inclusive back-off from OOVs.
Perplexity = 17.96, Entropy = 4.17 bits
Computation based on 2084 words.
Number of 3-grams hit = 2068  (99.23%)
Number of 2-grams hit = 15  (0.72%)
Number of 1-grams hit = 1  (0.05%)
14 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article349.text
Will force inclusive back-off from OOVs.
Perplexity = 19.17, Entropy = 4.26 bits
Computation based on 1935 words.
Number of 3-grams hit = 1927  (99.59%)
Number of 2-grams hit = 7  (0.36%)
Number of 1-grams hit = 1  (0.05%)
6 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article350.text
Will force inclusive back-off from OOVs.
Perplexity = 17.51, Entropy = 4.13 bits
Computation based on 361 words.
Number of 3-grams hit = 359  (99.45%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article351.text
Will force inclusive back-off from OOVs.
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 429 words.
Number of 3-grams hit = 424  (98.83%)
Number of 2-grams hit = 4  (0.93%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article352.text
Will force inclusive back-off from OOVs.
Perplexity = 15.81, Entropy = 3.98 bits
Computation based on 449 words.
Number of 3-grams hit = 445  (99.11%)
Number of 2-grams hit = 3  (0.67%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article353.text
Will force inclusive back-off from OOVs.
Perplexity = 16.40, Entropy = 4.04 bits
Computation based on 425 words.
Number of 3-grams hit = 421  (99.06%)
Number of 2-grams hit = 3  (0.71%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article354.text
Will force inclusive back-off from OOVs.
Perplexity = 16.93, Entropy = 4.08 bits
Computation based on 506 words.
Number of 3-grams hit = 502  (99.21%)
Number of 2-grams hit = 3  (0.59%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article355.text
Will force inclusive back-off from OOVs.
Perplexity = 14.25, Entropy = 3.83 bits
Computation based on 411 words.
Number of 3-grams hit = 407  (99.03%)
Number of 2-grams hit = 3  (0.73%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article356.text
Will force inclusive back-off from OOVs.
Perplexity = 14.49, Entropy = 3.86 bits
Computation based on 708 words.
Number of 3-grams hit = 705  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article357.text
Will force inclusive back-off from OOVs.
Perplexity = 20.59, Entropy = 4.36 bits
Computation based on 755 words.
Number of 3-grams hit = 753  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article358.text
Will force inclusive back-off from OOVs.
Perplexity = 17.62, Entropy = 4.14 bits
Computation based on 1189 words.
Number of 3-grams hit = 1179  (99.16%)
Number of 2-grams hit = 9  (0.76%)
Number of 1-grams hit = 1  (0.08%)
8 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article359.text
Will force inclusive back-off from OOVs.
Perplexity = 16.44, Entropy = 4.04 bits
Computation based on 1266 words.
Number of 3-grams hit = 1260  (99.53%)
Number of 2-grams hit = 5  (0.39%)
Number of 1-grams hit = 1  (0.08%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article360.text
Will force inclusive back-off from OOVs.
Perplexity = 16.58, Entropy = 4.05 bits
Computation based on 710 words.
Number of 3-grams hit = 706  (99.44%)
Number of 2-grams hit = 3  (0.42%)
Number of 1-grams hit = 1  (0.14%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article361.text
Will force inclusive back-off from OOVs.
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 1073 words.
Number of 3-grams hit = 1061  (98.88%)
Number of 2-grams hit = 11  (1.03%)
Number of 1-grams hit = 1  (0.09%)
12 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article362.text
Will force inclusive back-off from OOVs.
Perplexity = 19.53, Entropy = 4.29 bits
Computation based on 1414 words.
Number of 3-grams hit = 1403  (99.22%)
Number of 2-grams hit = 10  (0.71%)
Number of 1-grams hit = 1  (0.07%)
9 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article363.text
Will force inclusive back-off from OOVs.
Perplexity = 17.50, Entropy = 4.13 bits
Computation based on 1150 words.
Number of 3-grams hit = 1141  (99.22%)
Number of 2-grams hit = 8  (0.70%)
Number of 1-grams hit = 1  (0.09%)
7 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article364.text
Will force inclusive back-off from OOVs.
Perplexity = 15.65, Entropy = 3.97 bits
Computation based on 328 words.
Number of 3-grams hit = 325  (99.09%)
Number of 2-grams hit = 2  (0.61%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article365.text
Will force inclusive back-off from OOVs.
Perplexity = 17.90, Entropy = 4.16 bits
Computation based on 401 words.
Number of 3-grams hit = 393  (98.00%)
Number of 2-grams hit = 7  (1.75%)
Number of 1-grams hit = 1  (0.25%)
6 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article366.text
Will force inclusive back-off from OOVs.
Perplexity = 16.43, Entropy = 4.04 bits
Computation based on 348 words.
Number of 3-grams hit = 343  (98.56%)
Number of 2-grams hit = 4  (1.15%)
Number of 1-grams hit = 1  (0.29%)
3 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article367.text
Will force inclusive back-off from OOVs.
Perplexity = 19.63, Entropy = 4.29 bits
Computation based on 910 words.
Number of 3-grams hit = 907  (99.67%)
Number of 2-grams hit = 2  (0.22%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article368.text
Will force inclusive back-off from OOVs.
Perplexity = 13.21, Entropy = 3.72 bits
Computation based on 322 words.
Number of 3-grams hit = 320  (99.38%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article369.text
Will force inclusive back-off from OOVs.
Perplexity = 18.98, Entropy = 4.25 bits
Computation based on 313 words.
Number of 3-grams hit = 308  (98.40%)
Number of 2-grams hit = 4  (1.28%)
Number of 1-grams hit = 1  (0.32%)
3 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article370.text
Will force inclusive back-off from OOVs.
Perplexity = 18.33, Entropy = 4.20 bits
Computation based on 1637 words.
Number of 3-grams hit = 1625  (99.27%)
Number of 2-grams hit = 11  (0.67%)
Number of 1-grams hit = 1  (0.06%)
10 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article371.text
Will force inclusive back-off from OOVs.
Perplexity = 19.13, Entropy = 4.26 bits
Computation based on 259 words.
Number of 3-grams hit = 255  (98.46%)
Number of 2-grams hit = 3  (1.16%)
Number of 1-grams hit = 1  (0.39%)
3 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article372.text
Will force inclusive back-off from OOVs.
Perplexity = 18.01, Entropy = 4.17 bits
Computation based on 421 words.
Number of 3-grams hit = 419  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article373.text
Will force inclusive back-off from OOVs.
Perplexity = 18.30, Entropy = 4.19 bits
Computation based on 503 words.
Number of 3-grams hit = 499  (99.20%)
Number of 2-grams hit = 3  (0.60%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article374.text
Will force inclusive back-off from OOVs.
Perplexity = 17.70, Entropy = 4.15 bits
Computation based on 1509 words.
Number of 3-grams hit = 1493  (98.94%)
Number of 2-grams hit = 15  (0.99%)
Number of 1-grams hit = 1  (0.07%)
14 OOVs (0.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article375.text
Will force inclusive back-off from OOVs.
Perplexity = 20.37, Entropy = 4.35 bits
Computation based on 1610 words.
Number of 3-grams hit = 1607  (99.81%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
1 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article376.text
Will force inclusive back-off from OOVs.
Perplexity = 16.55, Entropy = 4.05 bits
Computation based on 561 words.
Number of 3-grams hit = 557  (99.29%)
Number of 2-grams hit = 3  (0.53%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article377.text
Will force inclusive back-off from OOVs.
Perplexity = 16.67, Entropy = 4.06 bits
Computation based on 740 words.
Number of 3-grams hit = 736  (99.46%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article378.text
Will force inclusive back-off from OOVs.
Perplexity = 14.34, Entropy = 3.84 bits
Computation based on 621 words.
Number of 3-grams hit = 618  (99.52%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article379.text
Will force inclusive back-off from OOVs.
Perplexity = 17.65, Entropy = 4.14 bits
Computation based on 2555 words.
Number of 3-grams hit = 2540  (99.41%)
Number of 2-grams hit = 14  (0.55%)
Number of 1-grams hit = 1  (0.04%)
13 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article380.text
Will force inclusive back-off from OOVs.
Perplexity = 15.82, Entropy = 3.98 bits
Computation based on 343 words.
Number of 3-grams hit = 338  (98.54%)
Number of 2-grams hit = 4  (1.17%)
Number of 1-grams hit = 1  (0.29%)
3 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article381.text
Will force inclusive back-off from OOVs.
Perplexity = 16.92, Entropy = 4.08 bits
Computation based on 626 words.
Number of 3-grams hit = 622  (99.36%)
Number of 2-grams hit = 3  (0.48%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article382.text
Will force inclusive back-off from OOVs.
Perplexity = 18.10, Entropy = 4.18 bits
Computation based on 1297 words.
Number of 3-grams hit = 1288  (99.31%)
Number of 2-grams hit = 8  (0.62%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article383.text
Will force inclusive back-off from OOVs.
Perplexity = 16.93, Entropy = 4.08 bits
Computation based on 1035 words.
Number of 3-grams hit = 1029  (99.42%)
Number of 2-grams hit = 5  (0.48%)
Number of 1-grams hit = 1  (0.10%)
4 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article384.text
Will force inclusive back-off from OOVs.
Perplexity = 16.44, Entropy = 4.04 bits
Computation based on 1000 words.
Number of 3-grams hit = 991  (99.10%)
Number of 2-grams hit = 8  (0.80%)
Number of 1-grams hit = 1  (0.10%)
8 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article385.text
Will force inclusive back-off from OOVs.
Perplexity = 17.61, Entropy = 4.14 bits
Computation based on 2601 words.
Number of 3-grams hit = 2594  (99.73%)
Number of 2-grams hit = 6  (0.23%)
Number of 1-grams hit = 1  (0.04%)
5 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article386.text
Will force inclusive back-off from OOVs.
Perplexity = 16.24, Entropy = 4.02 bits
Computation based on 416 words.
Number of 3-grams hit = 413  (99.28%)
Number of 2-grams hit = 2  (0.48%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article387.text
Will force inclusive back-off from OOVs.
Perplexity = 18.57, Entropy = 4.21 bits
Computation based on 1212 words.
Number of 3-grams hit = 1207  (99.59%)
Number of 2-grams hit = 4  (0.33%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article388.text
Will force inclusive back-off from OOVs.
Perplexity = 17.18, Entropy = 4.10 bits
Computation based on 4315 words.
Number of 3-grams hit = 4289  (99.40%)
Number of 2-grams hit = 25  (0.58%)
Number of 1-grams hit = 1  (0.02%)
24 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article389.text
Will force inclusive back-off from OOVs.
Perplexity = 15.29, Entropy = 3.93 bits
Computation based on 1118 words.
Number of 3-grams hit = 1113  (99.55%)
Number of 2-grams hit = 4  (0.36%)
Number of 1-grams hit = 1  (0.09%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article390.text
Will force inclusive back-off from OOVs.
Perplexity = 17.78, Entropy = 4.15 bits
Computation based on 1198 words.
Number of 3-grams hit = 1194  (99.67%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article391.text
Will force inclusive back-off from OOVs.
Perplexity = 18.18, Entropy = 4.18 bits
Computation based on 3534 words.
Number of 3-grams hit = 3525  (99.75%)
Number of 2-grams hit = 8  (0.23%)
Number of 1-grams hit = 1  (0.03%)
7 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article392.text
Will force inclusive back-off from OOVs.
Perplexity = 20.23, Entropy = 4.34 bits
Computation based on 229 words.
Number of 3-grams hit = 225  (98.25%)
Number of 2-grams hit = 3  (1.31%)
Number of 1-grams hit = 1  (0.44%)
2 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article393.text
Will force inclusive back-off from OOVs.
Perplexity = 14.44, Entropy = 3.85 bits
Computation based on 484 words.
Number of 3-grams hit = 480  (99.17%)
Number of 2-grams hit = 3  (0.62%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article394.text
Will force inclusive back-off from OOVs.
Perplexity = 16.93, Entropy = 4.08 bits
Computation based on 391 words.
Number of 3-grams hit = 385  (98.47%)
Number of 2-grams hit = 5  (1.28%)
Number of 1-grams hit = 1  (0.26%)
4 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article395.text
Will force inclusive back-off from OOVs.
Perplexity = 17.19, Entropy = 4.10 bits
Computation based on 307 words.
Number of 3-grams hit = 305  (99.35%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article396.text
Will force inclusive back-off from OOVs.
Perplexity = 19.36, Entropy = 4.27 bits
Computation based on 3050 words.
Number of 3-grams hit = 3046  (99.87%)
Number of 2-grams hit = 3  (0.10%)
Number of 1-grams hit = 1  (0.03%)
2 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article397.text
Will force inclusive back-off from OOVs.
Perplexity = 16.69, Entropy = 4.06 bits
Computation based on 3643 words.
Number of 3-grams hit = 3613  (99.18%)
Number of 2-grams hit = 29  (0.80%)
Number of 1-grams hit = 1  (0.03%)
30 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article398.text
Will force inclusive back-off from OOVs.
Perplexity = 17.35, Entropy = 4.12 bits
Computation based on 818 words.
Number of 3-grams hit = 814  (99.51%)
Number of 2-grams hit = 3  (0.37%)
Number of 1-grams hit = 1  (0.12%)
2 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article399.text
Will force inclusive back-off from OOVs.
Perplexity = 18.00, Entropy = 4.17 bits
Computation based on 3649 words.
Number of 3-grams hit = 3625  (99.34%)
Number of 2-grams hit = 23  (0.63%)
Number of 1-grams hit = 1  (0.03%)
23 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article400.text
Will force inclusive back-off from OOVs.
Perplexity = 18.39, Entropy = 4.20 bits
Computation based on 580 words.
Number of 3-grams hit = 573  (98.79%)
Number of 2-grams hit = 6  (1.03%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article401.text
Will force inclusive back-off from OOVs.
Perplexity = 14.68, Entropy = 3.88 bits
Computation based on 661 words.
Number of 3-grams hit = 657  (99.39%)
Number of 2-grams hit = 3  (0.45%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article402.text
Will force inclusive back-off from OOVs.
Perplexity = 16.38, Entropy = 4.03 bits
Computation based on 3663 words.
Number of 3-grams hit = 3654  (99.75%)
Number of 2-grams hit = 8  (0.22%)
Number of 1-grams hit = 1  (0.03%)
7 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article403.text
Will force inclusive back-off from OOVs.
Perplexity = 17.63, Entropy = 4.14 bits
Computation based on 619 words.
Number of 3-grams hit = 614  (99.19%)
Number of 2-grams hit = 4  (0.65%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article404.text
Will force inclusive back-off from OOVs.
Perplexity = 15.95, Entropy = 4.00 bits
Computation based on 211 words.
Number of 3-grams hit = 207  (98.10%)
Number of 2-grams hit = 3  (1.42%)
Number of 1-grams hit = 1  (0.47%)
3 OOVs (1.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article405.text
Will force inclusive back-off from OOVs.
Perplexity = 13.17, Entropy = 3.72 bits
Computation based on 159 words.
Number of 3-grams hit = 157  (98.74%)
Number of 2-grams hit = 1  (0.63%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article406.text
Will force inclusive back-off from OOVs.
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 981 words.
Number of 3-grams hit = 974  (99.29%)
Number of 2-grams hit = 6  (0.61%)
Number of 1-grams hit = 1  (0.10%)
5 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article407.text
Will force inclusive back-off from OOVs.
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 694 words.
Number of 3-grams hit = 689  (99.28%)
Number of 2-grams hit = 4  (0.58%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article408.text
Will force inclusive back-off from OOVs.
Perplexity = 15.06, Entropy = 3.91 bits
Computation based on 437 words.
Number of 3-grams hit = 432  (98.86%)
Number of 2-grams hit = 4  (0.92%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article409.text
Will force inclusive back-off from OOVs.
Perplexity = 18.45, Entropy = 4.21 bits
Computation based on 540 words.
Number of 3-grams hit = 534  (98.89%)
Number of 2-grams hit = 5  (0.93%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article410.text
Will force inclusive back-off from OOVs.
Perplexity = 20.69, Entropy = 4.37 bits
Computation based on 234 words.
Number of 3-grams hit = 231  (98.72%)
Number of 2-grams hit = 2  (0.85%)
Number of 1-grams hit = 1  (0.43%)
1 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article411.text
Will force inclusive back-off from OOVs.
Perplexity = 14.25, Entropy = 3.83 bits
Computation based on 438 words.
Number of 3-grams hit = 436  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article412.text
Will force inclusive back-off from OOVs.
Perplexity = 19.61, Entropy = 4.29 bits
Computation based on 4561 words.
Number of 3-grams hit = 4552  (99.80%)
Number of 2-grams hit = 8  (0.18%)
Number of 1-grams hit = 1  (0.02%)
7 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article413.text
Will force inclusive back-off from OOVs.
Perplexity = 17.26, Entropy = 4.11 bits
Computation based on 3839 words.
Number of 3-grams hit = 3810  (99.24%)
Number of 2-grams hit = 28  (0.73%)
Number of 1-grams hit = 1  (0.03%)
28 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article414.text
Will force inclusive back-off from OOVs.
Perplexity = 17.27, Entropy = 4.11 bits
Computation based on 398 words.
Number of 3-grams hit = 395  (99.25%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article415.text
Will force inclusive back-off from OOVs.
Perplexity = 15.70, Entropy = 3.97 bits
Computation based on 477 words.
Number of 3-grams hit = 472  (98.95%)
Number of 2-grams hit = 4  (0.84%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article416.text
Will force inclusive back-off from OOVs.
Perplexity = 15.03, Entropy = 3.91 bits
Computation based on 591 words.
Number of 3-grams hit = 589  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article417.text
Will force inclusive back-off from OOVs.
Perplexity = 19.05, Entropy = 4.25 bits
Computation based on 853 words.
Number of 3-grams hit = 850  (99.65%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.12%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article418.text
Will force inclusive back-off from OOVs.
Perplexity = 13.05, Entropy = 3.71 bits
Computation based on 809 words.
Number of 3-grams hit = 804  (99.38%)
Number of 2-grams hit = 4  (0.49%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article419.text
Will force inclusive back-off from OOVs.
Perplexity = 14.56, Entropy = 3.86 bits
Computation based on 551 words.
Number of 3-grams hit = 546  (99.09%)
Number of 2-grams hit = 4  (0.73%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article420.text
Will force inclusive back-off from OOVs.
Perplexity = 17.17, Entropy = 4.10 bits
Computation based on 611 words.
Number of 3-grams hit = 609  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article421.text
Will force inclusive back-off from OOVs.
Perplexity = 17.74, Entropy = 4.15 bits
Computation based on 815 words.
Number of 3-grams hit = 807  (99.02%)
Number of 2-grams hit = 7  (0.86%)
Number of 1-grams hit = 1  (0.12%)
6 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article422.text
Will force inclusive back-off from OOVs.
Perplexity = 16.84, Entropy = 4.07 bits
Computation based on 554 words.
Number of 3-grams hit = 547  (98.74%)
Number of 2-grams hit = 6  (1.08%)
Number of 1-grams hit = 1  (0.18%)
5 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article423.text
Will force inclusive back-off from OOVs.
Perplexity = 15.18, Entropy = 3.92 bits
Computation based on 612 words.
Number of 3-grams hit = 606  (99.02%)
Number of 2-grams hit = 5  (0.82%)
Number of 1-grams hit = 1  (0.16%)
4 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article424.text
Will force inclusive back-off from OOVs.
Perplexity = 17.58, Entropy = 4.14 bits
Computation based on 528 words.
Number of 3-grams hit = 525  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article425.text
Will force inclusive back-off from OOVs.
Perplexity = 16.43, Entropy = 4.04 bits
Computation based on 356 words.
Number of 3-grams hit = 353  (99.16%)
Number of 2-grams hit = 2  (0.56%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article426.text
Will force inclusive back-off from OOVs.
Perplexity = 17.69, Entropy = 4.14 bits
Computation based on 482 words.
Number of 3-grams hit = 477  (98.96%)
Number of 2-grams hit = 4  (0.83%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article427.text
Will force inclusive back-off from OOVs.
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 789 words.
Number of 3-grams hit = 786  (99.62%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article428.text
Will force inclusive back-off from OOVs.
Perplexity = 16.27, Entropy = 4.02 bits
Computation based on 624 words.
Number of 3-grams hit = 616  (98.72%)
Number of 2-grams hit = 7  (1.12%)
Number of 1-grams hit = 1  (0.16%)
6 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article429.text
Will force inclusive back-off from OOVs.
Perplexity = 19.19, Entropy = 4.26 bits
Computation based on 769 words.
Number of 3-grams hit = 764  (99.35%)
Number of 2-grams hit = 4  (0.52%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article430.text
Will force inclusive back-off from OOVs.
Perplexity = 16.07, Entropy = 4.01 bits
Computation based on 640 words.
Number of 3-grams hit = 637  (99.53%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article431.text
Will force inclusive back-off from OOVs.
Perplexity = 18.69, Entropy = 4.22 bits
Computation based on 566 words.
Number of 3-grams hit = 561  (99.12%)
Number of 2-grams hit = 4  (0.71%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article432.text
Will force inclusive back-off from OOVs.
Perplexity = 14.88, Entropy = 3.90 bits
Computation based on 352 words.
Number of 3-grams hit = 349  (99.15%)
Number of 2-grams hit = 2  (0.57%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article433.text
Will force inclusive back-off from OOVs.
Perplexity = 17.67, Entropy = 4.14 bits
Computation based on 967 words.
Number of 3-grams hit = 960  (99.28%)
Number of 2-grams hit = 6  (0.62%)
Number of 1-grams hit = 1  (0.10%)
5 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article434.text
Will force inclusive back-off from OOVs.
Perplexity = 14.44, Entropy = 3.85 bits
Computation based on 383 words.
Number of 3-grams hit = 381  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article435.text
Will force inclusive back-off from OOVs.
Perplexity = 18.42, Entropy = 4.20 bits
Computation based on 718 words.
Number of 3-grams hit = 715  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article436.text
Will force inclusive back-off from OOVs.
Perplexity = 16.05, Entropy = 4.00 bits
Computation based on 531 words.
Number of 3-grams hit = 528  (99.44%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article437.text
Will force inclusive back-off from OOVs.
Perplexity = 17.27, Entropy = 4.11 bits
Computation based on 1007 words.
Number of 3-grams hit = 1003  (99.60%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article438.text
Will force inclusive back-off from OOVs.
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 288 words.
Number of 3-grams hit = 283  (98.26%)
Number of 2-grams hit = 4  (1.39%)
Number of 1-grams hit = 1  (0.35%)
3 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article439.text
Will force inclusive back-off from OOVs.
Perplexity = 16.25, Entropy = 4.02 bits
Computation based on 520 words.
Number of 3-grams hit = 514  (98.85%)
Number of 2-grams hit = 5  (0.96%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article440.text
Will force inclusive back-off from OOVs.
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 687 words.
Number of 3-grams hit = 683  (99.42%)
Number of 2-grams hit = 3  (0.44%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article441.text
Will force inclusive back-off from OOVs.
Perplexity = 15.74, Entropy = 3.98 bits
Computation based on 410 words.
Number of 3-grams hit = 405  (98.78%)
Number of 2-grams hit = 4  (0.98%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article442.text
Will force inclusive back-off from OOVs.
Perplexity = 20.54, Entropy = 4.36 bits
Computation based on 10019 words.
Number of 3-grams hit = 10005  (99.86%)
Number of 2-grams hit = 13  (0.13%)
Number of 1-grams hit = 1  (0.01%)
12 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article443.text
Will force inclusive back-off from OOVs.
Perplexity = 17.67, Entropy = 4.14 bits
Computation based on 11770 words.
Number of 3-grams hit = 11686  (99.29%)
Number of 2-grams hit = 83  (0.71%)
Number of 1-grams hit = 1  (0.01%)
84 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article444.text
Will force inclusive back-off from OOVs.
Perplexity = 18.49, Entropy = 4.21 bits
Computation based on 650 words.
Number of 3-grams hit = 647  (99.54%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article445.text
Will force inclusive back-off from OOVs.
Perplexity = 17.70, Entropy = 4.15 bits
Computation based on 498 words.
Number of 3-grams hit = 493  (99.00%)
Number of 2-grams hit = 4  (0.80%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article446.text
Will force inclusive back-off from OOVs.
Perplexity = 16.10, Entropy = 4.01 bits
Computation based on 887 words.
Number of 3-grams hit = 878  (98.99%)
Number of 2-grams hit = 8  (0.90%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article447.text
Will force inclusive back-off from OOVs.
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 728 words.
Number of 3-grams hit = 723  (99.31%)
Number of 2-grams hit = 4  (0.55%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article448.text
Will force inclusive back-off from OOVs.
Perplexity = 20.80, Entropy = 4.38 bits
Computation based on 323 words.
Number of 3-grams hit = 320  (99.07%)
Number of 2-grams hit = 2  (0.62%)
Number of 1-grams hit = 1  (0.31%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article449.text
Will force inclusive back-off from OOVs.
Perplexity = 19.77, Entropy = 4.31 bits
Computation based on 483 words.
Number of 3-grams hit = 476  (98.55%)
Number of 2-grams hit = 6  (1.24%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article450.text
Will force inclusive back-off from OOVs.
Perplexity = 15.15, Entropy = 3.92 bits
Computation based on 356 words.
Number of 3-grams hit = 354  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article451.text
Will force inclusive back-off from OOVs.
Perplexity = 14.88, Entropy = 3.90 bits
Computation based on 574 words.
Number of 3-grams hit = 572  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article452.text
Will force inclusive back-off from OOVs.
Perplexity = 18.56, Entropy = 4.21 bits
Computation based on 237 words.
Number of 3-grams hit = 235  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article453.text
Will force inclusive back-off from OOVs.
Perplexity = 16.24, Entropy = 4.02 bits
Computation based on 625 words.
Number of 3-grams hit = 620  (99.20%)
Number of 2-grams hit = 4  (0.64%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article454.text
Will force inclusive back-off from OOVs.
Perplexity = 16.77, Entropy = 4.07 bits
Computation based on 542 words.
Number of 3-grams hit = 536  (98.89%)
Number of 2-grams hit = 5  (0.92%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article455.text
Will force inclusive back-off from OOVs.
Perplexity = 15.72, Entropy = 3.97 bits
Computation based on 380 words.
Number of 3-grams hit = 377  (99.21%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article456.text
Will force inclusive back-off from OOVs.
Perplexity = 16.55, Entropy = 4.05 bits
Computation based on 656 words.
Number of 3-grams hit = 653  (99.54%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article457.text
Will force inclusive back-off from OOVs.
Perplexity = 17.67, Entropy = 4.14 bits
Computation based on 333 words.
Number of 3-grams hit = 328  (98.50%)
Number of 2-grams hit = 4  (1.20%)
Number of 1-grams hit = 1  (0.30%)
4 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article458.text
Will force inclusive back-off from OOVs.
Perplexity = 18.56, Entropy = 4.21 bits
Computation based on 275 words.
Number of 3-grams hit = 272  (98.91%)
Number of 2-grams hit = 2  (0.73%)
Number of 1-grams hit = 1  (0.36%)
1 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article459.text
Will force inclusive back-off from OOVs.
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 696 words.
Number of 3-grams hit = 689  (98.99%)
Number of 2-grams hit = 6  (0.86%)
Number of 1-grams hit = 1  (0.14%)
5 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article460.text
Will force inclusive back-off from OOVs.
Perplexity = 18.84, Entropy = 4.24 bits
Computation based on 322 words.
Number of 3-grams hit = 315  (97.83%)
Number of 2-grams hit = 6  (1.86%)
Number of 1-grams hit = 1  (0.31%)
5 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article461.text
Will force inclusive back-off from OOVs.
Perplexity = 12.85, Entropy = 3.68 bits
Computation based on 419 words.
Number of 3-grams hit = 415  (99.05%)
Number of 2-grams hit = 3  (0.72%)
Number of 1-grams hit = 1  (0.24%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article462.text
Will force inclusive back-off from OOVs.
Perplexity = 16.34, Entropy = 4.03 bits
Computation based on 1399 words.
Number of 3-grams hit = 1386  (99.07%)
Number of 2-grams hit = 12  (0.86%)
Number of 1-grams hit = 1  (0.07%)
11 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article463.text
Will force inclusive back-off from OOVs.
Perplexity = 15.20, Entropy = 3.93 bits
Computation based on 551 words.
Number of 3-grams hit = 545  (98.91%)
Number of 2-grams hit = 5  (0.91%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article464.text
Will force inclusive back-off from OOVs.
Perplexity = 16.75, Entropy = 4.07 bits
Computation based on 438 words.
Number of 3-grams hit = 434  (99.09%)
Number of 2-grams hit = 3  (0.68%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article465.text
Will force inclusive back-off from OOVs.
Perplexity = 19.30, Entropy = 4.27 bits
Computation based on 1804 words.
Number of 3-grams hit = 1799  (99.72%)
Number of 2-grams hit = 4  (0.22%)
Number of 1-grams hit = 1  (0.06%)
3 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article466.text
Will force inclusive back-off from OOVs.
Perplexity = 18.09, Entropy = 4.18 bits
Computation based on 204 words.
Number of 3-grams hit = 201  (98.53%)
Number of 2-grams hit = 2  (0.98%)
Number of 1-grams hit = 1  (0.49%)
1 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article467.text
Will force inclusive back-off from OOVs.
Perplexity = 18.51, Entropy = 4.21 bits
Computation based on 1022 words.
Number of 3-grams hit = 1012  (99.02%)
Number of 2-grams hit = 9  (0.88%)
Number of 1-grams hit = 1  (0.10%)
8 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article468.text
Will force inclusive back-off from OOVs.
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 772 words.
Number of 3-grams hit = 764  (98.96%)
Number of 2-grams hit = 7  (0.91%)
Number of 1-grams hit = 1  (0.13%)
6 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article469.text
Will force inclusive back-off from OOVs.
Perplexity = 17.32, Entropy = 4.11 bits
Computation based on 355 words.
Number of 3-grams hit = 351  (98.87%)
Number of 2-grams hit = 3  (0.85%)
Number of 1-grams hit = 1  (0.28%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article470.text
Will force inclusive back-off from OOVs.
Perplexity = 13.75, Entropy = 3.78 bits
Computation based on 239 words.
Number of 3-grams hit = 236  (98.74%)
Number of 2-grams hit = 2  (0.84%)
Number of 1-grams hit = 1  (0.42%)
1 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article471.text
Will force inclusive back-off from OOVs.
Perplexity = 18.61, Entropy = 4.22 bits
Computation based on 1996 words.
Number of 3-grams hit = 1986  (99.50%)
Number of 2-grams hit = 9  (0.45%)
Number of 1-grams hit = 1  (0.05%)
8 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article472.text
Will force inclusive back-off from OOVs.
Perplexity = 16.85, Entropy = 4.07 bits
Computation based on 930 words.
Number of 3-grams hit = 922  (99.14%)
Number of 2-grams hit = 7  (0.75%)
Number of 1-grams hit = 1  (0.11%)
6 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article473.text
Will force inclusive back-off from OOVs.
Perplexity = 19.48, Entropy = 4.28 bits
Computation based on 217 words.
Number of 3-grams hit = 213  (98.16%)
Number of 2-grams hit = 3  (1.38%)
Number of 1-grams hit = 1  (0.46%)
2 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article474.text
Will force inclusive back-off from OOVs.
Perplexity = 17.32, Entropy = 4.11 bits
Computation based on 783 words.
Number of 3-grams hit = 780  (99.62%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article475.text
Will force inclusive back-off from OOVs.
Perplexity = 15.99, Entropy = 4.00 bits
Computation based on 540 words.
Number of 3-grams hit = 531  (98.33%)
Number of 2-grams hit = 8  (1.48%)
Number of 1-grams hit = 1  (0.19%)
8 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article476.text
Will force inclusive back-off from OOVs.
Perplexity = 15.62, Entropy = 3.97 bits
Computation based on 580 words.
Number of 3-grams hit = 574  (98.97%)
Number of 2-grams hit = 5  (0.86%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article477.text
Will force inclusive back-off from OOVs.
Perplexity = 17.14, Entropy = 4.10 bits
Computation based on 445 words.
Number of 3-grams hit = 442  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article478.text
Will force inclusive back-off from OOVs.
Perplexity = 16.48, Entropy = 4.04 bits
Computation based on 504 words.
Number of 3-grams hit = 499  (99.01%)
Number of 2-grams hit = 4  (0.79%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article479.text
Will force inclusive back-off from OOVs.
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 420 words.
Number of 3-grams hit = 415  (98.81%)
Number of 2-grams hit = 4  (0.95%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article480.text
Will force inclusive back-off from OOVs.
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 687 words.
Number of 3-grams hit = 683  (99.42%)
Number of 2-grams hit = 3  (0.44%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article481.text
Will force inclusive back-off from OOVs.
Perplexity = 17.07, Entropy = 4.09 bits
Computation based on 351 words.
Number of 3-grams hit = 349  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article482.text
Will force inclusive back-off from OOVs.
Perplexity = 18.16, Entropy = 4.18 bits
Computation based on 491 words.
Number of 3-grams hit = 485  (98.78%)
Number of 2-grams hit = 5  (1.02%)
Number of 1-grams hit = 1  (0.20%)
4 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article483.text
Will force inclusive back-off from OOVs.
Perplexity = 19.52, Entropy = 4.29 bits
Computation based on 1672 words.
Number of 3-grams hit = 1666  (99.64%)
Number of 2-grams hit = 5  (0.30%)
Number of 1-grams hit = 1  (0.06%)
4 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article484.text
Will force inclusive back-off from OOVs.
Perplexity = 16.27, Entropy = 4.02 bits
Computation based on 1418 words.
Number of 3-grams hit = 1402  (98.87%)
Number of 2-grams hit = 15  (1.06%)
Number of 1-grams hit = 1  (0.07%)
14 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article485.text
Will force inclusive back-off from OOVs.
Perplexity = 13.06, Entropy = 3.71 bits
Computation based on 376 words.
Number of 3-grams hit = 373  (99.20%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article486.text
Will force inclusive back-off from OOVs.
Perplexity = 19.02, Entropy = 4.25 bits
Computation based on 332 words.
Number of 3-grams hit = 329  (99.10%)
Number of 2-grams hit = 2  (0.60%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article487.text
Will force inclusive back-off from OOVs.
Perplexity = 17.43, Entropy = 4.12 bits
Computation based on 431 words.
Number of 3-grams hit = 429  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article488.text
Will force inclusive back-off from OOVs.
Perplexity = 19.07, Entropy = 4.25 bits
Computation based on 706 words.
Number of 3-grams hit = 700  (99.15%)
Number of 2-grams hit = 5  (0.71%)
Number of 1-grams hit = 1  (0.14%)
5 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article489.text
Will force inclusive back-off from OOVs.
Perplexity = 18.83, Entropy = 4.24 bits
Computation based on 545 words.
Number of 3-grams hit = 542  (99.45%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article490.text
Will force inclusive back-off from OOVs.
Perplexity = 17.11, Entropy = 4.10 bits
Computation based on 491 words.
Number of 3-grams hit = 487  (99.19%)
Number of 2-grams hit = 3  (0.61%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article491.text
Will force inclusive back-off from OOVs.
Perplexity = 16.31, Entropy = 4.03 bits
Computation based on 1386 words.
Number of 3-grams hit = 1375  (99.21%)
Number of 2-grams hit = 10  (0.72%)
Number of 1-grams hit = 1  (0.07%)
9 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article492.text
Will force inclusive back-off from OOVs.
Perplexity = 15.40, Entropy = 3.94 bits
Computation based on 434 words.
Number of 3-grams hit = 431  (99.31%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article493.text
Will force inclusive back-off from OOVs.
Perplexity = 14.91, Entropy = 3.90 bits
Computation based on 756 words.
Number of 3-grams hit = 751  (99.34%)
Number of 2-grams hit = 4  (0.53%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article494.text
Will force inclusive back-off from OOVs.
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 481 words.
Number of 3-grams hit = 474  (98.54%)
Number of 2-grams hit = 6  (1.25%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article495.text
Will force inclusive back-off from OOVs.
Perplexity = 15.22, Entropy = 3.93 bits
Computation based on 430 words.
Number of 3-grams hit = 428  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article496.text
Will force inclusive back-off from OOVs.
Perplexity = 14.95, Entropy = 3.90 bits
Computation based on 555 words.
Number of 3-grams hit = 550  (99.10%)
Number of 2-grams hit = 4  (0.72%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article497.text
Will force inclusive back-off from OOVs.
Perplexity = 18.18, Entropy = 4.18 bits
Computation based on 648 words.
Number of 3-grams hit = 642  (99.07%)
Number of 2-grams hit = 5  (0.77%)
Number of 1-grams hit = 1  (0.15%)
4 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article498.text
Will force inclusive back-off from OOVs.
Perplexity = 17.69, Entropy = 4.15 bits
Computation based on 1692 words.
Number of 3-grams hit = 1683  (99.47%)
Number of 2-grams hit = 8  (0.47%)
Number of 1-grams hit = 1  (0.06%)
7 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article499.text
Will force inclusive back-off from OOVs.
Perplexity = 19.63, Entropy = 4.30 bits
Computation based on 494 words.
Number of 3-grams hit = 489  (98.99%)
Number of 2-grams hit = 4  (0.81%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article500.text
Will force inclusive back-off from OOVs.
Perplexity = 15.28, Entropy = 3.93 bits
Computation based on 454 words.
Number of 3-grams hit = 449  (98.90%)
Number of 2-grams hit = 4  (0.88%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article501.text
Will force inclusive back-off from OOVs.
Perplexity = 15.85, Entropy = 3.99 bits
Computation based on 403 words.
Number of 3-grams hit = 400  (99.26%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article502.text
Will force inclusive back-off from OOVs.
Perplexity = 15.80, Entropy = 3.98 bits
Computation based on 505 words.
Number of 3-grams hit = 502  (99.41%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article503.text
Will force inclusive back-off from OOVs.
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 1011 words.
Number of 3-grams hit = 1002  (99.11%)
Number of 2-grams hit = 8  (0.79%)
Number of 1-grams hit = 1  (0.10%)
7 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article504.text
Will force inclusive back-off from OOVs.
Perplexity = 15.12, Entropy = 3.92 bits
Computation based on 612 words.
Number of 3-grams hit = 609  (99.51%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article505.text
Will force inclusive back-off from OOVs.
Perplexity = 14.74, Entropy = 3.88 bits
Computation based on 697 words.
Number of 3-grams hit = 690  (99.00%)
Number of 2-grams hit = 6  (0.86%)
Number of 1-grams hit = 1  (0.14%)
5 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article506.text
Will force inclusive back-off from OOVs.
Perplexity = 18.08, Entropy = 4.18 bits
Computation based on 535 words.
Number of 3-grams hit = 529  (98.88%)
Number of 2-grams hit = 5  (0.93%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article507.text
Will force inclusive back-off from OOVs.
Perplexity = 16.40, Entropy = 4.04 bits
Computation based on 1588 words.
Number of 3-grams hit = 1581  (99.56%)
Number of 2-grams hit = 6  (0.38%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article508.text
Will force inclusive back-off from OOVs.
Perplexity = 17.14, Entropy = 4.10 bits
Computation based on 656 words.
Number of 3-grams hit = 654  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article509.text
Will force inclusive back-off from OOVs.
Perplexity = 15.77, Entropy = 3.98 bits
Computation based on 350 words.
Number of 3-grams hit = 345  (98.57%)
Number of 2-grams hit = 4  (1.14%)
Number of 1-grams hit = 1  (0.29%)
3 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article510.text
Will force inclusive back-off from OOVs.
Perplexity = 16.40, Entropy = 4.04 bits
Computation based on 971 words.
Number of 3-grams hit = 968  (99.69%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.10%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article511.text
Will force inclusive back-off from OOVs.
Perplexity = 14.47, Entropy = 3.86 bits
Computation based on 629 words.
Number of 3-grams hit = 625  (99.36%)
Number of 2-grams hit = 3  (0.48%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article512.text
Will force inclusive back-off from OOVs.
Perplexity = 17.98, Entropy = 4.17 bits
Computation based on 1888 words.
Number of 3-grams hit = 1867  (98.89%)
Number of 2-grams hit = 20  (1.06%)
Number of 1-grams hit = 1  (0.05%)
20 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article513.text
Will force inclusive back-off from OOVs.
Perplexity = 12.60, Entropy = 3.65 bits
Computation based on 416 words.
Number of 3-grams hit = 414  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article514.text
Will force inclusive back-off from OOVs.
Perplexity = 16.82, Entropy = 4.07 bits
Computation based on 396 words.
Number of 3-grams hit = 393  (99.24%)
Number of 2-grams hit = 2  (0.51%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article515.text
Will force inclusive back-off from OOVs.
Perplexity = 14.92, Entropy = 3.90 bits
Computation based on 971 words.
Number of 3-grams hit = 966  (99.49%)
Number of 2-grams hit = 4  (0.41%)
Number of 1-grams hit = 1  (0.10%)
3 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article516.text
Will force inclusive back-off from OOVs.
Perplexity = 17.80, Entropy = 4.15 bits
Computation based on 410 words.
Number of 3-grams hit = 404  (98.54%)
Number of 2-grams hit = 5  (1.22%)
Number of 1-grams hit = 1  (0.24%)
4 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article517.text
Will force inclusive back-off from OOVs.
Perplexity = 16.60, Entropy = 4.05 bits
Computation based on 1054 words.
Number of 3-grams hit = 1044  (99.05%)
Number of 2-grams hit = 9  (0.85%)
Number of 1-grams hit = 1  (0.09%)
8 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article518.text
Will force inclusive back-off from OOVs.
Perplexity = 16.03, Entropy = 4.00 bits
Computation based on 587 words.
Number of 3-grams hit = 582  (99.15%)
Number of 2-grams hit = 4  (0.68%)
Number of 1-grams hit = 1  (0.17%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article519.text
Will force inclusive back-off from OOVs.
Perplexity = 16.24, Entropy = 4.02 bits
Computation based on 529 words.
Number of 3-grams hit = 525  (99.24%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article520.text
Will force inclusive back-off from OOVs.
Perplexity = 18.29, Entropy = 4.19 bits
Computation based on 609 words.
Number of 3-grams hit = 604  (99.18%)
Number of 2-grams hit = 4  (0.66%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article521.text
Will force inclusive back-off from OOVs.
Perplexity = 16.17, Entropy = 4.02 bits
Computation based on 518 words.
Number of 3-grams hit = 512  (98.84%)
Number of 2-grams hit = 5  (0.97%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article522.text
Will force inclusive back-off from OOVs.
Perplexity = 17.91, Entropy = 4.16 bits
Computation based on 2324 words.
Number of 3-grams hit = 2316  (99.66%)
Number of 2-grams hit = 7  (0.30%)
Number of 1-grams hit = 1  (0.04%)
6 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article523.text
Will force inclusive back-off from OOVs.
Perplexity = 17.22, Entropy = 4.11 bits
Computation based on 327 words.
Number of 3-grams hit = 325  (99.39%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article524.text
Will force inclusive back-off from OOVs.
Perplexity = 15.31, Entropy = 3.94 bits
Computation based on 1376 words.
Number of 3-grams hit = 1364  (99.13%)
Number of 2-grams hit = 11  (0.80%)
Number of 1-grams hit = 1  (0.07%)
11 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article525.text
Will force inclusive back-off from OOVs.
Perplexity = 17.59, Entropy = 4.14 bits
Computation based on 1962 words.
Number of 3-grams hit = 1946  (99.18%)
Number of 2-grams hit = 15  (0.76%)
Number of 1-grams hit = 1  (0.05%)
15 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article526.text
Will force inclusive back-off from OOVs.
Perplexity = 19.06, Entropy = 4.25 bits
Computation based on 1890 words.
Number of 3-grams hit = 1869  (98.89%)
Number of 2-grams hit = 20  (1.06%)
Number of 1-grams hit = 1  (0.05%)
19 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article527.text
Will force inclusive back-off from OOVs.
Perplexity = 16.70, Entropy = 4.06 bits
Computation based on 727 words.
Number of 3-grams hit = 716  (98.49%)
Number of 2-grams hit = 10  (1.38%)
Number of 1-grams hit = 1  (0.14%)
9 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article528.text
Will force inclusive back-off from OOVs.
Perplexity = 17.81, Entropy = 4.15 bits
Computation based on 723 words.
Number of 3-grams hit = 717  (99.17%)
Number of 2-grams hit = 5  (0.69%)
Number of 1-grams hit = 1  (0.14%)
4 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article529.text
Will force inclusive back-off from OOVs.
Perplexity = 15.63, Entropy = 3.97 bits
Computation based on 881 words.
Number of 3-grams hit = 872  (98.98%)
Number of 2-grams hit = 8  (0.91%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article530.text
Will force inclusive back-off from OOVs.
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 426 words.
Number of 3-grams hit = 424  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article531.text
Will force inclusive back-off from OOVs.
Perplexity = 17.02, Entropy = 4.09 bits
Computation based on 943 words.
Number of 3-grams hit = 936  (99.26%)
Number of 2-grams hit = 6  (0.64%)
Number of 1-grams hit = 1  (0.11%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article532.text
Will force inclusive back-off from OOVs.
Perplexity = 19.47, Entropy = 4.28 bits
Computation based on 927 words.
Number of 3-grams hit = 916  (98.81%)
Number of 2-grams hit = 10  (1.08%)
Number of 1-grams hit = 1  (0.11%)
9 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article533.text
Will force inclusive back-off from OOVs.
Perplexity = 17.37, Entropy = 4.12 bits
Computation based on 1134 words.
Number of 3-grams hit = 1128  (99.47%)
Number of 2-grams hit = 5  (0.44%)
Number of 1-grams hit = 1  (0.09%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article534.text
Will force inclusive back-off from OOVs.
Perplexity = 15.07, Entropy = 3.91 bits
Computation based on 501 words.
Number of 3-grams hit = 498  (99.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article535.text
Will force inclusive back-off from OOVs.
Perplexity = 14.03, Entropy = 3.81 bits
Computation based on 357 words.
Number of 3-grams hit = 354  (99.16%)
Number of 2-grams hit = 2  (0.56%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article536.text
Will force inclusive back-off from OOVs.
Perplexity = 17.39, Entropy = 4.12 bits
Computation based on 1510 words.
Number of 3-grams hit = 1503  (99.54%)
Number of 2-grams hit = 6  (0.40%)
Number of 1-grams hit = 1  (0.07%)
5 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article537.text
Will force inclusive back-off from OOVs.
Perplexity = 20.56, Entropy = 4.36 bits
Computation based on 996 words.
Number of 3-grams hit = 987  (99.10%)
Number of 2-grams hit = 8  (0.80%)
Number of 1-grams hit = 1  (0.10%)
7 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article538.text
Will force inclusive back-off from OOVs.
Perplexity = 16.98, Entropy = 4.09 bits
Computation based on 3417 words.
Number of 3-grams hit = 3392  (99.27%)
Number of 2-grams hit = 24  (0.70%)
Number of 1-grams hit = 1  (0.03%)
24 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article539.text
Will force inclusive back-off from OOVs.
Perplexity = 16.57, Entropy = 4.05 bits
Computation based on 534 words.
Number of 3-grams hit = 530  (99.25%)
Number of 2-grams hit = 3  (0.56%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article540.text
Will force inclusive back-off from OOVs.
Perplexity = 18.11, Entropy = 4.18 bits
Computation based on 2197 words.
Number of 3-grams hit = 2194  (99.86%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
1 OOVs (0.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article541.text
Will force inclusive back-off from OOVs.
Perplexity = 17.86, Entropy = 4.16 bits
Computation based on 491 words.
Number of 3-grams hit = 482  (98.17%)
Number of 2-grams hit = 8  (1.63%)
Number of 1-grams hit = 1  (0.20%)
7 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article542.text
Will force inclusive back-off from OOVs.
Perplexity = 19.50, Entropy = 4.29 bits
Computation based on 1590 words.
Number of 3-grams hit = 1585  (99.69%)
Number of 2-grams hit = 4  (0.25%)
Number of 1-grams hit = 1  (0.06%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article543.text
Will force inclusive back-off from OOVs.
Perplexity = 16.40, Entropy = 4.04 bits
Computation based on 539 words.
Number of 3-grams hit = 535  (99.26%)
Number of 2-grams hit = 3  (0.56%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article544.text
Will force inclusive back-off from OOVs.
Perplexity = 16.41, Entropy = 4.04 bits
Computation based on 1274 words.
Number of 3-grams hit = 1259  (98.82%)
Number of 2-grams hit = 14  (1.10%)
Number of 1-grams hit = 1  (0.08%)
13 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article545.text
Will force inclusive back-off from OOVs.
Perplexity = 17.19, Entropy = 4.10 bits
Computation based on 748 words.
Number of 3-grams hit = 743  (99.33%)
Number of 2-grams hit = 4  (0.53%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article546.text
Will force inclusive back-off from OOVs.
Perplexity = 16.58, Entropy = 4.05 bits
Computation based on 691 words.
Number of 3-grams hit = 688  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article547.text
Will force inclusive back-off from OOVs.
Perplexity = 16.21, Entropy = 4.02 bits
Computation based on 891 words.
Number of 3-grams hit = 885  (99.33%)
Number of 2-grams hit = 5  (0.56%)
Number of 1-grams hit = 1  (0.11%)
4 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article548.text
Will force inclusive back-off from OOVs.
Perplexity = 17.70, Entropy = 4.15 bits
Computation based on 618 words.
Number of 3-grams hit = 611  (98.87%)
Number of 2-grams hit = 6  (0.97%)
Number of 1-grams hit = 1  (0.16%)
5 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article549.text
Will force inclusive back-off from OOVs.
Perplexity = 15.80, Entropy = 3.98 bits
Computation based on 357 words.
Number of 3-grams hit = 353  (98.88%)
Number of 2-grams hit = 3  (0.84%)
Number of 1-grams hit = 1  (0.28%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article550.text
Will force inclusive back-off from OOVs.
Perplexity = 22.25, Entropy = 4.48 bits
Computation based on 184 words.
Number of 3-grams hit = 181  (98.37%)
Number of 2-grams hit = 2  (1.09%)
Number of 1-grams hit = 1  (0.54%)
1 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article551.text
Will force inclusive back-off from OOVs.
Perplexity = 19.67, Entropy = 4.30 bits
Computation based on 922 words.
Number of 3-grams hit = 919  (99.67%)
Number of 2-grams hit = 2  (0.22%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article552.text
Will force inclusive back-off from OOVs.
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 1075 words.
Number of 3-grams hit = 1071  (99.63%)
Number of 2-grams hit = 3  (0.28%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article553.text
Will force inclusive back-off from OOVs.
Perplexity = 21.17, Entropy = 4.40 bits
Computation based on 1239 words.
Number of 3-grams hit = 1235  (99.68%)
Number of 2-grams hit = 3  (0.24%)
Number of 1-grams hit = 1  (0.08%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article554.text
Will force inclusive back-off from OOVs.
Perplexity = 17.71, Entropy = 4.15 bits
Computation based on 4066 words.
Number of 3-grams hit = 4055  (99.73%)
Number of 2-grams hit = 10  (0.25%)
Number of 1-grams hit = 1  (0.02%)
9 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article555.text
Will force inclusive back-off from OOVs.
Perplexity = 17.05, Entropy = 4.09 bits
Computation based on 507 words.
Number of 3-grams hit = 504  (99.41%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article556.text
Will force inclusive back-off from OOVs.
Perplexity = 15.02, Entropy = 3.91 bits
Computation based on 567 words.
Number of 3-grams hit = 565  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article557.text
Will force inclusive back-off from OOVs.
Perplexity = 17.90, Entropy = 4.16 bits
Computation based on 380 words.
Number of 3-grams hit = 376  (98.95%)
Number of 2-grams hit = 3  (0.79%)
Number of 1-grams hit = 1  (0.26%)
3 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article558.text
Will force inclusive back-off from OOVs.
Perplexity = 17.65, Entropy = 4.14 bits
Computation based on 1728 words.
Number of 3-grams hit = 1725  (99.83%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
1 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article559.text
Will force inclusive back-off from OOVs.
Perplexity = 14.81, Entropy = 3.89 bits
Computation based on 635 words.
Number of 3-grams hit = 630  (99.21%)
Number of 2-grams hit = 4  (0.63%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article560.text
Will force inclusive back-off from OOVs.
Perplexity = 17.82, Entropy = 4.16 bits
Computation based on 548 words.
Number of 3-grams hit = 545  (99.45%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article561.text
Will force inclusive back-off from OOVs.
Perplexity = 14.88, Entropy = 3.90 bits
Computation based on 331 words.
Number of 3-grams hit = 327  (98.79%)
Number of 2-grams hit = 3  (0.91%)
Number of 1-grams hit = 1  (0.30%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article562.text
Will force inclusive back-off from OOVs.
Perplexity = 13.48, Entropy = 3.75 bits
Computation based on 311 words.
Number of 3-grams hit = 307  (98.71%)
Number of 2-grams hit = 3  (0.96%)
Number of 1-grams hit = 1  (0.32%)
2 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article563.text
Will force inclusive back-off from OOVs.
Perplexity = 18.44, Entropy = 4.20 bits
Computation based on 491 words.
Number of 3-grams hit = 488  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article564.text
Will force inclusive back-off from OOVs.
Perplexity = 17.06, Entropy = 4.09 bits
Computation based on 1328 words.
Number of 3-grams hit = 1315  (99.02%)
Number of 2-grams hit = 12  (0.90%)
Number of 1-grams hit = 1  (0.08%)
11 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article565.text
Will force inclusive back-off from OOVs.
Perplexity = 15.06, Entropy = 3.91 bits
Computation based on 1298 words.
Number of 3-grams hit = 1294  (99.69%)
Number of 2-grams hit = 3  (0.23%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article566.text
Will force inclusive back-off from OOVs.
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 4140 words.
Number of 3-grams hit = 4125  (99.64%)
Number of 2-grams hit = 14  (0.34%)
Number of 1-grams hit = 1  (0.02%)
13 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article567.text
Will force inclusive back-off from OOVs.
Perplexity = 15.68, Entropy = 3.97 bits
Computation based on 547 words.
Number of 3-grams hit = 543  (99.27%)
Number of 2-grams hit = 3  (0.55%)
Number of 1-grams hit = 1  (0.18%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article568.text
Will force inclusive back-off from OOVs.
Perplexity = 15.89, Entropy = 3.99 bits
Computation based on 329 words.
Number of 3-grams hit = 326  (99.09%)
Number of 2-grams hit = 2  (0.61%)
Number of 1-grams hit = 1  (0.30%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article569.text
Will force inclusive back-off from OOVs.
Perplexity = 17.14, Entropy = 4.10 bits
Computation based on 4332 words.
Number of 3-grams hit = 4307  (99.42%)
Number of 2-grams hit = 24  (0.55%)
Number of 1-grams hit = 1  (0.02%)
24 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article570.text
Will force inclusive back-off from OOVs.
Perplexity = 19.10, Entropy = 4.26 bits
Computation based on 816 words.
Number of 3-grams hit = 812  (99.51%)
Number of 2-grams hit = 3  (0.37%)
Number of 1-grams hit = 1  (0.12%)
2 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article571.text
Will force inclusive back-off from OOVs.
Perplexity = 13.52, Entropy = 3.76 bits
Computation based on 540 words.
Number of 3-grams hit = 538  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article572.text
Will force inclusive back-off from OOVs.
Perplexity = 17.42, Entropy = 4.12 bits
Computation based on 563 words.
Number of 3-grams hit = 553  (98.22%)
Number of 2-grams hit = 9  (1.60%)
Number of 1-grams hit = 1  (0.18%)
8 OOVs (1.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article573.text
Will force inclusive back-off from OOVs.
Perplexity = 20.03, Entropy = 4.32 bits
Computation based on 249 words.
Number of 3-grams hit = 244  (97.99%)
Number of 2-grams hit = 4  (1.61%)
Number of 1-grams hit = 1  (0.40%)
3 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article574.text
Will force inclusive back-off from OOVs.
Perplexity = 16.27, Entropy = 4.02 bits
Computation based on 1091 words.
Number of 3-grams hit = 1086  (99.54%)
Number of 2-grams hit = 4  (0.37%)
Number of 1-grams hit = 1  (0.09%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article575.text
Will force inclusive back-off from OOVs.
Perplexity = 16.13, Entropy = 4.01 bits
Computation based on 594 words.
Number of 3-grams hit = 591  (99.49%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article576.text
Will force inclusive back-off from OOVs.
Perplexity = 18.10, Entropy = 4.18 bits
Computation based on 441 words.
Number of 3-grams hit = 435  (98.64%)
Number of 2-grams hit = 5  (1.13%)
Number of 1-grams hit = 1  (0.23%)
4 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article577.text
Will force inclusive back-off from OOVs.
Perplexity = 18.24, Entropy = 4.19 bits
Computation based on 362 words.
Number of 3-grams hit = 359  (99.17%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article578.text
Will force inclusive back-off from OOVs.
Perplexity = 18.24, Entropy = 4.19 bits
Computation based on 405 words.
Number of 3-grams hit = 401  (99.01%)
Number of 2-grams hit = 3  (0.74%)
Number of 1-grams hit = 1  (0.25%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article579.text
Will force inclusive back-off from OOVs.
Perplexity = 16.25, Entropy = 4.02 bits
Computation based on 438 words.
Number of 3-grams hit = 432  (98.63%)
Number of 2-grams hit = 5  (1.14%)
Number of 1-grams hit = 1  (0.23%)
4 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article580.text
Will force inclusive back-off from OOVs.
Perplexity = 19.06, Entropy = 4.25 bits
Computation based on 535 words.
Number of 3-grams hit = 533  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article581.text
Will force inclusive back-off from OOVs.
Perplexity = 17.21, Entropy = 4.11 bits
Computation based on 538 words.
Number of 3-grams hit = 533  (99.07%)
Number of 2-grams hit = 4  (0.74%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article582.text
Will force inclusive back-off from OOVs.
Perplexity = 14.69, Entropy = 3.88 bits
Computation based on 508 words.
Number of 3-grams hit = 505  (99.41%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article583.text
Will force inclusive back-off from OOVs.
Perplexity = 16.11, Entropy = 4.01 bits
Computation based on 673 words.
Number of 3-grams hit = 669  (99.41%)
Number of 2-grams hit = 3  (0.45%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article584.text
Will force inclusive back-off from OOVs.
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 430 words.
Number of 3-grams hit = 424  (98.60%)
Number of 2-grams hit = 5  (1.16%)
Number of 1-grams hit = 1  (0.23%)
4 OOVs (0.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article585.text
Will force inclusive back-off from OOVs.
Perplexity = 18.21, Entropy = 4.19 bits
Computation based on 952 words.
Number of 3-grams hit = 948  (99.58%)
Number of 2-grams hit = 3  (0.32%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article586.text
Will force inclusive back-off from OOVs.
Perplexity = 19.53, Entropy = 4.29 bits
Computation based on 733 words.
Number of 3-grams hit = 731  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article587.text
Will force inclusive back-off from OOVs.
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 519 words.
Number of 3-grams hit = 511  (98.46%)
Number of 2-grams hit = 7  (1.35%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article588.text
Will force inclusive back-off from OOVs.
Perplexity = 17.90, Entropy = 4.16 bits
Computation based on 954 words.
Number of 3-grams hit = 946  (99.16%)
Number of 2-grams hit = 7  (0.73%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article589.text
Will force inclusive back-off from OOVs.
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 502 words.
Number of 3-grams hit = 498  (99.20%)
Number of 2-grams hit = 3  (0.60%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article590.text
Will force inclusive back-off from OOVs.
Perplexity = 17.18, Entropy = 4.10 bits
Computation based on 1390 words.
Number of 3-grams hit = 1385  (99.64%)
Number of 2-grams hit = 4  (0.29%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article591.text
Will force inclusive back-off from OOVs.
Perplexity = 19.51, Entropy = 4.29 bits
Computation based on 681 words.
Number of 3-grams hit = 673  (98.83%)
Number of 2-grams hit = 7  (1.03%)
Number of 1-grams hit = 1  (0.15%)
6 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article592.text
Will force inclusive back-off from OOVs.
Perplexity = 14.50, Entropy = 3.86 bits
Computation based on 340 words.
Number of 3-grams hit = 338  (99.41%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article593.text
Will force inclusive back-off from OOVs.
Perplexity = 18.97, Entropy = 4.25 bits
Computation based on 377 words.
Number of 3-grams hit = 374  (99.20%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article594.text
Will force inclusive back-off from OOVs.
Perplexity = 18.60, Entropy = 4.22 bits
Computation based on 586 words.
Number of 3-grams hit = 583  (99.49%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article595.text
Will force inclusive back-off from OOVs.
Perplexity = 15.82, Entropy = 3.98 bits
Computation based on 371 words.
Number of 3-grams hit = 369  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article596.text
Will force inclusive back-off from OOVs.
Perplexity = 17.52, Entropy = 4.13 bits
Computation based on 959 words.
Number of 3-grams hit = 953  (99.37%)
Number of 2-grams hit = 5  (0.52%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article597.text
Will force inclusive back-off from OOVs.
Perplexity = 15.17, Entropy = 3.92 bits
Computation based on 1032 words.
Number of 3-grams hit = 1028  (99.61%)
Number of 2-grams hit = 3  (0.29%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article598.text
Will force inclusive back-off from OOVs.
Perplexity = 15.55, Entropy = 3.96 bits
Computation based on 297 words.
Number of 3-grams hit = 293  (98.65%)
Number of 2-grams hit = 3  (1.01%)
Number of 1-grams hit = 1  (0.34%)
2 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article599.text
Will force inclusive back-off from OOVs.
Perplexity = 14.02, Entropy = 3.81 bits
Computation based on 803 words.
Number of 3-grams hit = 800  (99.63%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article600.text
Will force inclusive back-off from OOVs.
Perplexity = 19.68, Entropy = 4.30 bits
Computation based on 4669 words.
Number of 3-grams hit = 4663  (99.87%)
Number of 2-grams hit = 5  (0.11%)
Number of 1-grams hit = 1  (0.02%)
4 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article601.text
Will force inclusive back-off from OOVs.
Perplexity = 13.61, Entropy = 3.77 bits
Computation based on 1097 words.
Number of 3-grams hit = 1093  (99.64%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article602.text
Will force inclusive back-off from OOVs.
Perplexity = 16.66, Entropy = 4.06 bits
Computation based on 592 words.
Number of 3-grams hit = 590  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article603.text
Will force inclusive back-off from OOVs.
Perplexity = 16.51, Entropy = 4.05 bits
Computation based on 582 words.
Number of 3-grams hit = 575  (98.80%)
Number of 2-grams hit = 6  (1.03%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article604.text
Will force inclusive back-off from OOVs.
Perplexity = 16.52, Entropy = 4.05 bits
Computation based on 561 words.
Number of 3-grams hit = 559  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article605.text
Will force inclusive back-off from OOVs.
Perplexity = 17.57, Entropy = 4.14 bits
Computation based on 3152 words.
Number of 3-grams hit = 3146  (99.81%)
Number of 2-grams hit = 5  (0.16%)
Number of 1-grams hit = 1  (0.03%)
4 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article606.text
Will force inclusive back-off from OOVs.
Perplexity = 16.79, Entropy = 4.07 bits
Computation based on 910 words.
Number of 3-grams hit = 905  (99.45%)
Number of 2-grams hit = 4  (0.44%)
Number of 1-grams hit = 1  (0.11%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article607.text
Will force inclusive back-off from OOVs.
Perplexity = 17.69, Entropy = 4.14 bits
Computation based on 1055 words.
Number of 3-grams hit = 1045  (99.05%)
Number of 2-grams hit = 9  (0.85%)
Number of 1-grams hit = 1  (0.09%)
8 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article608.text
Will force inclusive back-off from OOVs.
Perplexity = 17.54, Entropy = 4.13 bits
Computation based on 615 words.
Number of 3-grams hit = 611  (99.35%)
Number of 2-grams hit = 3  (0.49%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article609.text
Will force inclusive back-off from OOVs.
Perplexity = 17.86, Entropy = 4.16 bits
Computation based on 4403 words.
Number of 3-grams hit = 4395  (99.82%)
Number of 2-grams hit = 7  (0.16%)
Number of 1-grams hit = 1  (0.02%)
6 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article610.text
Will force inclusive back-off from OOVs.
Perplexity = 17.78, Entropy = 4.15 bits
Computation based on 5927 words.
Number of 3-grams hit = 5889  (99.36%)
Number of 2-grams hit = 37  (0.62%)
Number of 1-grams hit = 1  (0.02%)
36 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article611.text
Will force inclusive back-off from OOVs.
Perplexity = 18.75, Entropy = 4.23 bits
Computation based on 776 words.
Number of 3-grams hit = 770  (99.23%)
Number of 2-grams hit = 5  (0.64%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article612.text
Will force inclusive back-off from OOVs.
Perplexity = 18.26, Entropy = 4.19 bits
Computation based on 1625 words.
Number of 3-grams hit = 1617  (99.51%)
Number of 2-grams hit = 7  (0.43%)
Number of 1-grams hit = 1  (0.06%)
7 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article613.text
Will force inclusive back-off from OOVs.
Perplexity = 17.67, Entropy = 4.14 bits
Computation based on 4183 words.
Number of 3-grams hit = 4176  (99.83%)
Number of 2-grams hit = 6  (0.14%)
Number of 1-grams hit = 1  (0.02%)
5 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article614.text
Will force inclusive back-off from OOVs.
Perplexity = 17.75, Entropy = 4.15 bits
Computation based on 1548 words.
Number of 3-grams hit = 1544  (99.74%)
Number of 2-grams hit = 3  (0.19%)
Number of 1-grams hit = 1  (0.06%)
2 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article615.text
Will force inclusive back-off from OOVs.
Perplexity = 16.63, Entropy = 4.06 bits
Computation based on 452 words.
Number of 3-grams hit = 447  (98.89%)
Number of 2-grams hit = 4  (0.88%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article616.text
Will force inclusive back-off from OOVs.
Perplexity = 18.03, Entropy = 4.17 bits
Computation based on 771 words.
Number of 3-grams hit = 768  (99.61%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article617.text
Will force inclusive back-off from OOVs.
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 1323 words.
Number of 3-grams hit = 1320  (99.77%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.08%)
1 OOVs (0.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article618.text
Will force inclusive back-off from OOVs.
Perplexity = 16.48, Entropy = 4.04 bits
Computation based on 3755 words.
Number of 3-grams hit = 3732  (99.39%)
Number of 2-grams hit = 22  (0.59%)
Number of 1-grams hit = 1  (0.03%)
22 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article619.text
Will force inclusive back-off from OOVs.
Perplexity = 17.78, Entropy = 4.15 bits
Computation based on 764 words.
Number of 3-grams hit = 759  (99.35%)
Number of 2-grams hit = 4  (0.52%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article620.text
Will force inclusive back-off from OOVs.
Perplexity = 16.77, Entropy = 4.07 bits
Computation based on 5287 words.
Number of 3-grams hit = 5243  (99.17%)
Number of 2-grams hit = 43  (0.81%)
Number of 1-grams hit = 1  (0.02%)
42 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article621.text
Will force inclusive back-off from OOVs.
Perplexity = 14.82, Entropy = 3.89 bits
Computation based on 321 words.
Number of 3-grams hit = 317  (98.75%)
Number of 2-grams hit = 3  (0.93%)
Number of 1-grams hit = 1  (0.31%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article622.text
Will force inclusive back-off from OOVs.
Perplexity = 17.81, Entropy = 4.15 bits
Computation based on 458 words.
Number of 3-grams hit = 455  (99.34%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article623.text
Will force inclusive back-off from OOVs.
Perplexity = 18.05, Entropy = 4.17 bits
Computation based on 1300 words.
Number of 3-grams hit = 1292  (99.38%)
Number of 2-grams hit = 7  (0.54%)
Number of 1-grams hit = 1  (0.08%)
8 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article624.text
Will force inclusive back-off from OOVs.
Perplexity = 16.53, Entropy = 4.05 bits
Computation based on 5132 words.
Number of 3-grams hit = 5096  (99.30%)
Number of 2-grams hit = 35  (0.68%)
Number of 1-grams hit = 1  (0.02%)
34 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article625.text
Will force inclusive back-off from OOVs.
Perplexity = 16.23, Entropy = 4.02 bits
Computation based on 1088 words.
Number of 3-grams hit = 1081  (99.36%)
Number of 2-grams hit = 6  (0.55%)
Number of 1-grams hit = 1  (0.09%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article626.text
Will force inclusive back-off from OOVs.
Perplexity = 16.65, Entropy = 4.06 bits
Computation based on 441 words.
Number of 3-grams hit = 433  (98.19%)
Number of 2-grams hit = 7  (1.59%)
Number of 1-grams hit = 1  (0.23%)
6 OOVs (1.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article627.text
Will force inclusive back-off from OOVs.
Perplexity = 15.26, Entropy = 3.93 bits
Computation based on 148 words.
Number of 3-grams hit = 145  (97.97%)
Number of 2-grams hit = 2  (1.35%)
Number of 1-grams hit = 1  (0.68%)
1 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article628.text
Will force inclusive back-off from OOVs.
Perplexity = 16.54, Entropy = 4.05 bits
Computation based on 1073 words.
Number of 3-grams hit = 1069  (99.63%)
Number of 2-grams hit = 3  (0.28%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article629.text
Will force inclusive back-off from OOVs.
Perplexity = 16.69, Entropy = 4.06 bits
Computation based on 1028 words.
Number of 3-grams hit = 1017  (98.93%)
Number of 2-grams hit = 10  (0.97%)
Number of 1-grams hit = 1  (0.10%)
9 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article630.text
Will force inclusive back-off from OOVs.
Perplexity = 15.00, Entropy = 3.91 bits
Computation based on 326 words.
Number of 3-grams hit = 321  (98.47%)
Number of 2-grams hit = 4  (1.23%)
Number of 1-grams hit = 1  (0.31%)
3 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article631.text
Will force inclusive back-off from OOVs.
Perplexity = 14.68, Entropy = 3.88 bits
Computation based on 872 words.
Number of 3-grams hit = 866  (99.31%)
Number of 2-grams hit = 5  (0.57%)
Number of 1-grams hit = 1  (0.11%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article632.text
Will force inclusive back-off from OOVs.
Perplexity = 16.71, Entropy = 4.06 bits
Computation based on 408 words.
Number of 3-grams hit = 404  (99.02%)
Number of 2-grams hit = 3  (0.74%)
Number of 1-grams hit = 1  (0.25%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article633.text
Will force inclusive back-off from OOVs.
Perplexity = 16.02, Entropy = 4.00 bits
Computation based on 529 words.
Number of 3-grams hit = 525  (99.24%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article634.text
Will force inclusive back-off from OOVs.
Perplexity = 18.80, Entropy = 4.23 bits
Computation based on 563 words.
Number of 3-grams hit = 561  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article635.text
Will force inclusive back-off from OOVs.
Perplexity = 19.72, Entropy = 4.30 bits
Computation based on 149 words.
Number of 3-grams hit = 144  (96.64%)
Number of 2-grams hit = 4  (2.68%)
Number of 1-grams hit = 1  (0.67%)
3 OOVs (1.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article636.text
Will force inclusive back-off from OOVs.
Perplexity = 14.89, Entropy = 3.90 bits
Computation based on 1341 words.
Number of 3-grams hit = 1336  (99.63%)
Number of 2-grams hit = 4  (0.30%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article637.text
Will force inclusive back-off from OOVs.
Perplexity = 16.32, Entropy = 4.03 bits
Computation based on 1030 words.
Number of 3-grams hit = 1023  (99.32%)
Number of 2-grams hit = 6  (0.58%)
Number of 1-grams hit = 1  (0.10%)
5 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article638.text
Will force inclusive back-off from OOVs.
Perplexity = 15.19, Entropy = 3.92 bits
Computation based on 527 words.
Number of 3-grams hit = 519  (98.48%)
Number of 2-grams hit = 7  (1.33%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article639.text
Will force inclusive back-off from OOVs.
Perplexity = 14.25, Entropy = 3.83 bits
Computation based on 524 words.
Number of 3-grams hit = 522  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article640.text
Will force inclusive back-off from OOVs.
Perplexity = 16.90, Entropy = 4.08 bits
Computation based on 201 words.
Number of 3-grams hit = 196  (97.51%)
Number of 2-grams hit = 4  (1.99%)
Number of 1-grams hit = 1  (0.50%)
3 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article641.text
Will force inclusive back-off from OOVs.
Perplexity = 16.52, Entropy = 4.05 bits
Computation based on 825 words.
Number of 3-grams hit = 818  (99.15%)
Number of 2-grams hit = 6  (0.73%)
Number of 1-grams hit = 1  (0.12%)
5 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article642.text
Will force inclusive back-off from OOVs.
Perplexity = 13.52, Entropy = 3.76 bits
Computation based on 441 words.
Number of 3-grams hit = 436  (98.87%)
Number of 2-grams hit = 4  (0.91%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article643.text
Will force inclusive back-off from OOVs.
Perplexity = 18.61, Entropy = 4.22 bits
Computation based on 486 words.
Number of 3-grams hit = 482  (99.18%)
Number of 2-grams hit = 3  (0.62%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article644.text
Will force inclusive back-off from OOVs.
Perplexity = 18.58, Entropy = 4.22 bits
Computation based on 1114 words.
Number of 3-grams hit = 1105  (99.19%)
Number of 2-grams hit = 8  (0.72%)
Number of 1-grams hit = 1  (0.09%)
7 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article645.text
Will force inclusive back-off from OOVs.
Perplexity = 16.05, Entropy = 4.00 bits
Computation based on 444 words.
Number of 3-grams hit = 440  (99.10%)
Number of 2-grams hit = 3  (0.68%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article646.text
Will force inclusive back-off from OOVs.
Perplexity = 16.18, Entropy = 4.02 bits
Computation based on 574 words.
Number of 3-grams hit = 571  (99.48%)
Number of 2-grams hit = 2  (0.35%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article647.text
Will force inclusive back-off from OOVs.
Perplexity = 16.63, Entropy = 4.06 bits
Computation based on 604 words.
Number of 3-grams hit = 596  (98.68%)
Number of 2-grams hit = 7  (1.16%)
Number of 1-grams hit = 1  (0.17%)
7 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article648.text
Will force inclusive back-off from OOVs.
Perplexity = 17.15, Entropy = 4.10 bits
Computation based on 472 words.
Number of 3-grams hit = 467  (98.94%)
Number of 2-grams hit = 4  (0.85%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article649.text
Will force inclusive back-off from OOVs.
Perplexity = 18.67, Entropy = 4.22 bits
Computation based on 526 words.
Number of 3-grams hit = 518  (98.48%)
Number of 2-grams hit = 7  (1.33%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article650.text
Will force inclusive back-off from OOVs.
Perplexity = 14.91, Entropy = 3.90 bits
Computation based on 534 words.
Number of 3-grams hit = 532  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article651.text
Will force inclusive back-off from OOVs.
Perplexity = 17.01, Entropy = 4.09 bits
Computation based on 461 words.
Number of 3-grams hit = 458  (99.35%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article652.text
Will force inclusive back-off from OOVs.
Perplexity = 18.11, Entropy = 4.18 bits
Computation based on 880 words.
Number of 3-grams hit = 878  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article653.text
Will force inclusive back-off from OOVs.
Perplexity = 18.94, Entropy = 4.24 bits
Computation based on 762 words.
Number of 3-grams hit = 757  (99.34%)
Number of 2-grams hit = 4  (0.52%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article654.text
Will force inclusive back-off from OOVs.
Perplexity = 17.36, Entropy = 4.12 bits
Computation based on 766 words.
Number of 3-grams hit = 757  (98.83%)
Number of 2-grams hit = 8  (1.04%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article655.text
Will force inclusive back-off from OOVs.
Perplexity = 16.99, Entropy = 4.09 bits
Computation based on 486 words.
Number of 3-grams hit = 483  (99.38%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article656.text
Will force inclusive back-off from OOVs.
Perplexity = 13.36, Entropy = 3.74 bits
Computation based on 463 words.
Number of 3-grams hit = 458  (98.92%)
Number of 2-grams hit = 4  (0.86%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article657.text
Will force inclusive back-off from OOVs.
Perplexity = 17.59, Entropy = 4.14 bits
Computation based on 371 words.
Number of 3-grams hit = 363  (97.84%)
Number of 2-grams hit = 7  (1.89%)
Number of 1-grams hit = 1  (0.27%)
6 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article658.text
Will force inclusive back-off from OOVs.
Perplexity = 17.63, Entropy = 4.14 bits
Computation based on 1000 words.
Number of 3-grams hit = 992  (99.20%)
Number of 2-grams hit = 7  (0.70%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article659.text
Will force inclusive back-off from OOVs.
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 989 words.
Number of 3-grams hit = 986  (99.70%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article660.text
Will force inclusive back-off from OOVs.
Perplexity = 18.64, Entropy = 4.22 bits
Computation based on 136 words.
Number of 3-grams hit = 131  (96.32%)
Number of 2-grams hit = 4  (2.94%)
Number of 1-grams hit = 1  (0.74%)
3 OOVs (2.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article661.text
Will force inclusive back-off from OOVs.
Perplexity = 16.33, Entropy = 4.03 bits
Computation based on 1237 words.
Number of 3-grams hit = 1227  (99.19%)
Number of 2-grams hit = 9  (0.73%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article662.text
Will force inclusive back-off from OOVs.
Perplexity = 17.13, Entropy = 4.10 bits
Computation based on 123 words.
Number of 3-grams hit = 118  (95.93%)
Number of 2-grams hit = 4  (3.25%)
Number of 1-grams hit = 1  (0.81%)
3 OOVs (2.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article663.text
Will force inclusive back-off from OOVs.
Perplexity = 16.00, Entropy = 4.00 bits
Computation based on 1358 words.
Number of 3-grams hit = 1354  (99.71%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article664.text
Will force inclusive back-off from OOVs.
Perplexity = 18.50, Entropy = 4.21 bits
Computation based on 698 words.
Number of 3-grams hit = 695  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article665.text
Will force inclusive back-off from OOVs.
Perplexity = 17.58, Entropy = 4.14 bits
Computation based on 753 words.
Number of 3-grams hit = 748  (99.34%)
Number of 2-grams hit = 4  (0.53%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article666.text
Will force inclusive back-off from OOVs.
Perplexity = 16.68, Entropy = 4.06 bits
Computation based on 381 words.
Number of 3-grams hit = 376  (98.69%)
Number of 2-grams hit = 4  (1.05%)
Number of 1-grams hit = 1  (0.26%)
4 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article667.text
Will force inclusive back-off from OOVs.
Perplexity = 21.71, Entropy = 4.44 bits
Computation based on 1300 words.
Number of 3-grams hit = 1297  (99.77%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.08%)
1 OOVs (0.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article668.text
Will force inclusive back-off from OOVs.
Perplexity = 20.08, Entropy = 4.33 bits
Computation based on 467 words.
Number of 3-grams hit = 464  (99.36%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article669.text
Will force inclusive back-off from OOVs.
Perplexity = 16.92, Entropy = 4.08 bits
Computation based on 486 words.
Number of 3-grams hit = 481  (98.97%)
Number of 2-grams hit = 4  (0.82%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article670.text
Will force inclusive back-off from OOVs.
Perplexity = 15.63, Entropy = 3.97 bits
Computation based on 446 words.
Number of 3-grams hit = 441  (98.88%)
Number of 2-grams hit = 4  (0.90%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article671.text
Will force inclusive back-off from OOVs.
Perplexity = 17.23, Entropy = 4.11 bits
Computation based on 2024 words.
Number of 3-grams hit = 2010  (99.31%)
Number of 2-grams hit = 13  (0.64%)
Number of 1-grams hit = 1  (0.05%)
12 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article672.text
Will force inclusive back-off from OOVs.
Perplexity = 19.33, Entropy = 4.27 bits
Computation based on 282 words.
Number of 3-grams hit = 274  (97.16%)
Number of 2-grams hit = 7  (2.48%)
Number of 1-grams hit = 1  (0.35%)
6 OOVs (2.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article673.text
Will force inclusive back-off from OOVs.
Perplexity = 20.03, Entropy = 4.32 bits
Computation based on 302 words.
Number of 3-grams hit = 298  (98.68%)
Number of 2-grams hit = 3  (0.99%)
Number of 1-grams hit = 1  (0.33%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article674.text
Will force inclusive back-off from OOVs.
Perplexity = 18.61, Entropy = 4.22 bits
Computation based on 141 words.
Number of 3-grams hit = 139  (98.58%)
Number of 2-grams hit = 1  (0.71%)
Number of 1-grams hit = 1  (0.71%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article675.text
Will force inclusive back-off from OOVs.
Perplexity = 17.35, Entropy = 4.12 bits
Computation based on 3857 words.
Number of 3-grams hit = 3838  (99.51%)
Number of 2-grams hit = 18  (0.47%)
Number of 1-grams hit = 1  (0.03%)
17 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article676.text
Will force inclusive back-off from OOVs.
Perplexity = 17.63, Entropy = 4.14 bits
Computation based on 495 words.
Number of 3-grams hit = 490  (98.99%)
Number of 2-grams hit = 4  (0.81%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article677.text
Will force inclusive back-off from OOVs.
Perplexity = 16.13, Entropy = 4.01 bits
Computation based on 641 words.
Number of 3-grams hit = 637  (99.38%)
Number of 2-grams hit = 3  (0.47%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article678.text
Will force inclusive back-off from OOVs.
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 340 words.
Number of 3-grams hit = 337  (99.12%)
Number of 2-grams hit = 2  (0.59%)
Number of 1-grams hit = 1  (0.29%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article679.text
Will force inclusive back-off from OOVs.
Perplexity = 18.67, Entropy = 4.22 bits
Computation based on 978 words.
Number of 3-grams hit = 965  (98.67%)
Number of 2-grams hit = 12  (1.23%)
Number of 1-grams hit = 1  (0.10%)
11 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article680.text
Will force inclusive back-off from OOVs.
Perplexity = 18.26, Entropy = 4.19 bits
Computation based on 316 words.
Number of 3-grams hit = 313  (99.05%)
Number of 2-grams hit = 2  (0.63%)
Number of 1-grams hit = 1  (0.32%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article681.text
Will force inclusive back-off from OOVs.
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 478 words.
Number of 3-grams hit = 476  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article682.text
Will force inclusive back-off from OOVs.
Perplexity = 17.38, Entropy = 4.12 bits
Computation based on 3663 words.
Number of 3-grams hit = 3635  (99.24%)
Number of 2-grams hit = 27  (0.74%)
Number of 1-grams hit = 1  (0.03%)
27 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article683.text
Will force inclusive back-off from OOVs.
Perplexity = 14.62, Entropy = 3.87 bits
Computation based on 266 words.
Number of 3-grams hit = 264  (99.25%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article684.text
Will force inclusive back-off from OOVs.
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 4351 words.
Number of 3-grams hit = 4342  (99.79%)
Number of 2-grams hit = 8  (0.18%)
Number of 1-grams hit = 1  (0.02%)
7 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article685.text
Will force inclusive back-off from OOVs.
Perplexity = 17.19, Entropy = 4.10 bits
Computation based on 456 words.
Number of 3-grams hit = 453  (99.34%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article686.text
Will force inclusive back-off from OOVs.
Perplexity = 16.60, Entropy = 4.05 bits
Computation based on 2213 words.
Number of 3-grams hit = 2195  (99.19%)
Number of 2-grams hit = 17  (0.77%)
Number of 1-grams hit = 1  (0.05%)
16 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article687.text
Will force inclusive back-off from OOVs.
Perplexity = 16.12, Entropy = 4.01 bits
Computation based on 511 words.
Number of 3-grams hit = 507  (99.22%)
Number of 2-grams hit = 3  (0.59%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article688.text
Will force inclusive back-off from OOVs.
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 304 words.
Number of 3-grams hit = 302  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article689.text
Will force inclusive back-off from OOVs.
Perplexity = 15.83, Entropy = 3.98 bits
Computation based on 507 words.
Number of 3-grams hit = 502  (99.01%)
Number of 2-grams hit = 4  (0.79%)
Number of 1-grams hit = 1  (0.20%)
3 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article690.text
Will force inclusive back-off from OOVs.
Perplexity = 17.92, Entropy = 4.16 bits
Computation based on 749 words.
Number of 3-grams hit = 742  (99.07%)
Number of 2-grams hit = 6  (0.80%)
Number of 1-grams hit = 1  (0.13%)
6 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article691.text
Will force inclusive back-off from OOVs.
Perplexity = 16.27, Entropy = 4.02 bits
Computation based on 871 words.
Number of 3-grams hit = 863  (99.08%)
Number of 2-grams hit = 7  (0.80%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article692.text
Will force inclusive back-off from OOVs.
Perplexity = 14.54, Entropy = 3.86 bits
Computation based on 430 words.
Number of 3-grams hit = 425  (98.84%)
Number of 2-grams hit = 4  (0.93%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article693.text
Will force inclusive back-off from OOVs.
Perplexity = 17.13, Entropy = 4.10 bits
Computation based on 943 words.
Number of 3-grams hit = 940  (99.68%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article694.text
Will force inclusive back-off from OOVs.
Perplexity = 18.31, Entropy = 4.19 bits
Computation based on 630 words.
Number of 3-grams hit = 628  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article695.text
Will force inclusive back-off from OOVs.
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 843 words.
Number of 3-grams hit = 831  (98.58%)
Number of 2-grams hit = 11  (1.30%)
Number of 1-grams hit = 1  (0.12%)
11 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article696.text
Will force inclusive back-off from OOVs.
Perplexity = 15.22, Entropy = 3.93 bits
Computation based on 499 words.
Number of 3-grams hit = 497  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article697.text
Will force inclusive back-off from OOVs.
Perplexity = 17.68, Entropy = 4.14 bits
Computation based on 936 words.
Number of 3-grams hit = 925  (98.82%)
Number of 2-grams hit = 10  (1.07%)
Number of 1-grams hit = 1  (0.11%)
9 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article698.text
Will force inclusive back-off from OOVs.
Perplexity = 15.12, Entropy = 3.92 bits
Computation based on 570 words.
Number of 3-grams hit = 564  (98.95%)
Number of 2-grams hit = 5  (0.88%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article699.text
Will force inclusive back-off from OOVs.
Perplexity = 16.16, Entropy = 4.01 bits
Computation based on 352 words.
Number of 3-grams hit = 350  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article700.text
Will force inclusive back-off from OOVs.
Perplexity = 18.75, Entropy = 4.23 bits
Computation based on 3334 words.
Number of 3-grams hit = 3323  (99.67%)
Number of 2-grams hit = 10  (0.30%)
Number of 1-grams hit = 1  (0.03%)
9 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article701.text
Will force inclusive back-off from OOVs.
Perplexity = 16.18, Entropy = 4.02 bits
Computation based on 806 words.
Number of 3-grams hit = 801  (99.38%)
Number of 2-grams hit = 4  (0.50%)
Number of 1-grams hit = 1  (0.12%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article702.text
Will force inclusive back-off from OOVs.
Perplexity = 16.59, Entropy = 4.05 bits
Computation based on 1828 words.
Number of 3-grams hit = 1817  (99.40%)
Number of 2-grams hit = 10  (0.55%)
Number of 1-grams hit = 1  (0.05%)
9 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article703.text
Will force inclusive back-off from OOVs.
Perplexity = 14.97, Entropy = 3.90 bits
Computation based on 425 words.
Number of 3-grams hit = 422  (99.29%)
Number of 2-grams hit = 2  (0.47%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article704.text
Will force inclusive back-off from OOVs.
Perplexity = 20.68, Entropy = 4.37 bits
Computation based on 307 words.
Number of 3-grams hit = 301  (98.05%)
Number of 2-grams hit = 5  (1.63%)
Number of 1-grams hit = 1  (0.33%)
4 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article705.text
Will force inclusive back-off from OOVs.
Perplexity = 17.85, Entropy = 4.16 bits
Computation based on 1009 words.
Number of 3-grams hit = 1001  (99.21%)
Number of 2-grams hit = 7  (0.69%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article706.text
Will force inclusive back-off from OOVs.
Perplexity = 16.72, Entropy = 4.06 bits
Computation based on 475 words.
Number of 3-grams hit = 471  (99.16%)
Number of 2-grams hit = 3  (0.63%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article707.text
Will force inclusive back-off from OOVs.
Perplexity = 14.85, Entropy = 3.89 bits
Computation based on 865 words.
Number of 3-grams hit = 857  (99.08%)
Number of 2-grams hit = 7  (0.81%)
Number of 1-grams hit = 1  (0.12%)
6 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article708.text
Will force inclusive back-off from OOVs.
Perplexity = 16.88, Entropy = 4.08 bits
Computation based on 526 words.
Number of 3-grams hit = 522  (99.24%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article709.text
Will force inclusive back-off from OOVs.
Perplexity = 19.08, Entropy = 4.25 bits
Computation based on 554 words.
Number of 3-grams hit = 548  (98.92%)
Number of 2-grams hit = 5  (0.90%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article710.text
Will force inclusive back-off from OOVs.
Perplexity = 16.89, Entropy = 4.08 bits
Computation based on 771 words.
Number of 3-grams hit = 766  (99.35%)
Number of 2-grams hit = 4  (0.52%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article711.text
Will force inclusive back-off from OOVs.
Perplexity = 15.60, Entropy = 3.96 bits
Computation based on 962 words.
Number of 3-grams hit = 960  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article712.text
Will force inclusive back-off from OOVs.
Perplexity = 17.10, Entropy = 4.10 bits
Computation based on 366 words.
Number of 3-grams hit = 361  (98.63%)
Number of 2-grams hit = 4  (1.09%)
Number of 1-grams hit = 1  (0.27%)
3 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article713.text
Will force inclusive back-off from OOVs.
Perplexity = 18.14, Entropy = 4.18 bits
Computation based on 888 words.
Number of 3-grams hit = 884  (99.55%)
Number of 2-grams hit = 3  (0.34%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article714.text
Will force inclusive back-off from OOVs.
Perplexity = 16.48, Entropy = 4.04 bits
Computation based on 365 words.
Number of 3-grams hit = 362  (99.18%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article715.text
Will force inclusive back-off from OOVs.
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 482 words.
Number of 3-grams hit = 475  (98.55%)
Number of 2-grams hit = 6  (1.24%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article716.text
Will force inclusive back-off from OOVs.
Perplexity = 16.30, Entropy = 4.03 bits
Computation based on 466 words.
Number of 3-grams hit = 462  (99.14%)
Number of 2-grams hit = 3  (0.64%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article717.text
Will force inclusive back-off from OOVs.
Perplexity = 14.76, Entropy = 3.88 bits
Computation based on 381 words.
Number of 3-grams hit = 379  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article718.text
Will force inclusive back-off from OOVs.
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 6935 words.
Number of 3-grams hit = 6884  (99.26%)
Number of 2-grams hit = 50  (0.72%)
Number of 1-grams hit = 1  (0.01%)
50 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article719.text
Will force inclusive back-off from OOVs.
Perplexity = 18.68, Entropy = 4.22 bits
Computation based on 1153 words.
Number of 3-grams hit = 1142  (99.05%)
Number of 2-grams hit = 10  (0.87%)
Number of 1-grams hit = 1  (0.09%)
9 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article720.text
Will force inclusive back-off from OOVs.
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 814 words.
Number of 3-grams hit = 812  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article721.text
Will force inclusive back-off from OOVs.
Perplexity = 17.06, Entropy = 4.09 bits
Computation based on 423 words.
Number of 3-grams hit = 420  (99.29%)
Number of 2-grams hit = 2  (0.47%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article722.text
Will force inclusive back-off from OOVs.
Perplexity = 17.78, Entropy = 4.15 bits
Computation based on 1018 words.
Number of 3-grams hit = 1014  (99.61%)
Number of 2-grams hit = 3  (0.29%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article723.text
Will force inclusive back-off from OOVs.
Perplexity = 14.00, Entropy = 3.81 bits
Computation based on 440 words.
Number of 3-grams hit = 438  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article724.text
Will force inclusive back-off from OOVs.
Perplexity = 13.84, Entropy = 3.79 bits
Computation based on 460 words.
Number of 3-grams hit = 456  (99.13%)
Number of 2-grams hit = 3  (0.65%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article725.text
Will force inclusive back-off from OOVs.
Perplexity = 16.67, Entropy = 4.06 bits
Computation based on 719 words.
Number of 3-grams hit = 714  (99.30%)
Number of 2-grams hit = 4  (0.56%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article726.text
Will force inclusive back-off from OOVs.
Perplexity = 18.84, Entropy = 4.24 bits
Computation based on 521 words.
Number of 3-grams hit = 518  (99.42%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article727.text
Will force inclusive back-off from OOVs.
Perplexity = 13.98, Entropy = 3.81 bits
Computation based on 493 words.
Number of 3-grams hit = 491  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article728.text
Will force inclusive back-off from OOVs.
Perplexity = 16.69, Entropy = 4.06 bits
Computation based on 421 words.
Number of 3-grams hit = 419  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article729.text
Will force inclusive back-off from OOVs.
Perplexity = 14.11, Entropy = 3.82 bits
Computation based on 437 words.
Number of 3-grams hit = 433  (99.08%)
Number of 2-grams hit = 3  (0.69%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article730.text
Will force inclusive back-off from OOVs.
Perplexity = 19.44, Entropy = 4.28 bits
Computation based on 8470 words.
Number of 3-grams hit = 8461  (99.89%)
Number of 2-grams hit = 8  (0.09%)
Number of 1-grams hit = 1  (0.01%)
7 OOVs (0.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article731.text
Will force inclusive back-off from OOVs.
Perplexity = 18.56, Entropy = 4.21 bits
Computation based on 1455 words.
Number of 3-grams hit = 1452  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
1 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article732.text
Will force inclusive back-off from OOVs.
Perplexity = 18.47, Entropy = 4.21 bits
Computation based on 698 words.
Number of 3-grams hit = 689  (98.71%)
Number of 2-grams hit = 8  (1.15%)
Number of 1-grams hit = 1  (0.14%)
7 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article733.text
Will force inclusive back-off from OOVs.
Perplexity = 18.58, Entropy = 4.22 bits
Computation based on 561 words.
Number of 3-grams hit = 558  (99.47%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article734.text
Will force inclusive back-off from OOVs.
Perplexity = 13.17, Entropy = 3.72 bits
Computation based on 417 words.
Number of 3-grams hit = 411  (98.56%)
Number of 2-grams hit = 5  (1.20%)
Number of 1-grams hit = 1  (0.24%)
4 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article735.text
Will force inclusive back-off from OOVs.
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 1178 words.
Number of 3-grams hit = 1170  (99.32%)
Number of 2-grams hit = 7  (0.59%)
Number of 1-grams hit = 1  (0.08%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article736.text
Will force inclusive back-off from OOVs.
Perplexity = 17.04, Entropy = 4.09 bits
Computation based on 666 words.
Number of 3-grams hit = 662  (99.40%)
Number of 2-grams hit = 3  (0.45%)
Number of 1-grams hit = 1  (0.15%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article737.text
Will force inclusive back-off from OOVs.
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 631 words.
Number of 3-grams hit = 627  (99.37%)
Number of 2-grams hit = 3  (0.48%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article738.text
Will force inclusive back-off from OOVs.
Perplexity = 16.43, Entropy = 4.04 bits
Computation based on 1009 words.
Number of 3-grams hit = 1005  (99.60%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article739.text
Will force inclusive back-off from OOVs.
Perplexity = 18.15, Entropy = 4.18 bits
Computation based on 440 words.
Number of 3-grams hit = 438  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article740.text
Will force inclusive back-off from OOVs.
Perplexity = 19.18, Entropy = 4.26 bits
Computation based on 1094 words.
Number of 3-grams hit = 1090  (99.63%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
2 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article741.text
Will force inclusive back-off from OOVs.
Perplexity = 16.01, Entropy = 4.00 bits
Computation based on 755 words.
Number of 3-grams hit = 750  (99.34%)
Number of 2-grams hit = 4  (0.53%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article742.text
Will force inclusive back-off from OOVs.
Perplexity = 16.91, Entropy = 4.08 bits
Computation based on 627 words.
Number of 3-grams hit = 622  (99.20%)
Number of 2-grams hit = 4  (0.64%)
Number of 1-grams hit = 1  (0.16%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article743.text
Will force inclusive back-off from OOVs.
Perplexity = 16.51, Entropy = 4.05 bits
Computation based on 1974 words.
Number of 3-grams hit = 1963  (99.44%)
Number of 2-grams hit = 10  (0.51%)
Number of 1-grams hit = 1  (0.05%)
9 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article744.text
Will force inclusive back-off from OOVs.
Perplexity = 16.46, Entropy = 4.04 bits
Computation based on 525 words.
Number of 3-grams hit = 521  (99.24%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article745.text
Will force inclusive back-off from OOVs.
Perplexity = 16.65, Entropy = 4.06 bits
Computation based on 692 words.
Number of 3-grams hit = 689  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article746.text
Will force inclusive back-off from OOVs.
Perplexity = 21.19, Entropy = 4.41 bits
Computation based on 1328 words.
Number of 3-grams hit = 1322  (99.55%)
Number of 2-grams hit = 5  (0.38%)
Number of 1-grams hit = 1  (0.08%)
4 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article747.text
Will force inclusive back-off from OOVs.
Perplexity = 16.89, Entropy = 4.08 bits
Computation based on 520 words.
Number of 3-grams hit = 518  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article748.text
Will force inclusive back-off from OOVs.
Perplexity = 16.47, Entropy = 4.04 bits
Computation based on 507 words.
Number of 3-grams hit = 503  (99.21%)
Number of 2-grams hit = 3  (0.59%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article749.text
Will force inclusive back-off from OOVs.
Perplexity = 14.43, Entropy = 3.85 bits
Computation based on 551 words.
Number of 3-grams hit = 548  (99.46%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article750.text
Will force inclusive back-off from OOVs.
Perplexity = 18.14, Entropy = 4.18 bits
Computation based on 467 words.
Number of 3-grams hit = 464  (99.36%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article751.text
Will force inclusive back-off from OOVs.
Perplexity = 15.87, Entropy = 3.99 bits
Computation based on 2010 words.
Number of 3-grams hit = 2002  (99.60%)
Number of 2-grams hit = 7  (0.35%)
Number of 1-grams hit = 1  (0.05%)
6 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article752.text
Will force inclusive back-off from OOVs.
Perplexity = 18.52, Entropy = 4.21 bits
Computation based on 429 words.
Number of 3-grams hit = 426  (99.30%)
Number of 2-grams hit = 2  (0.47%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article753.text
Will force inclusive back-off from OOVs.
Perplexity = 15.56, Entropy = 3.96 bits
Computation based on 530 words.
Number of 3-grams hit = 527  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article754.text
Will force inclusive back-off from OOVs.
Perplexity = 18.56, Entropy = 4.21 bits
Computation based on 1848 words.
Number of 3-grams hit = 1834  (99.24%)
Number of 2-grams hit = 13  (0.70%)
Number of 1-grams hit = 1  (0.05%)
14 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article755.text
Will force inclusive back-off from OOVs.
Perplexity = 17.23, Entropy = 4.11 bits
Computation based on 445 words.
Number of 3-grams hit = 437  (98.20%)
Number of 2-grams hit = 7  (1.57%)
Number of 1-grams hit = 1  (0.22%)
6 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article756.text
Will force inclusive back-off from OOVs.
Perplexity = 15.56, Entropy = 3.96 bits
Computation based on 703 words.
Number of 3-grams hit = 694  (98.72%)
Number of 2-grams hit = 8  (1.14%)
Number of 1-grams hit = 1  (0.14%)
7 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article757.text
Will force inclusive back-off from OOVs.
Perplexity = 17.27, Entropy = 4.11 bits
Computation based on 508 words.
Number of 3-grams hit = 504  (99.21%)
Number of 2-grams hit = 3  (0.59%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article758.text
Will force inclusive back-off from OOVs.
Perplexity = 17.63, Entropy = 4.14 bits
Computation based on 2493 words.
Number of 3-grams hit = 2469  (99.04%)
Number of 2-grams hit = 23  (0.92%)
Number of 1-grams hit = 1  (0.04%)
23 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article759.text
Will force inclusive back-off from OOVs.
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 3319 words.
Number of 3-grams hit = 3314  (99.85%)
Number of 2-grams hit = 4  (0.12%)
Number of 1-grams hit = 1  (0.03%)
3 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article760.text
Will force inclusive back-off from OOVs.
Perplexity = 17.04, Entropy = 4.09 bits
Computation based on 553 words.
Number of 3-grams hit = 548  (99.10%)
Number of 2-grams hit = 4  (0.72%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article761.text
Will force inclusive back-off from OOVs.
Perplexity = 17.50, Entropy = 4.13 bits
Computation based on 634 words.
Number of 3-grams hit = 631  (99.53%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article762.text
Will force inclusive back-off from OOVs.
Perplexity = 17.58, Entropy = 4.14 bits
Computation based on 782 words.
Number of 3-grams hit = 771  (98.59%)
Number of 2-grams hit = 10  (1.28%)
Number of 1-grams hit = 1  (0.13%)
10 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article763.text
Will force inclusive back-off from OOVs.
Perplexity = 18.34, Entropy = 4.20 bits
Computation based on 722 words.
Number of 3-grams hit = 717  (99.31%)
Number of 2-grams hit = 4  (0.55%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article764.text
Will force inclusive back-off from OOVs.
Perplexity = 16.11, Entropy = 4.01 bits
Computation based on 539 words.
Number of 3-grams hit = 535  (99.26%)
Number of 2-grams hit = 3  (0.56%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article765.text
Will force inclusive back-off from OOVs.
Perplexity = 13.58, Entropy = 3.76 bits
Computation based on 834 words.
Number of 3-grams hit = 828  (99.28%)
Number of 2-grams hit = 5  (0.60%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article766.text
Will force inclusive back-off from OOVs.
Perplexity = 17.81, Entropy = 4.15 bits
Computation based on 267 words.
Number of 3-grams hit = 264  (98.88%)
Number of 2-grams hit = 2  (0.75%)
Number of 1-grams hit = 1  (0.37%)
1 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article767.text
Will force inclusive back-off from OOVs.
Perplexity = 16.42, Entropy = 4.04 bits
Computation based on 4411 words.
Number of 3-grams hit = 4401  (99.77%)
Number of 2-grams hit = 9  (0.20%)
Number of 1-grams hit = 1  (0.02%)
8 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article768.text
Will force inclusive back-off from OOVs.
Perplexity = 17.83, Entropy = 4.16 bits
Computation based on 458 words.
Number of 3-grams hit = 452  (98.69%)
Number of 2-grams hit = 5  (1.09%)
Number of 1-grams hit = 1  (0.22%)
4 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article769.text
Will force inclusive back-off from OOVs.
Perplexity = 18.00, Entropy = 4.17 bits
Computation based on 680 words.
Number of 3-grams hit = 677  (99.56%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article770.text
Will force inclusive back-off from OOVs.
Perplexity = 17.32, Entropy = 4.11 bits
Computation based on 605 words.
Number of 3-grams hit = 598  (98.84%)
Number of 2-grams hit = 6  (0.99%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article771.text
Will force inclusive back-off from OOVs.
Perplexity = 17.74, Entropy = 4.15 bits
Computation based on 463 words.
Number of 3-grams hit = 455  (98.27%)
Number of 2-grams hit = 7  (1.51%)
Number of 1-grams hit = 1  (0.22%)
6 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article772.text
Will force inclusive back-off from OOVs.
Perplexity = 15.78, Entropy = 3.98 bits
Computation based on 544 words.
Number of 3-grams hit = 539  (99.08%)
Number of 2-grams hit = 4  (0.74%)
Number of 1-grams hit = 1  (0.18%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article773.text
Will force inclusive back-off from OOVs.
Perplexity = 17.07, Entropy = 4.09 bits
Computation based on 2976 words.
Number of 3-grams hit = 2953  (99.23%)
Number of 2-grams hit = 22  (0.74%)
Number of 1-grams hit = 1  (0.03%)
22 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article774.text
Will force inclusive back-off from OOVs.
Perplexity = 19.36, Entropy = 4.27 bits
Computation based on 2020 words.
Number of 3-grams hit = 2016  (99.80%)
Number of 2-grams hit = 3  (0.15%)
Number of 1-grams hit = 1  (0.05%)
2 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article775.text
Will force inclusive back-off from OOVs.
Perplexity = 19.41, Entropy = 4.28 bits
Computation based on 503 words.
Number of 3-grams hit = 499  (99.20%)
Number of 2-grams hit = 3  (0.60%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article776.text
Will force inclusive back-off from OOVs.
Perplexity = 18.37, Entropy = 4.20 bits
Computation based on 1580 words.
Number of 3-grams hit = 1574  (99.62%)
Number of 2-grams hit = 5  (0.32%)
Number of 1-grams hit = 1  (0.06%)
4 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article777.text
Will force inclusive back-off from OOVs.
Perplexity = 18.82, Entropy = 4.23 bits
Computation based on 9364 words.
Number of 3-grams hit = 9348  (99.83%)
Number of 2-grams hit = 15  (0.16%)
Number of 1-grams hit = 1  (0.01%)
14 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article778.text
Will force inclusive back-off from OOVs.
Perplexity = 17.93, Entropy = 4.16 bits
Computation based on 503 words.
Number of 3-grams hit = 499  (99.20%)
Number of 2-grams hit = 3  (0.60%)
Number of 1-grams hit = 1  (0.20%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article779.text
Will force inclusive back-off from OOVs.
Perplexity = 18.84, Entropy = 4.24 bits
Computation based on 933 words.
Number of 3-grams hit = 930  (99.68%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article780.text
Will force inclusive back-off from OOVs.
Perplexity = 19.27, Entropy = 4.27 bits
Computation based on 401 words.
Number of 3-grams hit = 398  (99.25%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article781.text
Will force inclusive back-off from OOVs.
Perplexity = 17.67, Entropy = 4.14 bits
Computation based on 735 words.
Number of 3-grams hit = 732  (99.59%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article782.text
Will force inclusive back-off from OOVs.
Perplexity = 18.39, Entropy = 4.20 bits
Computation based on 1356 words.
Number of 3-grams hit = 1347  (99.34%)
Number of 2-grams hit = 8  (0.59%)
Number of 1-grams hit = 1  (0.07%)
7 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article783.text
Will force inclusive back-off from OOVs.
Perplexity = 16.58, Entropy = 4.05 bits
Computation based on 502 words.
Number of 3-grams hit = 499  (99.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article784.text
Will force inclusive back-off from OOVs.
Perplexity = 18.42, Entropy = 4.20 bits
Computation based on 1724 words.
Number of 3-grams hit = 1717  (99.59%)
Number of 2-grams hit = 6  (0.35%)
Number of 1-grams hit = 1  (0.06%)
5 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article785.text
Will force inclusive back-off from OOVs.
Perplexity = 18.44, Entropy = 4.21 bits
Computation based on 2638 words.
Number of 3-grams hit = 2631  (99.73%)
Number of 2-grams hit = 6  (0.23%)
Number of 1-grams hit = 1  (0.04%)
5 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article786.text
Will force inclusive back-off from OOVs.
Perplexity = 18.29, Entropy = 4.19 bits
Computation based on 354 words.
Number of 3-grams hit = 352  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article787.text
Will force inclusive back-off from OOVs.
Perplexity = 13.97, Entropy = 3.80 bits
Computation based on 477 words.
Number of 3-grams hit = 474  (99.37%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article788.text
Will force inclusive back-off from OOVs.
Perplexity = 20.94, Entropy = 4.39 bits
Computation based on 1451 words.
Number of 3-grams hit = 1444  (99.52%)
Number of 2-grams hit = 6  (0.41%)
Number of 1-grams hit = 1  (0.07%)
5 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article789.text
Will force inclusive back-off from OOVs.
Perplexity = 17.57, Entropy = 4.14 bits
Computation based on 11419 words.
Number of 3-grams hit = 11344  (99.34%)
Number of 2-grams hit = 74  (0.65%)
Number of 1-grams hit = 1  (0.01%)
73 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article790.text
Will force inclusive back-off from OOVs.
Perplexity = 17.73, Entropy = 4.15 bits
Computation based on 932 words.
Number of 3-grams hit = 923  (99.03%)
Number of 2-grams hit = 8  (0.86%)
Number of 1-grams hit = 1  (0.11%)
8 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article791.text
Will force inclusive back-off from OOVs.
Perplexity = 17.12, Entropy = 4.10 bits
Computation based on 692 words.
Number of 3-grams hit = 684  (98.84%)
Number of 2-grams hit = 7  (1.01%)
Number of 1-grams hit = 1  (0.14%)
6 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article792.text
Will force inclusive back-off from OOVs.
Perplexity = 17.01, Entropy = 4.09 bits
Computation based on 2866 words.
Number of 3-grams hit = 2847  (99.34%)
Number of 2-grams hit = 18  (0.63%)
Number of 1-grams hit = 1  (0.03%)
18 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article793.text
Will force inclusive back-off from OOVs.
Perplexity = 18.12, Entropy = 4.18 bits
Computation based on 180 words.
Number of 3-grams hit = 178  (98.89%)
Number of 2-grams hit = 1  (0.56%)
Number of 1-grams hit = 1  (0.56%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article794.text
Will force inclusive back-off from OOVs.
Perplexity = 13.05, Entropy = 3.71 bits
Computation based on 504 words.
Number of 3-grams hit = 502  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article795.text
Will force inclusive back-off from OOVs.
Perplexity = 15.91, Entropy = 3.99 bits
Computation based on 421 words.
Number of 3-grams hit = 412  (97.86%)
Number of 2-grams hit = 8  (1.90%)
Number of 1-grams hit = 1  (0.24%)
7 OOVs (1.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article796.text
Will force inclusive back-off from OOVs.
Perplexity = 17.37, Entropy = 4.12 bits
Computation based on 1490 words.
Number of 3-grams hit = 1478  (99.19%)
Number of 2-grams hit = 11  (0.74%)
Number of 1-grams hit = 1  (0.07%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article797.text
Will force inclusive back-off from OOVs.
Perplexity = 20.32, Entropy = 4.34 bits
Computation based on 374 words.
Number of 3-grams hit = 371  (99.20%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.27%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article798.text
Will force inclusive back-off from OOVs.
Perplexity = 16.79, Entropy = 4.07 bits
Computation based on 459 words.
Number of 3-grams hit = 456  (99.35%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article799.text
Will force inclusive back-off from OOVs.
Perplexity = 15.87, Entropy = 3.99 bits
Computation based on 751 words.
Number of 3-grams hit = 749  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article800.text
Will force inclusive back-off from OOVs.
Perplexity = 17.23, Entropy = 4.11 bits
Computation based on 427 words.
Number of 3-grams hit = 421  (98.59%)
Number of 2-grams hit = 5  (1.17%)
Number of 1-grams hit = 1  (0.23%)
5 OOVs (1.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article801.text
Will force inclusive back-off from OOVs.
Perplexity = 20.22, Entropy = 4.34 bits
Computation based on 3112 words.
Number of 3-grams hit = 3107  (99.84%)
Number of 2-grams hit = 4  (0.13%)
Number of 1-grams hit = 1  (0.03%)
3 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article802.text
Will force inclusive back-off from OOVs.
Perplexity = 13.80, Entropy = 3.79 bits
Computation based on 2109 words.
Number of 3-grams hit = 2106  (99.86%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
1 OOVs (0.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article803.text
Will force inclusive back-off from OOVs.
Perplexity = 18.32, Entropy = 4.20 bits
Computation based on 4223 words.
Number of 3-grams hit = 4218  (99.88%)
Number of 2-grams hit = 4  (0.09%)
Number of 1-grams hit = 1  (0.02%)
3 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article804.text
Will force inclusive back-off from OOVs.
Perplexity = 16.91, Entropy = 4.08 bits
Computation based on 1462 words.
Number of 3-grams hit = 1454  (99.45%)
Number of 2-grams hit = 7  (0.48%)
Number of 1-grams hit = 1  (0.07%)
6 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article805.text
Will force inclusive back-off from OOVs.
Perplexity = 16.86, Entropy = 4.08 bits
Computation based on 919 words.
Number of 3-grams hit = 912  (99.24%)
Number of 2-grams hit = 6  (0.65%)
Number of 1-grams hit = 1  (0.11%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article806.text
Will force inclusive back-off from OOVs.
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 3283 words.
Number of 3-grams hit = 3263  (99.39%)
Number of 2-grams hit = 19  (0.58%)
Number of 1-grams hit = 1  (0.03%)
20 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article807.text
Will force inclusive back-off from OOVs.
Perplexity = 17.60, Entropy = 4.14 bits
Computation based on 709 words.
Number of 3-grams hit = 706  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article808.text
Will force inclusive back-off from OOVs.
Perplexity = 17.08, Entropy = 4.09 bits
Computation based on 1680 words.
Number of 3-grams hit = 1665  (99.11%)
Number of 2-grams hit = 14  (0.83%)
Number of 1-grams hit = 1  (0.06%)
14 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article809.text
Will force inclusive back-off from OOVs.
Perplexity = 18.54, Entropy = 4.21 bits
Computation based on 2024 words.
Number of 3-grams hit = 2020  (99.80%)
Number of 2-grams hit = 3  (0.15%)
Number of 1-grams hit = 1  (0.05%)
2 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article810.text
Will force inclusive back-off from OOVs.
Perplexity = 16.98, Entropy = 4.09 bits
Computation based on 871 words.
Number of 3-grams hit = 867  (99.54%)
Number of 2-grams hit = 3  (0.34%)
Number of 1-grams hit = 1  (0.11%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article811.text
Will force inclusive back-off from OOVs.
Perplexity = 17.62, Entropy = 4.14 bits
Computation based on 4475 words.
Number of 3-grams hit = 4439  (99.20%)
Number of 2-grams hit = 35  (0.78%)
Number of 1-grams hit = 1  (0.02%)
36 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article812.text
Will force inclusive back-off from OOVs.
Perplexity = 19.17, Entropy = 4.26 bits
Computation based on 3368 words.
Number of 3-grams hit = 3365  (99.91%)
Number of 2-grams hit = 2  (0.06%)
Number of 1-grams hit = 1  (0.03%)
1 OOVs (0.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article813.text
Will force inclusive back-off from OOVs.
Perplexity = 16.35, Entropy = 4.03 bits
Computation based on 1377 words.
Number of 3-grams hit = 1372  (99.64%)
Number of 2-grams hit = 4  (0.29%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article814.text
Will force inclusive back-off from OOVs.
Perplexity = 16.46, Entropy = 4.04 bits
Computation based on 850 words.
Number of 3-grams hit = 846  (99.53%)
Number of 2-grams hit = 3  (0.35%)
Number of 1-grams hit = 1  (0.12%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article815.text
Will force inclusive back-off from OOVs.
Perplexity = 16.75, Entropy = 4.07 bits
Computation based on 1263 words.
Number of 3-grams hit = 1250  (98.97%)
Number of 2-grams hit = 12  (0.95%)
Number of 1-grams hit = 1  (0.08%)
11 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article816.text
Will force inclusive back-off from OOVs.
Perplexity = 19.09, Entropy = 4.26 bits
Computation based on 1332 words.
Number of 3-grams hit = 1329  (99.77%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.08%)
1 OOVs (0.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article817.text
Will force inclusive back-off from OOVs.
Perplexity = 17.44, Entropy = 4.12 bits
Computation based on 528 words.
Number of 3-grams hit = 521  (98.67%)
Number of 2-grams hit = 6  (1.14%)
Number of 1-grams hit = 1  (0.19%)
5 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article818.text
Will force inclusive back-off from OOVs.
Perplexity = 17.97, Entropy = 4.17 bits
Computation based on 549 words.
Number of 3-grams hit = 543  (98.91%)
Number of 2-grams hit = 5  (0.91%)
Number of 1-grams hit = 1  (0.18%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article819.text
Will force inclusive back-off from OOVs.
Perplexity = 16.60, Entropy = 4.05 bits
Computation based on 1791 words.
Number of 3-grams hit = 1771  (98.88%)
Number of 2-grams hit = 19  (1.06%)
Number of 1-grams hit = 1  (0.06%)
18 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article820.text
Will force inclusive back-off from OOVs.
Perplexity = 15.29, Entropy = 3.93 bits
Computation based on 814 words.
Number of 3-grams hit = 806  (99.02%)
Number of 2-grams hit = 7  (0.86%)
Number of 1-grams hit = 1  (0.12%)
6 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article821.text
Will force inclusive back-off from OOVs.
Perplexity = 20.26, Entropy = 4.34 bits
Computation based on 1104 words.
Number of 3-grams hit = 1102  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article822.text
Will force inclusive back-off from OOVs.
Perplexity = 17.22, Entropy = 4.11 bits
Computation based on 2402 words.
Number of 3-grams hit = 2378  (99.00%)
Number of 2-grams hit = 23  (0.96%)
Number of 1-grams hit = 1  (0.04%)
22 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article823.text
Will force inclusive back-off from OOVs.
Perplexity = 16.76, Entropy = 4.07 bits
Computation based on 1301 words.
Number of 3-grams hit = 1291  (99.23%)
Number of 2-grams hit = 9  (0.69%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article824.text
Will force inclusive back-off from OOVs.
Perplexity = 15.50, Entropy = 3.95 bits
Computation based on 501 words.
Number of 3-grams hit = 498  (99.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article825.text
Will force inclusive back-off from OOVs.
Perplexity = 17.86, Entropy = 4.16 bits
Computation based on 567 words.
Number of 3-grams hit = 559  (98.59%)
Number of 2-grams hit = 7  (1.23%)
Number of 1-grams hit = 1  (0.18%)
6 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article826.text
Will force inclusive back-off from OOVs.
Perplexity = 19.31, Entropy = 4.27 bits
Computation based on 892 words.
Number of 3-grams hit = 889  (99.66%)
Number of 2-grams hit = 2  (0.22%)
Number of 1-grams hit = 1  (0.11%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article827.text
Will force inclusive back-off from OOVs.
Perplexity = 18.58, Entropy = 4.22 bits
Computation based on 642 words.
Number of 3-grams hit = 634  (98.75%)
Number of 2-grams hit = 7  (1.09%)
Number of 1-grams hit = 1  (0.16%)
6 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article828.text
Will force inclusive back-off from OOVs.
Perplexity = 19.94, Entropy = 4.32 bits
Computation based on 11962 words.
Number of 3-grams hit = 11926  (99.70%)
Number of 2-grams hit = 35  (0.29%)
Number of 1-grams hit = 1  (0.01%)
35 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article829.text
Will force inclusive back-off from OOVs.
Perplexity = 13.38, Entropy = 3.74 bits
Computation based on 536 words.
Number of 3-grams hit = 530  (98.88%)
Number of 2-grams hit = 5  (0.93%)
Number of 1-grams hit = 1  (0.19%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article830.text
Will force inclusive back-off from OOVs.
Perplexity = 17.48, Entropy = 4.13 bits
Computation based on 2314 words.
Number of 3-grams hit = 2307  (99.70%)
Number of 2-grams hit = 6  (0.26%)
Number of 1-grams hit = 1  (0.04%)
5 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article831.text
Will force inclusive back-off from OOVs.
Perplexity = 16.61, Entropy = 4.05 bits
Computation based on 1020 words.
Number of 3-grams hit = 1008  (98.82%)
Number of 2-grams hit = 11  (1.08%)
Number of 1-grams hit = 1  (0.10%)
10 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article832.text
Will force inclusive back-off from OOVs.
Perplexity = 16.39, Entropy = 4.03 bits
Computation based on 419 words.
Number of 3-grams hit = 414  (98.81%)
Number of 2-grams hit = 4  (0.95%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article833.text
Will force inclusive back-off from OOVs.
Perplexity = 16.49, Entropy = 4.04 bits
Computation based on 825 words.
Number of 3-grams hit = 819  (99.27%)
Number of 2-grams hit = 5  (0.61%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article834.text
Will force inclusive back-off from OOVs.
Perplexity = 18.14, Entropy = 4.18 bits
Computation based on 877 words.
Number of 3-grams hit = 870  (99.20%)
Number of 2-grams hit = 6  (0.68%)
Number of 1-grams hit = 1  (0.11%)
5 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article835.text
Will force inclusive back-off from OOVs.
Perplexity = 17.27, Entropy = 4.11 bits
Computation based on 458 words.
Number of 3-grams hit = 453  (98.91%)
Number of 2-grams hit = 4  (0.87%)
Number of 1-grams hit = 1  (0.22%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article836.text
Will force inclusive back-off from OOVs.
Perplexity = 16.27, Entropy = 4.02 bits
Computation based on 490 words.
Number of 3-grams hit = 487  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article837.text
Will force inclusive back-off from OOVs.
Perplexity = 13.36, Entropy = 3.74 bits
Computation based on 848 words.
Number of 3-grams hit = 842  (99.29%)
Number of 2-grams hit = 5  (0.59%)
Number of 1-grams hit = 1  (0.12%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article838.text
Will force inclusive back-off from OOVs.
Perplexity = 16.86, Entropy = 4.08 bits
Computation based on 2259 words.
Number of 3-grams hit = 2245  (99.38%)
Number of 2-grams hit = 13  (0.58%)
Number of 1-grams hit = 1  (0.04%)
12 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article839.text
Will force inclusive back-off from OOVs.
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 460 words.
Number of 3-grams hit = 457  (99.35%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article840.text
Will force inclusive back-off from OOVs.
Perplexity = 16.81, Entropy = 4.07 bits
Computation based on 519 words.
Number of 3-grams hit = 514  (99.04%)
Number of 2-grams hit = 4  (0.77%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article841.text
Will force inclusive back-off from OOVs.
Perplexity = 17.94, Entropy = 4.16 bits
Computation based on 912 words.
Number of 3-grams hit = 903  (99.01%)
Number of 2-grams hit = 8  (0.88%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article842.text
Will force inclusive back-off from OOVs.
Perplexity = 17.36, Entropy = 4.12 bits
Computation based on 475 words.
Number of 3-grams hit = 472  (99.37%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article843.text
Will force inclusive back-off from OOVs.
Perplexity = 18.41, Entropy = 4.20 bits
Computation based on 1305 words.
Number of 3-grams hit = 1296  (99.31%)
Number of 2-grams hit = 8  (0.61%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article844.text
Will force inclusive back-off from OOVs.
Perplexity = 17.64, Entropy = 4.14 bits
Computation based on 15704 words.
Number of 3-grams hit = 15602  (99.35%)
Number of 2-grams hit = 101  (0.64%)
Number of 1-grams hit = 1  (0.01%)
107 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article845.text
Will force inclusive back-off from OOVs.
Perplexity = 16.89, Entropy = 4.08 bits
Computation based on 602 words.
Number of 3-grams hit = 599  (99.50%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article846.text
Will force inclusive back-off from OOVs.
Perplexity = 17.96, Entropy = 4.17 bits
Computation based on 563 words.
Number of 3-grams hit = 560  (99.47%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article847.text
Will force inclusive back-off from OOVs.
Perplexity = 18.75, Entropy = 4.23 bits
Computation based on 1611 words.
Number of 3-grams hit = 1601  (99.38%)
Number of 2-grams hit = 9  (0.56%)
Number of 1-grams hit = 1  (0.06%)
10 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article848.text
Will force inclusive back-off from OOVs.
Perplexity = 16.63, Entropy = 4.06 bits
Computation based on 366 words.
Number of 3-grams hit = 364  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article849.text
Will force inclusive back-off from OOVs.
Perplexity = 17.84, Entropy = 4.16 bits
Computation based on 1555 words.
Number of 3-grams hit = 1553  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article850.text
Will force inclusive back-off from OOVs.
Perplexity = 18.21, Entropy = 4.19 bits
Computation based on 444 words.
Number of 3-grams hit = 440  (99.10%)
Number of 2-grams hit = 3  (0.68%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article851.text
Will force inclusive back-off from OOVs.
Perplexity = 17.54, Entropy = 4.13 bits
Computation based on 552 words.
Number of 3-grams hit = 543  (98.37%)
Number of 2-grams hit = 8  (1.45%)
Number of 1-grams hit = 1  (0.18%)
7 OOVs (1.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article852.text
Will force inclusive back-off from OOVs.
Perplexity = 17.73, Entropy = 4.15 bits
Computation based on 624 words.
Number of 3-grams hit = 621  (99.52%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article853.text
Will force inclusive back-off from OOVs.
Perplexity = 17.28, Entropy = 4.11 bits
Computation based on 1479 words.
Number of 3-grams hit = 1470  (99.39%)
Number of 2-grams hit = 8  (0.54%)
Number of 1-grams hit = 1  (0.07%)
7 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article854.text
Will force inclusive back-off from OOVs.
Perplexity = 17.43, Entropy = 4.12 bits
Computation based on 2661 words.
Number of 3-grams hit = 2651  (99.62%)
Number of 2-grams hit = 9  (0.34%)
Number of 1-grams hit = 1  (0.04%)
8 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article855.text
Will force inclusive back-off from OOVs.
Perplexity = 22.41, Entropy = 4.49 bits
Computation based on 419 words.
Number of 3-grams hit = 412  (98.33%)
Number of 2-grams hit = 6  (1.43%)
Number of 1-grams hit = 1  (0.24%)
5 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article856.text
Will force inclusive back-off from OOVs.
Perplexity = 16.90, Entropy = 4.08 bits
Computation based on 825 words.
Number of 3-grams hit = 815  (98.79%)
Number of 2-grams hit = 9  (1.09%)
Number of 1-grams hit = 1  (0.12%)
8 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article857.text
Will force inclusive back-off from OOVs.
Perplexity = 13.67, Entropy = 3.77 bits
Computation based on 353 words.
Number of 3-grams hit = 349  (98.87%)
Number of 2-grams hit = 3  (0.85%)
Number of 1-grams hit = 1  (0.28%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article858.text
Will force inclusive back-off from OOVs.
Perplexity = 20.93, Entropy = 4.39 bits
Computation based on 237 words.
Number of 3-grams hit = 235  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article859.text
Will force inclusive back-off from OOVs.
Perplexity = 14.59, Entropy = 3.87 bits
Computation based on 1106 words.
Number of 3-grams hit = 1099  (99.37%)
Number of 2-grams hit = 6  (0.54%)
Number of 1-grams hit = 1  (0.09%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article860.text
Will force inclusive back-off from OOVs.
Perplexity = 17.95, Entropy = 4.17 bits
Computation based on 451 words.
Number of 3-grams hit = 448  (99.33%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article861.text
Will force inclusive back-off from OOVs.
Perplexity = 14.91, Entropy = 3.90 bits
Computation based on 392 words.
Number of 3-grams hit = 389  (99.23%)
Number of 2-grams hit = 2  (0.51%)
Number of 1-grams hit = 1  (0.26%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article862.text
Will force inclusive back-off from OOVs.
Perplexity = 22.98, Entropy = 4.52 bits
Computation based on 451 words.
Number of 3-grams hit = 447  (99.11%)
Number of 2-grams hit = 3  (0.67%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article863.text
Will force inclusive back-off from OOVs.
Perplexity = 17.33, Entropy = 4.12 bits
Computation based on 658 words.
Number of 3-grams hit = 652  (99.09%)
Number of 2-grams hit = 5  (0.76%)
Number of 1-grams hit = 1  (0.15%)
4 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article864.text
Will force inclusive back-off from OOVs.
Perplexity = 12.94, Entropy = 3.69 bits
Computation based on 447 words.
Number of 3-grams hit = 443  (99.11%)
Number of 2-grams hit = 3  (0.67%)
Number of 1-grams hit = 1  (0.22%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article865.text
Will force inclusive back-off from OOVs.
Perplexity = 21.08, Entropy = 4.40 bits
Computation based on 6161 words.
Number of 3-grams hit = 6155  (99.90%)
Number of 2-grams hit = 5  (0.08%)
Number of 1-grams hit = 1  (0.02%)
4 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article866.text
Will force inclusive back-off from OOVs.
Perplexity = 15.84, Entropy = 3.99 bits
Computation based on 625 words.
Number of 3-grams hit = 621  (99.36%)
Number of 2-grams hit = 3  (0.48%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article867.text
Will force inclusive back-off from OOVs.
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 341 words.
Number of 3-grams hit = 337  (98.83%)
Number of 2-grams hit = 3  (0.88%)
Number of 1-grams hit = 1  (0.29%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article868.text
Will force inclusive back-off from OOVs.
Perplexity = 17.95, Entropy = 4.17 bits
Computation based on 1649 words.
Number of 3-grams hit = 1639  (99.39%)
Number of 2-grams hit = 9  (0.55%)
Number of 1-grams hit = 1  (0.06%)
8 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article869.text
Will force inclusive back-off from OOVs.
Perplexity = 17.47, Entropy = 4.13 bits
Computation based on 3099 words.
Number of 3-grams hit = 3081  (99.42%)
Number of 2-grams hit = 17  (0.55%)
Number of 1-grams hit = 1  (0.03%)
17 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article870.text
Will force inclusive back-off from OOVs.
Perplexity = 14.75, Entropy = 3.88 bits
Computation based on 768 words.
Number of 3-grams hit = 762  (99.22%)
Number of 2-grams hit = 5  (0.65%)
Number of 1-grams hit = 1  (0.13%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article871.text
Will force inclusive back-off from OOVs.
Perplexity = 17.30, Entropy = 4.11 bits
Computation based on 411 words.
Number of 3-grams hit = 406  (98.78%)
Number of 2-grams hit = 4  (0.97%)
Number of 1-grams hit = 1  (0.24%)
3 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article872.text
Will force inclusive back-off from OOVs.
Perplexity = 15.95, Entropy = 4.00 bits
Computation based on 295 words.
Number of 3-grams hit = 293  (99.32%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article873.text
Will force inclusive back-off from OOVs.
Perplexity = 16.19, Entropy = 4.02 bits
Computation based on 930 words.
Number of 3-grams hit = 923  (99.25%)
Number of 2-grams hit = 6  (0.65%)
Number of 1-grams hit = 1  (0.11%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article874.text
Will force inclusive back-off from OOVs.
Perplexity = 16.78, Entropy = 4.07 bits
Computation based on 426 words.
Number of 3-grams hit = 421  (98.83%)
Number of 2-grams hit = 4  (0.94%)
Number of 1-grams hit = 1  (0.23%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article875.text
Will force inclusive back-off from OOVs.
Perplexity = 14.50, Entropy = 3.86 bits
Computation based on 786 words.
Number of 3-grams hit = 777  (98.85%)
Number of 2-grams hit = 8  (1.02%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article876.text
Will force inclusive back-off from OOVs.
Perplexity = 17.76, Entropy = 4.15 bits
Computation based on 9154 words.
Number of 3-grams hit = 9077  (99.16%)
Number of 2-grams hit = 76  (0.83%)
Number of 1-grams hit = 1  (0.01%)
77 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article877.text
Will force inclusive back-off from OOVs.
Perplexity = 18.70, Entropy = 4.23 bits
Computation based on 455 words.
Number of 3-grams hit = 449  (98.68%)
Number of 2-grams hit = 5  (1.10%)
Number of 1-grams hit = 1  (0.22%)
4 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article878.text
Will force inclusive back-off from OOVs.
Perplexity = 16.44, Entropy = 4.04 bits
Computation based on 336 words.
Number of 3-grams hit = 332  (98.81%)
Number of 2-grams hit = 3  (0.89%)
Number of 1-grams hit = 1  (0.30%)
2 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article879.text
Will force inclusive back-off from OOVs.
Perplexity = 17.94, Entropy = 4.17 bits
Computation based on 539 words.
Number of 3-grams hit = 535  (99.26%)
Number of 2-grams hit = 3  (0.56%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article880.text
Will force inclusive back-off from OOVs.
Perplexity = 19.63, Entropy = 4.30 bits
Computation based on 460 words.
Number of 3-grams hit = 458  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article881.text
Will force inclusive back-off from OOVs.
Perplexity = 15.18, Entropy = 3.92 bits
Computation based on 342 words.
Number of 3-grams hit = 336  (98.25%)
Number of 2-grams hit = 5  (1.46%)
Number of 1-grams hit = 1  (0.29%)
4 OOVs (1.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article882.text
Will force inclusive back-off from OOVs.
Perplexity = 18.47, Entropy = 4.21 bits
Computation based on 589 words.
Number of 3-grams hit = 583  (98.98%)
Number of 2-grams hit = 5  (0.85%)
Number of 1-grams hit = 1  (0.17%)
4 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article883.text
Will force inclusive back-off from OOVs.
Perplexity = 17.39, Entropy = 4.12 bits
Computation based on 590 words.
Number of 3-grams hit = 586  (99.32%)
Number of 2-grams hit = 3  (0.51%)
Number of 1-grams hit = 1  (0.17%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article884.text
Will force inclusive back-off from OOVs.
Perplexity = 15.13, Entropy = 3.92 bits
Computation based on 729 words.
Number of 3-grams hit = 727  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article885.text
Will force inclusive back-off from OOVs.
Perplexity = 19.67, Entropy = 4.30 bits
Computation based on 485 words.
Number of 3-grams hit = 483  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article886.text
Will force inclusive back-off from OOVs.
Perplexity = 18.32, Entropy = 4.20 bits
Computation based on 470 words.
Number of 3-grams hit = 464  (98.72%)
Number of 2-grams hit = 5  (1.06%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article887.text
Will force inclusive back-off from OOVs.
Perplexity = 16.26, Entropy = 4.02 bits
Computation based on 480 words.
Number of 3-grams hit = 477  (99.38%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article888.text
Will force inclusive back-off from OOVs.
Perplexity = 16.01, Entropy = 4.00 bits
Computation based on 1202 words.
Number of 3-grams hit = 1193  (99.25%)
Number of 2-grams hit = 8  (0.67%)
Number of 1-grams hit = 1  (0.08%)
7 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article889.text
Will force inclusive back-off from OOVs.
Perplexity = 16.97, Entropy = 4.08 bits
Computation based on 4280 words.
Number of 3-grams hit = 4247  (99.23%)
Number of 2-grams hit = 32  (0.75%)
Number of 1-grams hit = 1  (0.02%)
33 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article890.text
Will force inclusive back-off from OOVs.
Perplexity = 13.93, Entropy = 3.80 bits
Computation based on 1047 words.
Number of 3-grams hit = 1041  (99.43%)
Number of 2-grams hit = 5  (0.48%)
Number of 1-grams hit = 1  (0.10%)
4 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article891.text
Will force inclusive back-off from OOVs.
Perplexity = 18.46, Entropy = 4.21 bits
Computation based on 801 words.
Number of 3-grams hit = 792  (98.88%)
Number of 2-grams hit = 8  (1.00%)
Number of 1-grams hit = 1  (0.12%)
7 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article892.text
Will force inclusive back-off from OOVs.
Perplexity = 18.94, Entropy = 4.24 bits
Computation based on 3160 words.
Number of 3-grams hit = 3155  (99.84%)
Number of 2-grams hit = 4  (0.13%)
Number of 1-grams hit = 1  (0.03%)
3 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article893.text
Will force inclusive back-off from OOVs.
Perplexity = 18.07, Entropy = 4.18 bits
Computation based on 628 words.
Number of 3-grams hit = 621  (98.89%)
Number of 2-grams hit = 6  (0.96%)
Number of 1-grams hit = 1  (0.16%)
5 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article894.text
Will force inclusive back-off from OOVs.
Perplexity = 17.16, Entropy = 4.10 bits
Computation based on 739 words.
Number of 3-grams hit = 729  (98.65%)
Number of 2-grams hit = 9  (1.22%)
Number of 1-grams hit = 1  (0.14%)
8 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article895.text
Will force inclusive back-off from OOVs.
Perplexity = 17.90, Entropy = 4.16 bits
Computation based on 167 words.
Number of 3-grams hit = 163  (97.60%)
Number of 2-grams hit = 3  (1.80%)
Number of 1-grams hit = 1  (0.60%)
2 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article896.text
Will force inclusive back-off from OOVs.
Perplexity = 19.40, Entropy = 4.28 bits
Computation based on 410 words.
Number of 3-grams hit = 403  (98.29%)
Number of 2-grams hit = 6  (1.46%)
Number of 1-grams hit = 1  (0.24%)
5 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article897.text
Will force inclusive back-off from OOVs.
Perplexity = 18.12, Entropy = 4.18 bits
Computation based on 698 words.
Number of 3-grams hit = 694  (99.43%)
Number of 2-grams hit = 3  (0.43%)
Number of 1-grams hit = 1  (0.14%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article898.text
Will force inclusive back-off from OOVs.
Perplexity = 18.96, Entropy = 4.24 bits
Computation based on 1591 words.
Number of 3-grams hit = 1578  (99.18%)
Number of 2-grams hit = 12  (0.75%)
Number of 1-grams hit = 1  (0.06%)
11 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article899.text
Will force inclusive back-off from OOVs.
Perplexity = 15.66, Entropy = 3.97 bits
Computation based on 202 words.
Number of 3-grams hit = 198  (98.02%)
Number of 2-grams hit = 3  (1.49%)
Number of 1-grams hit = 1  (0.50%)
2 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article900.text
Will force inclusive back-off from OOVs.
Perplexity = 15.75, Entropy = 3.98 bits
Computation based on 1046 words.
Number of 3-grams hit = 1039  (99.33%)
Number of 2-grams hit = 6  (0.57%)
Number of 1-grams hit = 1  (0.10%)
5 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article901.text
Will force inclusive back-off from OOVs.
Perplexity = 15.42, Entropy = 3.95 bits
Computation based on 399 words.
Number of 3-grams hit = 397  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article902.text
Will force inclusive back-off from OOVs.
Perplexity = 18.07, Entropy = 4.18 bits
Computation based on 2002 words.
Number of 3-grams hit = 1997  (99.75%)
Number of 2-grams hit = 4  (0.20%)
Number of 1-grams hit = 1  (0.05%)
3 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article903.text
Will force inclusive back-off from OOVs.
Perplexity = 16.94, Entropy = 4.08 bits
Computation based on 1186 words.
Number of 3-grams hit = 1181  (99.58%)
Number of 2-grams hit = 4  (0.34%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article904.text
Will force inclusive back-off from OOVs.
Perplexity = 17.26, Entropy = 4.11 bits
Computation based on 622 words.
Number of 3-grams hit = 619  (99.52%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article905.text
Will force inclusive back-off from OOVs.
Perplexity = 18.93, Entropy = 4.24 bits
Computation based on 348 words.
Number of 3-grams hit = 338  (97.13%)
Number of 2-grams hit = 9  (2.59%)
Number of 1-grams hit = 1  (0.29%)
9 OOVs (2.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article906.text
Will force inclusive back-off from OOVs.
Perplexity = 20.54, Entropy = 4.36 bits
Computation based on 436 words.
Number of 3-grams hit = 434  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article907.text
Will force inclusive back-off from OOVs.
Perplexity = 17.76, Entropy = 4.15 bits
Computation based on 445 words.
Number of 3-grams hit = 442  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article908.text
Will force inclusive back-off from OOVs.
Perplexity = 14.37, Entropy = 3.84 bits
Computation based on 525 words.
Number of 3-grams hit = 522  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article909.text
Will force inclusive back-off from OOVs.
Perplexity = 16.97, Entropy = 4.09 bits
Computation based on 412 words.
Number of 3-grams hit = 405  (98.30%)
Number of 2-grams hit = 6  (1.46%)
Number of 1-grams hit = 1  (0.24%)
5 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article910.text
Will force inclusive back-off from OOVs.
Perplexity = 17.79, Entropy = 4.15 bits
Computation based on 668 words.
Number of 3-grams hit = 663  (99.25%)
Number of 2-grams hit = 4  (0.60%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article911.text
Will force inclusive back-off from OOVs.
Perplexity = 18.20, Entropy = 4.19 bits
Computation based on 634 words.
Number of 3-grams hit = 629  (99.21%)
Number of 2-grams hit = 4  (0.63%)
Number of 1-grams hit = 1  (0.16%)
3 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article912.text
Will force inclusive back-off from OOVs.
Perplexity = 19.33, Entropy = 4.27 bits
Computation based on 672 words.
Number of 3-grams hit = 667  (99.26%)
Number of 2-grams hit = 4  (0.60%)
Number of 1-grams hit = 1  (0.15%)
3 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article913.text
Will force inclusive back-off from OOVs.
Perplexity = 18.06, Entropy = 4.17 bits
Computation based on 666 words.
Number of 3-grams hit = 660  (99.10%)
Number of 2-grams hit = 5  (0.75%)
Number of 1-grams hit = 1  (0.15%)
4 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article914.text
Will force inclusive back-off from OOVs.
Perplexity = 14.89, Entropy = 3.90 bits
Computation based on 530 words.
Number of 3-grams hit = 526  (99.25%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article915.text
Will force inclusive back-off from OOVs.
Perplexity = 17.44, Entropy = 4.12 bits
Computation based on 1203 words.
Number of 3-grams hit = 1193  (99.17%)
Number of 2-grams hit = 9  (0.75%)
Number of 1-grams hit = 1  (0.08%)
8 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article916.text
Will force inclusive back-off from OOVs.
Perplexity = 14.51, Entropy = 3.86 bits
Computation based on 940 words.
Number of 3-grams hit = 938  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article917.text
Will force inclusive back-off from OOVs.
Perplexity = 16.75, Entropy = 4.07 bits
Computation based on 1081 words.
Number of 3-grams hit = 1078  (99.72%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article918.text
Will force inclusive back-off from OOVs.
Perplexity = 17.20, Entropy = 4.10 bits
Computation based on 483 words.
Number of 3-grams hit = 479  (99.17%)
Number of 2-grams hit = 3  (0.62%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article919.text
Will force inclusive back-off from OOVs.
Perplexity = 18.00, Entropy = 4.17 bits
Computation based on 3288 words.
Number of 3-grams hit = 3259  (99.12%)
Number of 2-grams hit = 28  (0.85%)
Number of 1-grams hit = 1  (0.03%)
27 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article920.text
Will force inclusive back-off from OOVs.
Perplexity = 17.21, Entropy = 4.10 bits
Computation based on 915 words.
Number of 3-grams hit = 910  (99.45%)
Number of 2-grams hit = 4  (0.44%)
Number of 1-grams hit = 1  (0.11%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article921.text
Will force inclusive back-off from OOVs.
Perplexity = 15.95, Entropy = 4.00 bits
Computation based on 795 words.
Number of 3-grams hit = 790  (99.37%)
Number of 2-grams hit = 4  (0.50%)
Number of 1-grams hit = 1  (0.13%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article922.text
Will force inclusive back-off from OOVs.
Perplexity = 18.41, Entropy = 4.20 bits
Computation based on 469 words.
Number of 3-grams hit = 462  (98.51%)
Number of 2-grams hit = 6  (1.28%)
Number of 1-grams hit = 1  (0.21%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article923.text
Will force inclusive back-off from OOVs.
Perplexity = 18.10, Entropy = 4.18 bits
Computation based on 517 words.
Number of 3-grams hit = 512  (99.03%)
Number of 2-grams hit = 4  (0.77%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article924.text
Will force inclusive back-off from OOVs.
Perplexity = 17.96, Entropy = 4.17 bits
Computation based on 341 words.
Number of 3-grams hit = 337  (98.83%)
Number of 2-grams hit = 3  (0.88%)
Number of 1-grams hit = 1  (0.29%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article925.text
Will force inclusive back-off from OOVs.
Perplexity = 19.07, Entropy = 4.25 bits
Computation based on 3826 words.
Number of 3-grams hit = 3816  (99.74%)
Number of 2-grams hit = 9  (0.24%)
Number of 1-grams hit = 1  (0.03%)
8 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article926.text
Will force inclusive back-off from OOVs.
Perplexity = 18.13, Entropy = 4.18 bits
Computation based on 1334 words.
Number of 3-grams hit = 1329  (99.63%)
Number of 2-grams hit = 4  (0.30%)
Number of 1-grams hit = 1  (0.07%)
3 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article927.text
Will force inclusive back-off from OOVs.
Perplexity = 18.28, Entropy = 4.19 bits
Computation based on 404 words.
Number of 3-grams hit = 399  (98.76%)
Number of 2-grams hit = 4  (0.99%)
Number of 1-grams hit = 1  (0.25%)
3 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article928.text
Will force inclusive back-off from OOVs.
Perplexity = 16.02, Entropy = 4.00 bits
Computation based on 487 words.
Number of 3-grams hit = 483  (99.18%)
Number of 2-grams hit = 3  (0.62%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article929.text
Will force inclusive back-off from OOVs.
Perplexity = 15.66, Entropy = 3.97 bits
Computation based on 499 words.
Number of 3-grams hit = 492  (98.60%)
Number of 2-grams hit = 6  (1.20%)
Number of 1-grams hit = 1  (0.20%)
5 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article930.text
Will force inclusive back-off from OOVs.
Perplexity = 15.45, Entropy = 3.95 bits
Computation based on 434 words.
Number of 3-grams hit = 432  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article931.text
Will force inclusive back-off from OOVs.
Perplexity = 16.03, Entropy = 4.00 bits
Computation based on 596 words.
Number of 3-grams hit = 593  (99.50%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article932.text
Will force inclusive back-off from OOVs.
Perplexity = 15.12, Entropy = 3.92 bits
Computation based on 209 words.
Number of 3-grams hit = 204  (97.61%)
Number of 2-grams hit = 4  (1.91%)
Number of 1-grams hit = 1  (0.48%)
3 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article933.text
Will force inclusive back-off from OOVs.
Perplexity = 15.55, Entropy = 3.96 bits
Computation based on 411 words.
Number of 3-grams hit = 408  (99.27%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.24%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article934.text
Will force inclusive back-off from OOVs.
Perplexity = 15.35, Entropy = 3.94 bits
Computation based on 389 words.
Number of 3-grams hit = 385  (98.97%)
Number of 2-grams hit = 3  (0.77%)
Number of 1-grams hit = 1  (0.26%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article935.text
Will force inclusive back-off from OOVs.
Perplexity = 18.15, Entropy = 4.18 bits
Computation based on 1145 words.
Number of 3-grams hit = 1133  (98.95%)
Number of 2-grams hit = 11  (0.96%)
Number of 1-grams hit = 1  (0.09%)
11 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article936.text
Will force inclusive back-off from OOVs.
Perplexity = 15.68, Entropy = 3.97 bits
Computation based on 439 words.
Number of 3-grams hit = 435  (99.09%)
Number of 2-grams hit = 3  (0.68%)
Number of 1-grams hit = 1  (0.23%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article937.text
Will force inclusive back-off from OOVs.
Perplexity = 15.03, Entropy = 3.91 bits
Computation based on 226 words.
Number of 3-grams hit = 222  (98.23%)
Number of 2-grams hit = 3  (1.33%)
Number of 1-grams hit = 1  (0.44%)
2 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article938.text
Will force inclusive back-off from OOVs.
Perplexity = 16.94, Entropy = 4.08 bits
Computation based on 1292 words.
Number of 3-grams hit = 1282  (99.23%)
Number of 2-grams hit = 9  (0.70%)
Number of 1-grams hit = 1  (0.08%)
8 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article939.text
Will force inclusive back-off from OOVs.
Perplexity = 16.45, Entropy = 4.04 bits
Computation based on 481 words.
Number of 3-grams hit = 479  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article940.text
Will force inclusive back-off from OOVs.
Perplexity = 17.85, Entropy = 4.16 bits
Computation based on 1885 words.
Number of 3-grams hit = 1867  (99.05%)
Number of 2-grams hit = 17  (0.90%)
Number of 1-grams hit = 1  (0.05%)
16 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article941.text
Will force inclusive back-off from OOVs.
Perplexity = 15.72, Entropy = 3.97 bits
Computation based on 1216 words.
Number of 3-grams hit = 1212  (99.67%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article942.text
Will force inclusive back-off from OOVs.
Perplexity = 16.75, Entropy = 4.07 bits
Computation based on 526 words.
Number of 3-grams hit = 523  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article943.text
Will force inclusive back-off from OOVs.
Perplexity = 18.61, Entropy = 4.22 bits
Computation based on 1399 words.
Number of 3-grams hit = 1391  (99.43%)
Number of 2-grams hit = 7  (0.50%)
Number of 1-grams hit = 1  (0.07%)
6 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article944.text
Will force inclusive back-off from OOVs.
Perplexity = 17.39, Entropy = 4.12 bits
Computation based on 435 words.
Number of 3-grams hit = 432  (99.31%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article945.text
Will force inclusive back-off from OOVs.
Perplexity = 17.01, Entropy = 4.09 bits
Computation based on 3539 words.
Number of 3-grams hit = 3532  (99.80%)
Number of 2-grams hit = 6  (0.17%)
Number of 1-grams hit = 1  (0.03%)
5 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article946.text
Will force inclusive back-off from OOVs.
Perplexity = 17.90, Entropy = 4.16 bits
Computation based on 560 words.
Number of 3-grams hit = 552  (98.57%)
Number of 2-grams hit = 7  (1.25%)
Number of 1-grams hit = 1  (0.18%)
6 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article947.text
Will force inclusive back-off from OOVs.
Perplexity = 17.95, Entropy = 4.17 bits
Computation based on 428 words.
Number of 3-grams hit = 425  (99.30%)
Number of 2-grams hit = 2  (0.47%)
Number of 1-grams hit = 1  (0.23%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article948.text
Will force inclusive back-off from OOVs.
Perplexity = 18.45, Entropy = 4.21 bits
Computation based on 577 words.
Number of 3-grams hit = 570  (98.79%)
Number of 2-grams hit = 6  (1.04%)
Number of 1-grams hit = 1  (0.17%)
5 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article949.text
Will force inclusive back-off from OOVs.
Perplexity = 17.17, Entropy = 4.10 bits
Computation based on 1691 words.
Number of 3-grams hit = 1672  (98.88%)
Number of 2-grams hit = 18  (1.06%)
Number of 1-grams hit = 1  (0.06%)
19 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article950.text
Will force inclusive back-off from OOVs.
Perplexity = 14.41, Entropy = 3.85 bits
Computation based on 501 words.
Number of 3-grams hit = 493  (98.40%)
Number of 2-grams hit = 7  (1.40%)
Number of 1-grams hit = 1  (0.20%)
6 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article951.text
Will force inclusive back-off from OOVs.
Perplexity = 17.76, Entropy = 4.15 bits
Computation based on 566 words.
Number of 3-grams hit = 564  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article952.text
Will force inclusive back-off from OOVs.
Perplexity = 16.47, Entropy = 4.04 bits
Computation based on 1278 words.
Number of 3-grams hit = 1267  (99.14%)
Number of 2-grams hit = 10  (0.78%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article953.text
Will force inclusive back-off from OOVs.
Perplexity = 17.93, Entropy = 4.16 bits
Computation based on 2131 words.
Number of 3-grams hit = 2125  (99.72%)
Number of 2-grams hit = 5  (0.23%)
Number of 1-grams hit = 1  (0.05%)
4 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article954.text
Will force inclusive back-off from OOVs.
Perplexity = 19.98, Entropy = 4.32 bits
Computation based on 732 words.
Number of 3-grams hit = 729  (99.59%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article955.text
Will force inclusive back-off from OOVs.
Perplexity = 19.25, Entropy = 4.27 bits
Computation based on 1475 words.
Number of 3-grams hit = 1471  (99.73%)
Number of 2-grams hit = 3  (0.20%)
Number of 1-grams hit = 1  (0.07%)
2 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article956.text
Will force inclusive back-off from OOVs.
Perplexity = 12.92, Entropy = 3.69 bits
Computation based on 1031 words.
Number of 3-grams hit = 1026  (99.52%)
Number of 2-grams hit = 4  (0.39%)
Number of 1-grams hit = 1  (0.10%)
3 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article957.text
Will force inclusive back-off from OOVs.
Perplexity = 13.71, Entropy = 3.78 bits
Computation based on 304 words.
Number of 3-grams hit = 302  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article958.text
Will force inclusive back-off from OOVs.
Perplexity = 18.11, Entropy = 4.18 bits
Computation based on 935 words.
Number of 3-grams hit = 926  (99.04%)
Number of 2-grams hit = 8  (0.86%)
Number of 1-grams hit = 1  (0.11%)
8 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article959.text
Will force inclusive back-off from OOVs.
Perplexity = 18.05, Entropy = 4.17 bits
Computation based on 198 words.
Number of 3-grams hit = 194  (97.98%)
Number of 2-grams hit = 3  (1.52%)
Number of 1-grams hit = 1  (0.51%)
3 OOVs (1.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article960.text
Will force inclusive back-off from OOVs.
Perplexity = 18.08, Entropy = 4.18 bits
Computation based on 606 words.
Number of 3-grams hit = 602  (99.34%)
Number of 2-grams hit = 3  (0.50%)
Number of 1-grams hit = 1  (0.17%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article961.text
Will force inclusive back-off from OOVs.
Perplexity = 16.33, Entropy = 4.03 bits
Computation based on 492 words.
Number of 3-grams hit = 490  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article962.text
Will force inclusive back-off from OOVs.
Perplexity = 19.42, Entropy = 4.28 bits
Computation based on 2235 words.
Number of 3-grams hit = 2232  (99.87%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.04%)
1 OOVs (0.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article963.text
Will force inclusive back-off from OOVs.
Perplexity = 18.14, Entropy = 4.18 bits
Computation based on 2007 words.
Number of 3-grams hit = 1995  (99.40%)
Number of 2-grams hit = 11  (0.55%)
Number of 1-grams hit = 1  (0.05%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article964.text
Will force inclusive back-off from OOVs.
Perplexity = 18.80, Entropy = 4.23 bits
Computation based on 1006 words.
Number of 3-grams hit = 998  (99.20%)
Number of 2-grams hit = 7  (0.70%)
Number of 1-grams hit = 1  (0.10%)
6 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article965.text
Will force inclusive back-off from OOVs.
Perplexity = 16.17, Entropy = 4.02 bits
Computation based on 646 words.
Number of 3-grams hit = 643  (99.54%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article966.text
Will force inclusive back-off from OOVs.
Perplexity = 17.30, Entropy = 4.11 bits
Computation based on 782 words.
Number of 3-grams hit = 773  (98.85%)
Number of 2-grams hit = 8  (1.02%)
Number of 1-grams hit = 1  (0.13%)
7 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article967.text
Will force inclusive back-off from OOVs.
Perplexity = 19.29, Entropy = 4.27 bits
Computation based on 391 words.
Number of 3-grams hit = 389  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article968.text
Will force inclusive back-off from OOVs.
Perplexity = 18.60, Entropy = 4.22 bits
Computation based on 1058 words.
Number of 3-grams hit = 1056  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article969.text
Will force inclusive back-off from OOVs.
Perplexity = 13.86, Entropy = 3.79 bits
Computation based on 725 words.
Number of 3-grams hit = 722  (99.59%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article970.text
Will force inclusive back-off from OOVs.
Perplexity = 17.26, Entropy = 4.11 bits
Computation based on 740 words.
Number of 3-grams hit = 735  (99.32%)
Number of 2-grams hit = 4  (0.54%)
Number of 1-grams hit = 1  (0.14%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article971.text
Will force inclusive back-off from OOVs.
Perplexity = 18.43, Entropy = 4.20 bits
Computation based on 466 words.
Number of 3-grams hit = 461  (98.93%)
Number of 2-grams hit = 4  (0.86%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article972.text
Will force inclusive back-off from OOVs.
Perplexity = 17.83, Entropy = 4.16 bits
Computation based on 1851 words.
Number of 3-grams hit = 1838  (99.30%)
Number of 2-grams hit = 12  (0.65%)
Number of 1-grams hit = 1  (0.05%)
11 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article973.text
Will force inclusive back-off from OOVs.
Perplexity = 19.60, Entropy = 4.29 bits
Computation based on 609 words.
Number of 3-grams hit = 605  (99.34%)
Number of 2-grams hit = 3  (0.49%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article974.text
Will force inclusive back-off from OOVs.
Perplexity = 16.62, Entropy = 4.05 bits
Computation based on 944 words.
Number of 3-grams hit = 935  (99.05%)
Number of 2-grams hit = 8  (0.85%)
Number of 1-grams hit = 1  (0.11%)
7 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article975.text
Will force inclusive back-off from OOVs.
Perplexity = 17.08, Entropy = 4.09 bits
Computation based on 352 words.
Number of 3-grams hit = 349  (99.15%)
Number of 2-grams hit = 2  (0.57%)
Number of 1-grams hit = 1  (0.28%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article976.text
Will force inclusive back-off from OOVs.
Perplexity = 13.98, Entropy = 3.81 bits
Computation based on 360 words.
Number of 3-grams hit = 358  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article977.text
Will force inclusive back-off from OOVs.
Perplexity = 18.36, Entropy = 4.20 bits
Computation based on 861 words.
Number of 3-grams hit = 859  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article978.text
Will force inclusive back-off from OOVs.
Perplexity = 17.85, Entropy = 4.16 bits
Computation based on 472 words.
Number of 3-grams hit = 468  (99.15%)
Number of 2-grams hit = 3  (0.64%)
Number of 1-grams hit = 1  (0.21%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article979.text
Will force inclusive back-off from OOVs.
Perplexity = 18.53, Entropy = 4.21 bits
Computation based on 1552 words.
Number of 3-grams hit = 1544  (99.48%)
Number of 2-grams hit = 7  (0.45%)
Number of 1-grams hit = 1  (0.06%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article980.text
Will force inclusive back-off from OOVs.
Perplexity = 16.06, Entropy = 4.01 bits
Computation based on 609 words.
Number of 3-grams hit = 605  (99.34%)
Number of 2-grams hit = 3  (0.49%)
Number of 1-grams hit = 1  (0.16%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article981.text
Will force inclusive back-off from OOVs.
Perplexity = 14.24, Entropy = 3.83 bits
Computation based on 346 words.
Number of 3-grams hit = 344  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article982.text
Will force inclusive back-off from OOVs.
Perplexity = 14.93, Entropy = 3.90 bits
Computation based on 836 words.
Number of 3-grams hit = 833  (99.64%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article983.text
Will force inclusive back-off from OOVs.
Perplexity = 17.30, Entropy = 4.11 bits
Computation based on 466 words.
Number of 3-grams hit = 462  (99.14%)
Number of 2-grams hit = 3  (0.64%)
Number of 1-grams hit = 1  (0.21%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article984.text
Will force inclusive back-off from OOVs.
Perplexity = 16.92, Entropy = 4.08 bits
Computation based on 457 words.
Number of 3-grams hit = 454  (99.34%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article985.text
Will force inclusive back-off from OOVs.
Perplexity = 16.29, Entropy = 4.03 bits
Computation based on 1243 words.
Number of 3-grams hit = 1238  (99.60%)
Number of 2-grams hit = 4  (0.32%)
Number of 1-grams hit = 1  (0.08%)
3 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article986.text
Will force inclusive back-off from OOVs.
Perplexity = 16.46, Entropy = 4.04 bits
Computation based on 268 words.
Number of 3-grams hit = 263  (98.13%)
Number of 2-grams hit = 4  (1.49%)
Number of 1-grams hit = 1  (0.37%)
3 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article987.text
Will force inclusive back-off from OOVs.
Perplexity = 18.18, Entropy = 4.18 bits
Computation based on 1743 words.
Number of 3-grams hit = 1740  (99.83%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.06%)
1 OOVs (0.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article988.text
Will force inclusive back-off from OOVs.
Perplexity = 19.44, Entropy = 4.28 bits
Computation based on 1240 words.
Number of 3-grams hit = 1229  (99.11%)
Number of 2-grams hit = 10  (0.81%)
Number of 1-grams hit = 1  (0.08%)
9 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article989.text
Will force inclusive back-off from OOVs.
Perplexity = 18.64, Entropy = 4.22 bits
Computation based on 2276 words.
Number of 3-grams hit = 2271  (99.78%)
Number of 2-grams hit = 4  (0.18%)
Number of 1-grams hit = 1  (0.04%)
3 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article990.text
Will force inclusive back-off from OOVs.
Perplexity = 16.07, Entropy = 4.01 bits
Computation based on 659 words.
Number of 3-grams hit = 656  (99.54%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article991.text
Will force inclusive back-off from OOVs.
Perplexity = 17.07, Entropy = 4.09 bits
Computation based on 1536 words.
Number of 3-grams hit = 1527  (99.41%)
Number of 2-grams hit = 8  (0.52%)
Number of 1-grams hit = 1  (0.07%)
7 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article992.text
Will force inclusive back-off from OOVs.
Perplexity = 17.07, Entropy = 4.09 bits
Computation based on 532 words.
Number of 3-grams hit = 527  (99.06%)
Number of 2-grams hit = 4  (0.75%)
Number of 1-grams hit = 1  (0.19%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article993.text
Will force inclusive back-off from OOVs.
Perplexity = 17.36, Entropy = 4.12 bits
Computation based on 2446 words.
Number of 3-grams hit = 2425  (99.14%)
Number of 2-grams hit = 20  (0.82%)
Number of 1-grams hit = 1  (0.04%)
20 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article994.text
Will force inclusive back-off from OOVs.
Perplexity = 18.62, Entropy = 4.22 bits
Computation based on 473 words.
Number of 3-grams hit = 466  (98.52%)
Number of 2-grams hit = 6  (1.27%)
Number of 1-grams hit = 1  (0.21%)
6 OOVs (1.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article995.text
Will force inclusive back-off from OOVs.
Perplexity = 18.20, Entropy = 4.19 bits
Computation based on 7534 words.
Number of 3-grams hit = 7509  (99.67%)
Number of 2-grams hit = 24  (0.32%)
Number of 1-grams hit = 1  (0.01%)
23 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article996.text
Will force inclusive back-off from OOVs.
Perplexity = 15.89, Entropy = 3.99 bits
Computation based on 574 words.
Number of 3-grams hit = 567  (98.78%)
Number of 2-grams hit = 6  (1.05%)
Number of 1-grams hit = 1  (0.17%)
6 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article997.text
Will force inclusive back-off from OOVs.
Perplexity = 16.87, Entropy = 4.08 bits
Computation based on 5292 words.
Number of 3-grams hit = 5252  (99.24%)
Number of 2-grams hit = 39  (0.74%)
Number of 1-grams hit = 1  (0.02%)
39 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article998.text
Will force inclusive back-off from OOVs.
Perplexity = 18.46, Entropy = 4.21 bits
Computation based on 518 words.
Number of 3-grams hit = 511  (98.65%)
Number of 2-grams hit = 6  (1.16%)
Number of 1-grams hit = 1  (0.19%)
6 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article999.text
Will force inclusive back-off from OOVs.
Perplexity = 19.42, Entropy = 4.28 bits
Computation based on 471 words.
Number of 3-grams hit = 468  (99.36%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : 