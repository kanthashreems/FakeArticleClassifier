evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article0.text
Will force exclusive back-off from OOVs.
Perplexity = 97.88, Entropy = 6.61 bits
Computation based on 1255 words.
Number of 3-grams hit = 979  (78.01%)
Number of 2-grams hit = 222  (17.69%)
Number of 1-grams hit = 54  (4.30%)
7 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article1.text
Will force exclusive back-off from OOVs.
Perplexity = 87.68, Entropy = 6.45 bits
Computation based on 1499 words.
Number of 3-grams hit = 1201  (80.12%)
Number of 2-grams hit = 246  (16.41%)
Number of 1-grams hit = 52  (3.47%)
6 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article2.text
Will force exclusive back-off from OOVs.
Perplexity = 106.61, Entropy = 6.74 bits
Computation based on 540 words.
Number of 3-grams hit = 420  (77.78%)
Number of 2-grams hit = 104  (19.26%)
Number of 1-grams hit = 16  (2.96%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article3.text
Will force exclusive back-off from OOVs.
Perplexity = 73.87, Entropy = 6.21 bits
Computation based on 620 words.
Number of 3-grams hit = 505  (81.45%)
Number of 2-grams hit = 99  (15.97%)
Number of 1-grams hit = 16  (2.58%)
5 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article4.text
Will force exclusive back-off from OOVs.
Perplexity = 178.65, Entropy = 7.48 bits
Computation based on 395 words.
Number of 3-grams hit = 274  (69.37%)
Number of 2-grams hit = 93  (23.54%)
Number of 1-grams hit = 28  (7.09%)
3 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article5.text
Will force exclusive back-off from OOVs.
Perplexity = 125.22, Entropy = 6.97 bits
Computation based on 888 words.
Number of 3-grams hit = 679  (76.46%)
Number of 2-grams hit = 161  (18.13%)
Number of 1-grams hit = 48  (5.41%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article6.text
Will force exclusive back-off from OOVs.
Perplexity = 335.90, Entropy = 8.39 bits
Computation based on 281 words.
Number of 3-grams hit = 172  (61.21%)
Number of 2-grams hit = 74  (26.33%)
Number of 1-grams hit = 35  (12.46%)
6 OOVs (2.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article7.text
Will force exclusive back-off from OOVs.
Perplexity = 67.40, Entropy = 6.07 bits
Computation based on 605 words.
Number of 3-grams hit = 533  (88.10%)
Number of 2-grams hit = 53  (8.76%)
Number of 1-grams hit = 19  (3.14%)
6 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article8.text
Will force exclusive back-off from OOVs.
Perplexity = 205.67, Entropy = 7.68 bits
Computation based on 496 words.
Number of 3-grams hit = 328  (66.13%)
Number of 2-grams hit = 123  (24.80%)
Number of 1-grams hit = 45  (9.07%)
8 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article9.text
Will force exclusive back-off from OOVs.
Perplexity = 102.82, Entropy = 6.68 bits
Computation based on 314 words.
Number of 3-grams hit = 247  (78.66%)
Number of 2-grams hit = 54  (17.20%)
Number of 1-grams hit = 13  (4.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article10.text
Will force exclusive back-off from OOVs.
Perplexity = 129.93, Entropy = 7.02 bits
Computation based on 304 words.
Number of 3-grams hit = 225  (74.01%)
Number of 2-grams hit = 61  (20.07%)
Number of 1-grams hit = 18  (5.92%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article11.text
Will force exclusive back-off from OOVs.
Perplexity = 151.62, Entropy = 7.24 bits
Computation based on 302 words.
Number of 3-grams hit = 211  (69.87%)
Number of 2-grams hit = 72  (23.84%)
Number of 1-grams hit = 19  (6.29%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article12.text
Will force exclusive back-off from OOVs.
Perplexity = 69.88, Entropy = 6.13 bits
Computation based on 300 words.
Number of 3-grams hit = 241  (80.33%)
Number of 2-grams hit = 51  (17.00%)
Number of 1-grams hit = 8  (2.67%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article13.text
Will force exclusive back-off from OOVs.
Perplexity = 171.81, Entropy = 7.42 bits
Computation based on 453 words.
Number of 3-grams hit = 321  (70.86%)
Number of 2-grams hit = 92  (20.31%)
Number of 1-grams hit = 40  (8.83%)
11 OOVs (2.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article14.text
Will force exclusive back-off from OOVs.
Perplexity = 128.01, Entropy = 7.00 bits
Computation based on 468 words.
Number of 3-grams hit = 342  (73.08%)
Number of 2-grams hit = 89  (19.02%)
Number of 1-grams hit = 37  (7.91%)
7 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article15.text
Will force exclusive back-off from OOVs.
Perplexity = 116.70, Entropy = 6.87 bits
Computation based on 493 words.
Number of 3-grams hit = 374  (75.86%)
Number of 2-grams hit = 100  (20.28%)
Number of 1-grams hit = 19  (3.85%)
4 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article16.text
Will force exclusive back-off from OOVs.
Perplexity = 191.40, Entropy = 7.58 bits
Computation based on 365 words.
Number of 3-grams hit = 249  (68.22%)
Number of 2-grams hit = 92  (25.21%)
Number of 1-grams hit = 24  (6.58%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article17.text
Will force exclusive back-off from OOVs.
Perplexity = 155.58, Entropy = 7.28 bits
Computation based on 673 words.
Number of 3-grams hit = 488  (72.51%)
Number of 2-grams hit = 140  (20.80%)
Number of 1-grams hit = 45  (6.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article18.text
Will force exclusive back-off from OOVs.
Perplexity = 102.73, Entropy = 6.68 bits
Computation based on 406 words.
Number of 3-grams hit = 309  (76.11%)
Number of 2-grams hit = 78  (19.21%)
Number of 1-grams hit = 19  (4.68%)
6 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article19.text
Will force exclusive back-off from OOVs.
Perplexity = 129.99, Entropy = 7.02 bits
Computation based on 525 words.
Number of 3-grams hit = 385  (73.33%)
Number of 2-grams hit = 113  (21.52%)
Number of 1-grams hit = 27  (5.14%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article20.text
Will force exclusive back-off from OOVs.
Perplexity = 129.17, Entropy = 7.01 bits
Computation based on 348 words.
Number of 3-grams hit = 253  (72.70%)
Number of 2-grams hit = 69  (19.83%)
Number of 1-grams hit = 26  (7.47%)
4 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article21.text
Will force exclusive back-off from OOVs.
Perplexity = 172.51, Entropy = 7.43 bits
Computation based on 461 words.
Number of 3-grams hit = 316  (68.55%)
Number of 2-grams hit = 107  (23.21%)
Number of 1-grams hit = 38  (8.24%)
4 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article22.text
Will force exclusive back-off from OOVs.
Perplexity = 160.19, Entropy = 7.32 bits
Computation based on 360 words.
Number of 3-grams hit = 259  (71.94%)
Number of 2-grams hit = 80  (22.22%)
Number of 1-grams hit = 21  (5.83%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article23.text
Will force exclusive back-off from OOVs.
Perplexity = 203.30, Entropy = 7.67 bits
Computation based on 345 words.
Number of 3-grams hit = 223  (64.64%)
Number of 2-grams hit = 90  (26.09%)
Number of 1-grams hit = 32  (9.28%)
12 OOVs (3.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article24.text
Will force exclusive back-off from OOVs.
Perplexity = 120.90, Entropy = 6.92 bits
Computation based on 561 words.
Number of 3-grams hit = 423  (75.40%)
Number of 2-grams hit = 102  (18.18%)
Number of 1-grams hit = 36  (6.42%)
5 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article25.text
Will force exclusive back-off from OOVs.
Perplexity = 191.14, Entropy = 7.58 bits
Computation based on 373 words.
Number of 3-grams hit = 263  (70.51%)
Number of 2-grams hit = 78  (20.91%)
Number of 1-grams hit = 32  (8.58%)
7 OOVs (1.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article26.text
Will force exclusive back-off from OOVs.
Perplexity = 170.86, Entropy = 7.42 bits
Computation based on 1519 words.
Number of 3-grams hit = 1059  (69.72%)
Number of 2-grams hit = 366  (24.09%)
Number of 1-grams hit = 94  (6.19%)
13 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article27.text
Will force exclusive back-off from OOVs.
Perplexity = 246.77, Entropy = 7.95 bits
Computation based on 389 words.
Number of 3-grams hit = 257  (66.07%)
Number of 2-grams hit = 94  (24.16%)
Number of 1-grams hit = 38  (9.77%)
4 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article28.text
Will force exclusive back-off from OOVs.
Perplexity = 124.49, Entropy = 6.96 bits
Computation based on 1388 words.
Number of 3-grams hit = 1049  (75.58%)
Number of 2-grams hit = 258  (18.59%)
Number of 1-grams hit = 81  (5.84%)
11 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article29.text
Will force exclusive back-off from OOVs.
Perplexity = 170.71, Entropy = 7.42 bits
Computation based on 317 words.
Number of 3-grams hit = 226  (71.29%)
Number of 2-grams hit = 72  (22.71%)
Number of 1-grams hit = 19  (5.99%)
2 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article30.text
Will force exclusive back-off from OOVs.
Perplexity = 110.51, Entropy = 6.79 bits
Computation based on 475 words.
Number of 3-grams hit = 364  (76.63%)
Number of 2-grams hit = 89  (18.74%)
Number of 1-grams hit = 22  (4.63%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article31.text
Will force exclusive back-off from OOVs.
Perplexity = 148.18, Entropy = 7.21 bits
Computation based on 439 words.
Number of 3-grams hit = 326  (74.26%)
Number of 2-grams hit = 86  (19.59%)
Number of 1-grams hit = 27  (6.15%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article32.text
Will force exclusive back-off from OOVs.
Perplexity = 147.57, Entropy = 7.21 bits
Computation based on 475 words.
Number of 3-grams hit = 347  (73.05%)
Number of 2-grams hit = 93  (19.58%)
Number of 1-grams hit = 35  (7.37%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article33.text
Will force exclusive back-off from OOVs.
Perplexity = 201.77, Entropy = 7.66 bits
Computation based on 441 words.
Number of 3-grams hit = 308  (69.84%)
Number of 2-grams hit = 99  (22.45%)
Number of 1-grams hit = 34  (7.71%)
3 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article34.text
Will force exclusive back-off from OOVs.
Perplexity = 118.24, Entropy = 6.89 bits
Computation based on 304 words.
Number of 3-grams hit = 238  (78.29%)
Number of 2-grams hit = 55  (18.09%)
Number of 1-grams hit = 11  (3.62%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article35.text
Will force exclusive back-off from OOVs.
Perplexity = 131.91, Entropy = 7.04 bits
Computation based on 378 words.
Number of 3-grams hit = 285  (75.40%)
Number of 2-grams hit = 78  (20.63%)
Number of 1-grams hit = 15  (3.97%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article36.text
Will force exclusive back-off from OOVs.
Perplexity = 119.19, Entropy = 6.90 bits
Computation based on 1000 words.
Number of 3-grams hit = 756  (75.60%)
Number of 2-grams hit = 193  (19.30%)
Number of 1-grams hit = 51  (5.10%)
11 OOVs (1.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article37.text
Will force exclusive back-off from OOVs.
Perplexity = 127.73, Entropy = 7.00 bits
Computation based on 559 words.
Number of 3-grams hit = 404  (72.27%)
Number of 2-grams hit = 120  (21.47%)
Number of 1-grams hit = 35  (6.26%)
10 OOVs (1.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article38.text
Will force exclusive back-off from OOVs.
Perplexity = 165.66, Entropy = 7.37 bits
Computation based on 408 words.
Number of 3-grams hit = 275  (67.40%)
Number of 2-grams hit = 108  (26.47%)
Number of 1-grams hit = 25  (6.13%)
4 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article39.text
Will force exclusive back-off from OOVs.
Perplexity = 106.43, Entropy = 6.73 bits
Computation based on 2623 words.
Number of 3-grams hit = 2046  (78.00%)
Number of 2-grams hit = 457  (17.42%)
Number of 1-grams hit = 120  (4.57%)
37 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article40.text
Will force exclusive back-off from OOVs.
Perplexity = 62.58, Entropy = 5.97 bits
Computation based on 405 words.
Number of 3-grams hit = 335  (82.72%)
Number of 2-grams hit = 62  (15.31%)
Number of 1-grams hit = 8  (1.98%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article41.text
Will force exclusive back-off from OOVs.
Perplexity = 90.96, Entropy = 6.51 bits
Computation based on 969 words.
Number of 3-grams hit = 772  (79.67%)
Number of 2-grams hit = 162  (16.72%)
Number of 1-grams hit = 35  (3.61%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article42.text
Will force exclusive back-off from OOVs.
Perplexity = 126.48, Entropy = 6.98 bits
Computation based on 547 words.
Number of 3-grams hit = 418  (76.42%)
Number of 2-grams hit = 104  (19.01%)
Number of 1-grams hit = 25  (4.57%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article43.text
Will force exclusive back-off from OOVs.
Perplexity = 105.02, Entropy = 6.71 bits
Computation based on 449 words.
Number of 3-grams hit = 351  (78.17%)
Number of 2-grams hit = 75  (16.70%)
Number of 1-grams hit = 23  (5.12%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article44.text
Will force exclusive back-off from OOVs.
Perplexity = 119.19, Entropy = 6.90 bits
Computation based on 492 words.
Number of 3-grams hit = 373  (75.81%)
Number of 2-grams hit = 92  (18.70%)
Number of 1-grams hit = 27  (5.49%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article45.text
Will force exclusive back-off from OOVs.
Perplexity = 160.35, Entropy = 7.33 bits
Computation based on 464 words.
Number of 3-grams hit = 327  (70.47%)
Number of 2-grams hit = 112  (24.14%)
Number of 1-grams hit = 25  (5.39%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article46.text
Will force exclusive back-off from OOVs.
Perplexity = 199.29, Entropy = 7.64 bits
Computation based on 401 words.
Number of 3-grams hit = 265  (66.08%)
Number of 2-grams hit = 108  (26.93%)
Number of 1-grams hit = 28  (6.98%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article47.text
Will force exclusive back-off from OOVs.
Perplexity = 111.67, Entropy = 6.80 bits
Computation based on 2678 words.
Number of 3-grams hit = 2074  (77.45%)
Number of 2-grams hit = 497  (18.56%)
Number of 1-grams hit = 107  (4.00%)
6 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article48.text
Will force exclusive back-off from OOVs.
Perplexity = 233.17, Entropy = 7.87 bits
Computation based on 366 words.
Number of 3-grams hit = 244  (66.67%)
Number of 2-grams hit = 98  (26.78%)
Number of 1-grams hit = 24  (6.56%)
7 OOVs (1.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article49.text
Will force exclusive back-off from OOVs.
Perplexity = 90.62, Entropy = 6.50 bits
Computation based on 364 words.
Number of 3-grams hit = 286  (78.57%)
Number of 2-grams hit = 68  (18.68%)
Number of 1-grams hit = 10  (2.75%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article50.text
Will force exclusive back-off from OOVs.
Perplexity = 102.36, Entropy = 6.68 bits
Computation based on 522 words.
Number of 3-grams hit = 408  (78.16%)
Number of 2-grams hit = 90  (17.24%)
Number of 1-grams hit = 24  (4.60%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article51.text
Will force exclusive back-off from OOVs.
Perplexity = 216.63, Entropy = 7.76 bits
Computation based on 1102 words.
Number of 3-grams hit = 715  (64.88%)
Number of 2-grams hit = 299  (27.13%)
Number of 1-grams hit = 88  (7.99%)
19 OOVs (1.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article52.text
Will force exclusive back-off from OOVs.
Perplexity = 83.36, Entropy = 6.38 bits
Computation based on 360 words.
Number of 3-grams hit = 292  (81.11%)
Number of 2-grams hit = 60  (16.67%)
Number of 1-grams hit = 8  (2.22%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article53.text
Will force exclusive back-off from OOVs.
Perplexity = 160.39, Entropy = 7.33 bits
Computation based on 472 words.
Number of 3-grams hit = 328  (69.49%)
Number of 2-grams hit = 102  (21.61%)
Number of 1-grams hit = 42  (8.90%)
9 OOVs (1.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article54.text
Will force exclusive back-off from OOVs.
Perplexity = 184.97, Entropy = 7.53 bits
Computation based on 1063 words.
Number of 3-grams hit = 726  (68.30%)
Number of 2-grams hit = 241  (22.67%)
Number of 1-grams hit = 96  (9.03%)
21 OOVs (1.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article55.text
Will force exclusive back-off from OOVs.
Perplexity = 75.91, Entropy = 6.25 bits
Computation based on 533 words.
Number of 3-grams hit = 447  (83.86%)
Number of 2-grams hit = 62  (11.63%)
Number of 1-grams hit = 24  (4.50%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article56.text
Will force exclusive back-off from OOVs.
Perplexity = 149.73, Entropy = 7.23 bits
Computation based on 898 words.
Number of 3-grams hit = 652  (72.61%)
Number of 2-grams hit = 181  (20.16%)
Number of 1-grams hit = 65  (7.24%)
7 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article57.text
Will force exclusive back-off from OOVs.
Perplexity = 81.57, Entropy = 6.35 bits
Computation based on 1537 words.
Number of 3-grams hit = 1267  (82.43%)
Number of 2-grams hit = 225  (14.64%)
Number of 1-grams hit = 45  (2.93%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article58.text
Will force exclusive back-off from OOVs.
Perplexity = 118.92, Entropy = 6.89 bits
Computation based on 383 words.
Number of 3-grams hit = 297  (77.55%)
Number of 2-grams hit = 63  (16.45%)
Number of 1-grams hit = 23  (6.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article59.text
Will force exclusive back-off from OOVs.
Perplexity = 64.98, Entropy = 6.02 bits
Computation based on 208 words.
Number of 3-grams hit = 174  (83.65%)
Number of 2-grams hit = 28  (13.46%)
Number of 1-grams hit = 6  (2.88%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article60.text
Will force exclusive back-off from OOVs.
Perplexity = 106.69, Entropy = 6.74 bits
Computation based on 321 words.
Number of 3-grams hit = 232  (72.27%)
Number of 2-grams hit = 67  (20.87%)
Number of 1-grams hit = 22  (6.85%)
13 OOVs (3.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article61.text
Will force exclusive back-off from OOVs.
Perplexity = 130.88, Entropy = 7.03 bits
Computation based on 489 words.
Number of 3-grams hit = 362  (74.03%)
Number of 2-grams hit = 105  (21.47%)
Number of 1-grams hit = 22  (4.50%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article62.text
Will force exclusive back-off from OOVs.
Perplexity = 141.24, Entropy = 7.14 bits
Computation based on 1282 words.
Number of 3-grams hit = 910  (70.98%)
Number of 2-grams hit = 290  (22.62%)
Number of 1-grams hit = 82  (6.40%)
11 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article63.text
Will force exclusive back-off from OOVs.
Perplexity = 149.25, Entropy = 7.22 bits
Computation based on 965 words.
Number of 3-grams hit = 681  (70.57%)
Number of 2-grams hit = 210  (21.76%)
Number of 1-grams hit = 74  (7.67%)
25 OOVs (2.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article64.text
Will force exclusive back-off from OOVs.
Perplexity = 112.04, Entropy = 6.81 bits
Computation based on 4162 words.
Number of 3-grams hit = 3116  (74.87%)
Number of 2-grams hit = 816  (19.61%)
Number of 1-grams hit = 230  (5.53%)
54 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article65.text
Will force exclusive back-off from OOVs.
Perplexity = 102.01, Entropy = 6.67 bits
Computation based on 628 words.
Number of 3-grams hit = 483  (76.91%)
Number of 2-grams hit = 111  (17.68%)
Number of 1-grams hit = 34  (5.41%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article66.text
Will force exclusive back-off from OOVs.
Perplexity = 95.64, Entropy = 6.58 bits
Computation based on 472 words.
Number of 3-grams hit = 372  (78.81%)
Number of 2-grams hit = 83  (17.58%)
Number of 1-grams hit = 17  (3.60%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article67.text
Will force exclusive back-off from OOVs.
Perplexity = 174.66, Entropy = 7.45 bits
Computation based on 449 words.
Number of 3-grams hit = 299  (66.59%)
Number of 2-grams hit = 118  (26.28%)
Number of 1-grams hit = 32  (7.13%)
5 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article68.text
Will force exclusive back-off from OOVs.
Perplexity = 120.71, Entropy = 6.92 bits
Computation based on 1109 words.
Number of 3-grams hit = 857  (77.28%)
Number of 2-grams hit = 201  (18.12%)
Number of 1-grams hit = 51  (4.60%)
11 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article69.text
Will force exclusive back-off from OOVs.
Perplexity = 94.68, Entropy = 6.56 bits
Computation based on 380 words.
Number of 3-grams hit = 312  (82.11%)
Number of 2-grams hit = 51  (13.42%)
Number of 1-grams hit = 17  (4.47%)
3 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article70.text
Will force exclusive back-off from OOVs.
Perplexity = 85.57, Entropy = 6.42 bits
Computation based on 959 words.
Number of 3-grams hit = 776  (80.92%)
Number of 2-grams hit = 166  (17.31%)
Number of 1-grams hit = 17  (1.77%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article71.text
Will force exclusive back-off from OOVs.
Perplexity = 117.26, Entropy = 6.87 bits
Computation based on 550 words.
Number of 3-grams hit = 435  (79.09%)
Number of 2-grams hit = 97  (17.64%)
Number of 1-grams hit = 18  (3.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article72.text
Will force exclusive back-off from OOVs.
Perplexity = 132.60, Entropy = 7.05 bits
Computation based on 1302 words.
Number of 3-grams hit = 987  (75.81%)
Number of 2-grams hit = 247  (18.97%)
Number of 1-grams hit = 68  (5.22%)
6 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article73.text
Will force exclusive back-off from OOVs.
Perplexity = 96.79, Entropy = 6.60 bits
Computation based on 509 words.
Number of 3-grams hit = 409  (80.35%)
Number of 2-grams hit = 79  (15.52%)
Number of 1-grams hit = 21  (4.13%)
4 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article74.text
Will force exclusive back-off from OOVs.
Perplexity = 141.61, Entropy = 7.15 bits
Computation based on 818 words.
Number of 3-grams hit = 606  (74.08%)
Number of 2-grams hit = 168  (20.54%)
Number of 1-grams hit = 44  (5.38%)
5 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article75.text
Will force exclusive back-off from OOVs.
Perplexity = 102.92, Entropy = 6.69 bits
Computation based on 1190 words.
Number of 3-grams hit = 930  (78.15%)
Number of 2-grams hit = 209  (17.56%)
Number of 1-grams hit = 51  (4.29%)
4 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article76.text
Will force exclusive back-off from OOVs.
Perplexity = 86.23, Entropy = 6.43 bits
Computation based on 1377 words.
Number of 3-grams hit = 1117  (81.12%)
Number of 2-grams hit = 215  (15.61%)
Number of 1-grams hit = 45  (3.27%)
5 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article77.text
Will force exclusive back-off from OOVs.
Perplexity = 142.80, Entropy = 7.16 bits
Computation based on 288 words.
Number of 3-grams hit = 217  (75.35%)
Number of 2-grams hit = 50  (17.36%)
Number of 1-grams hit = 21  (7.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article78.text
Will force exclusive back-off from OOVs.
Perplexity = 103.24, Entropy = 6.69 bits
Computation based on 1118 words.
Number of 3-grams hit = 882  (78.89%)
Number of 2-grams hit = 193  (17.26%)
Number of 1-grams hit = 43  (3.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article79.text
Will force exclusive back-off from OOVs.
Perplexity = 161.28, Entropy = 7.33 bits
Computation based on 1065 words.
Number of 3-grams hit = 769  (72.21%)
Number of 2-grams hit = 216  (20.28%)
Number of 1-grams hit = 80  (7.51%)
8 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article80.text
Will force exclusive back-off from OOVs.
Perplexity = 129.97, Entropy = 7.02 bits
Computation based on 354 words.
Number of 3-grams hit = 230  (64.97%)
Number of 2-grams hit = 91  (25.71%)
Number of 1-grams hit = 33  (9.32%)
16 OOVs (4.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article81.text
Will force exclusive back-off from OOVs.
Perplexity = 108.05, Entropy = 6.76 bits
Computation based on 4029 words.
Number of 3-grams hit = 3151  (78.21%)
Number of 2-grams hit = 709  (17.60%)
Number of 1-grams hit = 169  (4.19%)
21 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article82.text
Will force exclusive back-off from OOVs.
Perplexity = 117.93, Entropy = 6.88 bits
Computation based on 467 words.
Number of 3-grams hit = 352  (75.37%)
Number of 2-grams hit = 97  (20.77%)
Number of 1-grams hit = 18  (3.85%)
4 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article83.text
Will force exclusive back-off from OOVs.
Perplexity = 94.31, Entropy = 6.56 bits
Computation based on 431 words.
Number of 3-grams hit = 348  (80.74%)
Number of 2-grams hit = 66  (15.31%)
Number of 1-grams hit = 17  (3.94%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article84.text
Will force exclusive back-off from OOVs.
Perplexity = 138.01, Entropy = 7.11 bits
Computation based on 399 words.
Number of 3-grams hit = 300  (75.19%)
Number of 2-grams hit = 82  (20.55%)
Number of 1-grams hit = 17  (4.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article85.text
Will force exclusive back-off from OOVs.
Perplexity = 122.12, Entropy = 6.93 bits
Computation based on 1214 words.
Number of 3-grams hit = 924  (76.11%)
Number of 2-grams hit = 231  (19.03%)
Number of 1-grams hit = 59  (4.86%)
5 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article86.text
Will force exclusive back-off from OOVs.
Perplexity = 160.29, Entropy = 7.32 bits
Computation based on 286 words.
Number of 3-grams hit = 206  (72.03%)
Number of 2-grams hit = 60  (20.98%)
Number of 1-grams hit = 20  (6.99%)
3 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article87.text
Will force exclusive back-off from OOVs.
Perplexity = 218.61, Entropy = 7.77 bits
Computation based on 622 words.
Number of 3-grams hit = 400  (64.31%)
Number of 2-grams hit = 167  (26.85%)
Number of 1-grams hit = 55  (8.84%)
10 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article88.text
Will force exclusive back-off from OOVs.
Perplexity = 104.44, Entropy = 6.71 bits
Computation based on 1476 words.
Number of 3-grams hit = 1142  (77.37%)
Number of 2-grams hit = 254  (17.21%)
Number of 1-grams hit = 80  (5.42%)
23 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article89.text
Will force exclusive back-off from OOVs.
Perplexity = 146.94, Entropy = 7.20 bits
Computation based on 390 words.
Number of 3-grams hit = 286  (73.33%)
Number of 2-grams hit = 83  (21.28%)
Number of 1-grams hit = 21  (5.38%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article90.text
Will force exclusive back-off from OOVs.
Perplexity = 141.67, Entropy = 7.15 bits
Computation based on 506 words.
Number of 3-grams hit = 357  (70.55%)
Number of 2-grams hit = 111  (21.94%)
Number of 1-grams hit = 38  (7.51%)
14 OOVs (2.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article91.text
Will force exclusive back-off from OOVs.
Perplexity = 104.15, Entropy = 6.70 bits
Computation based on 342 words.
Number of 3-grams hit = 269  (78.65%)
Number of 2-grams hit = 60  (17.54%)
Number of 1-grams hit = 13  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article92.text
Will force exclusive back-off from OOVs.
Perplexity = 117.46, Entropy = 6.88 bits
Computation based on 1024 words.
Number of 3-grams hit = 800  (78.12%)
Number of 2-grams hit = 176  (17.19%)
Number of 1-grams hit = 48  (4.69%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article93.text
Will force exclusive back-off from OOVs.
Perplexity = 143.42, Entropy = 7.16 bits
Computation based on 682 words.
Number of 3-grams hit = 508  (74.49%)
Number of 2-grams hit = 133  (19.50%)
Number of 1-grams hit = 41  (6.01%)
4 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article94.text
Will force exclusive back-off from OOVs.
Perplexity = 100.99, Entropy = 6.66 bits
Computation based on 1286 words.
Number of 3-grams hit = 1013  (78.77%)
Number of 2-grams hit = 228  (17.73%)
Number of 1-grams hit = 45  (3.50%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article95.text
Will force exclusive back-off from OOVs.
Perplexity = 97.16, Entropy = 6.60 bits
Computation based on 499 words.
Number of 3-grams hit = 393  (78.76%)
Number of 2-grams hit = 87  (17.43%)
Number of 1-grams hit = 19  (3.81%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article96.text
Will force exclusive back-off from OOVs.
Perplexity = 108.67, Entropy = 6.76 bits
Computation based on 397 words.
Number of 3-grams hit = 305  (76.83%)
Number of 2-grams hit = 75  (18.89%)
Number of 1-grams hit = 17  (4.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article97.text
Will force exclusive back-off from OOVs.
Perplexity = 100.74, Entropy = 6.65 bits
Computation based on 544 words.
Number of 3-grams hit = 427  (78.49%)
Number of 2-grams hit = 92  (16.91%)
Number of 1-grams hit = 25  (4.60%)
9 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article98.text
Will force exclusive back-off from OOVs.
Perplexity = 108.13, Entropy = 6.76 bits
Computation based on 363 words.
Number of 3-grams hit = 274  (75.48%)
Number of 2-grams hit = 74  (20.39%)
Number of 1-grams hit = 15  (4.13%)
3 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article99.text
Will force exclusive back-off from OOVs.
Perplexity = 199.43, Entropy = 7.64 bits
Computation based on 431 words.
Number of 3-grams hit = 268  (62.18%)
Number of 2-grams hit = 112  (25.99%)
Number of 1-grams hit = 51  (11.83%)
25 OOVs (5.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article100.text
Will force exclusive back-off from OOVs.
Perplexity = 124.79, Entropy = 6.96 bits
Computation based on 474 words.
Number of 3-grams hit = 362  (76.37%)
Number of 2-grams hit = 92  (19.41%)
Number of 1-grams hit = 20  (4.22%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article101.text
Will force exclusive back-off from OOVs.
Perplexity = 121.61, Entropy = 6.93 bits
Computation based on 486 words.
Number of 3-grams hit = 343  (70.58%)
Number of 2-grams hit = 98  (20.16%)
Number of 1-grams hit = 45  (9.26%)
21 OOVs (4.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article102.text
Will force exclusive back-off from OOVs.
Perplexity = 132.46, Entropy = 7.05 bits
Computation based on 412 words.
Number of 3-grams hit = 298  (72.33%)
Number of 2-grams hit = 90  (21.84%)
Number of 1-grams hit = 24  (5.83%)
3 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article103.text
Will force exclusive back-off from OOVs.
Perplexity = 236.70, Entropy = 7.89 bits
Computation based on 476 words.
Number of 3-grams hit = 309  (64.92%)
Number of 2-grams hit = 107  (22.48%)
Number of 1-grams hit = 60  (12.61%)
22 OOVs (4.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article104.text
Will force exclusive back-off from OOVs.
Perplexity = 88.83, Entropy = 6.47 bits
Computation based on 6340 words.
Number of 3-grams hit = 5123  (80.80%)
Number of 2-grams hit = 967  (15.25%)
Number of 1-grams hit = 250  (3.94%)
19 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article105.text
Will force exclusive back-off from OOVs.
Perplexity = 146.25, Entropy = 7.19 bits
Computation based on 446 words.
Number of 3-grams hit = 334  (74.89%)
Number of 2-grams hit = 80  (17.94%)
Number of 1-grams hit = 32  (7.17%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article106.text
Will force exclusive back-off from OOVs.
Perplexity = 201.45, Entropy = 7.65 bits
Computation based on 794 words.
Number of 3-grams hit = 530  (66.75%)
Number of 2-grams hit = 206  (25.94%)
Number of 1-grams hit = 58  (7.30%)
8 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article107.text
Will force exclusive back-off from OOVs.
Perplexity = 337.05, Entropy = 8.40 bits
Computation based on 474 words.
Number of 3-grams hit = 280  (59.07%)
Number of 2-grams hit = 130  (27.43%)
Number of 1-grams hit = 64  (13.50%)
11 OOVs (2.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article108.text
Will force exclusive back-off from OOVs.
Perplexity = 109.11, Entropy = 6.77 bits
Computation based on 528 words.
Number of 3-grams hit = 413  (78.22%)
Number of 2-grams hit = 100  (18.94%)
Number of 1-grams hit = 15  (2.84%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article109.text
Will force exclusive back-off from OOVs.
Perplexity = 105.80, Entropy = 6.73 bits
Computation based on 1123 words.
Number of 3-grams hit = 861  (76.67%)
Number of 2-grams hit = 209  (18.61%)
Number of 1-grams hit = 53  (4.72%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article110.text
Will force exclusive back-off from OOVs.
Perplexity = 121.22, Entropy = 6.92 bits
Computation based on 908 words.
Number of 3-grams hit = 688  (75.77%)
Number of 2-grams hit = 188  (20.70%)
Number of 1-grams hit = 32  (3.52%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article111.text
Will force exclusive back-off from OOVs.
Perplexity = 123.94, Entropy = 6.95 bits
Computation based on 1223 words.
Number of 3-grams hit = 915  (74.82%)
Number of 2-grams hit = 242  (19.79%)
Number of 1-grams hit = 66  (5.40%)
10 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article112.text
Will force exclusive back-off from OOVs.
Perplexity = 131.54, Entropy = 7.04 bits
Computation based on 328 words.
Number of 3-grams hit = 248  (75.61%)
Number of 2-grams hit = 64  (19.51%)
Number of 1-grams hit = 16  (4.88%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article113.text
Will force exclusive back-off from OOVs.
Perplexity = 72.70, Entropy = 6.18 bits
Computation based on 5818 words.
Number of 3-grams hit = 4896  (84.15%)
Number of 2-grams hit = 785  (13.49%)
Number of 1-grams hit = 137  (2.35%)
13 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article114.text
Will force exclusive back-off from OOVs.
Perplexity = 118.36, Entropy = 6.89 bits
Computation based on 499 words.
Number of 3-grams hit = 381  (76.35%)
Number of 2-grams hit = 101  (20.24%)
Number of 1-grams hit = 17  (3.41%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article115.text
Will force exclusive back-off from OOVs.
Perplexity = 114.78, Entropy = 6.84 bits
Computation based on 641 words.
Number of 3-grams hit = 488  (76.13%)
Number of 2-grams hit = 120  (18.72%)
Number of 1-grams hit = 33  (5.15%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article116.text
Will force exclusive back-off from OOVs.
Perplexity = 104.06, Entropy = 6.70 bits
Computation based on 1526 words.
Number of 3-grams hit = 1192  (78.11%)
Number of 2-grams hit = 278  (18.22%)
Number of 1-grams hit = 56  (3.67%)
13 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article117.text
Will force exclusive back-off from OOVs.
Perplexity = 126.41, Entropy = 6.98 bits
Computation based on 517 words.
Number of 3-grams hit = 387  (74.85%)
Number of 2-grams hit = 106  (20.50%)
Number of 1-grams hit = 24  (4.64%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article118.text
Will force exclusive back-off from OOVs.
Perplexity = 98.02, Entropy = 6.62 bits
Computation based on 610 words.
Number of 3-grams hit = 475  (77.87%)
Number of 2-grams hit = 106  (17.38%)
Number of 1-grams hit = 29  (4.75%)
5 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article119.text
Will force exclusive back-off from OOVs.
Perplexity = 113.02, Entropy = 6.82 bits
Computation based on 390 words.
Number of 3-grams hit = 304  (77.95%)
Number of 2-grams hit = 71  (18.21%)
Number of 1-grams hit = 15  (3.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article120.text
Will force exclusive back-off from OOVs.
Perplexity = 123.57, Entropy = 6.95 bits
Computation based on 557 words.
Number of 3-grams hit = 417  (74.87%)
Number of 2-grams hit = 110  (19.75%)
Number of 1-grams hit = 30  (5.39%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article121.text
Will force exclusive back-off from OOVs.
Perplexity = 108.86, Entropy = 6.77 bits
Computation based on 7137 words.
Number of 3-grams hit = 5580  (78.18%)
Number of 2-grams hit = 1257  (17.61%)
Number of 1-grams hit = 300  (4.20%)
24 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article122.text
Will force exclusive back-off from OOVs.
Perplexity = 59.78, Entropy = 5.90 bits
Computation based on 325 words.
Number of 3-grams hit = 280  (86.15%)
Number of 2-grams hit = 39  (12.00%)
Number of 1-grams hit = 6  (1.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article123.text
Will force exclusive back-off from OOVs.
Perplexity = 160.22, Entropy = 7.32 bits
Computation based on 524 words.
Number of 3-grams hit = 378  (72.14%)
Number of 2-grams hit = 124  (23.66%)
Number of 1-grams hit = 22  (4.20%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article124.text
Will force exclusive back-off from OOVs.
Perplexity = 91.74, Entropy = 6.52 bits
Computation based on 798 words.
Number of 3-grams hit = 643  (80.58%)
Number of 2-grams hit = 123  (15.41%)
Number of 1-grams hit = 32  (4.01%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article125.text
Will force exclusive back-off from OOVs.
Perplexity = 135.65, Entropy = 7.08 bits
Computation based on 484 words.
Number of 3-grams hit = 369  (76.24%)
Number of 2-grams hit = 88  (18.18%)
Number of 1-grams hit = 27  (5.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article126.text
Will force exclusive back-off from OOVs.
Perplexity = 88.22, Entropy = 6.46 bits
Computation based on 540 words.
Number of 3-grams hit = 437  (80.93%)
Number of 2-grams hit = 79  (14.63%)
Number of 1-grams hit = 24  (4.44%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article127.text
Will force exclusive back-off from OOVs.
Perplexity = 116.13, Entropy = 6.86 bits
Computation based on 923 words.
Number of 3-grams hit = 704  (76.27%)
Number of 2-grams hit = 176  (19.07%)
Number of 1-grams hit = 43  (4.66%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article128.text
Will force exclusive back-off from OOVs.
Perplexity = 268.93, Entropy = 8.07 bits
Computation based on 427 words.
Number of 3-grams hit = 261  (61.12%)
Number of 2-grams hit = 125  (29.27%)
Number of 1-grams hit = 41  (9.60%)
4 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article129.text
Will force exclusive back-off from OOVs.
Perplexity = 174.27, Entropy = 7.45 bits
Computation based on 469 words.
Number of 3-grams hit = 328  (69.94%)
Number of 2-grams hit = 108  (23.03%)
Number of 1-grams hit = 33  (7.04%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article130.text
Will force exclusive back-off from OOVs.
Perplexity = 108.56, Entropy = 6.76 bits
Computation based on 1527 words.
Number of 3-grams hit = 1179  (77.21%)
Number of 2-grams hit = 267  (17.49%)
Number of 1-grams hit = 81  (5.30%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article131.text
Will force exclusive back-off from OOVs.
Perplexity = 118.59, Entropy = 6.89 bits
Computation based on 6797 words.
Number of 3-grams hit = 5223  (76.84%)
Number of 2-grams hit = 1273  (18.73%)
Number of 1-grams hit = 301  (4.43%)
19 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article132.text
Will force exclusive back-off from OOVs.
Perplexity = 108.20, Entropy = 6.76 bits
Computation based on 1115 words.
Number of 3-grams hit = 849  (76.14%)
Number of 2-grams hit = 214  (19.19%)
Number of 1-grams hit = 52  (4.66%)
8 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article133.text
Will force exclusive back-off from OOVs.
Perplexity = 65.45, Entropy = 6.03 bits
Computation based on 243 words.
Number of 3-grams hit = 209  (86.01%)
Number of 2-grams hit = 26  (10.70%)
Number of 1-grams hit = 8  (3.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article134.text
Will force exclusive back-off from OOVs.
Perplexity = 117.36, Entropy = 6.87 bits
Computation based on 500 words.
Number of 3-grams hit = 394  (78.80%)
Number of 2-grams hit = 83  (16.60%)
Number of 1-grams hit = 23  (4.60%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article135.text
Will force exclusive back-off from OOVs.
Perplexity = 265.08, Entropy = 8.05 bits
Computation based on 482 words.
Number of 3-grams hit = 303  (62.86%)
Number of 2-grams hit = 131  (27.18%)
Number of 1-grams hit = 48  (9.96%)
9 OOVs (1.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article136.text
Will force exclusive back-off from OOVs.
Perplexity = 97.95, Entropy = 6.61 bits
Computation based on 526 words.
Number of 3-grams hit = 413  (78.52%)
Number of 2-grams hit = 93  (17.68%)
Number of 1-grams hit = 20  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article137.text
Will force exclusive back-off from OOVs.
Perplexity = 83.15, Entropy = 6.38 bits
Computation based on 329 words.
Number of 3-grams hit = 265  (80.55%)
Number of 2-grams hit = 52  (15.81%)
Number of 1-grams hit = 12  (3.65%)
4 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article138.text
Will force exclusive back-off from OOVs.
Perplexity = 86.36, Entropy = 6.43 bits
Computation based on 307 words.
Number of 3-grams hit = 244  (79.48%)
Number of 2-grams hit = 56  (18.24%)
Number of 1-grams hit = 7  (2.28%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article139.text
Will force exclusive back-off from OOVs.
Perplexity = 79.85, Entropy = 6.32 bits
Computation based on 271 words.
Number of 3-grams hit = 228  (84.13%)
Number of 2-grams hit = 32  (11.81%)
Number of 1-grams hit = 11  (4.06%)
4 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article140.text
Will force exclusive back-off from OOVs.
Perplexity = 171.46, Entropy = 7.42 bits
Computation based on 371 words.
Number of 3-grams hit = 249  (67.12%)
Number of 2-grams hit = 96  (25.88%)
Number of 1-grams hit = 26  (7.01%)
3 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article141.text
Will force exclusive back-off from OOVs.
Perplexity = 135.18, Entropy = 7.08 bits
Computation based on 434 words.
Number of 3-grams hit = 322  (74.19%)
Number of 2-grams hit = 97  (22.35%)
Number of 1-grams hit = 15  (3.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article142.text
Will force exclusive back-off from OOVs.
Perplexity = 130.06, Entropy = 7.02 bits
Computation based on 695 words.
Number of 3-grams hit = 526  (75.68%)
Number of 2-grams hit = 128  (18.42%)
Number of 1-grams hit = 41  (5.90%)
5 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article143.text
Will force exclusive back-off from OOVs.
Perplexity = 134.23, Entropy = 7.07 bits
Computation based on 377 words.
Number of 3-grams hit = 272  (72.15%)
Number of 2-grams hit = 84  (22.28%)
Number of 1-grams hit = 21  (5.57%)
4 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article144.text
Will force exclusive back-off from OOVs.
Perplexity = 182.98, Entropy = 7.52 bits
Computation based on 506 words.
Number of 3-grams hit = 340  (67.19%)
Number of 2-grams hit = 134  (26.48%)
Number of 1-grams hit = 32  (6.32%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article145.text
Will force exclusive back-off from OOVs.
Perplexity = 129.66, Entropy = 7.02 bits
Computation based on 1941 words.
Number of 3-grams hit = 1483  (76.40%)
Number of 2-grams hit = 367  (18.91%)
Number of 1-grams hit = 91  (4.69%)
3 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article146.text
Will force exclusive back-off from OOVs.
Perplexity = 110.38, Entropy = 6.79 bits
Computation based on 238 words.
Number of 3-grams hit = 182  (76.47%)
Number of 2-grams hit = 45  (18.91%)
Number of 1-grams hit = 11  (4.62%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article147.text
Will force exclusive back-off from OOVs.
Perplexity = 147.52, Entropy = 7.20 bits
Computation based on 657 words.
Number of 3-grams hit = 503  (76.56%)
Number of 2-grams hit = 118  (17.96%)
Number of 1-grams hit = 36  (5.48%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article148.text
Will force exclusive back-off from OOVs.
Perplexity = 153.58, Entropy = 7.26 bits
Computation based on 1733 words.
Number of 3-grams hit = 1255  (72.42%)
Number of 2-grams hit = 372  (21.47%)
Number of 1-grams hit = 106  (6.12%)
6 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article149.text
Will force exclusive back-off from OOVs.
Perplexity = 136.02, Entropy = 7.09 bits
Computation based on 557 words.
Number of 3-grams hit = 409  (73.43%)
Number of 2-grams hit = 115  (20.65%)
Number of 1-grams hit = 33  (5.92%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article150.text
Will force exclusive back-off from OOVs.
Perplexity = 98.33, Entropy = 6.62 bits
Computation based on 638 words.
Number of 3-grams hit = 497  (77.90%)
Number of 2-grams hit = 110  (17.24%)
Number of 1-grams hit = 31  (4.86%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article151.text
Will force exclusive back-off from OOVs.
Perplexity = 95.88, Entropy = 6.58 bits
Computation based on 173 words.
Number of 3-grams hit = 140  (80.92%)
Number of 2-grams hit = 29  (16.76%)
Number of 1-grams hit = 4  (2.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article152.text
Will force exclusive back-off from OOVs.
Perplexity = 103.43, Entropy = 6.69 bits
Computation based on 1817 words.
Number of 3-grams hit = 1410  (77.60%)
Number of 2-grams hit = 336  (18.49%)
Number of 1-grams hit = 71  (3.91%)
2 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article153.text
Will force exclusive back-off from OOVs.
Perplexity = 119.75, Entropy = 6.90 bits
Computation based on 588 words.
Number of 3-grams hit = 449  (76.36%)
Number of 2-grams hit = 106  (18.03%)
Number of 1-grams hit = 33  (5.61%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article154.text
Will force exclusive back-off from OOVs.
Perplexity = 106.73, Entropy = 6.74 bits
Computation based on 1059 words.
Number of 3-grams hit = 822  (77.62%)
Number of 2-grams hit = 196  (18.51%)
Number of 1-grams hit = 41  (3.87%)
5 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article155.text
Will force exclusive back-off from OOVs.
Perplexity = 114.15, Entropy = 6.83 bits
Computation based on 639 words.
Number of 3-grams hit = 495  (77.46%)
Number of 2-grams hit = 114  (17.84%)
Number of 1-grams hit = 30  (4.69%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article156.text
Will force exclusive back-off from OOVs.
Perplexity = 48.32, Entropy = 5.59 bits
Computation based on 1373 words.
Number of 3-grams hit = 1207  (87.91%)
Number of 2-grams hit = 141  (10.27%)
Number of 1-grams hit = 25  (1.82%)
5 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article157.text
Will force exclusive back-off from OOVs.
Perplexity = 206.54, Entropy = 7.69 bits
Computation based on 404 words.
Number of 3-grams hit = 282  (69.80%)
Number of 2-grams hit = 98  (24.26%)
Number of 1-grams hit = 24  (5.94%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article158.text
Will force exclusive back-off from OOVs.
Perplexity = 123.85, Entropy = 6.95 bits
Computation based on 574 words.
Number of 3-grams hit = 430  (74.91%)
Number of 2-grams hit = 121  (21.08%)
Number of 1-grams hit = 23  (4.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article159.text
Will force exclusive back-off from OOVs.
Perplexity = 58.55, Entropy = 5.87 bits
Computation based on 1205 words.
Number of 3-grams hit = 1029  (85.39%)
Number of 2-grams hit = 143  (11.87%)
Number of 1-grams hit = 33  (2.74%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article160.text
Will force exclusive back-off from OOVs.
Perplexity = 172.01, Entropy = 7.43 bits
Computation based on 534 words.
Number of 3-grams hit = 357  (66.85%)
Number of 2-grams hit = 138  (25.84%)
Number of 1-grams hit = 39  (7.30%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article161.text
Will force exclusive back-off from OOVs.
Perplexity = 320.99, Entropy = 8.33 bits
Computation based on 189 words.
Number of 3-grams hit = 105  (55.56%)
Number of 2-grams hit = 61  (32.28%)
Number of 1-grams hit = 23  (12.17%)
10 OOVs (5.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article162.text
Will force exclusive back-off from OOVs.
Perplexity = 115.26, Entropy = 6.85 bits
Computation based on 1148 words.
Number of 3-grams hit = 882  (76.83%)
Number of 2-grams hit = 221  (19.25%)
Number of 1-grams hit = 45  (3.92%)
6 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article163.text
Will force exclusive back-off from OOVs.
Perplexity = 130.01, Entropy = 7.02 bits
Computation based on 701 words.
Number of 3-grams hit = 523  (74.61%)
Number of 2-grams hit = 135  (19.26%)
Number of 1-grams hit = 43  (6.13%)
4 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article164.text
Will force exclusive back-off from OOVs.
Perplexity = 96.15, Entropy = 6.59 bits
Computation based on 1767 words.
Number of 3-grams hit = 1382  (78.21%)
Number of 2-grams hit = 313  (17.71%)
Number of 1-grams hit = 72  (4.07%)
10 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article165.text
Will force exclusive back-off from OOVs.
Perplexity = 208.87, Entropy = 7.71 bits
Computation based on 546 words.
Number of 3-grams hit = 363  (66.48%)
Number of 2-grams hit = 141  (25.82%)
Number of 1-grams hit = 42  (7.69%)
11 OOVs (1.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article166.text
Will force exclusive back-off from OOVs.
Perplexity = 94.62, Entropy = 6.56 bits
Computation based on 517 words.
Number of 3-grams hit = 415  (80.27%)
Number of 2-grams hit = 80  (15.47%)
Number of 1-grams hit = 22  (4.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article167.text
Will force exclusive back-off from OOVs.
Perplexity = 107.19, Entropy = 6.74 bits
Computation based on 1170 words.
Number of 3-grams hit = 910  (77.78%)
Number of 2-grams hit = 209  (17.86%)
Number of 1-grams hit = 51  (4.36%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article168.text
Will force exclusive back-off from OOVs.
Perplexity = 117.70, Entropy = 6.88 bits
Computation based on 524 words.
Number of 3-grams hit = 412  (78.63%)
Number of 2-grams hit = 83  (15.84%)
Number of 1-grams hit = 29  (5.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article169.text
Will force exclusive back-off from OOVs.
Perplexity = 122.58, Entropy = 6.94 bits
Computation based on 499 words.
Number of 3-grams hit = 378  (75.75%)
Number of 2-grams hit = 95  (19.04%)
Number of 1-grams hit = 26  (5.21%)
5 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article170.text
Will force exclusive back-off from OOVs.
Perplexity = 193.97, Entropy = 7.60 bits
Computation based on 1128 words.
Number of 3-grams hit = 753  (66.76%)
Number of 2-grams hit = 282  (25.00%)
Number of 1-grams hit = 93  (8.24%)
22 OOVs (1.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article171.text
Will force exclusive back-off from OOVs.
Perplexity = 289.44, Entropy = 8.18 bits
Computation based on 605 words.
Number of 3-grams hit = 376  (62.15%)
Number of 2-grams hit = 169  (27.93%)
Number of 1-grams hit = 60  (9.92%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article172.text
Will force exclusive back-off from OOVs.
Perplexity = 121.38, Entropy = 6.92 bits
Computation based on 681 words.
Number of 3-grams hit = 511  (75.04%)
Number of 2-grams hit = 140  (20.56%)
Number of 1-grams hit = 30  (4.41%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article173.text
Will force exclusive back-off from OOVs.
Perplexity = 170.50, Entropy = 7.41 bits
Computation based on 1009 words.
Number of 3-grams hit = 697  (69.08%)
Number of 2-grams hit = 234  (23.19%)
Number of 1-grams hit = 78  (7.73%)
12 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article174.text
Will force exclusive back-off from OOVs.
Perplexity = 111.00, Entropy = 6.79 bits
Computation based on 446 words.
Number of 3-grams hit = 346  (77.58%)
Number of 2-grams hit = 82  (18.39%)
Number of 1-grams hit = 18  (4.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article175.text
Will force exclusive back-off from OOVs.
Perplexity = 108.17, Entropy = 6.76 bits
Computation based on 458 words.
Number of 3-grams hit = 353  (77.07%)
Number of 2-grams hit = 85  (18.56%)
Number of 1-grams hit = 20  (4.37%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article176.text
Will force exclusive back-off from OOVs.
Perplexity = 142.85, Entropy = 7.16 bits
Computation based on 743 words.
Number of 3-grams hit = 538  (72.41%)
Number of 2-grams hit = 175  (23.55%)
Number of 1-grams hit = 30  (4.04%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article177.text
Will force exclusive back-off from OOVs.
Perplexity = 149.69, Entropy = 7.23 bits
Computation based on 511 words.
Number of 3-grams hit = 360  (70.45%)
Number of 2-grams hit = 119  (23.29%)
Number of 1-grams hit = 32  (6.26%)
7 OOVs (1.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article178.text
Will force exclusive back-off from OOVs.
Perplexity = 133.57, Entropy = 7.06 bits
Computation based on 168 words.
Number of 3-grams hit = 127  (75.60%)
Number of 2-grams hit = 29  (17.26%)
Number of 1-grams hit = 12  (7.14%)
1 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article179.text
Will force exclusive back-off from OOVs.
Perplexity = 175.66, Entropy = 7.46 bits
Computation based on 449 words.
Number of 3-grams hit = 320  (71.27%)
Number of 2-grams hit = 94  (20.94%)
Number of 1-grams hit = 35  (7.80%)
5 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article180.text
Will force exclusive back-off from OOVs.
Perplexity = 114.06, Entropy = 6.83 bits
Computation based on 427 words.
Number of 3-grams hit = 326  (76.35%)
Number of 2-grams hit = 77  (18.03%)
Number of 1-grams hit = 24  (5.62%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article181.text
Will force exclusive back-off from OOVs.
Perplexity = 77.02, Entropy = 6.27 bits
Computation based on 849 words.
Number of 3-grams hit = 708  (83.39%)
Number of 2-grams hit = 110  (12.96%)
Number of 1-grams hit = 31  (3.65%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article182.text
Will force exclusive back-off from OOVs.
Perplexity = 83.07, Entropy = 6.38 bits
Computation based on 366 words.
Number of 3-grams hit = 294  (80.33%)
Number of 2-grams hit = 59  (16.12%)
Number of 1-grams hit = 13  (3.55%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article183.text
Will force exclusive back-off from OOVs.
Perplexity = 89.81, Entropy = 6.49 bits
Computation based on 458 words.
Number of 3-grams hit = 361  (78.82%)
Number of 2-grams hit = 74  (16.16%)
Number of 1-grams hit = 23  (5.02%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article184.text
Will force exclusive back-off from OOVs.
Perplexity = 119.56, Entropy = 6.90 bits
Computation based on 5472 words.
Number of 3-grams hit = 4163  (76.08%)
Number of 2-grams hit = 1028  (18.79%)
Number of 1-grams hit = 281  (5.14%)
20 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article185.text
Will force exclusive back-off from OOVs.
Perplexity = 112.16, Entropy = 6.81 bits
Computation based on 975 words.
Number of 3-grams hit = 748  (76.72%)
Number of 2-grams hit = 189  (19.38%)
Number of 1-grams hit = 38  (3.90%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article186.text
Will force exclusive back-off from OOVs.
Perplexity = 92.40, Entropy = 6.53 bits
Computation based on 487 words.
Number of 3-grams hit = 385  (79.06%)
Number of 2-grams hit = 82  (16.84%)
Number of 1-grams hit = 20  (4.11%)
3 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article187.text
Will force exclusive back-off from OOVs.
Perplexity = 123.56, Entropy = 6.95 bits
Computation based on 6010 words.
Number of 3-grams hit = 4590  (76.37%)
Number of 2-grams hit = 1132  (18.84%)
Number of 1-grams hit = 288  (4.79%)
17 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article188.text
Will force exclusive back-off from OOVs.
Perplexity = 131.51, Entropy = 7.04 bits
Computation based on 684 words.
Number of 3-grams hit = 497  (72.66%)
Number of 2-grams hit = 159  (23.25%)
Number of 1-grams hit = 28  (4.09%)
7 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article189.text
Will force exclusive back-off from OOVs.
Perplexity = 179.59, Entropy = 7.49 bits
Computation based on 521 words.
Number of 3-grams hit = 351  (67.37%)
Number of 2-grams hit = 125  (23.99%)
Number of 1-grams hit = 45  (8.64%)
15 OOVs (2.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article190.text
Will force exclusive back-off from OOVs.
Perplexity = 198.83, Entropy = 7.64 bits
Computation based on 390 words.
Number of 3-grams hit = 269  (68.97%)
Number of 2-grams hit = 94  (24.10%)
Number of 1-grams hit = 27  (6.92%)
6 OOVs (1.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article191.text
Will force exclusive back-off from OOVs.
Perplexity = 119.30, Entropy = 6.90 bits
Computation based on 5442 words.
Number of 3-grams hit = 4186  (76.92%)
Number of 2-grams hit = 983  (18.06%)
Number of 1-grams hit = 273  (5.02%)
26 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article192.text
Will force exclusive back-off from OOVs.
Perplexity = 122.23, Entropy = 6.93 bits
Computation based on 767 words.
Number of 3-grams hit = 586  (76.40%)
Number of 2-grams hit = 157  (20.47%)
Number of 1-grams hit = 24  (3.13%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article193.text
Will force exclusive back-off from OOVs.
Perplexity = 85.80, Entropy = 6.42 bits
Computation based on 161 words.
Number of 3-grams hit = 124  (77.02%)
Number of 2-grams hit = 30  (18.63%)
Number of 1-grams hit = 7  (4.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article194.text
Will force exclusive back-off from OOVs.
Perplexity = 110.12, Entropy = 6.78 bits
Computation based on 604 words.
Number of 3-grams hit = 470  (77.81%)
Number of 2-grams hit = 105  (17.38%)
Number of 1-grams hit = 29  (4.80%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article195.text
Will force exclusive back-off from OOVs.
Perplexity = 110.08, Entropy = 6.78 bits
Computation based on 792 words.
Number of 3-grams hit = 611  (77.15%)
Number of 2-grams hit = 143  (18.06%)
Number of 1-grams hit = 38  (4.80%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article196.text
Will force exclusive back-off from OOVs.
Perplexity = 119.95, Entropy = 6.91 bits
Computation based on 685 words.
Number of 3-grams hit = 529  (77.23%)
Number of 2-grams hit = 134  (19.56%)
Number of 1-grams hit = 22  (3.21%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article197.text
Will force exclusive back-off from OOVs.
Perplexity = 112.57, Entropy = 6.81 bits
Computation based on 223 words.
Number of 3-grams hit = 170  (76.23%)
Number of 2-grams hit = 37  (16.59%)
Number of 1-grams hit = 16  (7.17%)
3 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article198.text
Will force exclusive back-off from OOVs.
Perplexity = 130.39, Entropy = 7.03 bits
Computation based on 718 words.
Number of 3-grams hit = 534  (74.37%)
Number of 2-grams hit = 144  (20.06%)
Number of 1-grams hit = 40  (5.57%)
3 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article199.text
Will force exclusive back-off from OOVs.
Perplexity = 91.03, Entropy = 6.51 bits
Computation based on 4477 words.
Number of 3-grams hit = 3591  (80.21%)
Number of 2-grams hit = 727  (16.24%)
Number of 1-grams hit = 159  (3.55%)
16 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article200.text
Will force exclusive back-off from OOVs.
Perplexity = 104.43, Entropy = 6.71 bits
Computation based on 638 words.
Number of 3-grams hit = 492  (77.12%)
Number of 2-grams hit = 116  (18.18%)
Number of 1-grams hit = 30  (4.70%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article201.text
Will force exclusive back-off from OOVs.
Perplexity = 181.04, Entropy = 7.50 bits
Computation based on 463 words.
Number of 3-grams hit = 315  (68.03%)
Number of 2-grams hit = 116  (25.05%)
Number of 1-grams hit = 32  (6.91%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article202.text
Will force exclusive back-off from OOVs.
Perplexity = 120.62, Entropy = 6.91 bits
Computation based on 1170 words.
Number of 3-grams hit = 883  (75.47%)
Number of 2-grams hit = 240  (20.51%)
Number of 1-grams hit = 47  (4.02%)
5 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article203.text
Will force exclusive back-off from OOVs.
Perplexity = 79.58, Entropy = 6.31 bits
Computation based on 4808 words.
Number of 3-grams hit = 3901  (81.14%)
Number of 2-grams hit = 781  (16.24%)
Number of 1-grams hit = 126  (2.62%)
22 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article204.text
Will force exclusive back-off from OOVs.
Perplexity = 88.13, Entropy = 6.46 bits
Computation based on 5390 words.
Number of 3-grams hit = 4360  (80.89%)
Number of 2-grams hit = 855  (15.86%)
Number of 1-grams hit = 175  (3.25%)
26 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article205.text
Will force exclusive back-off from OOVs.
Perplexity = 100.00, Entropy = 6.64 bits
Computation based on 553 words.
Number of 3-grams hit = 437  (79.02%)
Number of 2-grams hit = 97  (17.54%)
Number of 1-grams hit = 19  (3.44%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article206.text
Will force exclusive back-off from OOVs.
Perplexity = 154.44, Entropy = 7.27 bits
Computation based on 585 words.
Number of 3-grams hit = 415  (70.94%)
Number of 2-grams hit = 141  (24.10%)
Number of 1-grams hit = 29  (4.96%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article207.text
Will force exclusive back-off from OOVs.
Perplexity = 128.26, Entropy = 7.00 bits
Computation based on 722 words.
Number of 3-grams hit = 542  (75.07%)
Number of 2-grams hit = 143  (19.81%)
Number of 1-grams hit = 37  (5.12%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article208.text
Will force exclusive back-off from OOVs.
Perplexity = 101.03, Entropy = 6.66 bits
Computation based on 886 words.
Number of 3-grams hit = 703  (79.35%)
Number of 2-grams hit = 147  (16.59%)
Number of 1-grams hit = 36  (4.06%)
8 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article209.text
Will force exclusive back-off from OOVs.
Perplexity = 50.95, Entropy = 5.67 bits
Computation based on 269 words.
Number of 3-grams hit = 235  (87.36%)
Number of 2-grams hit = 26  (9.67%)
Number of 1-grams hit = 8  (2.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article210.text
Will force exclusive back-off from OOVs.
Perplexity = 157.74, Entropy = 7.30 bits
Computation based on 811 words.
Number of 3-grams hit = 551  (67.94%)
Number of 2-grams hit = 196  (24.17%)
Number of 1-grams hit = 64  (7.89%)
26 OOVs (3.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article211.text
Will force exclusive back-off from OOVs.
Perplexity = 199.10, Entropy = 7.64 bits
Computation based on 462 words.
Number of 3-grams hit = 307  (66.45%)
Number of 2-grams hit = 112  (24.24%)
Number of 1-grams hit = 43  (9.31%)
11 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article212.text
Will force exclusive back-off from OOVs.
Perplexity = 103.18, Entropy = 6.69 bits
Computation based on 416 words.
Number of 3-grams hit = 328  (78.85%)
Number of 2-grams hit = 74  (17.79%)
Number of 1-grams hit = 14  (3.37%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article213.text
Will force exclusive back-off from OOVs.
Perplexity = 113.65, Entropy = 6.83 bits
Computation based on 630 words.
Number of 3-grams hit = 487  (77.30%)
Number of 2-grams hit = 118  (18.73%)
Number of 1-grams hit = 25  (3.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article214.text
Will force exclusive back-off from OOVs.
Perplexity = 163.79, Entropy = 7.36 bits
Computation based on 444 words.
Number of 3-grams hit = 295  (66.44%)
Number of 2-grams hit = 108  (24.32%)
Number of 1-grams hit = 41  (9.23%)
20 OOVs (4.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article215.text
Will force exclusive back-off from OOVs.
Perplexity = 355.64, Entropy = 8.47 bits
Computation based on 685 words.
Number of 3-grams hit = 410  (59.85%)
Number of 2-grams hit = 199  (29.05%)
Number of 1-grams hit = 76  (11.09%)
11 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article216.text
Will force exclusive back-off from OOVs.
Perplexity = 283.25, Entropy = 8.15 bits
Computation based on 481 words.
Number of 3-grams hit = 301  (62.58%)
Number of 2-grams hit = 139  (28.90%)
Number of 1-grams hit = 41  (8.52%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article217.text
Will force exclusive back-off from OOVs.
Perplexity = 96.50, Entropy = 6.59 bits
Computation based on 302 words.
Number of 3-grams hit = 251  (83.11%)
Number of 2-grams hit = 41  (13.58%)
Number of 1-grams hit = 10  (3.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article218.text
Will force exclusive back-off from OOVs.
Perplexity = 74.07, Entropy = 6.21 bits
Computation based on 718 words.
Number of 3-grams hit = 597  (83.15%)
Number of 2-grams hit = 108  (15.04%)
Number of 1-grams hit = 13  (1.81%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article219.text
Will force exclusive back-off from OOVs.
Perplexity = 160.86, Entropy = 7.33 bits
Computation based on 406 words.
Number of 3-grams hit = 277  (68.23%)
Number of 2-grams hit = 102  (25.12%)
Number of 1-grams hit = 27  (6.65%)
5 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article220.text
Will force exclusive back-off from OOVs.
Perplexity = 68.38, Entropy = 6.10 bits
Computation based on 4293 words.
Number of 3-grams hit = 3608  (84.04%)
Number of 2-grams hit = 579  (13.49%)
Number of 1-grams hit = 106  (2.47%)
27 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article221.text
Will force exclusive back-off from OOVs.
Perplexity = 129.60, Entropy = 7.02 bits
Computation based on 1174 words.
Number of 3-grams hit = 874  (74.45%)
Number of 2-grams hit = 255  (21.72%)
Number of 1-grams hit = 45  (3.83%)
8 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article222.text
Will force exclusive back-off from OOVs.
Perplexity = 126.13, Entropy = 6.98 bits
Computation based on 4454 words.
Number of 3-grams hit = 3366  (75.57%)
Number of 2-grams hit = 852  (19.13%)
Number of 1-grams hit = 236  (5.30%)
17 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article223.text
Will force exclusive back-off from OOVs.
Perplexity = 116.22, Entropy = 6.86 bits
Computation based on 825 words.
Number of 3-grams hit = 624  (75.64%)
Number of 2-grams hit = 145  (17.58%)
Number of 1-grams hit = 56  (6.79%)
9 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article224.text
Will force exclusive back-off from OOVs.
Perplexity = 111.42, Entropy = 6.80 bits
Computation based on 380 words.
Number of 3-grams hit = 286  (75.26%)
Number of 2-grams hit = 76  (20.00%)
Number of 1-grams hit = 18  (4.74%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article225.text
Will force exclusive back-off from OOVs.
Perplexity = 229.95, Entropy = 7.85 bits
Computation based on 724 words.
Number of 3-grams hit = 479  (66.16%)
Number of 2-grams hit = 181  (25.00%)
Number of 1-grams hit = 64  (8.84%)
6 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article226.text
Will force exclusive back-off from OOVs.
Perplexity = 138.78, Entropy = 7.12 bits
Computation based on 1224 words.
Number of 3-grams hit = 875  (71.49%)
Number of 2-grams hit = 288  (23.53%)
Number of 1-grams hit = 61  (4.98%)
12 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article227.text
Will force exclusive back-off from OOVs.
Perplexity = 120.61, Entropy = 6.91 bits
Computation based on 936 words.
Number of 3-grams hit = 711  (75.96%)
Number of 2-grams hit = 184  (19.66%)
Number of 1-grams hit = 41  (4.38%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article228.text
Will force exclusive back-off from OOVs.
Perplexity = 164.56, Entropy = 7.36 bits
Computation based on 783 words.
Number of 3-grams hit = 569  (72.67%)
Number of 2-grams hit = 177  (22.61%)
Number of 1-grams hit = 37  (4.73%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article229.text
Will force exclusive back-off from OOVs.
Perplexity = 128.09, Entropy = 7.00 bits
Computation based on 471 words.
Number of 3-grams hit = 351  (74.52%)
Number of 2-grams hit = 100  (21.23%)
Number of 1-grams hit = 20  (4.25%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article230.text
Will force exclusive back-off from OOVs.
Perplexity = 71.48, Entropy = 6.16 bits
Computation based on 301 words.
Number of 3-grams hit = 243  (80.73%)
Number of 2-grams hit = 45  (14.95%)
Number of 1-grams hit = 13  (4.32%)
5 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article231.text
Will force exclusive back-off from OOVs.
Perplexity = 103.54, Entropy = 6.69 bits
Computation based on 801 words.
Number of 3-grams hit = 615  (76.78%)
Number of 2-grams hit = 155  (19.35%)
Number of 1-grams hit = 31  (3.87%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article232.text
Will force exclusive back-off from OOVs.
Perplexity = 132.94, Entropy = 7.05 bits
Computation based on 363 words.
Number of 3-grams hit = 264  (72.73%)
Number of 2-grams hit = 82  (22.59%)
Number of 1-grams hit = 17  (4.68%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article233.text
Will force exclusive back-off from OOVs.
Perplexity = 94.50, Entropy = 6.56 bits
Computation based on 668 words.
Number of 3-grams hit = 536  (80.24%)
Number of 2-grams hit = 110  (16.47%)
Number of 1-grams hit = 22  (3.29%)
5 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article234.text
Will force exclusive back-off from OOVs.
Perplexity = 115.19, Entropy = 6.85 bits
Computation based on 1331 words.
Number of 3-grams hit = 1034  (77.69%)
Number of 2-grams hit = 232  (17.43%)
Number of 1-grams hit = 65  (4.88%)
7 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article235.text
Will force exclusive back-off from OOVs.
Perplexity = 125.36, Entropy = 6.97 bits
Computation based on 349 words.
Number of 3-grams hit = 258  (73.93%)
Number of 2-grams hit = 75  (21.49%)
Number of 1-grams hit = 16  (4.58%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article236.text
Will force exclusive back-off from OOVs.
Perplexity = 260.12, Entropy = 8.02 bits
Computation based on 735 words.
Number of 3-grams hit = 461  (62.72%)
Number of 2-grams hit = 189  (25.71%)
Number of 1-grams hit = 85  (11.56%)
18 OOVs (2.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article237.text
Will force exclusive back-off from OOVs.
Perplexity = 155.50, Entropy = 7.28 bits
Computation based on 389 words.
Number of 3-grams hit = 283  (72.75%)
Number of 2-grams hit = 83  (21.34%)
Number of 1-grams hit = 23  (5.91%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article238.text
Will force exclusive back-off from OOVs.
Perplexity = 344.35, Entropy = 8.43 bits
Computation based on 300 words.
Number of 3-grams hit = 167  (55.67%)
Number of 2-grams hit = 88  (29.33%)
Number of 1-grams hit = 45  (15.00%)
13 OOVs (4.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article239.text
Will force exclusive back-off from OOVs.
Perplexity = 121.76, Entropy = 6.93 bits
Computation based on 471 words.
Number of 3-grams hit = 359  (76.22%)
Number of 2-grams hit = 89  (18.90%)
Number of 1-grams hit = 23  (4.88%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article240.text
Will force exclusive back-off from OOVs.
Perplexity = 192.01, Entropy = 7.59 bits
Computation based on 312 words.
Number of 3-grams hit = 208  (66.67%)
Number of 2-grams hit = 68  (21.79%)
Number of 1-grams hit = 36  (11.54%)
4 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article241.text
Will force exclusive back-off from OOVs.
Perplexity = 253.45, Entropy = 7.99 bits
Computation based on 301 words.
Number of 3-grams hit = 190  (63.12%)
Number of 2-grams hit = 83  (27.57%)
Number of 1-grams hit = 28  (9.30%)
6 OOVs (1.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article242.text
Will force exclusive back-off from OOVs.
Perplexity = 91.80, Entropy = 6.52 bits
Computation based on 291 words.
Number of 3-grams hit = 233  (80.07%)
Number of 2-grams hit = 44  (15.12%)
Number of 1-grams hit = 14  (4.81%)
3 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article243.text
Will force exclusive back-off from OOVs.
Perplexity = 135.73, Entropy = 7.08 bits
Computation based on 743 words.
Number of 3-grams hit = 550  (74.02%)
Number of 2-grams hit = 155  (20.86%)
Number of 1-grams hit = 38  (5.11%)
6 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article244.text
Will force exclusive back-off from OOVs.
Perplexity = 130.08, Entropy = 7.02 bits
Computation based on 424 words.
Number of 3-grams hit = 316  (74.53%)
Number of 2-grams hit = 82  (19.34%)
Number of 1-grams hit = 26  (6.13%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article245.text
Will force exclusive back-off from OOVs.
Perplexity = 84.24, Entropy = 6.40 bits
Computation based on 5029 words.
Number of 3-grams hit = 4011  (79.76%)
Number of 2-grams hit = 843  (16.76%)
Number of 1-grams hit = 175  (3.48%)
19 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article246.text
Will force exclusive back-off from OOVs.
Perplexity = 117.46, Entropy = 6.88 bits
Computation based on 275 words.
Number of 3-grams hit = 202  (73.45%)
Number of 2-grams hit = 56  (20.36%)
Number of 1-grams hit = 17  (6.18%)
2 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article247.text
Will force exclusive back-off from OOVs.
Perplexity = 86.11, Entropy = 6.43 bits
Computation based on 449 words.
Number of 3-grams hit = 359  (79.96%)
Number of 2-grams hit = 69  (15.37%)
Number of 1-grams hit = 21  (4.68%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article248.text
Will force exclusive back-off from OOVs.
Perplexity = 101.80, Entropy = 6.67 bits
Computation based on 355 words.
Number of 3-grams hit = 278  (78.31%)
Number of 2-grams hit = 66  (18.59%)
Number of 1-grams hit = 11  (3.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article249.text
Will force exclusive back-off from OOVs.
Perplexity = 81.33, Entropy = 6.35 bits
Computation based on 858 words.
Number of 3-grams hit = 699  (81.47%)
Number of 2-grams hit = 134  (15.62%)
Number of 1-grams hit = 25  (2.91%)
5 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article250.text
Will force exclusive back-off from OOVs.
Perplexity = 90.23, Entropy = 6.50 bits
Computation based on 4834 words.
Number of 3-grams hit = 3849  (79.62%)
Number of 2-grams hit = 824  (17.05%)
Number of 1-grams hit = 161  (3.33%)
30 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article251.text
Will force exclusive back-off from OOVs.
Perplexity = 105.76, Entropy = 6.72 bits
Computation based on 4971 words.
Number of 3-grams hit = 3877  (77.99%)
Number of 2-grams hit = 892  (17.94%)
Number of 1-grams hit = 202  (4.06%)
15 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article252.text
Will force exclusive back-off from OOVs.
Perplexity = 106.43, Entropy = 6.73 bits
Computation based on 525 words.
Number of 3-grams hit = 409  (77.90%)
Number of 2-grams hit = 95  (18.10%)
Number of 1-grams hit = 21  (4.00%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article253.text
Will force exclusive back-off from OOVs.
Perplexity = 117.25, Entropy = 6.87 bits
Computation based on 4945 words.
Number of 3-grams hit = 3790  (76.64%)
Number of 2-grams hit = 927  (18.75%)
Number of 1-grams hit = 228  (4.61%)
13 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article254.text
Will force exclusive back-off from OOVs.
Perplexity = 277.48, Entropy = 8.12 bits
Computation based on 774 words.
Number of 3-grams hit = 480  (62.02%)
Number of 2-grams hit = 227  (29.33%)
Number of 1-grams hit = 67  (8.66%)
9 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article255.text
Will force exclusive back-off from OOVs.
Perplexity = 136.71, Entropy = 7.09 bits
Computation based on 486 words.
Number of 3-grams hit = 349  (71.81%)
Number of 2-grams hit = 106  (21.81%)
Number of 1-grams hit = 31  (6.38%)
7 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article256.text
Will force exclusive back-off from OOVs.
Perplexity = 133.73, Entropy = 7.06 bits
Computation based on 783 words.
Number of 3-grams hit = 577  (73.69%)
Number of 2-grams hit = 169  (21.58%)
Number of 1-grams hit = 37  (4.73%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article257.text
Will force exclusive back-off from OOVs.
Perplexity = 62.89, Entropy = 5.97 bits
Computation based on 5750 words.
Number of 3-grams hit = 4881  (84.89%)
Number of 2-grams hit = 744  (12.94%)
Number of 1-grams hit = 125  (2.17%)
31 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article258.text
Will force exclusive back-off from OOVs.
Perplexity = 134.90, Entropy = 7.08 bits
Computation based on 409 words.
Number of 3-grams hit = 310  (75.79%)
Number of 2-grams hit = 76  (18.58%)
Number of 1-grams hit = 23  (5.62%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article259.text
Will force exclusive back-off from OOVs.
Perplexity = 183.53, Entropy = 7.52 bits
Computation based on 685 words.
Number of 3-grams hit = 453  (66.13%)
Number of 2-grams hit = 172  (25.11%)
Number of 1-grams hit = 60  (8.76%)
16 OOVs (2.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article260.text
Will force exclusive back-off from OOVs.
Perplexity = 107.78, Entropy = 6.75 bits
Computation based on 6973 words.
Number of 3-grams hit = 5434  (77.93%)
Number of 2-grams hit = 1244  (17.84%)
Number of 1-grams hit = 295  (4.23%)
22 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article261.text
Will force exclusive back-off from OOVs.
Perplexity = 73.94, Entropy = 6.21 bits
Computation based on 645 words.
Number of 3-grams hit = 531  (82.33%)
Number of 2-grams hit = 91  (14.11%)
Number of 1-grams hit = 23  (3.57%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article262.text
Will force exclusive back-off from OOVs.
Perplexity = 88.35, Entropy = 6.47 bits
Computation based on 661 words.
Number of 3-grams hit = 538  (81.39%)
Number of 2-grams hit = 103  (15.58%)
Number of 1-grams hit = 20  (3.03%)
3 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article263.text
Will force exclusive back-off from OOVs.
Perplexity = 103.48, Entropy = 6.69 bits
Computation based on 375 words.
Number of 3-grams hit = 292  (77.87%)
Number of 2-grams hit = 62  (16.53%)
Number of 1-grams hit = 21  (5.60%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article264.text
Will force exclusive back-off from OOVs.
Perplexity = 90.75, Entropy = 6.50 bits
Computation based on 798 words.
Number of 3-grams hit = 631  (79.07%)
Number of 2-grams hit = 138  (17.29%)
Number of 1-grams hit = 29  (3.63%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article265.text
Will force exclusive back-off from OOVs.
Perplexity = 89.55, Entropy = 6.48 bits
Computation based on 569 words.
Number of 3-grams hit = 443  (77.86%)
Number of 2-grams hit = 111  (19.51%)
Number of 1-grams hit = 15  (2.64%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article266.text
Will force exclusive back-off from OOVs.
Perplexity = 122.76, Entropy = 6.94 bits
Computation based on 435 words.
Number of 3-grams hit = 336  (77.24%)
Number of 2-grams hit = 78  (17.93%)
Number of 1-grams hit = 21  (4.83%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article267.text
Will force exclusive back-off from OOVs.
Perplexity = 113.73, Entropy = 6.83 bits
Computation based on 6221 words.
Number of 3-grams hit = 4792  (77.03%)
Number of 2-grams hit = 1133  (18.21%)
Number of 1-grams hit = 296  (4.76%)
31 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article268.text
Will force exclusive back-off from OOVs.
Perplexity = 121.52, Entropy = 6.93 bits
Computation based on 5821 words.
Number of 3-grams hit = 4447  (76.40%)
Number of 2-grams hit = 1083  (18.61%)
Number of 1-grams hit = 291  (5.00%)
19 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article269.text
Will force exclusive back-off from OOVs.
Perplexity = 145.45, Entropy = 7.18 bits
Computation based on 418 words.
Number of 3-grams hit = 311  (74.40%)
Number of 2-grams hit = 81  (19.38%)
Number of 1-grams hit = 26  (6.22%)
4 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article270.text
Will force exclusive back-off from OOVs.
Perplexity = 159.97, Entropy = 7.32 bits
Computation based on 375 words.
Number of 3-grams hit = 262  (69.87%)
Number of 2-grams hit = 87  (23.20%)
Number of 1-grams hit = 26  (6.93%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article271.text
Will force exclusive back-off from OOVs.
Perplexity = 103.99, Entropy = 6.70 bits
Computation based on 7295 words.
Number of 3-grams hit = 5667  (77.68%)
Number of 2-grams hit = 1328  (18.20%)
Number of 1-grams hit = 300  (4.11%)
52 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article272.text
Will force exclusive back-off from OOVs.
Perplexity = 74.69, Entropy = 6.22 bits
Computation based on 5195 words.
Number of 3-grams hit = 4269  (82.18%)
Number of 2-grams hit = 769  (14.80%)
Number of 1-grams hit = 157  (3.02%)
23 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article273.text
Will force exclusive back-off from OOVs.
Perplexity = 93.47, Entropy = 6.55 bits
Computation based on 638 words.
Number of 3-grams hit = 508  (79.62%)
Number of 2-grams hit = 108  (16.93%)
Number of 1-grams hit = 22  (3.45%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article274.text
Will force exclusive back-off from OOVs.
Perplexity = 113.15, Entropy = 6.82 bits
Computation based on 651 words.
Number of 3-grams hit = 489  (75.12%)
Number of 2-grams hit = 125  (19.20%)
Number of 1-grams hit = 37  (5.68%)
6 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article275.text
Will force exclusive back-off from OOVs.
Perplexity = 133.74, Entropy = 7.06 bits
Computation based on 1923 words.
Number of 3-grams hit = 1427  (74.21%)
Number of 2-grams hit = 408  (21.22%)
Number of 1-grams hit = 88  (4.58%)
4 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article276.text
Will force exclusive back-off from OOVs.
Perplexity = 206.64, Entropy = 7.69 bits
Computation based on 455 words.
Number of 3-grams hit = 290  (63.74%)
Number of 2-grams hit = 117  (25.71%)
Number of 1-grams hit = 48  (10.55%)
16 OOVs (3.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article277.text
Will force exclusive back-off from OOVs.
Perplexity = 119.26, Entropy = 6.90 bits
Computation based on 768 words.
Number of 3-grams hit = 586  (76.30%)
Number of 2-grams hit = 147  (19.14%)
Number of 1-grams hit = 35  (4.56%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article278.text
Will force exclusive back-off from OOVs.
Perplexity = 115.79, Entropy = 6.86 bits
Computation based on 806 words.
Number of 3-grams hit = 606  (75.19%)
Number of 2-grams hit = 167  (20.72%)
Number of 1-grams hit = 33  (4.09%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article279.text
Will force exclusive back-off from OOVs.
Perplexity = 90.93, Entropy = 6.51 bits
Computation based on 401 words.
Number of 3-grams hit = 325  (81.05%)
Number of 2-grams hit = 58  (14.46%)
Number of 1-grams hit = 18  (4.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article280.text
Will force exclusive back-off from OOVs.
Perplexity = 113.61, Entropy = 6.83 bits
Computation based on 1708 words.
Number of 3-grams hit = 1342  (78.57%)
Number of 2-grams hit = 293  (17.15%)
Number of 1-grams hit = 73  (4.27%)
2 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article281.text
Will force exclusive back-off from OOVs.
Perplexity = 134.21, Entropy = 7.07 bits
Computation based on 546 words.
Number of 3-grams hit = 379  (69.41%)
Number of 2-grams hit = 132  (24.18%)
Number of 1-grams hit = 35  (6.41%)
20 OOVs (3.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article282.text
Will force exclusive back-off from OOVs.
Perplexity = 87.11, Entropy = 6.44 bits
Computation based on 708 words.
Number of 3-grams hit = 567  (80.08%)
Number of 2-grams hit = 107  (15.11%)
Number of 1-grams hit = 34  (4.80%)
5 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article283.text
Will force exclusive back-off from OOVs.
Perplexity = 99.82, Entropy = 6.64 bits
Computation based on 1182 words.
Number of 3-grams hit = 936  (79.19%)
Number of 2-grams hit = 195  (16.50%)
Number of 1-grams hit = 51  (4.31%)
4 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article284.text
Will force exclusive back-off from OOVs.
Perplexity = 72.61, Entropy = 6.18 bits
Computation based on 1579 words.
Number of 3-grams hit = 1319  (83.53%)
Number of 2-grams hit = 213  (13.49%)
Number of 1-grams hit = 47  (2.98%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article285.text
Will force exclusive back-off from OOVs.
Perplexity = 104.87, Entropy = 6.71 bits
Computation based on 1094 words.
Number of 3-grams hit = 861  (78.70%)
Number of 2-grams hit = 195  (17.82%)
Number of 1-grams hit = 38  (3.47%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article286.text
Will force exclusive back-off from OOVs.
Perplexity = 181.06, Entropy = 7.50 bits
Computation based on 442 words.
Number of 3-grams hit = 316  (71.49%)
Number of 2-grams hit = 99  (22.40%)
Number of 1-grams hit = 27  (6.11%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article287.text
Will force exclusive back-off from OOVs.
Perplexity = 90.42, Entropy = 6.50 bits
Computation based on 1652 words.
Number of 3-grams hit = 1314  (79.54%)
Number of 2-grams hit = 278  (16.83%)
Number of 1-grams hit = 60  (3.63%)
6 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article288.text
Will force exclusive back-off from OOVs.
Perplexity = 115.04, Entropy = 6.85 bits
Computation based on 582 words.
Number of 3-grams hit = 443  (76.12%)
Number of 2-grams hit = 116  (19.93%)
Number of 1-grams hit = 23  (3.95%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article289.text
Will force exclusive back-off from OOVs.
Perplexity = 67.84, Entropy = 6.08 bits
Computation based on 704 words.
Number of 3-grams hit = 591  (83.95%)
Number of 2-grams hit = 100  (14.20%)
Number of 1-grams hit = 13  (1.85%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article290.text
Will force exclusive back-off from OOVs.
Perplexity = 104.45, Entropy = 6.71 bits
Computation based on 1280 words.
Number of 3-grams hit = 997  (77.89%)
Number of 2-grams hit = 235  (18.36%)
Number of 1-grams hit = 48  (3.75%)
13 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article291.text
Will force exclusive back-off from OOVs.
Perplexity = 181.91, Entropy = 7.51 bits
Computation based on 507 words.
Number of 3-grams hit = 345  (68.05%)
Number of 2-grams hit = 123  (24.26%)
Number of 1-grams hit = 39  (7.69%)
9 OOVs (1.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article292.text
Will force exclusive back-off from OOVs.
Perplexity = 100.74, Entropy = 6.65 bits
Computation based on 533 words.
Number of 3-grams hit = 421  (78.99%)
Number of 2-grams hit = 91  (17.07%)
Number of 1-grams hit = 21  (3.94%)
5 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article293.text
Will force exclusive back-off from OOVs.
Perplexity = 171.65, Entropy = 7.42 bits
Computation based on 552 words.
Number of 3-grams hit = 397  (71.92%)
Number of 2-grams hit = 117  (21.20%)
Number of 1-grams hit = 38  (6.88%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article294.text
Will force exclusive back-off from OOVs.
Perplexity = 248.63, Entropy = 7.96 bits
Computation based on 2100 words.
Number of 3-grams hit = 1366  (65.05%)
Number of 2-grams hit = 571  (27.19%)
Number of 1-grams hit = 163  (7.76%)
20 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article295.text
Will force exclusive back-off from OOVs.
Perplexity = 180.15, Entropy = 7.49 bits
Computation based on 583 words.
Number of 3-grams hit = 392  (67.24%)
Number of 2-grams hit = 151  (25.90%)
Number of 1-grams hit = 40  (6.86%)
10 OOVs (1.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article296.text
Will force exclusive back-off from OOVs.
Perplexity = 95.92, Entropy = 6.58 bits
Computation based on 1523 words.
Number of 3-grams hit = 1198  (78.66%)
Number of 2-grams hit = 272  (17.86%)
Number of 1-grams hit = 53  (3.48%)
7 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article297.text
Will force exclusive back-off from OOVs.
Perplexity = 68.87, Entropy = 6.11 bits
Computation based on 492 words.
Number of 3-grams hit = 418  (84.96%)
Number of 2-grams hit = 66  (13.41%)
Number of 1-grams hit = 8  (1.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article298.text
Will force exclusive back-off from OOVs.
Perplexity = 110.40, Entropy = 6.79 bits
Computation based on 3442 words.
Number of 3-grams hit = 2693  (78.24%)
Number of 2-grams hit = 602  (17.49%)
Number of 1-grams hit = 147  (4.27%)
12 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article299.text
Will force exclusive back-off from OOVs.
Perplexity = 163.15, Entropy = 7.35 bits
Computation based on 2703 words.
Number of 3-grams hit = 1939  (71.74%)
Number of 2-grams hit = 578  (21.38%)
Number of 1-grams hit = 186  (6.88%)
39 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article300.text
Will force exclusive back-off from OOVs.
Perplexity = 80.88, Entropy = 6.34 bits
Computation based on 2528 words.
Number of 3-grams hit = 2062  (81.57%)
Number of 2-grams hit = 398  (15.74%)
Number of 1-grams hit = 68  (2.69%)
11 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article301.text
Will force exclusive back-off from OOVs.
Perplexity = 127.44, Entropy = 6.99 bits
Computation based on 2136 words.
Number of 3-grams hit = 1616  (75.66%)
Number of 2-grams hit = 426  (19.94%)
Number of 1-grams hit = 94  (4.40%)
10 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article302.text
Will force exclusive back-off from OOVs.
Perplexity = 84.53, Entropy = 6.40 bits
Computation based on 622 words.
Number of 3-grams hit = 496  (79.74%)
Number of 2-grams hit = 95  (15.27%)
Number of 1-grams hit = 31  (4.98%)
4 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article303.text
Will force exclusive back-off from OOVs.
Perplexity = 106.36, Entropy = 6.73 bits
Computation based on 852 words.
Number of 3-grams hit = 660  (77.46%)
Number of 2-grams hit = 147  (17.25%)
Number of 1-grams hit = 45  (5.28%)
18 OOVs (2.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article304.text
Will force exclusive back-off from OOVs.
Perplexity = 243.94, Entropy = 7.93 bits
Computation based on 392 words.
Number of 3-grams hit = 238  (60.71%)
Number of 2-grams hit = 111  (28.32%)
Number of 1-grams hit = 43  (10.97%)
12 OOVs (2.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article305.text
Will force exclusive back-off from OOVs.
Perplexity = 119.18, Entropy = 6.90 bits
Computation based on 303 words.
Number of 3-grams hit = 229  (75.58%)
Number of 2-grams hit = 65  (21.45%)
Number of 1-grams hit = 9  (2.97%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article306.text
Will force exclusive back-off from OOVs.
Perplexity = 102.55, Entropy = 6.68 bits
Computation based on 589 words.
Number of 3-grams hit = 467  (79.29%)
Number of 2-grams hit = 90  (15.28%)
Number of 1-grams hit = 32  (5.43%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article307.text
Will force exclusive back-off from OOVs.
Perplexity = 122.53, Entropy = 6.94 bits
Computation based on 511 words.
Number of 3-grams hit = 389  (76.13%)
Number of 2-grams hit = 101  (19.77%)
Number of 1-grams hit = 21  (4.11%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article308.text
Will force exclusive back-off from OOVs.
Perplexity = 131.87, Entropy = 7.04 bits
Computation based on 392 words.
Number of 3-grams hit = 296  (75.51%)
Number of 2-grams hit = 81  (20.66%)
Number of 1-grams hit = 15  (3.83%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article309.text
Will force exclusive back-off from OOVs.
Perplexity = 96.00, Entropy = 6.58 bits
Computation based on 241 words.
Number of 3-grams hit = 198  (82.16%)
Number of 2-grams hit = 34  (14.11%)
Number of 1-grams hit = 9  (3.73%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article310.text
Will force exclusive back-off from OOVs.
Perplexity = 147.82, Entropy = 7.21 bits
Computation based on 480 words.
Number of 3-grams hit = 349  (72.71%)
Number of 2-grams hit = 106  (22.08%)
Number of 1-grams hit = 25  (5.21%)
4 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article311.text
Will force exclusive back-off from OOVs.
Perplexity = 112.56, Entropy = 6.81 bits
Computation based on 465 words.
Number of 3-grams hit = 349  (75.05%)
Number of 2-grams hit = 93  (20.00%)
Number of 1-grams hit = 23  (4.95%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article312.text
Will force exclusive back-off from OOVs.
Perplexity = 158.83, Entropy = 7.31 bits
Computation based on 594 words.
Number of 3-grams hit = 434  (73.06%)
Number of 2-grams hit = 122  (20.54%)
Number of 1-grams hit = 38  (6.40%)
4 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article313.text
Will force exclusive back-off from OOVs.
Perplexity = 181.89, Entropy = 7.51 bits
Computation based on 466 words.
Number of 3-grams hit = 315  (67.60%)
Number of 2-grams hit = 116  (24.89%)
Number of 1-grams hit = 35  (7.51%)
11 OOVs (2.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article314.text
Will force exclusive back-off from OOVs.
Perplexity = 88.59, Entropy = 6.47 bits
Computation based on 3281 words.
Number of 3-grams hit = 2618  (79.79%)
Number of 2-grams hit = 558  (17.01%)
Number of 1-grams hit = 105  (3.20%)
15 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article315.text
Will force exclusive back-off from OOVs.
Perplexity = 150.89, Entropy = 7.24 bits
Computation based on 1078 words.
Number of 3-grams hit = 779  (72.26%)
Number of 2-grams hit = 228  (21.15%)
Number of 1-grams hit = 71  (6.59%)
14 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article316.text
Will force exclusive back-off from OOVs.
Perplexity = 119.27, Entropy = 6.90 bits
Computation based on 329 words.
Number of 3-grams hit = 251  (76.29%)
Number of 2-grams hit = 62  (18.84%)
Number of 1-grams hit = 16  (4.86%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article317.text
Will force exclusive back-off from OOVs.
Perplexity = 117.25, Entropy = 6.87 bits
Computation based on 623 words.
Number of 3-grams hit = 475  (76.24%)
Number of 2-grams hit = 118  (18.94%)
Number of 1-grams hit = 30  (4.82%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article318.text
Will force exclusive back-off from OOVs.
Perplexity = 113.38, Entropy = 6.82 bits
Computation based on 3565 words.
Number of 3-grams hit = 2764  (77.53%)
Number of 2-grams hit = 652  (18.29%)
Number of 1-grams hit = 149  (4.18%)
9 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article319.text
Will force exclusive back-off from OOVs.
Perplexity = 333.92, Entropy = 8.38 bits
Computation based on 589 words.
Number of 3-grams hit = 364  (61.80%)
Number of 2-grams hit = 167  (28.35%)
Number of 1-grams hit = 58  (9.85%)
10 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article320.text
Will force exclusive back-off from OOVs.
Perplexity = 163.05, Entropy = 7.35 bits
Computation based on 376 words.
Number of 3-grams hit = 258  (68.62%)
Number of 2-grams hit = 101  (26.86%)
Number of 1-grams hit = 17  (4.52%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article321.text
Will force exclusive back-off from OOVs.
Perplexity = 129.81, Entropy = 7.02 bits
Computation based on 1074 words.
Number of 3-grams hit = 828  (77.09%)
Number of 2-grams hit = 191  (17.78%)
Number of 1-grams hit = 55  (5.12%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article322.text
Will force exclusive back-off from OOVs.
Perplexity = 105.47, Entropy = 6.72 bits
Computation based on 1666 words.
Number of 3-grams hit = 1295  (77.73%)
Number of 2-grams hit = 300  (18.01%)
Number of 1-grams hit = 71  (4.26%)
15 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article323.text
Will force exclusive back-off from OOVs.
Perplexity = 299.72, Entropy = 8.23 bits
Computation based on 808 words.
Number of 3-grams hit = 499  (61.76%)
Number of 2-grams hit = 238  (29.46%)
Number of 1-grams hit = 71  (8.79%)
12 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article324.text
Will force exclusive back-off from OOVs.
Perplexity = 117.08, Entropy = 6.87 bits
Computation based on 1642 words.
Number of 3-grams hit = 1263  (76.92%)
Number of 2-grams hit = 294  (17.90%)
Number of 1-grams hit = 85  (5.18%)
15 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article325.text
Will force exclusive back-off from OOVs.
Perplexity = 218.09, Entropy = 7.77 bits
Computation based on 1108 words.
Number of 3-grams hit = 720  (64.98%)
Number of 2-grams hit = 302  (27.26%)
Number of 1-grams hit = 86  (7.76%)
18 OOVs (1.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article326.text
Will force exclusive back-off from OOVs.
Perplexity = 130.24, Entropy = 7.03 bits
Computation based on 606 words.
Number of 3-grams hit = 454  (74.92%)
Number of 2-grams hit = 113  (18.65%)
Number of 1-grams hit = 39  (6.44%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article327.text
Will force exclusive back-off from OOVs.
Perplexity = 99.39, Entropy = 6.64 bits
Computation based on 1125 words.
Number of 3-grams hit = 881  (78.31%)
Number of 2-grams hit = 208  (18.49%)
Number of 1-grams hit = 36  (3.20%)
5 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article328.text
Will force exclusive back-off from OOVs.
Perplexity = 74.16, Entropy = 6.21 bits
Computation based on 250 words.
Number of 3-grams hit = 204  (81.60%)
Number of 2-grams hit = 40  (16.00%)
Number of 1-grams hit = 6  (2.40%)
1 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article329.text
Will force exclusive back-off from OOVs.
Perplexity = 82.42, Entropy = 6.36 bits
Computation based on 413 words.
Number of 3-grams hit = 335  (81.11%)
Number of 2-grams hit = 70  (16.95%)
Number of 1-grams hit = 8  (1.94%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article330.text
Will force exclusive back-off from OOVs.
Perplexity = 93.33, Entropy = 6.54 bits
Computation based on 1051 words.
Number of 3-grams hit = 822  (78.21%)
Number of 2-grams hit = 188  (17.89%)
Number of 1-grams hit = 41  (3.90%)
8 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article331.text
Will force exclusive back-off from OOVs.
Perplexity = 105.08, Entropy = 6.72 bits
Computation based on 1948 words.
Number of 3-grams hit = 1535  (78.80%)
Number of 2-grams hit = 332  (17.04%)
Number of 1-grams hit = 81  (4.16%)
10 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article332.text
Will force exclusive back-off from OOVs.
Perplexity = 91.69, Entropy = 6.52 bits
Computation based on 575 words.
Number of 3-grams hit = 468  (81.39%)
Number of 2-grams hit = 85  (14.78%)
Number of 1-grams hit = 22  (3.83%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article333.text
Will force exclusive back-off from OOVs.
Perplexity = 141.14, Entropy = 7.14 bits
Computation based on 2146 words.
Number of 3-grams hit = 1582  (73.72%)
Number of 2-grams hit = 443  (20.64%)
Number of 1-grams hit = 121  (5.64%)
7 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article334.text
Will force exclusive back-off from OOVs.
Perplexity = 124.07, Entropy = 6.96 bits
Computation based on 961 words.
Number of 3-grams hit = 728  (75.75%)
Number of 2-grams hit = 186  (19.35%)
Number of 1-grams hit = 47  (4.89%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article335.text
Will force exclusive back-off from OOVs.
Perplexity = 86.43, Entropy = 6.43 bits
Computation based on 1753 words.
Number of 3-grams hit = 1438  (82.03%)
Number of 2-grams hit = 259  (14.77%)
Number of 1-grams hit = 56  (3.19%)
8 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article336.text
Will force exclusive back-off from OOVs.
Perplexity = 107.48, Entropy = 6.75 bits
Computation based on 1140 words.
Number of 3-grams hit = 888  (77.89%)
Number of 2-grams hit = 212  (18.60%)
Number of 1-grams hit = 40  (3.51%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article337.text
Will force exclusive back-off from OOVs.
Perplexity = 94.78, Entropy = 6.57 bits
Computation based on 399 words.
Number of 3-grams hit = 314  (78.70%)
Number of 2-grams hit = 70  (17.54%)
Number of 1-grams hit = 15  (3.76%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article338.text
Will force exclusive back-off from OOVs.
Perplexity = 197.64, Entropy = 7.63 bits
Computation based on 3481 words.
Number of 3-grams hit = 2321  (66.68%)
Number of 2-grams hit = 860  (24.71%)
Number of 1-grams hit = 300  (8.62%)
91 OOVs (2.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article339.text
Will force exclusive back-off from OOVs.
Perplexity = 124.59, Entropy = 6.96 bits
Computation based on 373 words.
Number of 3-grams hit = 280  (75.07%)
Number of 2-grams hit = 79  (21.18%)
Number of 1-grams hit = 14  (3.75%)
3 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article340.text
Will force exclusive back-off from OOVs.
Perplexity = 117.15, Entropy = 6.87 bits
Computation based on 1153 words.
Number of 3-grams hit = 885  (76.76%)
Number of 2-grams hit = 221  (19.17%)
Number of 1-grams hit = 47  (4.08%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article341.text
Will force exclusive back-off from OOVs.
Perplexity = 97.18, Entropy = 6.60 bits
Computation based on 1370 words.
Number of 3-grams hit = 1073  (78.32%)
Number of 2-grams hit = 254  (18.54%)
Number of 1-grams hit = 43  (3.14%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article342.text
Will force exclusive back-off from OOVs.
Perplexity = 82.08, Entropy = 6.36 bits
Computation based on 1442 words.
Number of 3-grams hit = 1156  (80.17%)
Number of 2-grams hit = 228  (15.81%)
Number of 1-grams hit = 58  (4.02%)
9 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article343.text
Will force exclusive back-off from OOVs.
Perplexity = 90.57, Entropy = 6.50 bits
Computation based on 477 words.
Number of 3-grams hit = 373  (78.20%)
Number of 2-grams hit = 94  (19.71%)
Number of 1-grams hit = 10  (2.10%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article344.text
Will force exclusive back-off from OOVs.
Perplexity = 98.67, Entropy = 6.62 bits
Computation based on 581 words.
Number of 3-grams hit = 457  (78.66%)
Number of 2-grams hit = 107  (18.42%)
Number of 1-grams hit = 17  (2.93%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article345.text
Will force exclusive back-off from OOVs.
Perplexity = 117.42, Entropy = 6.88 bits
Computation based on 3153 words.
Number of 3-grams hit = 2411  (76.47%)
Number of 2-grams hit = 595  (18.87%)
Number of 1-grams hit = 147  (4.66%)
11 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article346.text
Will force exclusive back-off from OOVs.
Perplexity = 100.56, Entropy = 6.65 bits
Computation based on 1581 words.
Number of 3-grams hit = 1238  (78.30%)
Number of 2-grams hit = 283  (17.90%)
Number of 1-grams hit = 60  (3.80%)
4 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article347.text
Will force exclusive back-off from OOVs.
Perplexity = 127.69, Entropy = 7.00 bits
Computation based on 647 words.
Number of 3-grams hit = 494  (76.35%)
Number of 2-grams hit = 126  (19.47%)
Number of 1-grams hit = 27  (4.17%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article348.text
Will force exclusive back-off from OOVs.
Perplexity = 112.01, Entropy = 6.81 bits
Computation based on 2087 words.
Number of 3-grams hit = 1617  (77.48%)
Number of 2-grams hit = 359  (17.20%)
Number of 1-grams hit = 111  (5.32%)
11 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article349.text
Will force exclusive back-off from OOVs.
Perplexity = 93.96, Entropy = 6.55 bits
Computation based on 1922 words.
Number of 3-grams hit = 1522  (79.19%)
Number of 2-grams hit = 331  (17.22%)
Number of 1-grams hit = 69  (3.59%)
19 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article350.text
Will force exclusive back-off from OOVs.
Perplexity = 146.23, Entropy = 7.19 bits
Computation based on 357 words.
Number of 3-grams hit = 261  (73.11%)
Number of 2-grams hit = 72  (20.17%)
Number of 1-grams hit = 24  (6.72%)
4 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article351.text
Will force exclusive back-off from OOVs.
Perplexity = 96.74, Entropy = 6.60 bits
Computation based on 432 words.
Number of 3-grams hit = 332  (76.85%)
Number of 2-grams hit = 84  (19.44%)
Number of 1-grams hit = 16  (3.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article352.text
Will force exclusive back-off from OOVs.
Perplexity = 172.31, Entropy = 7.43 bits
Computation based on 446 words.
Number of 3-grams hit = 311  (69.73%)
Number of 2-grams hit = 96  (21.52%)
Number of 1-grams hit = 39  (8.74%)
5 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article353.text
Will force exclusive back-off from OOVs.
Perplexity = 114.20, Entropy = 6.84 bits
Computation based on 425 words.
Number of 3-grams hit = 325  (76.47%)
Number of 2-grams hit = 81  (19.06%)
Number of 1-grams hit = 19  (4.47%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article354.text
Will force exclusive back-off from OOVs.
Perplexity = 136.42, Entropy = 7.09 bits
Computation based on 508 words.
Number of 3-grams hit = 385  (75.79%)
Number of 2-grams hit = 90  (17.72%)
Number of 1-grams hit = 33  (6.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article355.text
Will force exclusive back-off from OOVs.
Perplexity = 172.49, Entropy = 7.43 bits
Computation based on 407 words.
Number of 3-grams hit = 270  (66.34%)
Number of 2-grams hit = 106  (26.04%)
Number of 1-grams hit = 31  (7.62%)
6 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article356.text
Will force exclusive back-off from OOVs.
Perplexity = 187.44, Entropy = 7.55 bits
Computation based on 691 words.
Number of 3-grams hit = 462  (66.86%)
Number of 2-grams hit = 180  (26.05%)
Number of 1-grams hit = 49  (7.09%)
18 OOVs (2.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article357.text
Will force exclusive back-off from OOVs.
Perplexity = 73.09, Entropy = 6.19 bits
Computation based on 747 words.
Number of 3-grams hit = 608  (81.39%)
Number of 2-grams hit = 116  (15.53%)
Number of 1-grams hit = 23  (3.08%)
8 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article358.text
Will force exclusive back-off from OOVs.
Perplexity = 199.74, Entropy = 7.64 bits
Computation based on 1164 words.
Number of 3-grams hit = 784  (67.35%)
Number of 2-grams hit = 275  (23.63%)
Number of 1-grams hit = 105  (9.02%)
33 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article359.text
Will force exclusive back-off from OOVs.
Perplexity = 68.60, Entropy = 6.10 bits
Computation based on 1254 words.
Number of 3-grams hit = 1086  (86.60%)
Number of 2-grams hit = 127  (10.13%)
Number of 1-grams hit = 41  (3.27%)
16 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article360.text
Will force exclusive back-off from OOVs.
Perplexity = 107.66, Entropy = 6.75 bits
Computation based on 710 words.
Number of 3-grams hit = 540  (76.06%)
Number of 2-grams hit = 144  (20.28%)
Number of 1-grams hit = 26  (3.66%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article361.text
Will force exclusive back-off from OOVs.
Perplexity = 125.72, Entropy = 6.97 bits
Computation based on 1080 words.
Number of 3-grams hit = 816  (75.56%)
Number of 2-grams hit = 199  (18.43%)
Number of 1-grams hit = 65  (6.02%)
5 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article362.text
Will force exclusive back-off from OOVs.
Perplexity = 90.42, Entropy = 6.50 bits
Computation based on 1417 words.
Number of 3-grams hit = 1159  (81.79%)
Number of 2-grams hit = 206  (14.54%)
Number of 1-grams hit = 52  (3.67%)
6 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article363.text
Will force exclusive back-off from OOVs.
Perplexity = 115.57, Entropy = 6.85 bits
Computation based on 1157 words.
Number of 3-grams hit = 897  (77.53%)
Number of 2-grams hit = 203  (17.55%)
Number of 1-grams hit = 57  (4.93%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article364.text
Will force exclusive back-off from OOVs.
Perplexity = 105.42, Entropy = 6.72 bits
Computation based on 326 words.
Number of 3-grams hit = 243  (74.54%)
Number of 2-grams hit = 70  (21.47%)
Number of 1-grams hit = 13  (3.99%)
3 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article365.text
Will force exclusive back-off from OOVs.
Perplexity = 99.86, Entropy = 6.64 bits
Computation based on 405 words.
Number of 3-grams hit = 316  (78.02%)
Number of 2-grams hit = 76  (18.77%)
Number of 1-grams hit = 13  (3.21%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article366.text
Will force exclusive back-off from OOVs.
Perplexity = 230.04, Entropy = 7.85 bits
Computation based on 345 words.
Number of 3-grams hit = 227  (65.80%)
Number of 2-grams hit = 84  (24.35%)
Number of 1-grams hit = 34  (9.86%)
6 OOVs (1.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article367.text
Will force exclusive back-off from OOVs.
Perplexity = 65.48, Entropy = 6.03 bits
Computation based on 906 words.
Number of 3-grams hit = 743  (82.01%)
Number of 2-grams hit = 128  (14.13%)
Number of 1-grams hit = 35  (3.86%)
5 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article368.text
Will force exclusive back-off from OOVs.
Perplexity = 207.10, Entropy = 7.69 bits
Computation based on 320 words.
Number of 3-grams hit = 214  (66.88%)
Number of 2-grams hit = 83  (25.94%)
Number of 1-grams hit = 23  (7.19%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article369.text
Will force exclusive back-off from OOVs.
Perplexity = 84.05, Entropy = 6.39 bits
Computation based on 316 words.
Number of 3-grams hit = 258  (81.65%)
Number of 2-grams hit = 46  (14.56%)
Number of 1-grams hit = 12  (3.80%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article370.text
Will force exclusive back-off from OOVs.
Perplexity = 109.93, Entropy = 6.78 bits
Computation based on 1644 words.
Number of 3-grams hit = 1293  (78.65%)
Number of 2-grams hit = 283  (17.21%)
Number of 1-grams hit = 68  (4.14%)
3 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article371.text
Will force exclusive back-off from OOVs.
Perplexity = 121.29, Entropy = 6.92 bits
Computation based on 262 words.
Number of 3-grams hit = 203  (77.48%)
Number of 2-grams hit = 41  (15.65%)
Number of 1-grams hit = 18  (6.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article372.text
Will force exclusive back-off from OOVs.
Perplexity = 122.41, Entropy = 6.94 bits
Computation based on 405 words.
Number of 3-grams hit = 297  (73.33%)
Number of 2-grams hit = 77  (19.01%)
Number of 1-grams hit = 31  (7.65%)
16 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article373.text
Will force exclusive back-off from OOVs.
Perplexity = 89.61, Entropy = 6.49 bits
Computation based on 505 words.
Number of 3-grams hit = 404  (80.00%)
Number of 2-grams hit = 89  (17.62%)
Number of 1-grams hit = 12  (2.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article374.text
Will force exclusive back-off from OOVs.
Perplexity = 99.62, Entropy = 6.64 bits
Computation based on 1521 words.
Number of 3-grams hit = 1184  (77.84%)
Number of 2-grams hit = 278  (18.28%)
Number of 1-grams hit = 59  (3.88%)
2 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article375.text
Will force exclusive back-off from OOVs.
Perplexity = 65.39, Entropy = 6.03 bits
Computation based on 1608 words.
Number of 3-grams hit = 1364  (84.83%)
Number of 2-grams hit = 209  (13.00%)
Number of 1-grams hit = 35  (2.18%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article376.text
Will force exclusive back-off from OOVs.
Perplexity = 121.84, Entropy = 6.93 bits
Computation based on 560 words.
Number of 3-grams hit = 423  (75.54%)
Number of 2-grams hit = 108  (19.29%)
Number of 1-grams hit = 29  (5.18%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article377.text
Will force exclusive back-off from OOVs.
Perplexity = 122.43, Entropy = 6.94 bits
Computation based on 738 words.
Number of 3-grams hit = 560  (75.88%)
Number of 2-grams hit = 134  (18.16%)
Number of 1-grams hit = 44  (5.96%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article378.text
Will force exclusive back-off from OOVs.
Perplexity = 232.43, Entropy = 7.86 bits
Computation based on 609 words.
Number of 3-grams hit = 392  (64.37%)
Number of 2-grams hit = 166  (27.26%)
Number of 1-grams hit = 51  (8.37%)
13 OOVs (2.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article379.text
Will force exclusive back-off from OOVs.
Perplexity = 99.05, Entropy = 6.63 bits
Computation based on 2558 words.
Number of 3-grams hit = 2033  (79.48%)
Number of 2-grams hit = 429  (16.77%)
Number of 1-grams hit = 96  (3.75%)
10 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article380.text
Will force exclusive back-off from OOVs.
Perplexity = 152.80, Entropy = 7.26 bits
Computation based on 346 words.
Number of 3-grams hit = 252  (72.83%)
Number of 2-grams hit = 77  (22.25%)
Number of 1-grams hit = 17  (4.91%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article381.text
Will force exclusive back-off from OOVs.
Perplexity = 46.57, Entropy = 5.54 bits
Computation based on 622 words.
Number of 3-grams hit = 570  (91.64%)
Number of 2-grams hit = 35  (5.63%)
Number of 1-grams hit = 17  (2.73%)
6 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article382.text
Will force exclusive back-off from OOVs.
Perplexity = 92.14, Entropy = 6.53 bits
Computation based on 1302 words.
Number of 3-grams hit = 1039  (79.80%)
Number of 2-grams hit = 215  (16.51%)
Number of 1-grams hit = 48  (3.69%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article383.text
Will force exclusive back-off from OOVs.
Perplexity = 124.08, Entropy = 6.96 bits
Computation based on 1031 words.
Number of 3-grams hit = 779  (75.56%)
Number of 2-grams hit = 191  (18.53%)
Number of 1-grams hit = 61  (5.92%)
8 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article384.text
Will force exclusive back-off from OOVs.
Perplexity = 127.49, Entropy = 6.99 bits
Computation based on 1005 words.
Number of 3-grams hit = 754  (75.02%)
Number of 2-grams hit = 202  (20.10%)
Number of 1-grams hit = 49  (4.88%)
3 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article385.text
Will force exclusive back-off from OOVs.
Perplexity = 50.20, Entropy = 5.65 bits
Computation based on 2592 words.
Number of 3-grams hit = 2411  (93.02%)
Number of 2-grams hit = 148  (5.71%)
Number of 1-grams hit = 33  (1.27%)
14 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article386.text
Will force exclusive back-off from OOVs.
Perplexity = 117.33, Entropy = 6.87 bits
Computation based on 405 words.
Number of 3-grams hit = 291  (71.85%)
Number of 2-grams hit = 88  (21.73%)
Number of 1-grams hit = 26  (6.42%)
12 OOVs (2.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article387.text
Will force exclusive back-off from OOVs.
Perplexity = 116.48, Entropy = 6.86 bits
Computation based on 1197 words.
Number of 3-grams hit = 902  (75.36%)
Number of 2-grams hit = 234  (19.55%)
Number of 1-grams hit = 61  (5.10%)
18 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article388.text
Will force exclusive back-off from OOVs.
Perplexity = 107.42, Entropy = 6.75 bits
Computation based on 4328 words.
Number of 3-grams hit = 3376  (78.00%)
Number of 2-grams hit = 746  (17.24%)
Number of 1-grams hit = 206  (4.76%)
11 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article389.text
Will force exclusive back-off from OOVs.
Perplexity = 147.75, Entropy = 7.21 bits
Computation based on 1099 words.
Number of 3-grams hit = 762  (69.34%)
Number of 2-grams hit = 259  (23.57%)
Number of 1-grams hit = 78  (7.10%)
22 OOVs (1.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article390.text
Will force exclusive back-off from OOVs.
Perplexity = 131.09, Entropy = 7.03 bits
Computation based on 1177 words.
Number of 3-grams hit = 852  (72.39%)
Number of 2-grams hit = 262  (22.26%)
Number of 1-grams hit = 63  (5.35%)
23 OOVs (1.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article391.text
Will force exclusive back-off from OOVs.
Perplexity = 100.62, Entropy = 6.65 bits
Computation based on 3505 words.
Number of 3-grams hit = 2697  (76.95%)
Number of 2-grams hit = 652  (18.60%)
Number of 1-grams hit = 156  (4.45%)
36 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article392.text
Will force exclusive back-off from OOVs.
Perplexity = 87.47, Entropy = 6.45 bits
Computation based on 231 words.
Number of 3-grams hit = 190  (82.25%)
Number of 2-grams hit = 36  (15.58%)
Number of 1-grams hit = 5  (2.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article393.text
Will force exclusive back-off from OOVs.
Perplexity = 216.86, Entropy = 7.76 bits
Computation based on 481 words.
Number of 3-grams hit = 312  (64.86%)
Number of 2-grams hit = 124  (25.78%)
Number of 1-grams hit = 45  (9.36%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article394.text
Will force exclusive back-off from OOVs.
Perplexity = 180.27, Entropy = 7.49 bits
Computation based on 390 words.
Number of 3-grams hit = 270  (69.23%)
Number of 2-grams hit = 86  (22.05%)
Number of 1-grams hit = 34  (8.72%)
5 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article395.text
Will force exclusive back-off from OOVs.
Perplexity = 81.53, Entropy = 6.35 bits
Computation based on 304 words.
Number of 3-grams hit = 249  (81.91%)
Number of 2-grams hit = 47  (15.46%)
Number of 1-grams hit = 8  (2.63%)
3 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article396.text
Will force exclusive back-off from OOVs.
Perplexity = 60.94, Entropy = 5.93 bits
Computation based on 3049 words.
Number of 3-grams hit = 2615  (85.77%)
Number of 2-grams hit = 373  (12.23%)
Number of 1-grams hit = 61  (2.00%)
3 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article397.text
Will force exclusive back-off from OOVs.
Perplexity = 123.07, Entropy = 6.94 bits
Computation based on 3657 words.
Number of 3-grams hit = 2779  (75.99%)
Number of 2-grams hit = 683  (18.68%)
Number of 1-grams hit = 195  (5.33%)
16 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article398.text
Will force exclusive back-off from OOVs.
Perplexity = 124.86, Entropy = 6.96 bits
Computation based on 815 words.
Number of 3-grams hit = 619  (75.95%)
Number of 2-grams hit = 159  (19.51%)
Number of 1-grams hit = 37  (4.54%)
5 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article399.text
Will force exclusive back-off from OOVs.
Perplexity = 117.41, Entropy = 6.88 bits
Computation based on 3657 words.
Number of 3-grams hit = 2845  (77.80%)
Number of 2-grams hit = 651  (17.80%)
Number of 1-grams hit = 161  (4.40%)
15 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article400.text
Will force exclusive back-off from OOVs.
Perplexity = 122.48, Entropy = 6.94 bits
Computation based on 582 words.
Number of 3-grams hit = 439  (75.43%)
Number of 2-grams hit = 122  (20.96%)
Number of 1-grams hit = 21  (3.61%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article401.text
Will force exclusive back-off from OOVs.
Perplexity = 180.92, Entropy = 7.50 bits
Computation based on 652 words.
Number of 3-grams hit = 435  (66.72%)
Number of 2-grams hit = 169  (25.92%)
Number of 1-grams hit = 48  (7.36%)
11 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article402.text
Will force exclusive back-off from OOVs.
Perplexity = 102.42, Entropy = 6.68 bits
Computation based on 3662 words.
Number of 3-grams hit = 2843  (77.64%)
Number of 2-grams hit = 675  (18.43%)
Number of 1-grams hit = 144  (3.93%)
8 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article403.text
Will force exclusive back-off from OOVs.
Perplexity = 128.44, Entropy = 7.00 bits
Computation based on 621 words.
Number of 3-grams hit = 467  (75.20%)
Number of 2-grams hit = 129  (20.77%)
Number of 1-grams hit = 25  (4.03%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article404.text
Will force exclusive back-off from OOVs.
Perplexity = 159.80, Entropy = 7.32 bits
Computation based on 213 words.
Number of 3-grams hit = 151  (70.89%)
Number of 2-grams hit = 46  (21.60%)
Number of 1-grams hit = 16  (7.51%)
1 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article405.text
Will force exclusive back-off from OOVs.
Perplexity = 201.67, Entropy = 7.66 bits
Computation based on 159 words.
Number of 3-grams hit = 109  (68.55%)
Number of 2-grams hit = 38  (23.90%)
Number of 1-grams hit = 12  (7.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article406.text
Will force exclusive back-off from OOVs.
Perplexity = 148.77, Entropy = 7.22 bits
Computation based on 982 words.
Number of 3-grams hit = 716  (72.91%)
Number of 2-grams hit = 212  (21.59%)
Number of 1-grams hit = 54  (5.50%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article407.text
Will force exclusive back-off from OOVs.
Perplexity = 116.49, Entropy = 6.86 bits
Computation based on 696 words.
Number of 3-grams hit = 536  (77.01%)
Number of 2-grams hit = 126  (18.10%)
Number of 1-grams hit = 34  (4.89%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article408.text
Will force exclusive back-off from OOVs.
Perplexity = 111.83, Entropy = 6.81 bits
Computation based on 438 words.
Number of 3-grams hit = 328  (74.89%)
Number of 2-grams hit = 88  (20.09%)
Number of 1-grams hit = 22  (5.02%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article409.text
Will force exclusive back-off from OOVs.
Perplexity = 115.68, Entropy = 6.85 bits
Computation based on 541 words.
Number of 3-grams hit = 417  (77.08%)
Number of 2-grams hit = 99  (18.30%)
Number of 1-grams hit = 25  (4.62%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article410.text
Will force exclusive back-off from OOVs.
Perplexity = 73.33, Entropy = 6.20 bits
Computation based on 235 words.
Number of 3-grams hit = 197  (83.83%)
Number of 2-grams hit = 32  (13.62%)
Number of 1-grams hit = 6  (2.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article411.text
Will force exclusive back-off from OOVs.
Perplexity = 92.75, Entropy = 6.54 bits
Computation based on 436 words.
Number of 3-grams hit = 339  (77.75%)
Number of 2-grams hit = 76  (17.43%)
Number of 1-grams hit = 21  (4.82%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article412.text
Will force exclusive back-off from OOVs.
Perplexity = 69.69, Entropy = 6.12 bits
Computation based on 4559 words.
Number of 3-grams hit = 3847  (84.38%)
Number of 2-grams hit = 625  (13.71%)
Number of 1-grams hit = 87  (1.91%)
9 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article413.text
Will force exclusive back-off from OOVs.
Perplexity = 106.08, Entropy = 6.73 bits
Computation based on 3853 words.
Number of 3-grams hit = 2998  (77.81%)
Number of 2-grams hit = 681  (17.67%)
Number of 1-grams hit = 174  (4.52%)
14 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article414.text
Will force exclusive back-off from OOVs.
Perplexity = 99.38, Entropy = 6.63 bits
Computation based on 399 words.
Number of 3-grams hit = 317  (79.45%)
Number of 2-grams hit = 64  (16.04%)
Number of 1-grams hit = 18  (4.51%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article415.text
Will force exclusive back-off from OOVs.
Perplexity = 205.33, Entropy = 7.68 bits
Computation based on 475 words.
Number of 3-grams hit = 321  (67.58%)
Number of 2-grams hit = 119  (25.05%)
Number of 1-grams hit = 35  (7.37%)
5 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article416.text
Will force exclusive back-off from OOVs.
Perplexity = 164.01, Entropy = 7.36 bits
Computation based on 585 words.
Number of 3-grams hit = 406  (69.40%)
Number of 2-grams hit = 148  (25.30%)
Number of 1-grams hit = 31  (5.30%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article417.text
Will force exclusive back-off from OOVs.
Perplexity = 94.30, Entropy = 6.56 bits
Computation based on 850 words.
Number of 3-grams hit = 668  (78.59%)
Number of 2-grams hit = 163  (19.18%)
Number of 1-grams hit = 19  (2.24%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article418.text
Will force exclusive back-off from OOVs.
Perplexity = 147.57, Entropy = 7.21 bits
Computation based on 807 words.
Number of 3-grams hit = 572  (70.88%)
Number of 2-grams hit = 183  (22.68%)
Number of 1-grams hit = 52  (6.44%)
5 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article419.text
Will force exclusive back-off from OOVs.
Perplexity = 132.99, Entropy = 7.06 bits
Computation based on 549 words.
Number of 3-grams hit = 398  (72.50%)
Number of 2-grams hit = 122  (22.22%)
Number of 1-grams hit = 29  (5.28%)
5 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article420.text
Will force exclusive back-off from OOVs.
Perplexity = 121.72, Entropy = 6.93 bits
Computation based on 610 words.
Number of 3-grams hit = 469  (76.89%)
Number of 2-grams hit = 117  (19.18%)
Number of 1-grams hit = 24  (3.93%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article421.text
Will force exclusive back-off from OOVs.
Perplexity = 83.48, Entropy = 6.38 bits
Computation based on 819 words.
Number of 3-grams hit = 657  (80.22%)
Number of 2-grams hit = 136  (16.61%)
Number of 1-grams hit = 26  (3.17%)
2 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article422.text
Will force exclusive back-off from OOVs.
Perplexity = 118.58, Entropy = 6.89 bits
Computation based on 551 words.
Number of 3-grams hit = 407  (73.87%)
Number of 2-grams hit = 103  (18.69%)
Number of 1-grams hit = 41  (7.44%)
8 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article423.text
Will force exclusive back-off from OOVs.
Perplexity = 223.48, Entropy = 7.80 bits
Computation based on 599 words.
Number of 3-grams hit = 384  (64.11%)
Number of 2-grams hit = 158  (26.38%)
Number of 1-grams hit = 57  (9.52%)
17 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article424.text
Will force exclusive back-off from OOVs.
Perplexity = 98.02, Entropy = 6.62 bits
Computation based on 524 words.
Number of 3-grams hit = 404  (77.10%)
Number of 2-grams hit = 96  (18.32%)
Number of 1-grams hit = 24  (4.58%)
5 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article425.text
Will force exclusive back-off from OOVs.
Perplexity = 103.03, Entropy = 6.69 bits
Computation based on 355 words.
Number of 3-grams hit = 270  (76.06%)
Number of 2-grams hit = 70  (19.72%)
Number of 1-grams hit = 15  (4.23%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article426.text
Will force exclusive back-off from OOVs.
Perplexity = 86.92, Entropy = 6.44 bits
Computation based on 484 words.
Number of 3-grams hit = 385  (79.55%)
Number of 2-grams hit = 80  (16.53%)
Number of 1-grams hit = 19  (3.93%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article427.text
Will force exclusive back-off from OOVs.
Perplexity = 96.54, Entropy = 6.59 bits
Computation based on 787 words.
Number of 3-grams hit = 613  (77.89%)
Number of 2-grams hit = 143  (18.17%)
Number of 1-grams hit = 31  (3.94%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article428.text
Will force exclusive back-off from OOVs.
Perplexity = 171.83, Entropy = 7.42 bits
Computation based on 626 words.
Number of 3-grams hit = 445  (71.09%)
Number of 2-grams hit = 145  (23.16%)
Number of 1-grams hit = 36  (5.75%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article429.text
Will force exclusive back-off from OOVs.
Perplexity = 111.82, Entropy = 6.81 bits
Computation based on 769 words.
Number of 3-grams hit = 614  (79.84%)
Number of 2-grams hit = 131  (17.04%)
Number of 1-grams hit = 24  (3.12%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article430.text
Will force exclusive back-off from OOVs.
Perplexity = 114.46, Entropy = 6.84 bits
Computation based on 639 words.
Number of 3-grams hit = 485  (75.90%)
Number of 2-grams hit = 127  (19.87%)
Number of 1-grams hit = 27  (4.23%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article431.text
Will force exclusive back-off from OOVs.
Perplexity = 88.75, Entropy = 6.47 bits
Computation based on 569 words.
Number of 3-grams hit = 459  (80.67%)
Number of 2-grams hit = 89  (15.64%)
Number of 1-grams hit = 21  (3.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article432.text
Will force exclusive back-off from OOVs.
Perplexity = 113.49, Entropy = 6.83 bits
Computation based on 351 words.
Number of 3-grams hit = 262  (74.64%)
Number of 2-grams hit = 74  (21.08%)
Number of 1-grams hit = 15  (4.27%)
2 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article433.text
Will force exclusive back-off from OOVs.
Perplexity = 89.59, Entropy = 6.49 bits
Computation based on 967 words.
Number of 3-grams hit = 764  (79.01%)
Number of 2-grams hit = 163  (16.86%)
Number of 1-grams hit = 40  (4.14%)
5 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article434.text
Will force exclusive back-off from OOVs.
Perplexity = 85.17, Entropy = 6.41 bits
Computation based on 381 words.
Number of 3-grams hit = 301  (79.00%)
Number of 2-grams hit = 62  (16.27%)
Number of 1-grams hit = 18  (4.72%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article435.text
Will force exclusive back-off from OOVs.
Perplexity = 84.05, Entropy = 6.39 bits
Computation based on 711 words.
Number of 3-grams hit = 561  (78.90%)
Number of 2-grams hit = 121  (17.02%)
Number of 1-grams hit = 29  (4.08%)
8 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article436.text
Will force exclusive back-off from OOVs.
Perplexity = 113.86, Entropy = 6.83 bits
Computation based on 531 words.
Number of 3-grams hit = 408  (76.84%)
Number of 2-grams hit = 102  (19.21%)
Number of 1-grams hit = 21  (3.95%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article437.text
Will force exclusive back-off from OOVs.
Perplexity = 128.42, Entropy = 7.00 bits
Computation based on 1007 words.
Number of 3-grams hit = 771  (76.56%)
Number of 2-grams hit = 184  (18.27%)
Number of 1-grams hit = 52  (5.16%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article438.text
Will force exclusive back-off from OOVs.
Perplexity = 117.48, Entropy = 6.88 bits
Computation based on 290 words.
Number of 3-grams hit = 215  (74.14%)
Number of 2-grams hit = 57  (19.66%)
Number of 1-grams hit = 18  (6.21%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article439.text
Will force exclusive back-off from OOVs.
Perplexity = 194.63, Entropy = 7.60 bits
Computation based on 516 words.
Number of 3-grams hit = 353  (68.41%)
Number of 2-grams hit = 124  (24.03%)
Number of 1-grams hit = 39  (7.56%)
8 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article440.text
Will force exclusive back-off from OOVs.
Perplexity = 91.14, Entropy = 6.51 bits
Computation based on 688 words.
Number of 3-grams hit = 536  (77.91%)
Number of 2-grams hit = 129  (18.75%)
Number of 1-grams hit = 23  (3.34%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article441.text
Will force exclusive back-off from OOVs.
Perplexity = 175.77, Entropy = 7.46 bits
Computation based on 400 words.
Number of 3-grams hit = 261  (65.25%)
Number of 2-grams hit = 105  (26.25%)
Number of 1-grams hit = 34  (8.50%)
13 OOVs (3.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article442.text
Will force exclusive back-off from OOVs.
Perplexity = 68.46, Entropy = 6.10 bits
Computation based on 10008 words.
Number of 3-grams hit = 8507  (85.00%)
Number of 2-grams hit = 1277  (12.76%)
Number of 1-grams hit = 224  (2.24%)
23 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article443.text
Will force exclusive back-off from OOVs.
Perplexity = 117.72, Entropy = 6.88 bits
Computation based on 11815 words.
Number of 3-grams hit = 9117  (77.16%)
Number of 2-grams hit = 2160  (18.28%)
Number of 1-grams hit = 538  (4.55%)
39 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article444.text
Will force exclusive back-off from OOVs.
Perplexity = 133.35, Entropy = 7.06 bits
Computation based on 643 words.
Number of 3-grams hit = 464  (72.16%)
Number of 2-grams hit = 143  (22.24%)
Number of 1-grams hit = 36  (5.60%)
8 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article445.text
Will force exclusive back-off from OOVs.
Perplexity = 78.47, Entropy = 6.29 bits
Computation based on 501 words.
Number of 3-grams hit = 408  (81.44%)
Number of 2-grams hit = 74  (14.77%)
Number of 1-grams hit = 19  (3.79%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article446.text
Will force exclusive back-off from OOVs.
Perplexity = 117.30, Entropy = 6.87 bits
Computation based on 894 words.
Number of 3-grams hit = 684  (76.51%)
Number of 2-grams hit = 177  (19.80%)
Number of 1-grams hit = 33  (3.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article447.text
Will force exclusive back-off from OOVs.
Perplexity = 129.58, Entropy = 7.02 bits
Computation based on 722 words.
Number of 3-grams hit = 523  (72.44%)
Number of 2-grams hit = 158  (21.88%)
Number of 1-grams hit = 41  (5.68%)
9 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article448.text
Will force exclusive back-off from OOVs.
Perplexity = 75.58, Entropy = 6.24 bits
Computation based on 323 words.
Number of 3-grams hit = 270  (83.59%)
Number of 2-grams hit = 46  (14.24%)
Number of 1-grams hit = 7  (2.17%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article449.text
Will force exclusive back-off from OOVs.
Perplexity = 103.13, Entropy = 6.69 bits
Computation based on 483 words.
Number of 3-grams hit = 388  (80.33%)
Number of 2-grams hit = 68  (14.08%)
Number of 1-grams hit = 27  (5.59%)
5 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article450.text
Will force exclusive back-off from OOVs.
Perplexity = 103.92, Entropy = 6.70 bits
Computation based on 356 words.
Number of 3-grams hit = 280  (78.65%)
Number of 2-grams hit = 67  (18.82%)
Number of 1-grams hit = 9  (2.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article451.text
Will force exclusive back-off from OOVs.
Perplexity = 109.10, Entropy = 6.77 bits
Computation based on 571 words.
Number of 3-grams hit = 424  (74.26%)
Number of 2-grams hit = 117  (20.49%)
Number of 1-grams hit = 30  (5.25%)
3 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article452.text
Will force exclusive back-off from OOVs.
Perplexity = 100.41, Entropy = 6.65 bits
Computation based on 237 words.
Number of 3-grams hit = 188  (79.32%)
Number of 2-grams hit = 36  (15.19%)
Number of 1-grams hit = 13  (5.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article453.text
Will force exclusive back-off from OOVs.
Perplexity = 139.93, Entropy = 7.13 bits
Computation based on 625 words.
Number of 3-grams hit = 467  (74.72%)
Number of 2-grams hit = 130  (20.80%)
Number of 1-grams hit = 28  (4.48%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article454.text
Will force exclusive back-off from OOVs.
Perplexity = 110.39, Entropy = 6.79 bits
Computation based on 544 words.
Number of 3-grams hit = 417  (76.65%)
Number of 2-grams hit = 102  (18.75%)
Number of 1-grams hit = 25  (4.60%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article455.text
Will force exclusive back-off from OOVs.
Perplexity = 90.59, Entropy = 6.50 bits
Computation based on 379 words.
Number of 3-grams hit = 298  (78.63%)
Number of 2-grams hit = 68  (17.94%)
Number of 1-grams hit = 13  (3.43%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article456.text
Will force exclusive back-off from OOVs.
Perplexity = 119.31, Entropy = 6.90 bits
Computation based on 656 words.
Number of 3-grams hit = 504  (76.83%)
Number of 2-grams hit = 121  (18.45%)
Number of 1-grams hit = 31  (4.73%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article457.text
Will force exclusive back-off from OOVs.
Perplexity = 125.36, Entropy = 6.97 bits
Computation based on 337 words.
Number of 3-grams hit = 261  (77.45%)
Number of 2-grams hit = 60  (17.80%)
Number of 1-grams hit = 16  (4.75%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article458.text
Will force exclusive back-off from OOVs.
Perplexity = 106.33, Entropy = 6.73 bits
Computation based on 276 words.
Number of 3-grams hit = 219  (79.35%)
Number of 2-grams hit = 48  (17.39%)
Number of 1-grams hit = 9  (3.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article459.text
Will force exclusive back-off from OOVs.
Perplexity = 108.15, Entropy = 6.76 bits
Computation based on 700 words.
Number of 3-grams hit = 545  (77.86%)
Number of 2-grams hit = 122  (17.43%)
Number of 1-grams hit = 33  (4.71%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article460.text
Will force exclusive back-off from OOVs.
Perplexity = 137.08, Entropy = 7.10 bits
Computation based on 326 words.
Number of 3-grams hit = 243  (74.54%)
Number of 2-grams hit = 67  (20.55%)
Number of 1-grams hit = 16  (4.91%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article461.text
Will force exclusive back-off from OOVs.
Perplexity = 153.44, Entropy = 7.26 bits
Computation based on 419 words.
Number of 3-grams hit = 288  (68.74%)
Number of 2-grams hit = 106  (25.30%)
Number of 1-grams hit = 25  (5.97%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article462.text
Will force exclusive back-off from OOVs.
Perplexity = 135.95, Entropy = 7.09 bits
Computation based on 1406 words.
Number of 3-grams hit = 1053  (74.89%)
Number of 2-grams hit = 280  (19.91%)
Number of 1-grams hit = 73  (5.19%)
4 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article463.text
Will force exclusive back-off from OOVs.
Perplexity = 131.11, Entropy = 7.03 bits
Computation based on 543 words.
Number of 3-grams hit = 395  (72.74%)
Number of 2-grams hit = 116  (21.36%)
Number of 1-grams hit = 32  (5.89%)
12 OOVs (2.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article464.text
Will force exclusive back-off from OOVs.
Perplexity = 177.05, Entropy = 7.47 bits
Computation based on 421 words.
Number of 3-grams hit = 279  (66.27%)
Number of 2-grams hit = 104  (24.70%)
Number of 1-grams hit = 38  (9.03%)
19 OOVs (4.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article465.text
Will force exclusive back-off from OOVs.
Perplexity = 72.88, Entropy = 6.19 bits
Computation based on 1805 words.
Number of 3-grams hit = 1503  (83.27%)
Number of 2-grams hit = 261  (14.46%)
Number of 1-grams hit = 41  (2.27%)
2 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article466.text
Will force exclusive back-off from OOVs.
Perplexity = 98.86, Entropy = 6.63 bits
Computation based on 204 words.
Number of 3-grams hit = 163  (79.90%)
Number of 2-grams hit = 32  (15.69%)
Number of 1-grams hit = 9  (4.41%)
1 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article467.text
Will force exclusive back-off from OOVs.
Perplexity = 91.56, Entropy = 6.52 bits
Computation based on 1028 words.
Number of 3-grams hit = 822  (79.96%)
Number of 2-grams hit = 168  (16.34%)
Number of 1-grams hit = 38  (3.70%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article468.text
Will force exclusive back-off from OOVs.
Perplexity = 119.90, Entropy = 6.91 bits
Computation based on 776 words.
Number of 3-grams hit = 590  (76.03%)
Number of 2-grams hit = 146  (18.81%)
Number of 1-grams hit = 40  (5.15%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article469.text
Will force exclusive back-off from OOVs.
Perplexity = 121.89, Entropy = 6.93 bits
Computation based on 355 words.
Number of 3-grams hit = 267  (75.21%)
Number of 2-grams hit = 65  (18.31%)
Number of 1-grams hit = 23  (6.48%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article470.text
Will force exclusive back-off from OOVs.
Perplexity = 262.96, Entropy = 8.04 bits
Computation based on 235 words.
Number of 3-grams hit = 147  (62.55%)
Number of 2-grams hit = 63  (26.81%)
Number of 1-grams hit = 25  (10.64%)
5 OOVs (2.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article471.text
Will force exclusive back-off from OOVs.
Perplexity = 109.52, Entropy = 6.78 bits
Computation based on 1994 words.
Number of 3-grams hit = 1576  (79.04%)
Number of 2-grams hit = 337  (16.90%)
Number of 1-grams hit = 81  (4.06%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article472.text
Will force exclusive back-off from OOVs.
Perplexity = 180.45, Entropy = 7.50 bits
Computation based on 930 words.
Number of 3-grams hit = 645  (69.35%)
Number of 2-grams hit = 220  (23.66%)
Number of 1-grams hit = 65  (6.99%)
6 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article473.text
Will force exclusive back-off from OOVs.
Perplexity = 94.35, Entropy = 6.56 bits
Computation based on 219 words.
Number of 3-grams hit = 173  (79.00%)
Number of 2-grams hit = 34  (15.53%)
Number of 1-grams hit = 12  (5.48%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article474.text
Will force exclusive back-off from OOVs.
Perplexity = 141.00, Entropy = 7.14 bits
Computation based on 775 words.
Number of 3-grams hit = 565  (72.90%)
Number of 2-grams hit = 167  (21.55%)
Number of 1-grams hit = 43  (5.55%)
9 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article475.text
Will force exclusive back-off from OOVs.
Perplexity = 132.11, Entropy = 7.05 bits
Computation based on 546 words.
Number of 3-grams hit = 394  (72.16%)
Number of 2-grams hit = 130  (23.81%)
Number of 1-grams hit = 22  (4.03%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article476.text
Will force exclusive back-off from OOVs.
Perplexity = 157.05, Entropy = 7.30 bits
Computation based on 582 words.
Number of 3-grams hit = 432  (74.23%)
Number of 2-grams hit = 110  (18.90%)
Number of 1-grams hit = 40  (6.87%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article477.text
Will force exclusive back-off from OOVs.
Perplexity = 109.32, Entropy = 6.77 bits
Computation based on 444 words.
Number of 3-grams hit = 342  (77.03%)
Number of 2-grams hit = 78  (17.57%)
Number of 1-grams hit = 24  (5.41%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article478.text
Will force exclusive back-off from OOVs.
Perplexity = 148.83, Entropy = 7.22 bits
Computation based on 507 words.
Number of 3-grams hit = 382  (75.35%)
Number of 2-grams hit = 93  (18.34%)
Number of 1-grams hit = 32  (6.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article479.text
Will force exclusive back-off from OOVs.
Perplexity = 112.31, Entropy = 6.81 bits
Computation based on 421 words.
Number of 3-grams hit = 319  (75.77%)
Number of 2-grams hit = 85  (20.19%)
Number of 1-grams hit = 17  (4.04%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article480.text
Will force exclusive back-off from OOVs.
Perplexity = 90.31, Entropy = 6.50 bits
Computation based on 684 words.
Number of 3-grams hit = 547  (79.97%)
Number of 2-grams hit = 113  (16.52%)
Number of 1-grams hit = 24  (3.51%)
5 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article481.text
Will force exclusive back-off from OOVs.
Perplexity = 116.22, Entropy = 6.86 bits
Computation based on 349 words.
Number of 3-grams hit = 267  (76.50%)
Number of 2-grams hit = 63  (18.05%)
Number of 1-grams hit = 19  (5.44%)
2 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article482.text
Will force exclusive back-off from OOVs.
Perplexity = 80.17, Entropy = 6.33 bits
Computation based on 494 words.
Number of 3-grams hit = 401  (81.17%)
Number of 2-grams hit = 76  (15.38%)
Number of 1-grams hit = 17  (3.44%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article483.text
Will force exclusive back-off from OOVs.
Perplexity = 83.76, Entropy = 6.39 bits
Computation based on 1664 words.
Number of 3-grams hit = 1320  (79.33%)
Number of 2-grams hit = 280  (16.83%)
Number of 1-grams hit = 64  (3.85%)
12 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article484.text
Will force exclusive back-off from OOVs.
Perplexity = 122.30, Entropy = 6.93 bits
Computation based on 1427 words.
Number of 3-grams hit = 1084  (75.96%)
Number of 2-grams hit = 268  (18.78%)
Number of 1-grams hit = 75  (5.26%)
5 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article485.text
Will force exclusive back-off from OOVs.
Perplexity = 287.42, Entropy = 8.17 bits
Computation based on 372 words.
Number of 3-grams hit = 230  (61.83%)
Number of 2-grams hit = 103  (27.69%)
Number of 1-grams hit = 39  (10.48%)
5 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article486.text
Will force exclusive back-off from OOVs.
Perplexity = 150.46, Entropy = 7.23 bits
Computation based on 331 words.
Number of 3-grams hit = 244  (73.72%)
Number of 2-grams hit = 70  (21.15%)
Number of 1-grams hit = 17  (5.14%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article487.text
Will force exclusive back-off from OOVs.
Perplexity = 106.86, Entropy = 6.74 bits
Computation based on 429 words.
Number of 3-grams hit = 334  (77.86%)
Number of 2-grams hit = 79  (18.41%)
Number of 1-grams hit = 16  (3.73%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article488.text
Will force exclusive back-off from OOVs.
Perplexity = 92.58, Entropy = 6.53 bits
Computation based on 710 words.
Number of 3-grams hit = 565  (79.58%)
Number of 2-grams hit = 126  (17.75%)
Number of 1-grams hit = 19  (2.68%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article489.text
Will force exclusive back-off from OOVs.
Perplexity = 77.54, Entropy = 6.28 bits
Computation based on 537 words.
Number of 3-grams hit = 434  (80.82%)
Number of 2-grams hit = 85  (15.83%)
Number of 1-grams hit = 18  (3.35%)
9 OOVs (1.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article490.text
Will force exclusive back-off from OOVs.
Perplexity = 195.76, Entropy = 7.61 bits
Computation based on 479 words.
Number of 3-grams hit = 317  (66.18%)
Number of 2-grams hit = 118  (24.63%)
Number of 1-grams hit = 44  (9.19%)
14 OOVs (2.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article491.text
Will force exclusive back-off from OOVs.
Perplexity = 131.16, Entropy = 7.04 bits
Computation based on 1388 words.
Number of 3-grams hit = 1040  (74.93%)
Number of 2-grams hit = 266  (19.16%)
Number of 1-grams hit = 82  (5.91%)
7 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article492.text
Will force exclusive back-off from OOVs.
Perplexity = 115.25, Entropy = 6.85 bits
Computation based on 433 words.
Number of 3-grams hit = 326  (75.29%)
Number of 2-grams hit = 95  (21.94%)
Number of 1-grams hit = 12  (2.77%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article493.text
Will force exclusive back-off from OOVs.
Perplexity = 168.01, Entropy = 7.39 bits
Computation based on 759 words.
Number of 3-grams hit = 543  (71.54%)
Number of 2-grams hit = 174  (22.92%)
Number of 1-grams hit = 42  (5.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article494.text
Will force exclusive back-off from OOVs.
Perplexity = 123.25, Entropy = 6.95 bits
Computation based on 485 words.
Number of 3-grams hit = 366  (75.46%)
Number of 2-grams hit = 96  (19.79%)
Number of 1-grams hit = 23  (4.74%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article495.text
Will force exclusive back-off from OOVs.
Perplexity = 114.80, Entropy = 6.84 bits
Computation based on 426 words.
Number of 3-grams hit = 307  (72.07%)
Number of 2-grams hit = 96  (22.54%)
Number of 1-grams hit = 23  (5.40%)
4 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article496.text
Will force exclusive back-off from OOVs.
Perplexity = 245.39, Entropy = 7.94 bits
Computation based on 546 words.
Number of 3-grams hit = 338  (61.90%)
Number of 2-grams hit = 164  (30.04%)
Number of 1-grams hit = 44  (8.06%)
12 OOVs (2.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article497.text
Will force exclusive back-off from OOVs.
Perplexity = 111.35, Entropy = 6.80 bits
Computation based on 647 words.
Number of 3-grams hit = 506  (78.21%)
Number of 2-grams hit = 100  (15.46%)
Number of 1-grams hit = 41  (6.34%)
5 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article498.text
Will force exclusive back-off from OOVs.
Perplexity = 284.20, Entropy = 8.15 bits
Computation based on 1659 words.
Number of 3-grams hit = 1035  (62.39%)
Number of 2-grams hit = 440  (26.52%)
Number of 1-grams hit = 184  (11.09%)
40 OOVs (2.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article499.text
Will force exclusive back-off from OOVs.
Perplexity = 100.33, Entropy = 6.65 bits
Computation based on 495 words.
Number of 3-grams hit = 399  (80.61%)
Number of 2-grams hit = 80  (16.16%)
Number of 1-grams hit = 16  (3.23%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article500.text
Will force exclusive back-off from OOVs.
Perplexity = 122.07, Entropy = 6.93 bits
Computation based on 457 words.
Number of 3-grams hit = 349  (76.37%)
Number of 2-grams hit = 88  (19.26%)
Number of 1-grams hit = 20  (4.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article501.text
Will force exclusive back-off from OOVs.
Perplexity = 105.37, Entropy = 6.72 bits
Computation based on 403 words.
Number of 3-grams hit = 305  (75.68%)
Number of 2-grams hit = 83  (20.60%)
Number of 1-grams hit = 15  (3.72%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article502.text
Will force exclusive back-off from OOVs.
Perplexity = 131.92, Entropy = 7.04 bits
Computation based on 505 words.
Number of 3-grams hit = 385  (76.24%)
Number of 2-grams hit = 96  (19.01%)
Number of 1-grams hit = 24  (4.75%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article503.text
Will force exclusive back-off from OOVs.
Perplexity = 110.89, Entropy = 6.79 bits
Computation based on 1013 words.
Number of 3-grams hit = 790  (77.99%)
Number of 2-grams hit = 175  (17.28%)
Number of 1-grams hit = 48  (4.74%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article504.text
Will force exclusive back-off from OOVs.
Perplexity = 110.76, Entropy = 6.79 bits
Computation based on 613 words.
Number of 3-grams hit = 471  (76.84%)
Number of 2-grams hit = 121  (19.74%)
Number of 1-grams hit = 21  (3.43%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article505.text
Will force exclusive back-off from OOVs.
Perplexity = 159.12, Entropy = 7.31 bits
Computation based on 701 words.
Number of 3-grams hit = 496  (70.76%)
Number of 2-grams hit = 164  (23.40%)
Number of 1-grams hit = 41  (5.85%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article506.text
Will force exclusive back-off from OOVs.
Perplexity = 195.56, Entropy = 7.61 bits
Computation based on 522 words.
Number of 3-grams hit = 346  (66.28%)
Number of 2-grams hit = 116  (22.22%)
Number of 1-grams hit = 60  (11.49%)
17 OOVs (3.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article507.text
Will force exclusive back-off from OOVs.
Perplexity = 95.00, Entropy = 6.57 bits
Computation based on 1584 words.
Number of 3-grams hit = 1228  (77.53%)
Number of 2-grams hit = 298  (18.81%)
Number of 1-grams hit = 58  (3.66%)
9 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article508.text
Will force exclusive back-off from OOVs.
Perplexity = 57.84, Entropy = 5.85 bits
Computation based on 655 words.
Number of 3-grams hit = 560  (85.50%)
Number of 2-grams hit = 84  (12.82%)
Number of 1-grams hit = 11  (1.68%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article509.text
Will force exclusive back-off from OOVs.
Perplexity = 134.75, Entropy = 7.07 bits
Computation based on 353 words.
Number of 3-grams hit = 263  (74.50%)
Number of 2-grams hit = 68  (19.26%)
Number of 1-grams hit = 22  (6.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article510.text
Will force exclusive back-off from OOVs.
Perplexity = 129.03, Entropy = 7.01 bits
Computation based on 968 words.
Number of 3-grams hit = 712  (73.55%)
Number of 2-grams hit = 201  (20.76%)
Number of 1-grams hit = 55  (5.68%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article511.text
Will force exclusive back-off from OOVs.
Perplexity = 183.84, Entropy = 7.52 bits
Computation based on 623 words.
Number of 3-grams hit = 423  (67.90%)
Number of 2-grams hit = 157  (25.20%)
Number of 1-grams hit = 43  (6.90%)
8 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article512.text
Will force exclusive back-off from OOVs.
Perplexity = 111.09, Entropy = 6.80 bits
Computation based on 1900 words.
Number of 3-grams hit = 1474  (77.58%)
Number of 2-grams hit = 344  (18.11%)
Number of 1-grams hit = 82  (4.32%)
8 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article513.text
Will force exclusive back-off from OOVs.
Perplexity = 240.83, Entropy = 7.91 bits
Computation based on 414 words.
Number of 3-grams hit = 264  (63.77%)
Number of 2-grams hit = 115  (27.78%)
Number of 1-grams hit = 35  (8.45%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article514.text
Will force exclusive back-off from OOVs.
Perplexity = 82.68, Entropy = 6.37 bits
Computation based on 397 words.
Number of 3-grams hit = 320  (80.60%)
Number of 2-grams hit = 63  (15.87%)
Number of 1-grams hit = 14  (3.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article515.text
Will force exclusive back-off from OOVs.
Perplexity = 150.99, Entropy = 7.24 bits
Computation based on 972 words.
Number of 3-grams hit = 695  (71.50%)
Number of 2-grams hit = 236  (24.28%)
Number of 1-grams hit = 41  (4.22%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article516.text
Will force exclusive back-off from OOVs.
Perplexity = 105.18, Entropy = 6.72 bits
Computation based on 412 words.
Number of 3-grams hit = 321  (77.91%)
Number of 2-grams hit = 73  (17.72%)
Number of 1-grams hit = 18  (4.37%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article517.text
Will force exclusive back-off from OOVs.
Perplexity = 156.50, Entropy = 7.29 bits
Computation based on 1060 words.
Number of 3-grams hit = 772  (72.83%)
Number of 2-grams hit = 228  (21.51%)
Number of 1-grams hit = 60  (5.66%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article518.text
Will force exclusive back-off from OOVs.
Perplexity = 110.42, Entropy = 6.79 bits
Computation based on 584 words.
Number of 3-grams hit = 450  (77.05%)
Number of 2-grams hit = 107  (18.32%)
Number of 1-grams hit = 27  (4.62%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article519.text
Will force exclusive back-off from OOVs.
Perplexity = 130.69, Entropy = 7.03 bits
Computation based on 525 words.
Number of 3-grams hit = 384  (73.14%)
Number of 2-grams hit = 113  (21.52%)
Number of 1-grams hit = 28  (5.33%)
6 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article520.text
Will force exclusive back-off from OOVs.
Perplexity = 91.47, Entropy = 6.52 bits
Computation based on 610 words.
Number of 3-grams hit = 489  (80.16%)
Number of 2-grams hit = 100  (16.39%)
Number of 1-grams hit = 21  (3.44%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article521.text
Will force exclusive back-off from OOVs.
Perplexity = 177.47, Entropy = 7.47 bits
Computation based on 511 words.
Number of 3-grams hit = 353  (69.08%)
Number of 2-grams hit = 121  (23.68%)
Number of 1-grams hit = 37  (7.24%)
11 OOVs (2.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article522.text
Will force exclusive back-off from OOVs.
Perplexity = 103.00, Entropy = 6.69 bits
Computation based on 2319 words.
Number of 3-grams hit = 1824  (78.65%)
Number of 2-grams hit = 417  (17.98%)
Number of 1-grams hit = 78  (3.36%)
11 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article523.text
Will force exclusive back-off from OOVs.
Perplexity = 113.64, Entropy = 6.83 bits
Computation based on 327 words.
Number of 3-grams hit = 253  (77.37%)
Number of 2-grams hit = 58  (17.74%)
Number of 1-grams hit = 16  (4.89%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article524.text
Will force exclusive back-off from OOVs.
Perplexity = 136.75, Entropy = 7.10 bits
Computation based on 1383 words.
Number of 3-grams hit = 1011  (73.10%)
Number of 2-grams hit = 301  (21.76%)
Number of 1-grams hit = 71  (5.13%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article525.text
Will force exclusive back-off from OOVs.
Perplexity = 121.32, Entropy = 6.92 bits
Computation based on 1967 words.
Number of 3-grams hit = 1491  (75.80%)
Number of 2-grams hit = 378  (19.22%)
Number of 1-grams hit = 98  (4.98%)
10 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article526.text
Will force exclusive back-off from OOVs.
Perplexity = 109.32, Entropy = 6.77 bits
Computation based on 1900 words.
Number of 3-grams hit = 1491  (78.47%)
Number of 2-grams hit = 318  (16.74%)
Number of 1-grams hit = 91  (4.79%)
9 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article527.text
Will force exclusive back-off from OOVs.
Perplexity = 118.24, Entropy = 6.89 bits
Computation based on 733 words.
Number of 3-grams hit = 559  (76.26%)
Number of 2-grams hit = 142  (19.37%)
Number of 1-grams hit = 32  (4.37%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article528.text
Will force exclusive back-off from OOVs.
Perplexity = 117.16, Entropy = 6.87 bits
Computation based on 723 words.
Number of 3-grams hit = 561  (77.59%)
Number of 2-grams hit = 126  (17.43%)
Number of 1-grams hit = 36  (4.98%)
4 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article529.text
Will force exclusive back-off from OOVs.
Perplexity = 167.31, Entropy = 7.39 bits
Computation based on 886 words.
Number of 3-grams hit = 640  (72.23%)
Number of 2-grams hit = 190  (21.44%)
Number of 1-grams hit = 56  (6.32%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article530.text
Will force exclusive back-off from OOVs.
Perplexity = 118.73, Entropy = 6.89 bits
Computation based on 420 words.
Number of 3-grams hit = 313  (74.52%)
Number of 2-grams hit = 87  (20.71%)
Number of 1-grams hit = 20  (4.76%)
6 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article531.text
Will force exclusive back-off from OOVs.
Perplexity = 135.43, Entropy = 7.08 bits
Computation based on 944 words.
Number of 3-grams hit = 690  (73.09%)
Number of 2-grams hit = 204  (21.61%)
Number of 1-grams hit = 50  (5.30%)
4 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article532.text
Will force exclusive back-off from OOVs.
Perplexity = 104.42, Entropy = 6.71 bits
Computation based on 933 words.
Number of 3-grams hit = 731  (78.35%)
Number of 2-grams hit = 164  (17.58%)
Number of 1-grams hit = 38  (4.07%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article533.text
Will force exclusive back-off from OOVs.
Perplexity = 86.53, Entropy = 6.44 bits
Computation based on 1137 words.
Number of 3-grams hit = 917  (80.65%)
Number of 2-grams hit = 191  (16.80%)
Number of 1-grams hit = 29  (2.55%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article534.text
Will force exclusive back-off from OOVs.
Perplexity = 90.77, Entropy = 6.50 bits
Computation based on 494 words.
Number of 3-grams hit = 374  (75.71%)
Number of 2-grams hit = 94  (19.03%)
Number of 1-grams hit = 26  (5.26%)
8 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article535.text
Will force exclusive back-off from OOVs.
Perplexity = 249.04, Entropy = 7.96 bits
Computation based on 358 words.
Number of 3-grams hit = 226  (63.13%)
Number of 2-grams hit = 106  (29.61%)
Number of 1-grams hit = 26  (7.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article536.text
Will force exclusive back-off from OOVs.
Perplexity = 98.87, Entropy = 6.63 bits
Computation based on 1511 words.
Number of 3-grams hit = 1187  (78.56%)
Number of 2-grams hit = 274  (18.13%)
Number of 1-grams hit = 50  (3.31%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article537.text
Will force exclusive back-off from OOVs.
Perplexity = 86.90, Entropy = 6.44 bits
Computation based on 999 words.
Number of 3-grams hit = 806  (80.68%)
Number of 2-grams hit = 150  (15.02%)
Number of 1-grams hit = 43  (4.30%)
4 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article538.text
Will force exclusive back-off from OOVs.
Perplexity = 115.47, Entropy = 6.85 bits
Computation based on 3432 words.
Number of 3-grams hit = 2635  (76.78%)
Number of 2-grams hit = 633  (18.44%)
Number of 1-grams hit = 164  (4.78%)
9 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article539.text
Will force exclusive back-off from OOVs.
Perplexity = 181.74, Entropy = 7.51 bits
Computation based on 534 words.
Number of 3-grams hit = 384  (71.91%)
Number of 2-grams hit = 104  (19.48%)
Number of 1-grams hit = 46  (8.61%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article540.text
Will force exclusive back-off from OOVs.
Perplexity = 98.89, Entropy = 6.63 bits
Computation based on 2193 words.
Number of 3-grams hit = 1729  (78.84%)
Number of 2-grams hit = 391  (17.83%)
Number of 1-grams hit = 73  (3.33%)
5 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article541.text
Will force exclusive back-off from OOVs.
Perplexity = 134.41, Entropy = 7.07 bits
Computation based on 497 words.
Number of 3-grams hit = 369  (74.25%)
Number of 2-grams hit = 103  (20.72%)
Number of 1-grams hit = 25  (5.03%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article542.text
Will force exclusive back-off from OOVs.
Perplexity = 72.92, Entropy = 6.19 bits
Computation based on 1590 words.
Number of 3-grams hit = 1329  (83.58%)
Number of 2-grams hit = 222  (13.96%)
Number of 1-grams hit = 39  (2.45%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article543.text
Will force exclusive back-off from OOVs.
Perplexity = 163.71, Entropy = 7.36 bits
Computation based on 534 words.
Number of 3-grams hit = 384  (71.91%)
Number of 2-grams hit = 111  (20.79%)
Number of 1-grams hit = 39  (7.30%)
7 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article544.text
Will force exclusive back-off from OOVs.
Perplexity = 142.50, Entropy = 7.15 bits
Computation based on 1280 words.
Number of 3-grams hit = 943  (73.67%)
Number of 2-grams hit = 269  (21.02%)
Number of 1-grams hit = 68  (5.31%)
7 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article545.text
Will force exclusive back-off from OOVs.
Perplexity = 94.33, Entropy = 6.56 bits
Computation based on 749 words.
Number of 3-grams hit = 594  (79.31%)
Number of 2-grams hit = 126  (16.82%)
Number of 1-grams hit = 29  (3.87%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article546.text
Will force exclusive back-off from OOVs.
Perplexity = 72.98, Entropy = 6.19 bits
Computation based on 685 words.
Number of 3-grams hit = 587  (85.69%)
Number of 2-grams hit = 74  (10.80%)
Number of 1-grams hit = 24  (3.50%)
7 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article547.text
Will force exclusive back-off from OOVs.
Perplexity = 239.68, Entropy = 7.90 bits
Computation based on 880 words.
Number of 3-grams hit = 584  (66.36%)
Number of 2-grams hit = 231  (26.25%)
Number of 1-grams hit = 65  (7.39%)
15 OOVs (1.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article548.text
Will force exclusive back-off from OOVs.
Perplexity = 107.61, Entropy = 6.75 bits
Computation based on 623 words.
Number of 3-grams hit = 488  (78.33%)
Number of 2-grams hit = 115  (18.46%)
Number of 1-grams hit = 20  (3.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article549.text
Will force exclusive back-off from OOVs.
Perplexity = 170.12, Entropy = 7.41 bits
Computation based on 358 words.
Number of 3-grams hit = 253  (70.67%)
Number of 2-grams hit = 87  (24.30%)
Number of 1-grams hit = 18  (5.03%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article550.text
Will force exclusive back-off from OOVs.
Perplexity = 70.92, Entropy = 6.15 bits
Computation based on 185 words.
Number of 3-grams hit = 158  (85.41%)
Number of 2-grams hit = 20  (10.81%)
Number of 1-grams hit = 7  (3.78%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article551.text
Will force exclusive back-off from OOVs.
Perplexity = 86.75, Entropy = 6.44 bits
Computation based on 918 words.
Number of 3-grams hit = 738  (80.39%)
Number of 2-grams hit = 158  (17.21%)
Number of 1-grams hit = 22  (2.40%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article552.text
Will force exclusive back-off from OOVs.
Perplexity = 146.83, Entropy = 7.20 bits
Computation based on 1051 words.
Number of 3-grams hit = 726  (69.08%)
Number of 2-grams hit = 253  (24.07%)
Number of 1-grams hit = 72  (6.85%)
26 OOVs (2.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article553.text
Will force exclusive back-off from OOVs.
Perplexity = 80.27, Entropy = 6.33 bits
Computation based on 1239 words.
Number of 3-grams hit = 1046  (84.42%)
Number of 2-grams hit = 164  (13.24%)
Number of 1-grams hit = 29  (2.34%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article554.text
Will force exclusive back-off from OOVs.
Perplexity = 113.08, Entropy = 6.82 bits
Computation based on 4043 words.
Number of 3-grams hit = 3084  (76.28%)
Number of 2-grams hit = 771  (19.07%)
Number of 1-grams hit = 188  (4.65%)
32 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article555.text
Will force exclusive back-off from OOVs.
Perplexity = 154.13, Entropy = 7.27 bits
Computation based on 505 words.
Number of 3-grams hit = 374  (74.06%)
Number of 2-grams hit = 104  (20.59%)
Number of 1-grams hit = 27  (5.35%)
3 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article556.text
Will force exclusive back-off from OOVs.
Perplexity = 247.30, Entropy = 7.95 bits
Computation based on 555 words.
Number of 3-grams hit = 347  (62.52%)
Number of 2-grams hit = 153  (27.57%)
Number of 1-grams hit = 55  (9.91%)
12 OOVs (2.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article557.text
Will force exclusive back-off from OOVs.
Perplexity = 74.80, Entropy = 6.22 bits
Computation based on 382 words.
Number of 3-grams hit = 308  (80.63%)
Number of 2-grams hit = 60  (15.71%)
Number of 1-grams hit = 14  (3.66%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article558.text
Will force exclusive back-off from OOVs.
Perplexity = 117.17, Entropy = 6.87 bits
Computation based on 1710 words.
Number of 3-grams hit = 1281  (74.91%)
Number of 2-grams hit = 344  (20.12%)
Number of 1-grams hit = 85  (4.97%)
19 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article559.text
Will force exclusive back-off from OOVs.
Perplexity = 168.91, Entropy = 7.40 bits
Computation based on 629 words.
Number of 3-grams hit = 438  (69.63%)
Number of 2-grams hit = 150  (23.85%)
Number of 1-grams hit = 41  (6.52%)
9 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article560.text
Will force exclusive back-off from OOVs.
Perplexity = 74.70, Entropy = 6.22 bits
Computation based on 547 words.
Number of 3-grams hit = 449  (82.08%)
Number of 2-grams hit = 78  (14.26%)
Number of 1-grams hit = 20  (3.66%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article561.text
Will force exclusive back-off from OOVs.
Perplexity = 186.09, Entropy = 7.54 bits
Computation based on 330 words.
Number of 3-grams hit = 217  (65.76%)
Number of 2-grams hit = 87  (26.36%)
Number of 1-grams hit = 26  (7.88%)
3 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article562.text
Will force exclusive back-off from OOVs.
Perplexity = 94.54, Entropy = 6.56 bits
Computation based on 312 words.
Number of 3-grams hit = 245  (78.53%)
Number of 2-grams hit = 55  (17.63%)
Number of 1-grams hit = 12  (3.85%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article563.text
Will force exclusive back-off from OOVs.
Perplexity = 69.04, Entropy = 6.11 bits
Computation based on 492 words.
Number of 3-grams hit = 413  (83.94%)
Number of 2-grams hit = 68  (13.82%)
Number of 1-grams hit = 11  (2.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article564.text
Will force exclusive back-off from OOVs.
Perplexity = 115.20, Entropy = 6.85 bits
Computation based on 1335 words.
Number of 3-grams hit = 1023  (76.63%)
Number of 2-grams hit = 248  (18.58%)
Number of 1-grams hit = 64  (4.79%)
4 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article565.text
Will force exclusive back-off from OOVs.
Perplexity = 142.05, Entropy = 7.15 bits
Computation based on 1286 words.
Number of 3-grams hit = 917  (71.31%)
Number of 2-grams hit = 287  (22.32%)
Number of 1-grams hit = 82  (6.38%)
15 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article566.text
Will force exclusive back-off from OOVs.
Perplexity = 91.04, Entropy = 6.51 bits
Computation based on 4132 words.
Number of 3-grams hit = 3211  (77.71%)
Number of 2-grams hit = 760  (18.39%)
Number of 1-grams hit = 161  (3.90%)
21 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article567.text
Will force exclusive back-off from OOVs.
Perplexity = 155.23, Entropy = 7.28 bits
Computation based on 547 words.
Number of 3-grams hit = 386  (70.57%)
Number of 2-grams hit = 118  (21.57%)
Number of 1-grams hit = 43  (7.86%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article568.text
Will force exclusive back-off from OOVs.
Perplexity = 88.69, Entropy = 6.47 bits
Computation based on 329 words.
Number of 3-grams hit = 263  (79.94%)
Number of 2-grams hit = 56  (17.02%)
Number of 1-grams hit = 10  (3.04%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article569.text
Will force exclusive back-off from OOVs.
Perplexity = 107.84, Entropy = 6.75 bits
Computation based on 4344 words.
Number of 3-grams hit = 3353  (77.19%)
Number of 2-grams hit = 812  (18.69%)
Number of 1-grams hit = 179  (4.12%)
12 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article570.text
Will force exclusive back-off from OOVs.
Perplexity = 111.58, Entropy = 6.80 bits
Computation based on 815 words.
Number of 3-grams hit = 636  (78.04%)
Number of 2-grams hit = 144  (17.67%)
Number of 1-grams hit = 35  (4.29%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article571.text
Will force exclusive back-off from OOVs.
Perplexity = 165.38, Entropy = 7.37 bits
Computation based on 534 words.
Number of 3-grams hit = 371  (69.48%)
Number of 2-grams hit = 123  (23.03%)
Number of 1-grams hit = 40  (7.49%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article572.text
Will force exclusive back-off from OOVs.
Perplexity = 123.99, Entropy = 6.95 bits
Computation based on 570 words.
Number of 3-grams hit = 425  (74.56%)
Number of 2-grams hit = 113  (19.82%)
Number of 1-grams hit = 32  (5.61%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article573.text
Will force exclusive back-off from OOVs.
Perplexity = 121.27, Entropy = 6.92 bits
Computation based on 251 words.
Number of 3-grams hit = 187  (74.50%)
Number of 2-grams hit = 51  (20.32%)
Number of 1-grams hit = 13  (5.18%)
1 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article574.text
Will force exclusive back-off from OOVs.
Perplexity = 117.47, Entropy = 6.88 bits
Computation based on 1089 words.
Number of 3-grams hit = 830  (76.22%)
Number of 2-grams hit = 207  (19.01%)
Number of 1-grams hit = 52  (4.78%)
5 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article575.text
Will force exclusive back-off from OOVs.
Perplexity = 115.69, Entropy = 6.85 bits
Computation based on 595 words.
Number of 3-grams hit = 458  (76.97%)
Number of 2-grams hit = 108  (18.15%)
Number of 1-grams hit = 29  (4.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article576.text
Will force exclusive back-off from OOVs.
Perplexity = 323.24, Entropy = 8.34 bits
Computation based on 426 words.
Number of 3-grams hit = 253  (59.39%)
Number of 2-grams hit = 113  (26.53%)
Number of 1-grams hit = 60  (14.08%)
19 OOVs (4.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article577.text
Will force exclusive back-off from OOVs.
Perplexity = 100.23, Entropy = 6.65 bits
Computation based on 363 words.
Number of 3-grams hit = 287  (79.06%)
Number of 2-grams hit = 62  (17.08%)
Number of 1-grams hit = 14  (3.86%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article578.text
Will force exclusive back-off from OOVs.
Perplexity = 98.97, Entropy = 6.63 bits
Computation based on 406 words.
Number of 3-grams hit = 319  (78.57%)
Number of 2-grams hit = 67  (16.50%)
Number of 1-grams hit = 20  (4.93%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article579.text
Will force exclusive back-off from OOVs.
Perplexity = 146.10, Entropy = 7.19 bits
Computation based on 440 words.
Number of 3-grams hit = 322  (73.18%)
Number of 2-grams hit = 90  (20.45%)
Number of 1-grams hit = 28  (6.36%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article580.text
Will force exclusive back-off from OOVs.
Perplexity = 109.11, Entropy = 6.77 bits
Computation based on 532 words.
Number of 3-grams hit = 419  (78.76%)
Number of 2-grams hit = 92  (17.29%)
Number of 1-grams hit = 21  (3.95%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article581.text
Will force exclusive back-off from OOVs.
Perplexity = 149.85, Entropy = 7.23 bits
Computation based on 533 words.
Number of 3-grams hit = 373  (69.98%)
Number of 2-grams hit = 118  (22.14%)
Number of 1-grams hit = 42  (7.88%)
8 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article582.text
Will force exclusive back-off from OOVs.
Perplexity = 165.44, Entropy = 7.37 bits
Computation based on 500 words.
Number of 3-grams hit = 356  (71.20%)
Number of 2-grams hit = 109  (21.80%)
Number of 1-grams hit = 35  (7.00%)
9 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article583.text
Will force exclusive back-off from OOVs.
Perplexity = 136.42, Entropy = 7.09 bits
Computation based on 664 words.
Number of 3-grams hit = 474  (71.39%)
Number of 2-grams hit = 147  (22.14%)
Number of 1-grams hit = 43  (6.48%)
11 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article584.text
Will force exclusive back-off from OOVs.
Perplexity = 110.34, Entropy = 6.79 bits
Computation based on 433 words.
Number of 3-grams hit = 330  (76.21%)
Number of 2-grams hit = 84  (19.40%)
Number of 1-grams hit = 19  (4.39%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article585.text
Will force exclusive back-off from OOVs.
Perplexity = 88.10, Entropy = 6.46 bits
Computation based on 946 words.
Number of 3-grams hit = 766  (80.97%)
Number of 2-grams hit = 145  (15.33%)
Number of 1-grams hit = 35  (3.70%)
8 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article586.text
Will force exclusive back-off from OOVs.
Perplexity = 88.60, Entropy = 6.47 bits
Computation based on 733 words.
Number of 3-grams hit = 602  (82.13%)
Number of 2-grams hit = 107  (14.60%)
Number of 1-grams hit = 24  (3.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article587.text
Will force exclusive back-off from OOVs.
Perplexity = 120.28, Entropy = 6.91 bits
Computation based on 522 words.
Number of 3-grams hit = 400  (76.63%)
Number of 2-grams hit = 101  (19.35%)
Number of 1-grams hit = 21  (4.02%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article588.text
Will force exclusive back-off from OOVs.
Perplexity = 111.07, Entropy = 6.80 bits
Computation based on 955 words.
Number of 3-grams hit = 737  (77.17%)
Number of 2-grams hit = 169  (17.70%)
Number of 1-grams hit = 49  (5.13%)
5 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article589.text
Will force exclusive back-off from OOVs.
Perplexity = 102.49, Entropy = 6.68 bits
Computation based on 504 words.
Number of 3-grams hit = 395  (78.37%)
Number of 2-grams hit = 93  (18.45%)
Number of 1-grams hit = 16  (3.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article590.text
Will force exclusive back-off from OOVs.
Perplexity = 72.47, Entropy = 6.18 bits
Computation based on 1387 words.
Number of 3-grams hit = 1135  (81.83%)
Number of 2-grams hit = 211  (15.21%)
Number of 1-grams hit = 41  (2.96%)
6 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article591.text
Will force exclusive back-off from OOVs.
Perplexity = 92.63, Entropy = 6.53 bits
Computation based on 686 words.
Number of 3-grams hit = 553  (80.61%)
Number of 2-grams hit = 107  (15.60%)
Number of 1-grams hit = 26  (3.79%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article592.text
Will force exclusive back-off from OOVs.
Perplexity = 143.37, Entropy = 7.16 bits
Computation based on 339 words.
Number of 3-grams hit = 242  (71.39%)
Number of 2-grams hit = 78  (23.01%)
Number of 1-grams hit = 19  (5.60%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article593.text
Will force exclusive back-off from OOVs.
Perplexity = 73.54, Entropy = 6.20 bits
Computation based on 376 words.
Number of 3-grams hit = 303  (80.59%)
Number of 2-grams hit = 63  (16.76%)
Number of 1-grams hit = 10  (2.66%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article594.text
Will force exclusive back-off from OOVs.
Perplexity = 98.21, Entropy = 6.62 bits
Computation based on 586 words.
Number of 3-grams hit = 458  (78.16%)
Number of 2-grams hit = 112  (19.11%)
Number of 1-grams hit = 16  (2.73%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article595.text
Will force exclusive back-off from OOVs.
Perplexity = 91.51, Entropy = 6.52 bits
Computation based on 369 words.
Number of 3-grams hit = 287  (77.78%)
Number of 2-grams hit = 69  (18.70%)
Number of 1-grams hit = 13  (3.52%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article596.text
Will force exclusive back-off from OOVs.
Perplexity = 131.70, Entropy = 7.04 bits
Computation based on 963 words.
Number of 3-grams hit = 729  (75.70%)
Number of 2-grams hit = 181  (18.80%)
Number of 1-grams hit = 53  (5.50%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article597.text
Will force exclusive back-off from OOVs.
Perplexity = 161.47, Entropy = 7.34 bits
Computation based on 1027 words.
Number of 3-grams hit = 721  (70.20%)
Number of 2-grams hit = 242  (23.56%)
Number of 1-grams hit = 64  (6.23%)
7 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article598.text
Will force exclusive back-off from OOVs.
Perplexity = 123.07, Entropy = 6.94 bits
Computation based on 296 words.
Number of 3-grams hit = 221  (74.66%)
Number of 2-grams hit = 58  (19.59%)
Number of 1-grams hit = 17  (5.74%)
3 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article599.text
Will force exclusive back-off from OOVs.
Perplexity = 140.91, Entropy = 7.14 bits
Computation based on 802 words.
Number of 3-grams hit = 573  (71.45%)
Number of 2-grams hit = 188  (23.44%)
Number of 1-grams hit = 41  (5.11%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article600.text
Will force exclusive back-off from OOVs.
Perplexity = 58.95, Entropy = 5.88 bits
Computation based on 4658 words.
Number of 3-grams hit = 3967  (85.17%)
Number of 2-grams hit = 578  (12.41%)
Number of 1-grams hit = 113  (2.43%)
15 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article601.text
Will force exclusive back-off from OOVs.
Perplexity = 153.56, Entropy = 7.26 bits
Computation based on 1088 words.
Number of 3-grams hit = 777  (71.42%)
Number of 2-grams hit = 242  (22.24%)
Number of 1-grams hit = 69  (6.34%)
11 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article602.text
Will force exclusive back-off from OOVs.
Perplexity = 97.09, Entropy = 6.60 bits
Computation based on 590 words.
Number of 3-grams hit = 471  (79.83%)
Number of 2-grams hit = 95  (16.10%)
Number of 1-grams hit = 24  (4.07%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article603.text
Will force exclusive back-off from OOVs.
Perplexity = 151.55, Entropy = 7.24 bits
Computation based on 584 words.
Number of 3-grams hit = 431  (73.80%)
Number of 2-grams hit = 112  (19.18%)
Number of 1-grams hit = 41  (7.02%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article604.text
Will force exclusive back-off from OOVs.
Perplexity = 77.52, Entropy = 6.28 bits
Computation based on 560 words.
Number of 3-grams hit = 466  (83.21%)
Number of 2-grams hit = 77  (13.75%)
Number of 1-grams hit = 17  (3.04%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article605.text
Will force exclusive back-off from OOVs.
Perplexity = 98.06, Entropy = 6.62 bits
Computation based on 3116 words.
Number of 3-grams hit = 2408  (77.28%)
Number of 2-grams hit = 565  (18.13%)
Number of 1-grams hit = 143  (4.59%)
40 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article606.text
Will force exclusive back-off from OOVs.
Perplexity = 127.46, Entropy = 6.99 bits
Computation based on 900 words.
Number of 3-grams hit = 648  (72.00%)
Number of 2-grams hit = 203  (22.56%)
Number of 1-grams hit = 49  (5.44%)
13 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article607.text
Will force exclusive back-off from OOVs.
Perplexity = 104.06, Entropy = 6.70 bits
Computation based on 1058 words.
Number of 3-grams hit = 836  (79.02%)
Number of 2-grams hit = 176  (16.64%)
Number of 1-grams hit = 46  (4.35%)
5 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article608.text
Will force exclusive back-off from OOVs.
Perplexity = 102.52, Entropy = 6.68 bits
Computation based on 617 words.
Number of 3-grams hit = 487  (78.93%)
Number of 2-grams hit = 106  (17.18%)
Number of 1-grams hit = 24  (3.89%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article609.text
Will force exclusive back-off from OOVs.
Perplexity = 103.78, Entropy = 6.70 bits
Computation based on 4371 words.
Number of 3-grams hit = 3381  (77.35%)
Number of 2-grams hit = 801  (18.33%)
Number of 1-grams hit = 189  (4.32%)
38 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article610.text
Will force exclusive back-off from OOVs.
Perplexity = 113.31, Entropy = 6.82 bits
Computation based on 5950 words.
Number of 3-grams hit = 4623  (77.70%)
Number of 2-grams hit = 1060  (17.82%)
Number of 1-grams hit = 267  (4.49%)
13 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article611.text
Will force exclusive back-off from OOVs.
Perplexity = 105.58, Entropy = 6.72 bits
Computation based on 776 words.
Number of 3-grams hit = 611  (78.74%)
Number of 2-grams hit = 123  (15.85%)
Number of 1-grams hit = 42  (5.41%)
4 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article612.text
Will force exclusive back-off from OOVs.
Perplexity = 70.90, Entropy = 6.15 bits
Computation based on 1629 words.
Number of 3-grams hit = 1336  (82.01%)
Number of 2-grams hit = 255  (15.65%)
Number of 1-grams hit = 38  (2.33%)
3 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article613.text
Will force exclusive back-off from OOVs.
Perplexity = 97.01, Entropy = 6.60 bits
Computation based on 4146 words.
Number of 3-grams hit = 3179  (76.68%)
Number of 2-grams hit = 786  (18.96%)
Number of 1-grams hit = 181  (4.37%)
42 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article614.text
Will force exclusive back-off from OOVs.
Perplexity = 85.80, Entropy = 6.42 bits
Computation based on 1524 words.
Number of 3-grams hit = 1188  (77.95%)
Number of 2-grams hit = 275  (18.04%)
Number of 1-grams hit = 61  (4.00%)
26 OOVs (1.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article615.text
Will force exclusive back-off from OOVs.
Perplexity = 88.98, Entropy = 6.48 bits
Computation based on 452 words.
Number of 3-grams hit = 352  (77.88%)
Number of 2-grams hit = 79  (17.48%)
Number of 1-grams hit = 21  (4.65%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article616.text
Will force exclusive back-off from OOVs.
Perplexity = 113.01, Entropy = 6.82 bits
Computation based on 767 words.
Number of 3-grams hit = 587  (76.53%)
Number of 2-grams hit = 140  (18.25%)
Number of 1-grams hit = 40  (5.22%)
5 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article617.text
Will force exclusive back-off from OOVs.
Perplexity = 93.35, Entropy = 6.54 bits
Computation based on 1319 words.
Number of 3-grams hit = 1038  (78.70%)
Number of 2-grams hit = 240  (18.20%)
Number of 1-grams hit = 41  (3.11%)
5 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article618.text
Will force exclusive back-off from OOVs.
Perplexity = 118.43, Entropy = 6.89 bits
Computation based on 3769 words.
Number of 3-grams hit = 2888  (76.63%)
Number of 2-grams hit = 703  (18.65%)
Number of 1-grams hit = 178  (4.72%)
8 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article619.text
Will force exclusive back-off from OOVs.
Perplexity = 90.77, Entropy = 6.50 bits
Computation based on 761 words.
Number of 3-grams hit = 595  (78.19%)
Number of 2-grams hit = 135  (17.74%)
Number of 1-grams hit = 31  (4.07%)
6 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article620.text
Will force exclusive back-off from OOVs.
Perplexity = 119.61, Entropy = 6.90 bits
Computation based on 5314 words.
Number of 3-grams hit = 4055  (76.31%)
Number of 2-grams hit = 993  (18.69%)
Number of 1-grams hit = 266  (5.01%)
15 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article621.text
Will force exclusive back-off from OOVs.
Perplexity = 103.60, Entropy = 6.69 bits
Computation based on 321 words.
Number of 3-grams hit = 238  (74.14%)
Number of 2-grams hit = 60  (18.69%)
Number of 1-grams hit = 23  (7.17%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article622.text
Will force exclusive back-off from OOVs.
Perplexity = 150.37, Entropy = 7.23 bits
Computation based on 452 words.
Number of 3-grams hit = 324  (71.68%)
Number of 2-grams hit = 93  (20.58%)
Number of 1-grams hit = 35  (7.74%)
7 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article623.text
Will force exclusive back-off from OOVs.
Perplexity = 110.24, Entropy = 6.78 bits
Computation based on 1306 words.
Number of 3-grams hit = 1015  (77.72%)
Number of 2-grams hit = 233  (17.84%)
Number of 1-grams hit = 58  (4.44%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article624.text
Will force exclusive back-off from OOVs.
Perplexity = 125.27, Entropy = 6.97 bits
Computation based on 5146 words.
Number of 3-grams hit = 3888  (75.55%)
Number of 2-grams hit = 1010  (19.63%)
Number of 1-grams hit = 248  (4.82%)
20 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article625.text
Will force exclusive back-off from OOVs.
Perplexity = 123.21, Entropy = 6.94 bits
Computation based on 1088 words.
Number of 3-grams hit = 826  (75.92%)
Number of 2-grams hit = 204  (18.75%)
Number of 1-grams hit = 58  (5.33%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article626.text
Will force exclusive back-off from OOVs.
Perplexity = 115.04, Entropy = 6.85 bits
Computation based on 446 words.
Number of 3-grams hit = 330  (73.99%)
Number of 2-grams hit = 88  (19.73%)
Number of 1-grams hit = 28  (6.28%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article627.text
Will force exclusive back-off from OOVs.
Perplexity = 293.91, Entropy = 8.20 bits
Computation based on 141 words.
Number of 3-grams hit = 83  (58.87%)
Number of 2-grams hit = 37  (26.24%)
Number of 1-grams hit = 21  (14.89%)
8 OOVs (5.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article628.text
Will force exclusive back-off from OOVs.
Perplexity = 130.08, Entropy = 7.02 bits
Computation based on 1057 words.
Number of 3-grams hit = 779  (73.70%)
Number of 2-grams hit = 209  (19.77%)
Number of 1-grams hit = 69  (6.53%)
18 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article629.text
Will force exclusive back-off from OOVs.
Perplexity = 129.77, Entropy = 7.02 bits
Computation based on 1027 words.
Number of 3-grams hit = 764  (74.39%)
Number of 2-grams hit = 200  (19.47%)
Number of 1-grams hit = 63  (6.13%)
10 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article630.text
Will force exclusive back-off from OOVs.
Perplexity = 181.23, Entropy = 7.50 bits
Computation based on 328 words.
Number of 3-grams hit = 227  (69.21%)
Number of 2-grams hit = 74  (22.56%)
Number of 1-grams hit = 27  (8.23%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article631.text
Will force exclusive back-off from OOVs.
Perplexity = 196.46, Entropy = 7.62 bits
Computation based on 857 words.
Number of 3-grams hit = 554  (64.64%)
Number of 2-grams hit = 220  (25.67%)
Number of 1-grams hit = 83  (9.68%)
19 OOVs (2.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article632.text
Will force exclusive back-off from OOVs.
Perplexity = 148.24, Entropy = 7.21 bits
Computation based on 408 words.
Number of 3-grams hit = 305  (74.75%)
Number of 2-grams hit = 83  (20.34%)
Number of 1-grams hit = 20  (4.90%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article633.text
Will force exclusive back-off from OOVs.
Perplexity = 115.48, Entropy = 6.85 bits
Computation based on 529 words.
Number of 3-grams hit = 403  (76.18%)
Number of 2-grams hit = 99  (18.71%)
Number of 1-grams hit = 27  (5.10%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article634.text
Will force exclusive back-off from OOVs.
Perplexity = 62.77, Entropy = 5.97 bits
Computation based on 560 words.
Number of 3-grams hit = 473  (84.46%)
Number of 2-grams hit = 74  (13.21%)
Number of 1-grams hit = 13  (2.32%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article635.text
Will force exclusive back-off from OOVs.
Perplexity = 136.83, Entropy = 7.10 bits
Computation based on 151 words.
Number of 3-grams hit = 111  (73.51%)
Number of 2-grams hit = 34  (22.52%)
Number of 1-grams hit = 6  (3.97%)
1 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article636.text
Will force exclusive back-off from OOVs.
Perplexity = 259.74, Entropy = 8.02 bits
Computation based on 1322 words.
Number of 3-grams hit = 857  (64.83%)
Number of 2-grams hit = 334  (25.26%)
Number of 1-grams hit = 131  (9.91%)
22 OOVs (1.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article637.text
Will force exclusive back-off from OOVs.
Perplexity = 100.73, Entropy = 6.65 bits
Computation based on 1034 words.
Number of 3-grams hit = 811  (78.43%)
Number of 2-grams hit = 172  (16.63%)
Number of 1-grams hit = 51  (4.93%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article638.text
Will force exclusive back-off from OOVs.
Perplexity = 217.64, Entropy = 7.77 bits
Computation based on 526 words.
Number of 3-grams hit = 342  (65.02%)
Number of 2-grams hit = 133  (25.29%)
Number of 1-grams hit = 51  (9.70%)
7 OOVs (1.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article639.text
Will force exclusive back-off from OOVs.
Perplexity = 162.51, Entropy = 7.34 bits
Computation based on 522 words.
Number of 3-grams hit = 367  (70.31%)
Number of 2-grams hit = 119  (22.80%)
Number of 1-grams hit = 36  (6.90%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article640.text
Will force exclusive back-off from OOVs.
Perplexity = 93.57, Entropy = 6.55 bits
Computation based on 204 words.
Number of 3-grams hit = 160  (78.43%)
Number of 2-grams hit = 35  (17.16%)
Number of 1-grams hit = 9  (4.41%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article641.text
Will force exclusive back-off from OOVs.
Perplexity = 160.11, Entropy = 7.32 bits
Computation based on 825 words.
Number of 3-grams hit = 608  (73.70%)
Number of 2-grams hit = 169  (20.48%)
Number of 1-grams hit = 48  (5.82%)
5 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article642.text
Will force exclusive back-off from OOVs.
Perplexity = 250.57, Entropy = 7.97 bits
Computation based on 429 words.
Number of 3-grams hit = 267  (62.24%)
Number of 2-grams hit = 114  (26.57%)
Number of 1-grams hit = 48  (11.19%)
15 OOVs (3.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article643.text
Will force exclusive back-off from OOVs.
Perplexity = 114.74, Entropy = 6.84 bits
Computation based on 488 words.
Number of 3-grams hit = 381  (78.07%)
Number of 2-grams hit = 97  (19.88%)
Number of 1-grams hit = 10  (2.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article644.text
Will force exclusive back-off from OOVs.
Perplexity = 102.53, Entropy = 6.68 bits
Computation based on 1116 words.
Number of 3-grams hit = 889  (79.66%)
Number of 2-grams hit = 183  (16.40%)
Number of 1-grams hit = 44  (3.94%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article645.text
Will force exclusive back-off from OOVs.
Perplexity = 104.55, Entropy = 6.71 bits
Computation based on 440 words.
Number of 3-grams hit = 334  (75.91%)
Number of 2-grams hit = 87  (19.77%)
Number of 1-grams hit = 19  (4.32%)
6 OOVs (1.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article646.text
Will force exclusive back-off from OOVs.
Perplexity = 114.45, Entropy = 6.84 bits
Computation based on 573 words.
Number of 3-grams hit = 443  (77.31%)
Number of 2-grams hit = 104  (18.15%)
Number of 1-grams hit = 26  (4.54%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article647.text
Will force exclusive back-off from OOVs.
Perplexity = 165.65, Entropy = 7.37 bits
Computation based on 608 words.
Number of 3-grams hit = 438  (72.04%)
Number of 2-grams hit = 142  (23.36%)
Number of 1-grams hit = 28  (4.61%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article648.text
Will force exclusive back-off from OOVs.
Perplexity = 108.05, Entropy = 6.76 bits
Computation based on 475 words.
Number of 3-grams hit = 365  (76.84%)
Number of 2-grams hit = 93  (19.58%)
Number of 1-grams hit = 17  (3.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article649.text
Will force exclusive back-off from OOVs.
Perplexity = 132.06, Entropy = 7.05 bits
Computation based on 530 words.
Number of 3-grams hit = 399  (75.28%)
Number of 2-grams hit = 101  (19.06%)
Number of 1-grams hit = 30  (5.66%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article650.text
Will force exclusive back-off from OOVs.
Perplexity = 139.69, Entropy = 7.13 bits
Computation based on 529 words.
Number of 3-grams hit = 391  (73.91%)
Number of 2-grams hit = 106  (20.04%)
Number of 1-grams hit = 32  (6.05%)
5 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article651.text
Will force exclusive back-off from OOVs.
Perplexity = 101.94, Entropy = 6.67 bits
Computation based on 459 words.
Number of 3-grams hit = 346  (75.38%)
Number of 2-grams hit = 94  (20.48%)
Number of 1-grams hit = 19  (4.14%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article652.text
Will force exclusive back-off from OOVs.
Perplexity = 65.04, Entropy = 6.02 bits
Computation based on 878 words.
Number of 3-grams hit = 744  (84.74%)
Number of 2-grams hit = 118  (13.44%)
Number of 1-grams hit = 16  (1.82%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article653.text
Will force exclusive back-off from OOVs.
Perplexity = 99.56, Entropy = 6.64 bits
Computation based on 765 words.
Number of 3-grams hit = 613  (80.13%)
Number of 2-grams hit = 121  (15.82%)
Number of 1-grams hit = 31  (4.05%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article654.text
Will force exclusive back-off from OOVs.
Perplexity = 124.96, Entropy = 6.97 bits
Computation based on 772 words.
Number of 3-grams hit = 590  (76.42%)
Number of 2-grams hit = 143  (18.52%)
Number of 1-grams hit = 39  (5.05%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article655.text
Will force exclusive back-off from OOVs.
Perplexity = 131.87, Entropy = 7.04 bits
Computation based on 481 words.
Number of 3-grams hit = 358  (74.43%)
Number of 2-grams hit = 87  (18.09%)
Number of 1-grams hit = 36  (7.48%)
6 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article656.text
Will force exclusive back-off from OOVs.
Perplexity = 264.73, Entropy = 8.05 bits
Computation based on 461 words.
Number of 3-grams hit = 290  (62.91%)
Number of 2-grams hit = 124  (26.90%)
Number of 1-grams hit = 47  (10.20%)
5 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article657.text
Will force exclusive back-off from OOVs.
Perplexity = 98.78, Entropy = 6.63 bits
Computation based on 376 words.
Number of 3-grams hit = 292  (77.66%)
Number of 2-grams hit = 57  (15.16%)
Number of 1-grams hit = 27  (7.18%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article658.text
Will force exclusive back-off from OOVs.
Perplexity = 96.02, Entropy = 6.59 bits
Computation based on 1004 words.
Number of 3-grams hit = 797  (79.38%)
Number of 2-grams hit = 168  (16.73%)
Number of 1-grams hit = 39  (3.88%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article659.text
Will force exclusive back-off from OOVs.
Perplexity = 136.79, Entropy = 7.10 bits
Computation based on 964 words.
Number of 3-grams hit = 677  (70.23%)
Number of 2-grams hit = 209  (21.68%)
Number of 1-grams hit = 78  (8.09%)
26 OOVs (2.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article660.text
Will force exclusive back-off from OOVs.
Perplexity = 172.49, Entropy = 7.43 bits
Computation based on 139 words.
Number of 3-grams hit = 99  (71.22%)
Number of 2-grams hit = 26  (18.71%)
Number of 1-grams hit = 14  (10.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article661.text
Will force exclusive back-off from OOVs.
Perplexity = 119.89, Entropy = 6.91 bits
Computation based on 1244 words.
Number of 3-grams hit = 951  (76.45%)
Number of 2-grams hit = 224  (18.01%)
Number of 1-grams hit = 69  (5.55%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article662.text
Will force exclusive back-off from OOVs.
Perplexity = 430.09, Entropy = 8.75 bits
Computation based on 123 words.
Number of 3-grams hit = 71  (57.72%)
Number of 2-grams hit = 34  (27.64%)
Number of 1-grams hit = 18  (14.63%)
3 OOVs (2.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article663.text
Will force exclusive back-off from OOVs.
Perplexity = 197.50, Entropy = 7.63 bits
Computation based on 1338 words.
Number of 3-grams hit = 898  (67.12%)
Number of 2-grams hit = 352  (26.31%)
Number of 1-grams hit = 88  (6.58%)
22 OOVs (1.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article664.text
Will force exclusive back-off from OOVs.
Perplexity = 83.91, Entropy = 6.39 bits
Computation based on 692 words.
Number of 3-grams hit = 562  (81.21%)
Number of 2-grams hit = 102  (14.74%)
Number of 1-grams hit = 28  (4.05%)
7 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article665.text
Will force exclusive back-off from OOVs.
Perplexity = 94.78, Entropy = 6.57 bits
Computation based on 756 words.
Number of 3-grams hit = 601  (79.50%)
Number of 2-grams hit = 126  (16.67%)
Number of 1-grams hit = 29  (3.84%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article666.text
Will force exclusive back-off from OOVs.
Perplexity = 145.28, Entropy = 7.18 bits
Computation based on 383 words.
Number of 3-grams hit = 278  (72.58%)
Number of 2-grams hit = 83  (21.67%)
Number of 1-grams hit = 22  (5.74%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article667.text
Will force exclusive back-off from OOVs.
Perplexity = 64.54, Entropy = 6.01 bits
Computation based on 1292 words.
Number of 3-grams hit = 1096  (84.83%)
Number of 2-grams hit = 172  (13.31%)
Number of 1-grams hit = 24  (1.86%)
9 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article668.text
Will force exclusive back-off from OOVs.
Perplexity = 88.34, Entropy = 6.46 bits
Computation based on 466 words.
Number of 3-grams hit = 377  (80.90%)
Number of 2-grams hit = 77  (16.52%)
Number of 1-grams hit = 12  (2.58%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article669.text
Will force exclusive back-off from OOVs.
Perplexity = 133.14, Entropy = 7.06 bits
Computation based on 485 words.
Number of 3-grams hit = 366  (75.46%)
Number of 2-grams hit = 90  (18.56%)
Number of 1-grams hit = 29  (5.98%)
4 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article670.text
Will force exclusive back-off from OOVs.
Perplexity = 71.83, Entropy = 6.17 bits
Computation based on 448 words.
Number of 3-grams hit = 360  (80.36%)
Number of 2-grams hit = 73  (16.29%)
Number of 1-grams hit = 15  (3.35%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article671.text
Will force exclusive back-off from OOVs.
Perplexity = 119.29, Entropy = 6.90 bits
Computation based on 2031 words.
Number of 3-grams hit = 1561  (76.86%)
Number of 2-grams hit = 372  (18.32%)
Number of 1-grams hit = 98  (4.83%)
5 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article672.text
Will force exclusive back-off from OOVs.
Perplexity = 157.02, Entropy = 7.29 bits
Computation based on 284 words.
Number of 3-grams hit = 210  (73.94%)
Number of 2-grams hit = 52  (18.31%)
Number of 1-grams hit = 22  (7.75%)
4 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article673.text
Will force exclusive back-off from OOVs.
Perplexity = 95.94, Entropy = 6.58 bits
Computation based on 301 words.
Number of 3-grams hit = 239  (79.40%)
Number of 2-grams hit = 47  (15.61%)
Number of 1-grams hit = 15  (4.98%)
3 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article674.text
Will force exclusive back-off from OOVs.
Perplexity = 74.78, Entropy = 6.22 bits
Computation based on 141 words.
Number of 3-grams hit = 113  (80.14%)
Number of 2-grams hit = 24  (17.02%)
Number of 1-grams hit = 4  (2.84%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article675.text
Will force exclusive back-off from OOVs.
Perplexity = 112.10, Entropy = 6.81 bits
Computation based on 3859 words.
Number of 3-grams hit = 3002  (77.79%)
Number of 2-grams hit = 669  (17.34%)
Number of 1-grams hit = 188  (4.87%)
15 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article676.text
Will force exclusive back-off from OOVs.
Perplexity = 120.03, Entropy = 6.91 bits
Computation based on 496 words.
Number of 3-grams hit = 385  (77.62%)
Number of 2-grams hit = 86  (17.34%)
Number of 1-grams hit = 25  (5.04%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article677.text
Will force exclusive back-off from OOVs.
Perplexity = 137.92, Entropy = 7.11 bits
Computation based on 641 words.
Number of 3-grams hit = 466  (72.70%)
Number of 2-grams hit = 145  (22.62%)
Number of 1-grams hit = 30  (4.68%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article678.text
Will force exclusive back-off from OOVs.
Perplexity = 121.98, Entropy = 6.93 bits
Computation based on 340 words.
Number of 3-grams hit = 259  (76.18%)
Number of 2-grams hit = 66  (19.41%)
Number of 1-grams hit = 15  (4.41%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article679.text
Will force exclusive back-off from OOVs.
Perplexity = 117.23, Entropy = 6.87 bits
Computation based on 986 words.
Number of 3-grams hit = 756  (76.67%)
Number of 2-grams hit = 184  (18.66%)
Number of 1-grams hit = 46  (4.67%)
3 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article680.text
Will force exclusive back-off from OOVs.
Perplexity = 88.67, Entropy = 6.47 bits
Computation based on 317 words.
Number of 3-grams hit = 250  (78.86%)
Number of 2-grams hit = 51  (16.09%)
Number of 1-grams hit = 16  (5.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article681.text
Will force exclusive back-off from OOVs.
Perplexity = 85.98, Entropy = 6.43 bits
Computation based on 474 words.
Number of 3-grams hit = 382  (80.59%)
Number of 2-grams hit = 71  (14.98%)
Number of 1-grams hit = 21  (4.43%)
4 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article682.text
Will force exclusive back-off from OOVs.
Perplexity = 107.48, Entropy = 6.75 bits
Computation based on 3680 words.
Number of 3-grams hit = 2880  (78.26%)
Number of 2-grams hit = 649  (17.64%)
Number of 1-grams hit = 151  (4.10%)
10 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article683.text
Will force exclusive back-off from OOVs.
Perplexity = 150.57, Entropy = 7.23 bits
Computation based on 261 words.
Number of 3-grams hit = 183  (70.11%)
Number of 2-grams hit = 62  (23.75%)
Number of 1-grams hit = 16  (6.13%)
5 OOVs (1.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article684.text
Will force exclusive back-off from OOVs.
Perplexity = 94.58, Entropy = 6.56 bits
Computation based on 4339 words.
Number of 3-grams hit = 3419  (78.80%)
Number of 2-grams hit = 770  (17.75%)
Number of 1-grams hit = 150  (3.46%)
19 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article685.text
Will force exclusive back-off from OOVs.
Perplexity = 176.31, Entropy = 7.46 bits
Computation based on 455 words.
Number of 3-grams hit = 330  (72.53%)
Number of 2-grams hit = 97  (21.32%)
Number of 1-grams hit = 28  (6.15%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article686.text
Will force exclusive back-off from OOVs.
Perplexity = 123.92, Entropy = 6.95 bits
Computation based on 2213 words.
Number of 3-grams hit = 1669  (75.42%)
Number of 2-grams hit = 430  (19.43%)
Number of 1-grams hit = 114  (5.15%)
16 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article687.text
Will force exclusive back-off from OOVs.
Perplexity = 104.56, Entropy = 6.71 bits
Computation based on 512 words.
Number of 3-grams hit = 388  (75.78%)
Number of 2-grams hit = 102  (19.92%)
Number of 1-grams hit = 22  (4.30%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article688.text
Will force exclusive back-off from OOVs.
Perplexity = 99.50, Entropy = 6.64 bits
Computation based on 303 words.
Number of 3-grams hit = 239  (78.88%)
Number of 2-grams hit = 54  (17.82%)
Number of 1-grams hit = 10  (3.30%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article689.text
Will force exclusive back-off from OOVs.
Perplexity = 186.68, Entropy = 7.54 bits
Computation based on 508 words.
Number of 3-grams hit = 356  (70.08%)
Number of 2-grams hit = 114  (22.44%)
Number of 1-grams hit = 38  (7.48%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article690.text
Will force exclusive back-off from OOVs.
Perplexity = 100.04, Entropy = 6.64 bits
Computation based on 754 words.
Number of 3-grams hit = 600  (79.58%)
Number of 2-grams hit = 131  (17.37%)
Number of 1-grams hit = 23  (3.05%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article691.text
Will force exclusive back-off from OOVs.
Perplexity = 123.05, Entropy = 6.94 bits
Computation based on 872 words.
Number of 3-grams hit = 662  (75.92%)
Number of 2-grams hit = 158  (18.12%)
Number of 1-grams hit = 52  (5.96%)
6 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article692.text
Will force exclusive back-off from OOVs.
Perplexity = 331.88, Entropy = 8.37 bits
Computation based on 427 words.
Number of 3-grams hit = 257  (60.19%)
Number of 2-grams hit = 124  (29.04%)
Number of 1-grams hit = 46  (10.77%)
6 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article693.text
Will force exclusive back-off from OOVs.
Perplexity = 114.69, Entropy = 6.84 bits
Computation based on 922 words.
Number of 3-grams hit = 686  (74.40%)
Number of 2-grams hit = 183  (19.85%)
Number of 1-grams hit = 53  (5.75%)
22 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article694.text
Will force exclusive back-off from OOVs.
Perplexity = 89.05, Entropy = 6.48 bits
Computation based on 626 words.
Number of 3-grams hit = 502  (80.19%)
Number of 2-grams hit = 105  (16.77%)
Number of 1-grams hit = 19  (3.04%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article695.text
Will force exclusive back-off from OOVs.
Perplexity = 99.45, Entropy = 6.64 bits
Computation based on 849 words.
Number of 3-grams hit = 659  (77.62%)
Number of 2-grams hit = 147  (17.31%)
Number of 1-grams hit = 43  (5.06%)
5 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article696.text
Will force exclusive back-off from OOVs.
Perplexity = 107.30, Entropy = 6.75 bits
Computation based on 496 words.
Number of 3-grams hit = 365  (73.59%)
Number of 2-grams hit = 113  (22.78%)
Number of 1-grams hit = 18  (3.63%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article697.text
Will force exclusive back-off from OOVs.
Perplexity = 116.75, Entropy = 6.87 bits
Computation based on 942 words.
Number of 3-grams hit = 735  (78.03%)
Number of 2-grams hit = 161  (17.09%)
Number of 1-grams hit = 46  (4.88%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article698.text
Will force exclusive back-off from OOVs.
Perplexity = 131.85, Entropy = 7.04 bits
Computation based on 572 words.
Number of 3-grams hit = 427  (74.65%)
Number of 2-grams hit = 119  (20.80%)
Number of 1-grams hit = 26  (4.55%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article699.text
Will force exclusive back-off from OOVs.
Perplexity = 109.45, Entropy = 6.77 bits
Computation based on 352 words.
Number of 3-grams hit = 275  (78.12%)
Number of 2-grams hit = 65  (18.47%)
Number of 1-grams hit = 12  (3.41%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article700.text
Will force exclusive back-off from OOVs.
Perplexity = 38.80, Entropy = 5.28 bits
Computation based on 3290 words.
Number of 3-grams hit = 3182  (96.72%)
Number of 2-grams hit = 54  (1.64%)
Number of 1-grams hit = 54  (1.64%)
53 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article701.text
Will force exclusive back-off from OOVs.
Perplexity = 120.72, Entropy = 6.92 bits
Computation based on 807 words.
Number of 3-grams hit = 613  (75.96%)
Number of 2-grams hit = 151  (18.71%)
Number of 1-grams hit = 43  (5.33%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article702.text
Will force exclusive back-off from OOVs.
Perplexity = 220.02, Entropy = 7.78 bits
Computation based on 1792 words.
Number of 3-grams hit = 1195  (66.69%)
Number of 2-grams hit = 428  (23.88%)
Number of 1-grams hit = 169  (9.43%)
45 OOVs (2.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article703.text
Will force exclusive back-off from OOVs.
Perplexity = 198.31, Entropy = 7.63 bits
Computation based on 421 words.
Number of 3-grams hit = 287  (68.17%)
Number of 2-grams hit = 101  (23.99%)
Number of 1-grams hit = 33  (7.84%)
5 OOVs (1.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article704.text
Will force exclusive back-off from OOVs.
Perplexity = 112.47, Entropy = 6.81 bits
Computation based on 311 words.
Number of 3-grams hit = 244  (78.46%)
Number of 2-grams hit = 54  (17.36%)
Number of 1-grams hit = 13  (4.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article705.text
Will force exclusive back-off from OOVs.
Perplexity = 116.54, Entropy = 6.86 bits
Computation based on 1010 words.
Number of 3-grams hit = 784  (77.62%)
Number of 2-grams hit = 168  (16.63%)
Number of 1-grams hit = 58  (5.74%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article706.text
Will force exclusive back-off from OOVs.
Perplexity = 133.25, Entropy = 7.06 bits
Computation based on 476 words.
Number of 3-grams hit = 349  (73.32%)
Number of 2-grams hit = 99  (20.80%)
Number of 1-grams hit = 28  (5.88%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article707.text
Will force exclusive back-off from OOVs.
Perplexity = 230.21, Entropy = 7.85 bits
Computation based on 869 words.
Number of 3-grams hit = 588  (67.66%)
Number of 2-grams hit = 220  (25.32%)
Number of 1-grams hit = 61  (7.02%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article708.text
Will force exclusive back-off from OOVs.
Perplexity = 127.94, Entropy = 7.00 bits
Computation based on 523 words.
Number of 3-grams hit = 383  (73.23%)
Number of 2-grams hit = 113  (21.61%)
Number of 1-grams hit = 27  (5.16%)
5 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article709.text
Will force exclusive back-off from OOVs.
Perplexity = 176.56, Entropy = 7.46 bits
Computation based on 548 words.
Number of 3-grams hit = 389  (70.99%)
Number of 2-grams hit = 122  (22.26%)
Number of 1-grams hit = 37  (6.75%)
10 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article710.text
Will force exclusive back-off from OOVs.
Perplexity = 126.17, Entropy = 6.98 bits
Computation based on 753 words.
Number of 3-grams hit = 560  (74.37%)
Number of 2-grams hit = 143  (18.99%)
Number of 1-grams hit = 50  (6.64%)
21 OOVs (2.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article711.text
Will force exclusive back-off from OOVs.
Perplexity = 108.53, Entropy = 6.76 bits
Computation based on 959 words.
Number of 3-grams hit = 712  (74.24%)
Number of 2-grams hit = 210  (21.90%)
Number of 1-grams hit = 37  (3.86%)
3 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article712.text
Will force exclusive back-off from OOVs.
Perplexity = 146.78, Entropy = 7.20 bits
Computation based on 365 words.
Number of 3-grams hit = 261  (71.51%)
Number of 2-grams hit = 73  (20.00%)
Number of 1-grams hit = 31  (8.49%)
4 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article713.text
Will force exclusive back-off from OOVs.
Perplexity = 88.74, Entropy = 6.47 bits
Computation based on 887 words.
Number of 3-grams hit = 707  (79.71%)
Number of 2-grams hit = 150  (16.91%)
Number of 1-grams hit = 30  (3.38%)
3 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article714.text
Will force exclusive back-off from OOVs.
Perplexity = 123.75, Entropy = 6.95 bits
Computation based on 364 words.
Number of 3-grams hit = 271  (74.45%)
Number of 2-grams hit = 75  (20.60%)
Number of 1-grams hit = 18  (4.95%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article715.text
Will force exclusive back-off from OOVs.
Perplexity = 116.39, Entropy = 6.86 bits
Computation based on 486 words.
Number of 3-grams hit = 373  (76.75%)
Number of 2-grams hit = 93  (19.14%)
Number of 1-grams hit = 20  (4.12%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article716.text
Will force exclusive back-off from OOVs.
Perplexity = 175.86, Entropy = 7.46 bits
Computation based on 452 words.
Number of 3-grams hit = 299  (66.15%)
Number of 2-grams hit = 101  (22.35%)
Number of 1-grams hit = 52  (11.50%)
16 OOVs (3.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article717.text
Will force exclusive back-off from OOVs.
Perplexity = 109.64, Entropy = 6.78 bits
Computation based on 377 words.
Number of 3-grams hit = 290  (76.92%)
Number of 2-grams hit = 63  (16.71%)
Number of 1-grams hit = 24  (6.37%)
4 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article718.text
Will force exclusive back-off from OOVs.
Perplexity = 123.02, Entropy = 6.94 bits
Computation based on 6958 words.
Number of 3-grams hit = 5334  (76.66%)
Number of 2-grams hit = 1281  (18.41%)
Number of 1-grams hit = 343  (4.93%)
27 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article719.text
Will force exclusive back-off from OOVs.
Perplexity = 93.70, Entropy = 6.55 bits
Computation based on 1162 words.
Number of 3-grams hit = 929  (79.95%)
Number of 2-grams hit = 190  (16.35%)
Number of 1-grams hit = 43  (3.70%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article720.text
Will force exclusive back-off from OOVs.
Perplexity = 109.60, Entropy = 6.78 bits
Computation based on 806 words.
Number of 3-grams hit = 620  (76.92%)
Number of 2-grams hit = 152  (18.86%)
Number of 1-grams hit = 34  (4.22%)
8 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article721.text
Will force exclusive back-off from OOVs.
Perplexity = 164.33, Entropy = 7.36 bits
Computation based on 423 words.
Number of 3-grams hit = 310  (73.29%)
Number of 2-grams hit = 96  (22.70%)
Number of 1-grams hit = 17  (4.02%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article722.text
Will force exclusive back-off from OOVs.
Perplexity = 88.29, Entropy = 6.46 bits
Computation based on 1012 words.
Number of 3-grams hit = 817  (80.73%)
Number of 2-grams hit = 156  (15.42%)
Number of 1-grams hit = 39  (3.85%)
8 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article723.text
Will force exclusive back-off from OOVs.
Perplexity = 145.72, Entropy = 7.19 bits
Computation based on 427 words.
Number of 3-grams hit = 289  (67.68%)
Number of 2-grams hit = 104  (24.36%)
Number of 1-grams hit = 34  (7.96%)
13 OOVs (2.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article724.text
Will force exclusive back-off from OOVs.
Perplexity = 219.19, Entropy = 7.78 bits
Computation based on 458 words.
Number of 3-grams hit = 315  (68.78%)
Number of 2-grams hit = 108  (23.58%)
Number of 1-grams hit = 35  (7.64%)
4 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article725.text
Will force exclusive back-off from OOVs.
Perplexity = 135.59, Entropy = 7.08 bits
Computation based on 710 words.
Number of 3-grams hit = 520  (73.24%)
Number of 2-grams hit = 137  (19.30%)
Number of 1-grams hit = 53  (7.46%)
12 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article726.text
Will force exclusive back-off from OOVs.
Perplexity = 83.17, Entropy = 6.38 bits
Computation based on 519 words.
Number of 3-grams hit = 417  (80.35%)
Number of 2-grams hit = 88  (16.96%)
Number of 1-grams hit = 14  (2.70%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article727.text
Will force exclusive back-off from OOVs.
Perplexity = 193.93, Entropy = 7.60 bits
Computation based on 482 words.
Number of 3-grams hit = 317  (65.77%)
Number of 2-grams hit = 128  (26.56%)
Number of 1-grams hit = 37  (7.68%)
11 OOVs (2.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article728.text
Will force exclusive back-off from OOVs.
Perplexity = 142.66, Entropy = 7.16 bits
Computation based on 416 words.
Number of 3-grams hit = 307  (73.80%)
Number of 2-grams hit = 79  (18.99%)
Number of 1-grams hit = 30  (7.21%)
5 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article729.text
Will force exclusive back-off from OOVs.
Perplexity = 189.32, Entropy = 7.56 bits
Computation based on 437 words.
Number of 3-grams hit = 306  (70.02%)
Number of 2-grams hit = 103  (23.57%)
Number of 1-grams hit = 28  (6.41%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article730.text
Will force exclusive back-off from OOVs.
Perplexity = 98.89, Entropy = 6.63 bits
Computation based on 8423 words.
Number of 3-grams hit = 6636  (78.78%)
Number of 2-grams hit = 1472  (17.48%)
Number of 1-grams hit = 315  (3.74%)
54 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article731.text
Will force exclusive back-off from OOVs.
Perplexity = 75.23, Entropy = 6.23 bits
Computation based on 1450 words.
Number of 3-grams hit = 1190  (82.07%)
Number of 2-grams hit = 218  (15.03%)
Number of 1-grams hit = 42  (2.90%)
6 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article732.text
Will force exclusive back-off from OOVs.
Perplexity = 114.54, Entropy = 6.84 bits
Computation based on 702 words.
Number of 3-grams hit = 539  (76.78%)
Number of 2-grams hit = 134  (19.09%)
Number of 1-grams hit = 29  (4.13%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article733.text
Will force exclusive back-off from OOVs.
Perplexity = 99.33, Entropy = 6.63 bits
Computation based on 559 words.
Number of 3-grams hit = 438  (78.35%)
Number of 2-grams hit = 95  (16.99%)
Number of 1-grams hit = 26  (4.65%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article734.text
Will force exclusive back-off from OOVs.
Perplexity = 375.43, Entropy = 8.55 bits
Computation based on 414 words.
Number of 3-grams hit = 239  (57.73%)
Number of 2-grams hit = 125  (30.19%)
Number of 1-grams hit = 50  (12.08%)
7 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article735.text
Will force exclusive back-off from OOVs.
Perplexity = 101.37, Entropy = 6.66 bits
Computation based on 1178 words.
Number of 3-grams hit = 939  (79.71%)
Number of 2-grams hit = 195  (16.55%)
Number of 1-grams hit = 44  (3.74%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article736.text
Will force exclusive back-off from OOVs.
Perplexity = 133.56, Entropy = 7.06 bits
Computation based on 667 words.
Number of 3-grams hit = 501  (75.11%)
Number of 2-grams hit = 137  (20.54%)
Number of 1-grams hit = 29  (4.35%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article737.text
Will force exclusive back-off from OOVs.
Perplexity = 83.52, Entropy = 6.38 bits
Computation based on 633 words.
Number of 3-grams hit = 521  (82.31%)
Number of 2-grams hit = 90  (14.22%)
Number of 1-grams hit = 22  (3.48%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article738.text
Will force exclusive back-off from OOVs.
Perplexity = 190.03, Entropy = 7.57 bits
Computation based on 1002 words.
Number of 3-grams hit = 683  (68.16%)
Number of 2-grams hit = 267  (26.65%)
Number of 1-grams hit = 52  (5.19%)
9 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article739.text
Will force exclusive back-off from OOVs.
Perplexity = 115.11, Entropy = 6.85 bits
Computation based on 433 words.
Number of 3-grams hit = 322  (74.36%)
Number of 2-grams hit = 84  (19.40%)
Number of 1-grams hit = 27  (6.24%)
7 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article740.text
Will force exclusive back-off from OOVs.
Perplexity = 83.15, Entropy = 6.38 bits
Computation based on 1093 words.
Number of 3-grams hit = 881  (80.60%)
Number of 2-grams hit = 180  (16.47%)
Number of 1-grams hit = 32  (2.93%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article741.text
Will force exclusive back-off from OOVs.
Perplexity = 124.47, Entropy = 6.96 bits
Computation based on 756 words.
Number of 3-grams hit = 563  (74.47%)
Number of 2-grams hit = 160  (21.16%)
Number of 1-grams hit = 33  (4.37%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article742.text
Will force exclusive back-off from OOVs.
Perplexity = 221.15, Entropy = 7.79 bits
Computation based on 609 words.
Number of 3-grams hit = 383  (62.89%)
Number of 2-grams hit = 168  (27.59%)
Number of 1-grams hit = 58  (9.52%)
22 OOVs (3.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article743.text
Will force exclusive back-off from OOVs.
Perplexity = 138.12, Entropy = 7.11 bits
Computation based on 1973 words.
Number of 3-grams hit = 1481  (75.06%)
Number of 2-grams hit = 389  (19.72%)
Number of 1-grams hit = 103  (5.22%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article744.text
Will force exclusive back-off from OOVs.
Perplexity = 87.95, Entropy = 6.46 bits
Computation based on 525 words.
Number of 3-grams hit = 412  (78.48%)
Number of 2-grams hit = 85  (16.19%)
Number of 1-grams hit = 28  (5.33%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article745.text
Will force exclusive back-off from OOVs.
Perplexity = 97.06, Entropy = 6.60 bits
Computation based on 687 words.
Number of 3-grams hit = 524  (76.27%)
Number of 2-grams hit = 130  (18.92%)
Number of 1-grams hit = 33  (4.80%)
6 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article746.text
Will force exclusive back-off from OOVs.
Perplexity = 92.20, Entropy = 6.53 bits
Computation based on 1313 words.
Number of 3-grams hit = 1046  (79.66%)
Number of 2-grams hit = 220  (16.76%)
Number of 1-grams hit = 47  (3.58%)
19 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article747.text
Will force exclusive back-off from OOVs.
Perplexity = 57.05, Entropy = 5.83 bits
Computation based on 518 words.
Number of 3-grams hit = 439  (84.75%)
Number of 2-grams hit = 70  (13.51%)
Number of 1-grams hit = 9  (1.74%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article748.text
Will force exclusive back-off from OOVs.
Perplexity = 165.61, Entropy = 7.37 bits
Computation based on 500 words.
Number of 3-grams hit = 356  (71.20%)
Number of 2-grams hit = 98  (19.60%)
Number of 1-grams hit = 46  (9.20%)
9 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article749.text
Will force exclusive back-off from OOVs.
Perplexity = 110.90, Entropy = 6.79 bits
Computation based on 550 words.
Number of 3-grams hit = 420  (76.36%)
Number of 2-grams hit = 105  (19.09%)
Number of 1-grams hit = 25  (4.55%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article750.text
Will force exclusive back-off from OOVs.
Perplexity = 93.25, Entropy = 6.54 bits
Computation based on 467 words.
Number of 3-grams hit = 382  (81.80%)
Number of 2-grams hit = 64  (13.70%)
Number of 1-grams hit = 21  (4.50%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article751.text
Will force exclusive back-off from OOVs.
Perplexity = 118.04, Entropy = 6.88 bits
Computation based on 2005 words.
Number of 3-grams hit = 1492  (74.41%)
Number of 2-grams hit = 407  (20.30%)
Number of 1-grams hit = 106  (5.29%)
11 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article752.text
Will force exclusive back-off from OOVs.
Perplexity = 101.47, Entropy = 6.66 bits
Computation based on 427 words.
Number of 3-grams hit = 336  (78.69%)
Number of 2-grams hit = 70  (16.39%)
Number of 1-grams hit = 21  (4.92%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article753.text
Will force exclusive back-off from OOVs.
Perplexity = 100.71, Entropy = 6.65 bits
Computation based on 531 words.
Number of 3-grams hit = 413  (77.78%)
Number of 2-grams hit = 97  (18.27%)
Number of 1-grams hit = 21  (3.95%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article754.text
Will force exclusive back-off from OOVs.
Perplexity = 105.20, Entropy = 6.72 bits
Computation based on 1850 words.
Number of 3-grams hit = 1439  (77.78%)
Number of 2-grams hit = 318  (17.19%)
Number of 1-grams hit = 93  (5.03%)
12 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article755.text
Will force exclusive back-off from OOVs.
Perplexity = 126.90, Entropy = 6.99 bits
Computation based on 448 words.
Number of 3-grams hit = 331  (73.88%)
Number of 2-grams hit = 92  (20.54%)
Number of 1-grams hit = 25  (5.58%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article756.text
Will force exclusive back-off from OOVs.
Perplexity = 148.62, Entropy = 7.22 bits
Computation based on 709 words.
Number of 3-grams hit = 519  (73.20%)
Number of 2-grams hit = 161  (22.71%)
Number of 1-grams hit = 29  (4.09%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article757.text
Will force exclusive back-off from OOVs.
Perplexity = 42.82, Entropy = 5.42 bits
Computation based on 506 words.
Number of 3-grams hit = 480  (94.86%)
Number of 2-grams hit = 17  (3.36%)
Number of 1-grams hit = 9  (1.78%)
4 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article758.text
Will force exclusive back-off from OOVs.
Perplexity = 118.68, Entropy = 6.89 bits
Computation based on 2506 words.
Number of 3-grams hit = 1923  (76.74%)
Number of 2-grams hit = 474  (18.91%)
Number of 1-grams hit = 109  (4.35%)
10 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article759.text
Will force exclusive back-off from OOVs.
Perplexity = 88.23, Entropy = 6.46 bits
Computation based on 3315 words.
Number of 3-grams hit = 2672  (80.60%)
Number of 2-grams hit = 556  (16.77%)
Number of 1-grams hit = 87  (2.62%)
7 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article760.text
Will force exclusive back-off from OOVs.
Perplexity = 43.16, Entropy = 5.43 bits
Computation based on 550 words.
Number of 3-grams hit = 520  (94.55%)
Number of 2-grams hit = 18  (3.27%)
Number of 1-grams hit = 12  (2.18%)
6 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article761.text
Will force exclusive back-off from OOVs.
Perplexity = 80.32, Entropy = 6.33 bits
Computation based on 629 words.
Number of 3-grams hit = 512  (81.40%)
Number of 2-grams hit = 93  (14.79%)
Number of 1-grams hit = 24  (3.82%)
6 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article762.text
Will force exclusive back-off from OOVs.
Perplexity = 123.85, Entropy = 6.95 bits
Computation based on 790 words.
Number of 3-grams hit = 602  (76.20%)
Number of 2-grams hit = 144  (18.23%)
Number of 1-grams hit = 44  (5.57%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article763.text
Will force exclusive back-off from OOVs.
Perplexity = 98.98, Entropy = 6.63 bits
Computation based on 722 words.
Number of 3-grams hit = 578  (80.06%)
Number of 2-grams hit = 115  (15.93%)
Number of 1-grams hit = 29  (4.02%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article764.text
Will force exclusive back-off from OOVs.
Perplexity = 169.44, Entropy = 7.40 bits
Computation based on 534 words.
Number of 3-grams hit = 380  (71.16%)
Number of 2-grams hit = 116  (21.72%)
Number of 1-grams hit = 38  (7.12%)
7 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article765.text
Will force exclusive back-off from OOVs.
Perplexity = 232.40, Entropy = 7.86 bits
Computation based on 828 words.
Number of 3-grams hit = 545  (65.82%)
Number of 2-grams hit = 226  (27.29%)
Number of 1-grams hit = 57  (6.88%)
10 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article766.text
Will force exclusive back-off from OOVs.
Perplexity = 105.37, Entropy = 6.72 bits
Computation based on 263 words.
Number of 3-grams hit = 195  (74.14%)
Number of 2-grams hit = 50  (19.01%)
Number of 1-grams hit = 18  (6.84%)
5 OOVs (1.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article767.text
Will force exclusive back-off from OOVs.
Perplexity = 116.81, Entropy = 6.87 bits
Computation based on 4386 words.
Number of 3-grams hit = 3339  (76.13%)
Number of 2-grams hit = 879  (20.04%)
Number of 1-grams hit = 168  (3.83%)
33 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article768.text
Will force exclusive back-off from OOVs.
Perplexity = 81.26, Entropy = 6.34 bits
Computation based on 461 words.
Number of 3-grams hit = 371  (80.48%)
Number of 2-grams hit = 70  (15.18%)
Number of 1-grams hit = 20  (4.34%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article769.text
Will force exclusive back-off from OOVs.
Perplexity = 56.36, Entropy = 5.82 bits
Computation based on 677 words.
Number of 3-grams hit = 586  (86.56%)
Number of 2-grams hit = 80  (11.82%)
Number of 1-grams hit = 11  (1.62%)
4 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article770.text
Will force exclusive back-off from OOVs.
Perplexity = 126.99, Entropy = 6.99 bits
Computation based on 606 words.
Number of 3-grams hit = 452  (74.59%)
Number of 2-grams hit = 118  (19.47%)
Number of 1-grams hit = 36  (5.94%)
4 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article771.text
Will force exclusive back-off from OOVs.
Perplexity = 111.13, Entropy = 6.80 bits
Computation based on 468 words.
Number of 3-grams hit = 363  (77.56%)
Number of 2-grams hit = 90  (19.23%)
Number of 1-grams hit = 15  (3.21%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article772.text
Will force exclusive back-off from OOVs.
Perplexity = 100.04, Entropy = 6.64 bits
Computation based on 541 words.
Number of 3-grams hit = 407  (75.23%)
Number of 2-grams hit = 104  (19.22%)
Number of 1-grams hit = 30  (5.55%)
6 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article773.text
Will force exclusive back-off from OOVs.
Perplexity = 112.25, Entropy = 6.81 bits
Computation based on 2984 words.
Number of 3-grams hit = 2298  (77.01%)
Number of 2-grams hit = 559  (18.73%)
Number of 1-grams hit = 127  (4.26%)
14 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article774.text
Will force exclusive back-off from OOVs.
Perplexity = 84.51, Entropy = 6.40 bits
Computation based on 2009 words.
Number of 3-grams hit = 1629  (81.09%)
Number of 2-grams hit = 319  (15.88%)
Number of 1-grams hit = 61  (3.04%)
13 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article775.text
Will force exclusive back-off from OOVs.
Perplexity = 90.44, Entropy = 6.50 bits
Computation based on 498 words.
Number of 3-grams hit = 391  (78.51%)
Number of 2-grams hit = 86  (17.27%)
Number of 1-grams hit = 21  (4.22%)
7 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article776.text
Will force exclusive back-off from OOVs.
Perplexity = 89.40, Entropy = 6.48 bits
Computation based on 1575 words.
Number of 3-grams hit = 1250  (79.37%)
Number of 2-grams hit = 273  (17.33%)
Number of 1-grams hit = 52  (3.30%)
9 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article777.text
Will force exclusive back-off from OOVs.
Perplexity = 87.78, Entropy = 6.46 bits
Computation based on 9319 words.
Number of 3-grams hit = 7398  (79.39%)
Number of 2-grams hit = 1589  (17.05%)
Number of 1-grams hit = 332  (3.56%)
59 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article778.text
Will force exclusive back-off from OOVs.
Perplexity = 119.07, Entropy = 6.90 bits
Computation based on 503 words.
Number of 3-grams hit = 394  (78.33%)
Number of 2-grams hit = 84  (16.70%)
Number of 1-grams hit = 25  (4.97%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article779.text
Will force exclusive back-off from OOVs.
Perplexity = 109.70, Entropy = 6.78 bits
Computation based on 932 words.
Number of 3-grams hit = 733  (78.65%)
Number of 2-grams hit = 163  (17.49%)
Number of 1-grams hit = 36  (3.86%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article780.text
Will force exclusive back-off from OOVs.
Perplexity = 91.29, Entropy = 6.51 bits
Computation based on 401 words.
Number of 3-grams hit = 319  (79.55%)
Number of 2-grams hit = 66  (16.46%)
Number of 1-grams hit = 16  (3.99%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article781.text
Will force exclusive back-off from OOVs.
Perplexity = 109.36, Entropy = 6.77 bits
Computation based on 732 words.
Number of 3-grams hit = 563  (76.91%)
Number of 2-grams hit = 140  (19.13%)
Number of 1-grams hit = 29  (3.96%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article782.text
Will force exclusive back-off from OOVs.
Perplexity = 98.82, Entropy = 6.63 bits
Computation based on 1362 words.
Number of 3-grams hit = 1087  (79.81%)
Number of 2-grams hit = 224  (16.45%)
Number of 1-grams hit = 51  (3.74%)
1 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article783.text
Will force exclusive back-off from OOVs.
Perplexity = 107.16, Entropy = 6.74 bits
Computation based on 501 words.
Number of 3-grams hit = 393  (78.44%)
Number of 2-grams hit = 88  (17.56%)
Number of 1-grams hit = 20  (3.99%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article784.text
Will force exclusive back-off from OOVs.
Perplexity = 95.25, Entropy = 6.57 bits
Computation based on 1721 words.
Number of 3-grams hit = 1376  (79.95%)
Number of 2-grams hit = 268  (15.57%)
Number of 1-grams hit = 77  (4.47%)
8 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article785.text
Will force exclusive back-off from OOVs.
Perplexity = 96.09, Entropy = 6.59 bits
Computation based on 2613 words.
Number of 3-grams hit = 2058  (78.76%)
Number of 2-grams hit = 442  (16.92%)
Number of 1-grams hit = 113  (4.32%)
30 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article786.text
Will force exclusive back-off from OOVs.
Perplexity = 85.16, Entropy = 6.41 bits
Computation based on 353 words.
Number of 3-grams hit = 283  (80.17%)
Number of 2-grams hit = 63  (17.85%)
Number of 1-grams hit = 7  (1.98%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article787.text
Will force exclusive back-off from OOVs.
Perplexity = 217.03, Entropy = 7.76 bits
Computation based on 472 words.
Number of 3-grams hit = 312  (66.10%)
Number of 2-grams hit = 128  (27.12%)
Number of 1-grams hit = 32  (6.78%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article788.text
Will force exclusive back-off from OOVs.
Perplexity = 55.24, Entropy = 5.79 bits
Computation based on 1453 words.
Number of 3-grams hit = 1274  (87.68%)
Number of 2-grams hit = 162  (11.15%)
Number of 1-grams hit = 17  (1.17%)
3 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article789.text
Will force exclusive back-off from OOVs.
Perplexity = 114.23, Entropy = 6.84 bits
Computation based on 11445 words.
Number of 3-grams hit = 8892  (77.69%)
Number of 2-grams hit = 2015  (17.61%)
Number of 1-grams hit = 538  (4.70%)
47 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article790.text
Will force exclusive back-off from OOVs.
Perplexity = 127.47, Entropy = 6.99 bits
Computation based on 935 words.
Number of 3-grams hit = 703  (75.19%)
Number of 2-grams hit = 182  (19.47%)
Number of 1-grams hit = 50  (5.35%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article791.text
Will force exclusive back-off from OOVs.
Perplexity = 108.64, Entropy = 6.76 bits
Computation based on 696 words.
Number of 3-grams hit = 532  (76.44%)
Number of 2-grams hit = 134  (19.25%)
Number of 1-grams hit = 30  (4.31%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article792.text
Will force exclusive back-off from OOVs.
Perplexity = 116.93, Entropy = 6.87 bits
Computation based on 2871 words.
Number of 3-grams hit = 2194  (76.42%)
Number of 2-grams hit = 530  (18.46%)
Number of 1-grams hit = 147  (5.12%)
13 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article793.text
Will force exclusive back-off from OOVs.
Perplexity = 97.24, Entropy = 6.60 bits
Computation based on 180 words.
Number of 3-grams hit = 140  (77.78%)
Number of 2-grams hit = 32  (17.78%)
Number of 1-grams hit = 8  (4.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article794.text
Will force exclusive back-off from OOVs.
Perplexity = 186.52, Entropy = 7.54 bits
Computation based on 500 words.
Number of 3-grams hit = 335  (67.00%)
Number of 2-grams hit = 132  (26.40%)
Number of 1-grams hit = 33  (6.60%)
4 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article795.text
Will force exclusive back-off from OOVs.
Perplexity = 234.30, Entropy = 7.87 bits
Computation based on 426 words.
Number of 3-grams hit = 295  (69.25%)
Number of 2-grams hit = 94  (22.07%)
Number of 1-grams hit = 37  (8.69%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article796.text
Will force exclusive back-off from OOVs.
Perplexity = 132.84, Entropy = 7.05 bits
Computation based on 1490 words.
Number of 3-grams hit = 1113  (74.70%)
Number of 2-grams hit = 293  (19.66%)
Number of 1-grams hit = 84  (5.64%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article797.text
Will force exclusive back-off from OOVs.
Perplexity = 74.65, Entropy = 6.22 bits
Computation based on 375 words.
Number of 3-grams hit = 316  (84.27%)
Number of 2-grams hit = 50  (13.33%)
Number of 1-grams hit = 9  (2.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article798.text
Will force exclusive back-off from OOVs.
Perplexity = 112.14, Entropy = 6.81 bits
Computation based on 458 words.
Number of 3-grams hit = 352  (76.86%)
Number of 2-grams hit = 89  (19.43%)
Number of 1-grams hit = 17  (3.71%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article799.text
Will force exclusive back-off from OOVs.
Perplexity = 166.10, Entropy = 7.38 bits
Computation based on 741 words.
Number of 3-grams hit = 525  (70.85%)
Number of 2-grams hit = 167  (22.54%)
Number of 1-grams hit = 49  (6.61%)
10 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article800.text
Will force exclusive back-off from OOVs.
Perplexity = 109.29, Entropy = 6.77 bits
Computation based on 431 words.
Number of 3-grams hit = 333  (77.26%)
Number of 2-grams hit = 80  (18.56%)
Number of 1-grams hit = 18  (4.18%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article801.text
Will force exclusive back-off from OOVs.
Perplexity = 76.60, Entropy = 6.26 bits
Computation based on 3087 words.
Number of 3-grams hit = 2546  (82.47%)
Number of 2-grams hit = 467  (15.13%)
Number of 1-grams hit = 74  (2.40%)
28 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article802.text
Will force exclusive back-off from OOVs.
Perplexity = 160.42, Entropy = 7.33 bits
Computation based on 2082 words.
Number of 3-grams hit = 1448  (69.55%)
Number of 2-grams hit = 496  (23.82%)
Number of 1-grams hit = 138  (6.63%)
28 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article803.text
Will force exclusive back-off from OOVs.
Perplexity = 99.82, Entropy = 6.64 bits
Computation based on 4196 words.
Number of 3-grams hit = 3270  (77.93%)
Number of 2-grams hit = 758  (18.06%)
Number of 1-grams hit = 168  (4.00%)
30 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article804.text
Will force exclusive back-off from OOVs.
Perplexity = 286.81, Entropy = 8.16 bits
Computation based on 1431 words.
Number of 3-grams hit = 881  (61.57%)
Number of 2-grams hit = 396  (27.67%)
Number of 1-grams hit = 154  (10.76%)
37 OOVs (2.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article805.text
Will force exclusive back-off from OOVs.
Perplexity = 123.64, Entropy = 6.95 bits
Computation based on 919 words.
Number of 3-grams hit = 703  (76.50%)
Number of 2-grams hit = 174  (18.93%)
Number of 1-grams hit = 42  (4.57%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article806.text
Will force exclusive back-off from OOVs.
Perplexity = 105.05, Entropy = 6.71 bits
Computation based on 3287 words.
Number of 3-grams hit = 2567  (78.10%)
Number of 2-grams hit = 598  (18.19%)
Number of 1-grams hit = 122  (3.71%)
16 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article807.text
Will force exclusive back-off from OOVs.
Perplexity = 82.02, Entropy = 6.36 bits
Computation based on 708 words.
Number of 3-grams hit = 569  (80.37%)
Number of 2-grams hit = 119  (16.81%)
Number of 1-grams hit = 20  (2.82%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article808.text
Will force exclusive back-off from OOVs.
Perplexity = 125.64, Entropy = 6.97 bits
Computation based on 1682 words.
Number of 3-grams hit = 1281  (76.16%)
Number of 2-grams hit = 304  (18.07%)
Number of 1-grams hit = 97  (5.77%)
12 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article809.text
Will force exclusive back-off from OOVs.
Perplexity = 119.74, Entropy = 6.90 bits
Computation based on 2017 words.
Number of 3-grams hit = 1547  (76.70%)
Number of 2-grams hit = 382  (18.94%)
Number of 1-grams hit = 88  (4.36%)
9 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article810.text
Will force exclusive back-off from OOVs.
Perplexity = 100.54, Entropy = 6.65 bits
Computation based on 869 words.
Number of 3-grams hit = 680  (78.25%)
Number of 2-grams hit = 160  (18.41%)
Number of 1-grams hit = 29  (3.34%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article811.text
Will force exclusive back-off from OOVs.
Perplexity = 112.84, Entropy = 6.82 bits
Computation based on 4497 words.
Number of 3-grams hit = 3489  (77.59%)
Number of 2-grams hit = 802  (17.83%)
Number of 1-grams hit = 206  (4.58%)
14 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article812.text
Will force exclusive back-off from OOVs.
Perplexity = 94.94, Entropy = 6.57 bits
Computation based on 3353 words.
Number of 3-grams hit = 2671  (79.66%)
Number of 2-grams hit = 576  (17.18%)
Number of 1-grams hit = 106  (3.16%)
16 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article813.text
Will force exclusive back-off from OOVs.
Perplexity = 201.67, Entropy = 7.66 bits
Computation based on 1357 words.
Number of 3-grams hit = 908  (66.91%)
Number of 2-grams hit = 338  (24.91%)
Number of 1-grams hit = 111  (8.18%)
23 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article814.text
Will force exclusive back-off from OOVs.
Perplexity = 68.31, Entropy = 6.09 bits
Computation based on 851 words.
Number of 3-grams hit = 704  (82.73%)
Number of 2-grams hit = 127  (14.92%)
Number of 1-grams hit = 20  (2.35%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article815.text
Will force exclusive back-off from OOVs.
Perplexity = 108.06, Entropy = 6.76 bits
Computation based on 1264 words.
Number of 3-grams hit = 972  (76.90%)
Number of 2-grams hit = 228  (18.04%)
Number of 1-grams hit = 64  (5.06%)
10 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article816.text
Will force exclusive back-off from OOVs.
Perplexity = 79.32, Entropy = 6.31 bits
Computation based on 1331 words.
Number of 3-grams hit = 1081  (81.22%)
Number of 2-grams hit = 211  (15.85%)
Number of 1-grams hit = 39  (2.93%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article817.text
Will force exclusive back-off from OOVs.
Perplexity = 123.71, Entropy = 6.95 bits
Computation based on 533 words.
Number of 3-grams hit = 408  (76.55%)
Number of 2-grams hit = 100  (18.76%)
Number of 1-grams hit = 25  (4.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article818.text
Will force exclusive back-off from OOVs.
Perplexity = 212.95, Entropy = 7.73 bits
Computation based on 532 words.
Number of 3-grams hit = 348  (65.41%)
Number of 2-grams hit = 119  (22.37%)
Number of 1-grams hit = 65  (12.22%)
21 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article819.text
Will force exclusive back-off from OOVs.
Perplexity = 117.00, Entropy = 6.87 bits
Computation based on 1803 words.
Number of 3-grams hit = 1380  (76.54%)
Number of 2-grams hit = 329  (18.25%)
Number of 1-grams hit = 94  (5.21%)
6 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article820.text
Will force exclusive back-off from OOVs.
Perplexity = 132.09, Entropy = 7.05 bits
Computation based on 816 words.
Number of 3-grams hit = 592  (72.55%)
Number of 2-grams hit = 185  (22.67%)
Number of 1-grams hit = 39  (4.78%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article821.text
Will force exclusive back-off from OOVs.
Perplexity = 72.19, Entropy = 6.17 bits
Computation based on 1100 words.
Number of 3-grams hit = 927  (84.27%)
Number of 2-grams hit = 146  (13.27%)
Number of 1-grams hit = 27  (2.45%)
4 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article822.text
Will force exclusive back-off from OOVs.
Perplexity = 118.70, Entropy = 6.89 bits
Computation based on 2420 words.
Number of 3-grams hit = 1860  (76.86%)
Number of 2-grams hit = 454  (18.76%)
Number of 1-grams hit = 106  (4.38%)
4 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article823.text
Will force exclusive back-off from OOVs.
Perplexity = 114.06, Entropy = 6.83 bits
Computation based on 1307 words.
Number of 3-grams hit = 996  (76.21%)
Number of 2-grams hit = 257  (19.66%)
Number of 1-grams hit = 54  (4.13%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article824.text
Will force exclusive back-off from OOVs.
Perplexity = 199.47, Entropy = 7.64 bits
Computation based on 486 words.
Number of 3-grams hit = 309  (63.58%)
Number of 2-grams hit = 130  (26.75%)
Number of 1-grams hit = 47  (9.67%)
16 OOVs (3.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article825.text
Will force exclusive back-off from OOVs.
Perplexity = 104.01, Entropy = 6.70 bits
Computation based on 571 words.
Number of 3-grams hit = 444  (77.76%)
Number of 2-grams hit = 104  (18.21%)
Number of 1-grams hit = 23  (4.03%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article826.text
Will force exclusive back-off from OOVs.
Perplexity = 94.83, Entropy = 6.57 bits
Computation based on 889 words.
Number of 3-grams hit = 709  (79.75%)
Number of 2-grams hit = 149  (16.76%)
Number of 1-grams hit = 31  (3.49%)
4 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article827.text
Will force exclusive back-off from OOVs.
Perplexity = 164.76, Entropy = 7.36 bits
Computation based on 627 words.
Number of 3-grams hit = 426  (67.94%)
Number of 2-grams hit = 144  (22.97%)
Number of 1-grams hit = 57  (9.09%)
21 OOVs (3.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article828.text
Will force exclusive back-off from OOVs.
Perplexity = 115.07, Entropy = 6.85 bits
Computation based on 11850 words.
Number of 3-grams hit = 9100  (76.79%)
Number of 2-grams hit = 2169  (18.30%)
Number of 1-grams hit = 581  (4.90%)
147 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article829.text
Will force exclusive back-off from OOVs.
Perplexity = 230.74, Entropy = 7.85 bits
Computation based on 531 words.
Number of 3-grams hit = 345  (64.97%)
Number of 2-grams hit = 140  (26.37%)
Number of 1-grams hit = 46  (8.66%)
9 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article830.text
Will force exclusive back-off from OOVs.
Perplexity = 195.14, Entropy = 7.61 bits
Computation based on 2276 words.
Number of 3-grams hit = 1549  (68.06%)
Number of 2-grams hit = 511  (22.45%)
Number of 1-grams hit = 216  (9.49%)
43 OOVs (1.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article831.text
Will force exclusive back-off from OOVs.
Perplexity = 142.34, Entropy = 7.15 bits
Computation based on 1025 words.
Number of 3-grams hit = 758  (73.95%)
Number of 2-grams hit = 203  (19.80%)
Number of 1-grams hit = 64  (6.24%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article832.text
Will force exclusive back-off from OOVs.
Perplexity = 104.98, Entropy = 6.71 bits
Computation based on 419 words.
Number of 3-grams hit = 320  (76.37%)
Number of 2-grams hit = 79  (18.85%)
Number of 1-grams hit = 20  (4.77%)
3 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article833.text
Will force exclusive back-off from OOVs.
Perplexity = 118.44, Entropy = 6.89 bits
Computation based on 826 words.
Number of 3-grams hit = 639  (77.36%)
Number of 2-grams hit = 148  (17.92%)
Number of 1-grams hit = 39  (4.72%)
3 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article834.text
Will force exclusive back-off from OOVs.
Perplexity = 97.65, Entropy = 6.61 bits
Computation based on 881 words.
Number of 3-grams hit = 693  (78.66%)
Number of 2-grams hit = 159  (18.05%)
Number of 1-grams hit = 29  (3.29%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article835.text
Will force exclusive back-off from OOVs.
Perplexity = 120.69, Entropy = 6.92 bits
Computation based on 460 words.
Number of 3-grams hit = 346  (75.22%)
Number of 2-grams hit = 94  (20.43%)
Number of 1-grams hit = 20  (4.35%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article836.text
Will force exclusive back-off from OOVs.
Perplexity = 92.99, Entropy = 6.54 bits
Computation based on 483 words.
Number of 3-grams hit = 374  (77.43%)
Number of 2-grams hit = 88  (18.22%)
Number of 1-grams hit = 21  (4.35%)
8 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article837.text
Will force exclusive back-off from OOVs.
Perplexity = 205.37, Entropy = 7.68 bits
Computation based on 845 words.
Number of 3-grams hit = 550  (65.09%)
Number of 2-grams hit = 225  (26.63%)
Number of 1-grams hit = 70  (8.28%)
7 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article838.text
Will force exclusive back-off from OOVs.
Perplexity = 161.20, Entropy = 7.33 bits
Computation based on 2236 words.
Number of 3-grams hit = 1579  (70.62%)
Number of 2-grams hit = 513  (22.94%)
Number of 1-grams hit = 144  (6.44%)
35 OOVs (1.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article839.text
Will force exclusive back-off from OOVs.
Perplexity = 129.11, Entropy = 7.01 bits
Computation based on 461 words.
Number of 3-grams hit = 344  (74.62%)
Number of 2-grams hit = 101  (21.91%)
Number of 1-grams hit = 16  (3.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article840.text
Will force exclusive back-off from OOVs.
Perplexity = 105.49, Entropy = 6.72 bits
Computation based on 521 words.
Number of 3-grams hit = 401  (76.97%)
Number of 2-grams hit = 95  (18.23%)
Number of 1-grams hit = 25  (4.80%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article841.text
Will force exclusive back-off from OOVs.
Perplexity = 108.90, Entropy = 6.77 bits
Computation based on 916 words.
Number of 3-grams hit = 715  (78.06%)
Number of 2-grams hit = 157  (17.14%)
Number of 1-grams hit = 44  (4.80%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article842.text
Will force exclusive back-off from OOVs.
Perplexity = 131.79, Entropy = 7.04 bits
Computation based on 473 words.
Number of 3-grams hit = 355  (75.05%)
Number of 2-grams hit = 95  (20.08%)
Number of 1-grams hit = 23  (4.86%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article843.text
Will force exclusive back-off from OOVs.
Perplexity = 91.94, Entropy = 6.52 bits
Computation based on 1310 words.
Number of 3-grams hit = 1068  (81.53%)
Number of 2-grams hit = 193  (14.73%)
Number of 1-grams hit = 49  (3.74%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article844.text
Will force exclusive back-off from OOVs.
Perplexity = 109.00, Entropy = 6.77 bits
Computation based on 15758 words.
Number of 3-grams hit = 12269  (77.86%)
Number of 2-grams hit = 2800  (17.77%)
Number of 1-grams hit = 689  (4.37%)
53 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article845.text
Will force exclusive back-off from OOVs.
Perplexity = 103.93, Entropy = 6.70 bits
Computation based on 601 words.
Number of 3-grams hit = 469  (78.04%)
Number of 2-grams hit = 104  (17.30%)
Number of 1-grams hit = 28  (4.66%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article846.text
Will force exclusive back-off from OOVs.
Perplexity = 134.35, Entropy = 7.07 bits
Computation based on 556 words.
Number of 3-grams hit = 407  (73.20%)
Number of 2-grams hit = 111  (19.96%)
Number of 1-grams hit = 38  (6.83%)
8 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article847.text
Will force exclusive back-off from OOVs.
Perplexity = 85.99, Entropy = 6.43 bits
Computation based on 1613 words.
Number of 3-grams hit = 1297  (80.41%)
Number of 2-grams hit = 258  (16.00%)
Number of 1-grams hit = 58  (3.60%)
8 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article848.text
Will force exclusive back-off from OOVs.
Perplexity = 74.57, Entropy = 6.22 bits
Computation based on 366 words.
Number of 3-grams hit = 291  (79.51%)
Number of 2-grams hit = 66  (18.03%)
Number of 1-grams hit = 9  (2.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article849.text
Will force exclusive back-off from OOVs.
Perplexity = 124.92, Entropy = 6.96 bits
Computation based on 1550 words.
Number of 3-grams hit = 1160  (74.84%)
Number of 2-grams hit = 319  (20.58%)
Number of 1-grams hit = 71  (4.58%)
5 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article850.text
Will force exclusive back-off from OOVs.
Perplexity = 114.05, Entropy = 6.83 bits
Computation based on 443 words.
Number of 3-grams hit = 342  (77.20%)
Number of 2-grams hit = 84  (18.96%)
Number of 1-grams hit = 17  (3.84%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article851.text
Will force exclusive back-off from OOVs.
Perplexity = 129.60, Entropy = 7.02 bits
Computation based on 558 words.
Number of 3-grams hit = 432  (77.42%)
Number of 2-grams hit = 96  (17.20%)
Number of 1-grams hit = 30  (5.38%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article852.text
Will force exclusive back-off from OOVs.
Perplexity = 112.06, Entropy = 6.81 bits
Computation based on 622 words.
Number of 3-grams hit = 488  (78.46%)
Number of 2-grams hit = 108  (17.36%)
Number of 1-grams hit = 26  (4.18%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article853.text
Will force exclusive back-off from OOVs.
Perplexity = 107.02, Entropy = 6.74 bits
Computation based on 1476 words.
Number of 3-grams hit = 1138  (77.10%)
Number of 2-grams hit = 257  (17.41%)
Number of 1-grams hit = 81  (5.49%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article854.text
Will force exclusive back-off from OOVs.
Perplexity = 124.59, Entropy = 6.96 bits
Computation based on 2651 words.
Number of 3-grams hit = 2002  (75.52%)
Number of 2-grams hit = 519  (19.58%)
Number of 1-grams hit = 130  (4.90%)
18 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article855.text
Will force exclusive back-off from OOVs.
Perplexity = 73.16, Entropy = 6.19 bits
Computation based on 423 words.
Number of 3-grams hit = 358  (84.63%)
Number of 2-grams hit = 54  (12.77%)
Number of 1-grams hit = 11  (2.60%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article856.text
Will force exclusive back-off from OOVs.
Perplexity = 122.99, Entropy = 6.94 bits
Computation based on 824 words.
Number of 3-grams hit = 606  (73.54%)
Number of 2-grams hit = 164  (19.90%)
Number of 1-grams hit = 54  (6.55%)
9 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article857.text
Will force exclusive back-off from OOVs.
Perplexity = 151.28, Entropy = 7.24 bits
Computation based on 354 words.
Number of 3-grams hit = 242  (68.36%)
Number of 2-grams hit = 90  (25.42%)
Number of 1-grams hit = 22  (6.21%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article858.text
Will force exclusive back-off from OOVs.
Perplexity = 83.03, Entropy = 6.38 bits
Computation based on 237 words.
Number of 3-grams hit = 202  (85.23%)
Number of 2-grams hit = 31  (13.08%)
Number of 1-grams hit = 4  (1.69%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article859.text
Will force exclusive back-off from OOVs.
Perplexity = 200.29, Entropy = 7.65 bits
Computation based on 1095 words.
Number of 3-grams hit = 728  (66.48%)
Number of 2-grams hit = 296  (27.03%)
Number of 1-grams hit = 71  (6.48%)
16 OOVs (1.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article860.text
Will force exclusive back-off from OOVs.
Perplexity = 106.54, Entropy = 6.74 bits
Computation based on 450 words.
Number of 3-grams hit = 349  (77.56%)
Number of 2-grams hit = 85  (18.89%)
Number of 1-grams hit = 16  (3.56%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article861.text
Will force exclusive back-off from OOVs.
Perplexity = 111.16, Entropy = 6.80 bits
Computation based on 393 words.
Number of 3-grams hit = 294  (74.81%)
Number of 2-grams hit = 79  (20.10%)
Number of 1-grams hit = 20  (5.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article862.text
Will force exclusive back-off from OOVs.
Perplexity = 85.06, Entropy = 6.41 bits
Computation based on 452 words.
Number of 3-grams hit = 377  (83.41%)
Number of 2-grams hit = 57  (12.61%)
Number of 1-grams hit = 18  (3.98%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article863.text
Will force exclusive back-off from OOVs.
Perplexity = 106.76, Entropy = 6.74 bits
Computation based on 662 words.
Number of 3-grams hit = 513  (77.49%)
Number of 2-grams hit = 122  (18.43%)
Number of 1-grams hit = 27  (4.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article864.text
Will force exclusive back-off from OOVs.
Perplexity = 369.35, Entropy = 8.53 bits
Computation based on 442 words.
Number of 3-grams hit = 263  (59.50%)
Number of 2-grams hit = 139  (31.45%)
Number of 1-grams hit = 40  (9.05%)
7 OOVs (1.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article865.text
Will force exclusive back-off from OOVs.
Perplexity = 61.60, Entropy = 5.94 bits
Computation based on 6140 words.
Number of 3-grams hit = 5207  (84.80%)
Number of 2-grams hit = 770  (12.54%)
Number of 1-grams hit = 163  (2.65%)
25 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article866.text
Will force exclusive back-off from OOVs.
Perplexity = 205.24, Entropy = 7.68 bits
Computation based on 615 words.
Number of 3-grams hit = 410  (66.67%)
Number of 2-grams hit = 143  (23.25%)
Number of 1-grams hit = 62  (10.08%)
12 OOVs (1.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article867.text
Will force exclusive back-off from OOVs.
Perplexity = 112.58, Entropy = 6.81 bits
Computation based on 341 words.
Number of 3-grams hit = 268  (78.59%)
Number of 2-grams hit = 53  (15.54%)
Number of 1-grams hit = 20  (5.87%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article868.text
Will force exclusive back-off from OOVs.
Perplexity = 102.44, Entropy = 6.68 bits
Computation based on 1652 words.
Number of 3-grams hit = 1303  (78.87%)
Number of 2-grams hit = 277  (16.77%)
Number of 1-grams hit = 72  (4.36%)
5 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article869.text
Will force exclusive back-off from OOVs.
Perplexity = 104.43, Entropy = 6.71 bits
Computation based on 3106 words.
Number of 3-grams hit = 2444  (78.69%)
Number of 2-grams hit = 551  (17.74%)
Number of 1-grams hit = 111  (3.57%)
10 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article870.text
Will force exclusive back-off from OOVs.
Perplexity = 133.09, Entropy = 7.06 bits
Computation based on 768 words.
Number of 3-grams hit = 576  (75.00%)
Number of 2-grams hit = 151  (19.66%)
Number of 1-grams hit = 41  (5.34%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article871.text
Will force exclusive back-off from OOVs.
Perplexity = 130.55, Entropy = 7.03 bits
Computation based on 412 words.
Number of 3-grams hit = 310  (75.24%)
Number of 2-grams hit = 89  (21.60%)
Number of 1-grams hit = 13  (3.16%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article872.text
Will force exclusive back-off from OOVs.
Perplexity = 136.48, Entropy = 7.09 bits
Computation based on 294 words.
Number of 3-grams hit = 213  (72.45%)
Number of 2-grams hit = 71  (24.15%)
Number of 1-grams hit = 10  (3.40%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article873.text
Will force exclusive back-off from OOVs.
Perplexity = 133.86, Entropy = 7.06 bits
Computation based on 930 words.
Number of 3-grams hit = 695  (74.73%)
Number of 2-grams hit = 181  (19.46%)
Number of 1-grams hit = 54  (5.81%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article874.text
Will force exclusive back-off from OOVs.
Perplexity = 107.09, Entropy = 6.74 bits
Computation based on 429 words.
Number of 3-grams hit = 329  (76.69%)
Number of 2-grams hit = 85  (19.81%)
Number of 1-grams hit = 15  (3.50%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article875.text
Will force exclusive back-off from OOVs.
Perplexity = 157.61, Entropy = 7.30 bits
Computation based on 786 words.
Number of 3-grams hit = 553  (70.36%)
Number of 2-grams hit = 176  (22.39%)
Number of 1-grams hit = 57  (7.25%)
7 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article876.text
Will force exclusive back-off from OOVs.
Perplexity = 119.08, Entropy = 6.90 bits
Computation based on 9198 words.
Number of 3-grams hit = 7058  (76.73%)
Number of 2-grams hit = 1704  (18.53%)
Number of 1-grams hit = 436  (4.74%)
33 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article877.text
Will force exclusive back-off from OOVs.
Perplexity = 77.62, Entropy = 6.28 bits
Computation based on 458 words.
Number of 3-grams hit = 373  (81.44%)
Number of 2-grams hit = 64  (13.97%)
Number of 1-grams hit = 21  (4.59%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article878.text
Will force exclusive back-off from OOVs.
Perplexity = 86.32, Entropy = 6.43 bits
Computation based on 333 words.
Number of 3-grams hit = 265  (79.58%)
Number of 2-grams hit = 50  (15.02%)
Number of 1-grams hit = 18  (5.41%)
5 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article879.text
Will force exclusive back-off from OOVs.
Perplexity = 135.60, Entropy = 7.08 bits
Computation based on 539 words.
Number of 3-grams hit = 407  (75.51%)
Number of 2-grams hit = 108  (20.04%)
Number of 1-grams hit = 24  (4.45%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article880.text
Will force exclusive back-off from OOVs.
Perplexity = 106.88, Entropy = 6.74 bits
Computation based on 459 words.
Number of 3-grams hit = 370  (80.61%)
Number of 2-grams hit = 73  (15.90%)
Number of 1-grams hit = 16  (3.49%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article881.text
Will force exclusive back-off from OOVs.
Perplexity = 175.49, Entropy = 7.46 bits
Computation based on 345 words.
Number of 3-grams hit = 245  (71.01%)
Number of 2-grams hit = 83  (24.06%)
Number of 1-grams hit = 17  (4.93%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article882.text
Will force exclusive back-off from OOVs.
Perplexity = 99.73, Entropy = 6.64 bits
Computation based on 591 words.
Number of 3-grams hit = 475  (80.37%)
Number of 2-grams hit = 95  (16.07%)
Number of 1-grams hit = 21  (3.55%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article883.text
Will force exclusive back-off from OOVs.
Perplexity = 91.82, Entropy = 6.52 bits
Computation based on 591 words.
Number of 3-grams hit = 474  (80.20%)
Number of 2-grams hit = 99  (16.75%)
Number of 1-grams hit = 18  (3.05%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article884.text
Will force exclusive back-off from OOVs.
Perplexity = 95.12, Entropy = 6.57 bits
Computation based on 727 words.
Number of 3-grams hit = 561  (77.17%)
Number of 2-grams hit = 141  (19.39%)
Number of 1-grams hit = 25  (3.44%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article885.text
Will force exclusive back-off from OOVs.
Perplexity = 80.06, Entropy = 6.32 bits
Computation based on 482 words.
Number of 3-grams hit = 397  (82.37%)
Number of 2-grams hit = 72  (14.94%)
Number of 1-grams hit = 13  (2.70%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article886.text
Will force exclusive back-off from OOVs.
Perplexity = 113.12, Entropy = 6.82 bits
Computation based on 473 words.
Number of 3-grams hit = 370  (78.22%)
Number of 2-grams hit = 83  (17.55%)
Number of 1-grams hit = 20  (4.23%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article887.text
Will force exclusive back-off from OOVs.
Perplexity = 87.68, Entropy = 6.45 bits
Computation based on 481 words.
Number of 3-grams hit = 392  (81.50%)
Number of 2-grams hit = 80  (16.63%)
Number of 1-grams hit = 9  (1.87%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article888.text
Will force exclusive back-off from OOVs.
Perplexity = 129.67, Entropy = 7.02 bits
Computation based on 1207 words.
Number of 3-grams hit = 904  (74.90%)
Number of 2-grams hit = 239  (19.80%)
Number of 1-grams hit = 64  (5.30%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article889.text
Will force exclusive back-off from OOVs.
Perplexity = 124.04, Entropy = 6.95 bits
Computation based on 4303 words.
Number of 3-grams hit = 3279  (76.20%)
Number of 2-grams hit = 821  (19.08%)
Number of 1-grams hit = 203  (4.72%)
10 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article890.text
Will force exclusive back-off from OOVs.
Perplexity = 91.58, Entropy = 6.52 bits
Computation based on 1038 words.
Number of 3-grams hit = 785  (75.63%)
Number of 2-grams hit = 185  (17.82%)
Number of 1-grams hit = 68  (6.55%)
13 OOVs (1.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article891.text
Will force exclusive back-off from OOVs.
Perplexity = 102.57, Entropy = 6.68 bits
Computation based on 804 words.
Number of 3-grams hit = 628  (78.11%)
Number of 2-grams hit = 136  (16.92%)
Number of 1-grams hit = 40  (4.98%)
4 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article892.text
Will force exclusive back-off from OOVs.
Perplexity = 74.98, Entropy = 6.23 bits
Computation based on 3156 words.
Number of 3-grams hit = 2597  (82.29%)
Number of 2-grams hit = 476  (15.08%)
Number of 1-grams hit = 83  (2.63%)
7 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article893.text
Will force exclusive back-off from OOVs.
Perplexity = 90.74, Entropy = 6.50 bits
Computation based on 625 words.
Number of 3-grams hit = 503  (80.48%)
Number of 2-grams hit = 90  (14.40%)
Number of 1-grams hit = 32  (5.12%)
8 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article894.text
Will force exclusive back-off from OOVs.
Perplexity = 144.98, Entropy = 7.18 bits
Computation based on 743 words.
Number of 3-grams hit = 547  (73.62%)
Number of 2-grams hit = 157  (21.13%)
Number of 1-grams hit = 39  (5.25%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article895.text
Will force exclusive back-off from OOVs.
Perplexity = 110.46, Entropy = 6.79 bits
Computation based on 169 words.
Number of 3-grams hit = 132  (78.11%)
Number of 2-grams hit = 30  (17.75%)
Number of 1-grams hit = 7  (4.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article896.text
Will force exclusive back-off from OOVs.
Perplexity = 125.34, Entropy = 6.97 bits
Computation based on 413 words.
Number of 3-grams hit = 317  (76.76%)
Number of 2-grams hit = 68  (16.46%)
Number of 1-grams hit = 28  (6.78%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article897.text
Will force exclusive back-off from OOVs.
Perplexity = 85.18, Entropy = 6.41 bits
Computation based on 696 words.
Number of 3-grams hit = 556  (79.89%)
Number of 2-grams hit = 117  (16.81%)
Number of 1-grams hit = 23  (3.30%)
4 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article898.text
Will force exclusive back-off from OOVs.
Perplexity = 110.43, Entropy = 6.79 bits
Computation based on 1592 words.
Number of 3-grams hit = 1243  (78.08%)
Number of 2-grams hit = 274  (17.21%)
Number of 1-grams hit = 75  (4.71%)
10 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article899.text
Will force exclusive back-off from OOVs.
Perplexity = 222.75, Entropy = 7.80 bits
Computation based on 199 words.
Number of 3-grams hit = 125  (62.81%)
Number of 2-grams hit = 53  (26.63%)
Number of 1-grams hit = 21  (10.55%)
5 OOVs (2.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article900.text
Will force exclusive back-off from OOVs.
Perplexity = 137.79, Entropy = 7.11 bits
Computation based on 1046 words.
Number of 3-grams hit = 767  (73.33%)
Number of 2-grams hit = 223  (21.32%)
Number of 1-grams hit = 56  (5.35%)
5 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article901.text
Will force exclusive back-off from OOVs.
Perplexity = 168.89, Entropy = 7.40 bits
Computation based on 388 words.
Number of 3-grams hit = 260  (67.01%)
Number of 2-grams hit = 92  (23.71%)
Number of 1-grams hit = 36  (9.28%)
11 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article902.text
Will force exclusive back-off from OOVs.
Perplexity = 100.42, Entropy = 6.65 bits
Computation based on 1995 words.
Number of 3-grams hit = 1561  (78.25%)
Number of 2-grams hit = 362  (18.15%)
Number of 1-grams hit = 72  (3.61%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article903.text
Will force exclusive back-off from OOVs.
Perplexity = 110.16, Entropy = 6.78 bits
Computation based on 1184 words.
Number of 3-grams hit = 909  (76.77%)
Number of 2-grams hit = 223  (18.83%)
Number of 1-grams hit = 52  (4.39%)
5 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article904.text
Will force exclusive back-off from OOVs.
Perplexity = 74.36, Entropy = 6.22 bits
Computation based on 623 words.
Number of 3-grams hit = 518  (83.15%)
Number of 2-grams hit = 94  (15.09%)
Number of 1-grams hit = 11  (1.77%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article905.text
Will force exclusive back-off from OOVs.
Perplexity = 223.71, Entropy = 7.81 bits
Computation based on 354 words.
Number of 3-grams hit = 236  (66.67%)
Number of 2-grams hit = 90  (25.42%)
Number of 1-grams hit = 28  (7.91%)
3 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article906.text
Will force exclusive back-off from OOVs.
Perplexity = 78.38, Entropy = 6.29 bits
Computation based on 429 words.
Number of 3-grams hit = 351  (81.82%)
Number of 2-grams hit = 55  (12.82%)
Number of 1-grams hit = 23  (5.36%)
7 OOVs (1.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article907.text
Will force exclusive back-off from OOVs.
Perplexity = 100.80, Entropy = 6.66 bits
Computation based on 445 words.
Number of 3-grams hit = 353  (79.33%)
Number of 2-grams hit = 75  (16.85%)
Number of 1-grams hit = 17  (3.82%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article908.text
Will force exclusive back-off from OOVs.
Perplexity = 100.05, Entropy = 6.64 bits
Computation based on 524 words.
Number of 3-grams hit = 399  (76.15%)
Number of 2-grams hit = 103  (19.66%)
Number of 1-grams hit = 22  (4.20%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article909.text
Will force exclusive back-off from OOVs.
Perplexity = 190.32, Entropy = 7.57 bits
Computation based on 413 words.
Number of 3-grams hit = 297  (71.91%)
Number of 2-grams hit = 83  (20.10%)
Number of 1-grams hit = 33  (7.99%)
4 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article910.text
Will force exclusive back-off from OOVs.
Perplexity = 103.43, Entropy = 6.69 bits
Computation based on 667 words.
Number of 3-grams hit = 523  (78.41%)
Number of 2-grams hit = 111  (16.64%)
Number of 1-grams hit = 33  (4.95%)
4 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article911.text
Will force exclusive back-off from OOVs.
Perplexity = 103.32, Entropy = 6.69 bits
Computation based on 635 words.
Number of 3-grams hit = 501  (78.90%)
Number of 2-grams hit = 109  (17.17%)
Number of 1-grams hit = 25  (3.94%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article912.text
Will force exclusive back-off from OOVs.
Perplexity = 74.53, Entropy = 6.22 bits
Computation based on 675 words.
Number of 3-grams hit = 559  (82.81%)
Number of 2-grams hit = 95  (14.07%)
Number of 1-grams hit = 21  (3.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article913.text
Will force exclusive back-off from OOVs.
Perplexity = 94.13, Entropy = 6.56 bits
Computation based on 670 words.
Number of 3-grams hit = 536  (80.00%)
Number of 2-grams hit = 110  (16.42%)
Number of 1-grams hit = 24  (3.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article914.text
Will force exclusive back-off from OOVs.
Perplexity = 130.90, Entropy = 7.03 bits
Computation based on 529 words.
Number of 3-grams hit = 381  (72.02%)
Number of 2-grams hit = 119  (22.50%)
Number of 1-grams hit = 29  (5.48%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article915.text
Will force exclusive back-off from OOVs.
Perplexity = 129.36, Entropy = 7.02 bits
Computation based on 1206 words.
Number of 3-grams hit = 917  (76.04%)
Number of 2-grams hit = 236  (19.57%)
Number of 1-grams hit = 53  (4.39%)
5 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article916.text
Will force exclusive back-off from OOVs.
Perplexity = 149.73, Entropy = 7.23 bits
Computation based on 939 words.
Number of 3-grams hit = 683  (72.74%)
Number of 2-grams hit = 202  (21.51%)
Number of 1-grams hit = 54  (5.75%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article917.text
Will force exclusive back-off from OOVs.
Perplexity = 78.75, Entropy = 6.30 bits
Computation based on 1076 words.
Number of 3-grams hit = 870  (80.86%)
Number of 2-grams hit = 163  (15.15%)
Number of 1-grams hit = 43  (4.00%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article918.text
Will force exclusive back-off from OOVs.
Perplexity = 117.99, Entropy = 6.88 bits
Computation based on 482 words.
Number of 3-grams hit = 367  (76.14%)
Number of 2-grams hit = 84  (17.43%)
Number of 1-grams hit = 31  (6.43%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article919.text
Will force exclusive back-off from OOVs.
Perplexity = 115.49, Entropy = 6.85 bits
Computation based on 3300 words.
Number of 3-grams hit = 2555  (77.42%)
Number of 2-grams hit = 597  (18.09%)
Number of 1-grams hit = 148  (4.48%)
15 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article920.text
Will force exclusive back-off from OOVs.
Perplexity = 110.60, Entropy = 6.79 bits
Computation based on 915 words.
Number of 3-grams hit = 719  (78.58%)
Number of 2-grams hit = 151  (16.50%)
Number of 1-grams hit = 45  (4.92%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article921.text
Will force exclusive back-off from OOVs.
Perplexity = 122.85, Entropy = 6.94 bits
Computation based on 795 words.
Number of 3-grams hit = 599  (75.35%)
Number of 2-grams hit = 161  (20.25%)
Number of 1-grams hit = 35  (4.40%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article922.text
Will force exclusive back-off from OOVs.
Perplexity = 92.16, Entropy = 6.53 bits
Computation based on 474 words.
Number of 3-grams hit = 376  (79.32%)
Number of 2-grams hit = 75  (15.82%)
Number of 1-grams hit = 23  (4.85%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article923.text
Will force exclusive back-off from OOVs.
Perplexity = 97.66, Entropy = 6.61 bits
Computation based on 519 words.
Number of 3-grams hit = 414  (79.77%)
Number of 2-grams hit = 89  (17.15%)
Number of 1-grams hit = 16  (3.08%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article924.text
Will force exclusive back-off from OOVs.
Perplexity = 69.33, Entropy = 6.12 bits
Computation based on 341 words.
Number of 3-grams hit = 278  (81.52%)
Number of 2-grams hit = 50  (14.66%)
Number of 1-grams hit = 13  (3.81%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article925.text
Will force exclusive back-off from OOVs.
Perplexity = 78.69, Entropy = 6.30 bits
Computation based on 3813 words.
Number of 3-grams hit = 3155  (82.74%)
Number of 2-grams hit = 546  (14.32%)
Number of 1-grams hit = 112  (2.94%)
21 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article926.text
Will force exclusive back-off from OOVs.
Perplexity = 75.40, Entropy = 6.24 bits
Computation based on 1337 words.
Number of 3-grams hit = 1121  (83.84%)
Number of 2-grams hit = 184  (13.76%)
Number of 1-grams hit = 32  (2.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article927.text
Will force exclusive back-off from OOVs.
Perplexity = 189.57, Entropy = 7.57 bits
Computation based on 403 words.
Number of 3-grams hit = 277  (68.73%)
Number of 2-grams hit = 93  (23.08%)
Number of 1-grams hit = 33  (8.19%)
4 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article928.text
Will force exclusive back-off from OOVs.
Perplexity = 112.80, Entropy = 6.82 bits
Computation based on 483 words.
Number of 3-grams hit = 363  (75.16%)
Number of 2-grams hit = 100  (20.70%)
Number of 1-grams hit = 20  (4.14%)
6 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article929.text
Will force exclusive back-off from OOVs.
Perplexity = 164.51, Entropy = 7.36 bits
Computation based on 502 words.
Number of 3-grams hit = 361  (71.91%)
Number of 2-grams hit = 121  (24.10%)
Number of 1-grams hit = 20  (3.98%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article930.text
Will force exclusive back-off from OOVs.
Perplexity = 205.84, Entropy = 7.69 bits
Computation based on 417 words.
Number of 3-grams hit = 281  (67.39%)
Number of 2-grams hit = 101  (24.22%)
Number of 1-grams hit = 35  (8.39%)
17 OOVs (3.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article931.text
Will force exclusive back-off from OOVs.
Perplexity = 123.82, Entropy = 6.95 bits
Computation based on 595 words.
Number of 3-grams hit = 449  (75.46%)
Number of 2-grams hit = 123  (20.67%)
Number of 1-grams hit = 23  (3.87%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article932.text
Will force exclusive back-off from OOVs.
Perplexity = 133.68, Entropy = 7.06 bits
Computation based on 211 words.
Number of 3-grams hit = 149  (70.62%)
Number of 2-grams hit = 50  (23.70%)
Number of 1-grams hit = 12  (5.69%)
1 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article933.text
Will force exclusive back-off from OOVs.
Perplexity = 104.11, Entropy = 6.70 bits
Computation based on 410 words.
Number of 3-grams hit = 318  (77.56%)
Number of 2-grams hit = 72  (17.56%)
Number of 1-grams hit = 20  (4.88%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article934.text
Will force exclusive back-off from OOVs.
Perplexity = 169.09, Entropy = 7.40 bits
Computation based on 384 words.
Number of 3-grams hit = 246  (64.06%)
Number of 2-grams hit = 106  (27.60%)
Number of 1-grams hit = 32  (8.33%)
7 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article935.text
Will force exclusive back-off from OOVs.
Perplexity = 104.26, Entropy = 6.70 bits
Computation based on 1153 words.
Number of 3-grams hit = 906  (78.58%)
Number of 2-grams hit = 197  (17.09%)
Number of 1-grams hit = 50  (4.34%)
3 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article936.text
Will force exclusive back-off from OOVs.
Perplexity = 175.02, Entropy = 7.45 bits
Computation based on 435 words.
Number of 3-grams hit = 293  (67.36%)
Number of 2-grams hit = 100  (22.99%)
Number of 1-grams hit = 42  (9.66%)
6 OOVs (1.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article937.text
Will force exclusive back-off from OOVs.
Perplexity = 165.15, Entropy = 7.37 bits
Computation based on 228 words.
Number of 3-grams hit = 162  (71.05%)
Number of 2-grams hit = 51  (22.37%)
Number of 1-grams hit = 15  (6.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article938.text
Will force exclusive back-off from OOVs.
Perplexity = 120.15, Entropy = 6.91 bits
Computation based on 1295 words.
Number of 3-grams hit = 982  (75.83%)
Number of 2-grams hit = 245  (18.92%)
Number of 1-grams hit = 68  (5.25%)
5 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article939.text
Will force exclusive back-off from OOVs.
Perplexity = 95.43, Entropy = 6.58 bits
Computation based on 476 words.
Number of 3-grams hit = 371  (77.94%)
Number of 2-grams hit = 84  (17.65%)
Number of 1-grams hit = 21  (4.41%)
5 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article940.text
Will force exclusive back-off from OOVs.
Perplexity = 108.60, Entropy = 6.76 bits
Computation based on 1898 words.
Number of 3-grams hit = 1505  (79.29%)
Number of 2-grams hit = 308  (16.23%)
Number of 1-grams hit = 85  (4.48%)
3 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article941.text
Will force exclusive back-off from OOVs.
Perplexity = 103.89, Entropy = 6.70 bits
Computation based on 1216 words.
Number of 3-grams hit = 938  (77.14%)
Number of 2-grams hit = 225  (18.50%)
Number of 1-grams hit = 53  (4.36%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article942.text
Will force exclusive back-off from OOVs.
Perplexity = 98.47, Entropy = 6.62 bits
Computation based on 524 words.
Number of 3-grams hit = 412  (78.63%)
Number of 2-grams hit = 86  (16.41%)
Number of 1-grams hit = 26  (4.96%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article943.text
Will force exclusive back-off from OOVs.
Perplexity = 100.48, Entropy = 6.65 bits
Computation based on 1395 words.
Number of 3-grams hit = 1108  (79.43%)
Number of 2-grams hit = 232  (16.63%)
Number of 1-grams hit = 55  (3.94%)
10 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article944.text
Will force exclusive back-off from OOVs.
Perplexity = 102.80, Entropy = 6.68 bits
Computation based on 436 words.
Number of 3-grams hit = 347  (79.59%)
Number of 2-grams hit = 75  (17.20%)
Number of 1-grams hit = 14  (3.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article945.text
Will force exclusive back-off from OOVs.
Perplexity = 94.87, Entropy = 6.57 bits
Computation based on 3522 words.
Number of 3-grams hit = 2789  (79.19%)
Number of 2-grams hit = 619  (17.58%)
Number of 1-grams hit = 114  (3.24%)
22 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article946.text
Will force exclusive back-off from OOVs.
Perplexity = 180.21, Entropy = 7.49 bits
Computation based on 551 words.
Number of 3-grams hit = 384  (69.69%)
Number of 2-grams hit = 113  (20.51%)
Number of 1-grams hit = 54  (9.80%)
15 OOVs (2.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article947.text
Will force exclusive back-off from OOVs.
Perplexity = 156.15, Entropy = 7.29 bits
Computation based on 422 words.
Number of 3-grams hit = 298  (70.62%)
Number of 2-grams hit = 97  (22.99%)
Number of 1-grams hit = 27  (6.40%)
7 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article948.text
Will force exclusive back-off from OOVs.
Perplexity = 121.05, Entropy = 6.92 bits
Computation based on 581 words.
Number of 3-grams hit = 442  (76.08%)
Number of 2-grams hit = 113  (19.45%)
Number of 1-grams hit = 26  (4.48%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article949.text
Will force exclusive back-off from OOVs.
Perplexity = 135.27, Entropy = 7.08 bits
Computation based on 1701 words.
Number of 3-grams hit = 1278  (75.13%)
Number of 2-grams hit = 339  (19.93%)
Number of 1-grams hit = 84  (4.94%)
9 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article950.text
Will force exclusive back-off from OOVs.
Perplexity = 265.21, Entropy = 8.05 bits
Computation based on 500 words.
Number of 3-grams hit = 323  (64.60%)
Number of 2-grams hit = 128  (25.60%)
Number of 1-grams hit = 49  (9.80%)
7 OOVs (1.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article951.text
Will force exclusive back-off from OOVs.
Perplexity = 91.47, Entropy = 6.52 bits
Computation based on 564 words.
Number of 3-grams hit = 440  (78.01%)
Number of 2-grams hit = 109  (19.33%)
Number of 1-grams hit = 15  (2.66%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article952.text
Will force exclusive back-off from OOVs.
Perplexity = 121.76, Entropy = 6.93 bits
Computation based on 1283 words.
Number of 3-grams hit = 983  (76.62%)
Number of 2-grams hit = 234  (18.24%)
Number of 1-grams hit = 66  (5.14%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article953.text
Will force exclusive back-off from OOVs.
Perplexity = 108.99, Entropy = 6.77 bits
Computation based on 2117 words.
Number of 3-grams hit = 1630  (77.00%)
Number of 2-grams hit = 406  (19.18%)
Number of 1-grams hit = 81  (3.83%)
18 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article954.text
Will force exclusive back-off from OOVs.
Perplexity = 107.80, Entropy = 6.75 bits
Computation based on 730 words.
Number of 3-grams hit = 567  (77.67%)
Number of 2-grams hit = 131  (17.95%)
Number of 1-grams hit = 32  (4.38%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article955.text
Will force exclusive back-off from OOVs.
Perplexity = 67.49, Entropy = 6.08 bits
Computation based on 1475 words.
Number of 3-grams hit = 1253  (84.95%)
Number of 2-grams hit = 194  (13.15%)
Number of 1-grams hit = 28  (1.90%)
2 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article956.text
Will force exclusive back-off from OOVs.
Perplexity = 228.29, Entropy = 7.83 bits
Computation based on 1026 words.
Number of 3-grams hit = 660  (64.33%)
Number of 2-grams hit = 280  (27.29%)
Number of 1-grams hit = 86  (8.38%)
8 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article957.text
Will force exclusive back-off from OOVs.
Perplexity = 182.92, Entropy = 7.52 bits
Computation based on 302 words.
Number of 3-grams hit = 212  (70.20%)
Number of 2-grams hit = 62  (20.53%)
Number of 1-grams hit = 28  (9.27%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article958.text
Will force exclusive back-off from OOVs.
Perplexity = 120.07, Entropy = 6.91 bits
Computation based on 940 words.
Number of 3-grams hit = 736  (78.30%)
Number of 2-grams hit = 159  (16.91%)
Number of 1-grams hit = 45  (4.79%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article959.text
Will force exclusive back-off from OOVs.
Perplexity = 106.89, Entropy = 6.74 bits
Computation based on 201 words.
Number of 3-grams hit = 150  (74.63%)
Number of 2-grams hit = 40  (19.90%)
Number of 1-grams hit = 11  (5.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article960.text
Will force exclusive back-off from OOVs.
Perplexity = 102.17, Entropy = 6.67 bits
Computation based on 605 words.
Number of 3-grams hit = 483  (79.83%)
Number of 2-grams hit = 100  (16.53%)
Number of 1-grams hit = 22  (3.64%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article961.text
Will force exclusive back-off from OOVs.
Perplexity = 151.11, Entropy = 7.24 bits
Computation based on 475 words.
Number of 3-grams hit = 328  (69.05%)
Number of 2-grams hit = 113  (23.79%)
Number of 1-grams hit = 34  (7.16%)
17 OOVs (3.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article962.text
Will force exclusive back-off from OOVs.
Perplexity = 89.85, Entropy = 6.49 bits
Computation based on 2235 words.
Number of 3-grams hit = 1808  (80.89%)
Number of 2-grams hit = 361  (16.15%)
Number of 1-grams hit = 66  (2.95%)
1 OOVs (0.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article963.text
Will force exclusive back-off from OOVs.
Perplexity = 92.91, Entropy = 6.54 bits
Computation based on 2012 words.
Number of 3-grams hit = 1609  (79.97%)
Number of 2-grams hit = 323  (16.05%)
Number of 1-grams hit = 80  (3.98%)
5 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article964.text
Will force exclusive back-off from OOVs.
Perplexity = 88.42, Entropy = 6.47 bits
Computation based on 1010 words.
Number of 3-grams hit = 816  (80.79%)
Number of 2-grams hit = 167  (16.53%)
Number of 1-grams hit = 27  (2.67%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article965.text
Will force exclusive back-off from OOVs.
Perplexity = 137.82, Entropy = 7.11 bits
Computation based on 643 words.
Number of 3-grams hit = 470  (73.09%)
Number of 2-grams hit = 129  (20.06%)
Number of 1-grams hit = 44  (6.84%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article966.text
Will force exclusive back-off from OOVs.
Perplexity = 150.13, Entropy = 7.23 bits
Computation based on 789 words.
Number of 3-grams hit = 584  (74.02%)
Number of 2-grams hit = 161  (20.41%)
Number of 1-grams hit = 44  (5.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article967.text
Will force exclusive back-off from OOVs.
Perplexity = 102.23, Entropy = 6.68 bits
Computation based on 388 words.
Number of 3-grams hit = 305  (78.61%)
Number of 2-grams hit = 67  (17.27%)
Number of 1-grams hit = 16  (4.12%)
3 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article968.text
Will force exclusive back-off from OOVs.
Perplexity = 64.78, Entropy = 6.02 bits
Computation based on 1057 words.
Number of 3-grams hit = 896  (84.77%)
Number of 2-grams hit = 135  (12.77%)
Number of 1-grams hit = 26  (2.46%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article969.text
Will force exclusive back-off from OOVs.
Perplexity = 116.89, Entropy = 6.87 bits
Computation based on 719 words.
Number of 3-grams hit = 530  (73.71%)
Number of 2-grams hit = 150  (20.86%)
Number of 1-grams hit = 39  (5.42%)
7 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article970.text
Will force exclusive back-off from OOVs.
Perplexity = 105.99, Entropy = 6.73 bits
Computation based on 739 words.
Number of 3-grams hit = 575  (77.81%)
Number of 2-grams hit = 126  (17.05%)
Number of 1-grams hit = 38  (5.14%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article971.text
Will force exclusive back-off from OOVs.
Perplexity = 116.99, Entropy = 6.87 bits
Computation based on 468 words.
Number of 3-grams hit = 361  (77.14%)
Number of 2-grams hit = 84  (17.95%)
Number of 1-grams hit = 23  (4.91%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article972.text
Will force exclusive back-off from OOVs.
Perplexity = 113.94, Entropy = 6.83 bits
Computation based on 1855 words.
Number of 3-grams hit = 1438  (77.52%)
Number of 2-grams hit = 332  (17.90%)
Number of 1-grams hit = 85  (4.58%)
7 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article973.text
Will force exclusive back-off from OOVs.
Perplexity = 89.62, Entropy = 6.49 bits
Computation based on 605 words.
Number of 3-grams hit = 489  (80.83%)
Number of 2-grams hit = 99  (16.36%)
Number of 1-grams hit = 17  (2.81%)
6 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article974.text
Will force exclusive back-off from OOVs.
Perplexity = 147.37, Entropy = 7.20 bits
Computation based on 942 words.
Number of 3-grams hit = 685  (72.72%)
Number of 2-grams hit = 200  (21.23%)
Number of 1-grams hit = 57  (6.05%)
9 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article975.text
Will force exclusive back-off from OOVs.
Perplexity = 103.67, Entropy = 6.70 bits
Computation based on 353 words.
Number of 3-grams hit = 272  (77.05%)
Number of 2-grams hit = 63  (17.85%)
Number of 1-grams hit = 18  (5.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article976.text
Will force exclusive back-off from OOVs.
Perplexity = 242.10, Entropy = 7.92 bits
Computation based on 347 words.
Number of 3-grams hit = 205  (59.08%)
Number of 2-grams hit = 108  (31.12%)
Number of 1-grams hit = 34  (9.80%)
13 OOVs (3.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article977.text
Will force exclusive back-off from OOVs.
Perplexity = 80.16, Entropy = 6.32 bits
Computation based on 857 words.
Number of 3-grams hit = 694  (80.98%)
Number of 2-grams hit = 131  (15.29%)
Number of 1-grams hit = 32  (3.73%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article978.text
Will force exclusive back-off from OOVs.
Perplexity = 116.06, Entropy = 6.86 bits
Computation based on 464 words.
Number of 3-grams hit = 342  (73.71%)
Number of 2-grams hit = 92  (19.83%)
Number of 1-grams hit = 30  (6.47%)
10 OOVs (2.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article979.text
Will force exclusive back-off from OOVs.
Perplexity = 81.34, Entropy = 6.35 bits
Computation based on 1554 words.
Number of 3-grams hit = 1243  (79.99%)
Number of 2-grams hit = 260  (16.73%)
Number of 1-grams hit = 51  (3.28%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article980.text
Will force exclusive back-off from OOVs.
Perplexity = 138.92, Entropy = 7.12 bits
Computation based on 598 words.
Number of 3-grams hit = 426  (71.24%)
Number of 2-grams hit = 129  (21.57%)
Number of 1-grams hit = 43  (7.19%)
13 OOVs (2.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article981.text
Will force exclusive back-off from OOVs.
Perplexity = 141.92, Entropy = 7.15 bits
Computation based on 345 words.
Number of 3-grams hit = 251  (72.75%)
Number of 2-grams hit = 76  (22.03%)
Number of 1-grams hit = 18  (5.22%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article982.text
Will force exclusive back-off from OOVs.
Perplexity = 95.57, Entropy = 6.58 bits
Computation based on 825 words.
Number of 3-grams hit = 623  (75.52%)
Number of 2-grams hit = 167  (20.24%)
Number of 1-grams hit = 35  (4.24%)
12 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article983.text
Will force exclusive back-off from OOVs.
Perplexity = 124.92, Entropy = 6.96 bits
Computation based on 468 words.
Number of 3-grams hit = 354  (75.64%)
Number of 2-grams hit = 86  (18.38%)
Number of 1-grams hit = 28  (5.98%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article984.text
Will force exclusive back-off from OOVs.
Perplexity = 63.05, Entropy = 5.98 bits
Computation based on 458 words.
Number of 3-grams hit = 382  (83.41%)
Number of 2-grams hit = 66  (14.41%)
Number of 1-grams hit = 10  (2.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article985.text
Will force exclusive back-off from OOVs.
Perplexity = 129.84, Entropy = 7.02 bits
Computation based on 1233 words.
Number of 3-grams hit = 918  (74.45%)
Number of 2-grams hit = 253  (20.52%)
Number of 1-grams hit = 62  (5.03%)
13 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article986.text
Will force exclusive back-off from OOVs.
Perplexity = 97.13, Entropy = 6.60 bits
Computation based on 269 words.
Number of 3-grams hit = 212  (78.81%)
Number of 2-grams hit = 48  (17.84%)
Number of 1-grams hit = 9  (3.35%)
2 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article987.text
Will force exclusive back-off from OOVs.
Perplexity = 69.62, Entropy = 6.12 bits
Computation based on 1737 words.
Number of 3-grams hit = 1437  (82.73%)
Number of 2-grams hit = 268  (15.43%)
Number of 1-grams hit = 32  (1.84%)
7 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article988.text
Will force exclusive back-off from OOVs.
Perplexity = 119.02, Entropy = 6.90 bits
Computation based on 1241 words.
Number of 3-grams hit = 969  (78.08%)
Number of 2-grams hit = 208  (16.76%)
Number of 1-grams hit = 64  (5.16%)
8 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article989.text
Will force exclusive back-off from OOVs.
Perplexity = 69.95, Entropy = 6.13 bits
Computation based on 2270 words.
Number of 3-grams hit = 1903  (83.83%)
Number of 2-grams hit = 300  (13.22%)
Number of 1-grams hit = 67  (2.95%)
9 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article990.text
Will force exclusive back-off from OOVs.
Perplexity = 60.94, Entropy = 5.93 bits
Computation based on 653 words.
Number of 3-grams hit = 543  (83.15%)
Number of 2-grams hit = 87  (13.32%)
Number of 1-grams hit = 23  (3.52%)
7 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article991.text
Will force exclusive back-off from OOVs.
Perplexity = 96.55, Entropy = 6.59 bits
Computation based on 1540 words.
Number of 3-grams hit = 1212  (78.70%)
Number of 2-grams hit = 272  (17.66%)
Number of 1-grams hit = 56  (3.64%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article992.text
Will force exclusive back-off from OOVs.
Perplexity = 178.60, Entropy = 7.48 bits
Computation based on 510 words.
Number of 3-grams hit = 331  (64.90%)
Number of 2-grams hit = 126  (24.71%)
Number of 1-grams hit = 53  (10.39%)
25 OOVs (4.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article993.text
Will force exclusive back-off from OOVs.
Perplexity = 113.62, Entropy = 6.83 bits
Computation based on 2457 words.
Number of 3-grams hit = 1905  (77.53%)
Number of 2-grams hit = 438  (17.83%)
Number of 1-grams hit = 114  (4.64%)
9 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article994.text
Will force exclusive back-off from OOVs.
Perplexity = 98.30, Entropy = 6.62 bits
Computation based on 477 words.
Number of 3-grams hit = 381  (79.87%)
Number of 2-grams hit = 74  (15.51%)
Number of 1-grams hit = 22  (4.61%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article995.text
Will force exclusive back-off from OOVs.
Perplexity = 198.37, Entropy = 7.63 bits
Computation based on 7311 words.
Number of 3-grams hit = 4907  (67.12%)
Number of 2-grams hit = 1793  (24.52%)
Number of 1-grams hit = 611  (8.36%)
246 OOVs (3.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article996.text
Will force exclusive back-off from OOVs.
Perplexity = 136.31, Entropy = 7.09 bits
Computation based on 580 words.
Number of 3-grams hit = 432  (74.48%)
Number of 2-grams hit = 120  (20.69%)
Number of 1-grams hit = 28  (4.83%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article997.text
Will force exclusive back-off from OOVs.
Perplexity = 118.14, Entropy = 6.88 bits
Computation based on 5318 words.
Number of 3-grams hit = 4050  (76.16%)
Number of 2-grams hit = 1021  (19.20%)
Number of 1-grams hit = 247  (4.64%)
13 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article998.text
Will force exclusive back-off from OOVs.
Perplexity = 89.29, Entropy = 6.48 bits
Computation based on 523 words.
Number of 3-grams hit = 422  (80.69%)
Number of 2-grams hit = 83  (15.87%)
Number of 1-grams hit = 18  (3.44%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article999.text
Will force exclusive back-off from OOVs.
Perplexity = 77.59, Entropy = 6.28 bits
Computation based on 468 words.
Number of 3-grams hit = 385  (82.26%)
Number of 2-grams hit = 67  (14.32%)
Number of 1-grams hit = 16  (3.42%)
4 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : 