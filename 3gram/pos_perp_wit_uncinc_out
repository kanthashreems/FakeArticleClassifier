evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle0.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1366 words.
Number of 3-grams hit = 1364  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle1.out
Will force inclusive back-off from OOVs.
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1620 words.
Number of 3-grams hit = 1618  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle2.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 591 words.
Number of 3-grams hit = 589  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle3.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 661 words.
Number of 3-grams hit = 659  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle4.out
Will force inclusive back-off from OOVs.
Perplexity = 2.98, Entropy = 1.58 bits
Computation based on 425 words.
Number of 3-grams hit = 423  (99.53%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle5.out
Will force inclusive back-off from OOVs.
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 957 words.
Number of 3-grams hit = 954  (99.69%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle6.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 317 words.
Number of 3-grams hit = 315  (99.37%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle7.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 665 words.
Number of 3-grams hit = 663  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle8.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 528 words.
Number of 3-grams hit = 526  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle9.out
Will force inclusive back-off from OOVs.
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 345 words.
Number of 3-grams hit = 343  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle10.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 327 words.
Number of 3-grams hit = 325  (99.39%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle11.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 325 words.
Number of 3-grams hit = 323  (99.38%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle12.out
Will force inclusive back-off from OOVs.
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 326 words.
Number of 3-grams hit = 324  (99.39%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle13.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 488 words.
Number of 3-grams hit = 485  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle14.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 523 words.
Number of 3-grams hit = 521  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle15.out
Will force inclusive back-off from OOVs.
Perplexity = 3.24, Entropy = 1.69 bits
Computation based on 543 words.
Number of 3-grams hit = 540  (99.45%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle16.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 400 words.
Number of 3-grams hit = 398  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle17.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 722 words.
Number of 3-grams hit = 719  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle18.out
Will force inclusive back-off from OOVs.
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 445 words.
Number of 3-grams hit = 443  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle19.out
Will force inclusive back-off from OOVs.
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 560 words.
Number of 3-grams hit = 557  (99.46%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle20.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 386 words.
Number of 3-grams hit = 384  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle21.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 500 words.
Number of 3-grams hit = 498  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle22.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 387 words.
Number of 3-grams hit = 385  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle23.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 378 words.
Number of 3-grams hit = 376  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle24.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 599 words.
Number of 3-grams hit = 597  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle25.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 413 words.
Number of 3-grams hit = 411  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle26.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1649 words.
Number of 3-grams hit = 1647  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle27.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 424 words.
Number of 3-grams hit = 422  (99.53%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle28.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1511 words.
Number of 3-grams hit = 1507  (99.74%)
Number of 2-grams hit = 3  (0.20%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle29.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 346 words.
Number of 3-grams hit = 344  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle30.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 523 words.
Number of 3-grams hit = 521  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle31.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 481 words.
Number of 3-grams hit = 479  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle32.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 516 words.
Number of 3-grams hit = 514  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle33.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 476 words.
Number of 3-grams hit = 474  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle34.out
Will force inclusive back-off from OOVs.
Perplexity = 3.00, Entropy = 1.59 bits
Computation based on 326 words.
Number of 3-grams hit = 324  (99.39%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle35.out
Will force inclusive back-off from OOVs.
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 402 words.
Number of 3-grams hit = 400  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle36.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1088 words.
Number of 3-grams hit = 1086  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle37.out
Will force inclusive back-off from OOVs.
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 604 words.
Number of 3-grams hit = 601  (99.50%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle38.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 444 words.
Number of 3-grams hit = 442  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle39.out
Will force inclusive back-off from OOVs.
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 2871 words.
Number of 3-grams hit = 2868  (99.90%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle40.out
Will force inclusive back-off from OOVs.
Perplexity = 4.18, Entropy = 2.06 bits
Computation based on 442 words.
Number of 3-grams hit = 439  (99.32%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle41.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1060 words.
Number of 3-grams hit = 1057  (99.72%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle42.out
Will force inclusive back-off from OOVs.
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 591 words.
Number of 3-grams hit = 588  (99.49%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle43.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 490 words.
Number of 3-grams hit = 487  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle44.out
Will force inclusive back-off from OOVs.
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 549 words.
Number of 3-grams hit = 547  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle45.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 502 words.
Number of 3-grams hit = 500  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle46.out
Will force inclusive back-off from OOVs.
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 436 words.
Number of 3-grams hit = 434  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle47.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 2914 words.
Number of 3-grams hit = 2911  (99.90%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle48.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 400 words.
Number of 3-grams hit = 396  (99.00%)
Number of 2-grams hit = 3  (0.75%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle49.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 395 words.
Number of 3-grams hit = 393  (99.49%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle50.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 563 words.
Number of 3-grams hit = 560  (99.47%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle51.out
Will force inclusive back-off from OOVs.
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 1184 words.
Number of 3-grams hit = 1182  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle52.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 401 words.
Number of 3-grams hit = 398  (99.25%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle53.out
Will force inclusive back-off from OOVs.
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 521 words.
Number of 3-grams hit = 519  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle54.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1201 words.
Number of 3-grams hit = 1199  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle55.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 578 words.
Number of 3-grams hit = 576  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle56.out
Will force inclusive back-off from OOVs.
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 988 words.
Number of 3-grams hit = 984  (99.60%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle57.out
Will force inclusive back-off from OOVs.
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1691 words.
Number of 3-grams hit = 1688  (99.82%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle58.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 416 words.
Number of 3-grams hit = 414  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle59.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 231 words.
Number of 3-grams hit = 228  (98.70%)
Number of 2-grams hit = 2  (0.87%)
Number of 1-grams hit = 1  (0.43%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle60.out
Will force inclusive back-off from OOVs.
Perplexity = 3.03, Entropy = 1.60 bits
Computation based on 354 words.
Number of 3-grams hit = 352  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle61.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 530 words.
Number of 3-grams hit = 528  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle62.out
Will force inclusive back-off from OOVs.
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 1398 words.
Number of 3-grams hit = 1395  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle63.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1066 words.
Number of 3-grams hit = 1064  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle64.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 4550 words.
Number of 3-grams hit = 4543  (99.85%)
Number of 2-grams hit = 6  (0.13%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle65.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 697 words.
Number of 3-grams hit = 694  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle66.out
Will force inclusive back-off from OOVs.
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 514 words.
Number of 3-grams hit = 512  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle67.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 484 words.
Number of 3-grams hit = 482  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle68.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1206 words.
Number of 3-grams hit = 1203  (99.75%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle69.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 407 words.
Number of 3-grams hit = 405  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle70.out
Will force inclusive back-off from OOVs.
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1071 words.
Number of 3-grams hit = 1067  (99.63%)
Number of 2-grams hit = 3  (0.28%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle71.out
Will force inclusive back-off from OOVs.
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 591 words.
Number of 3-grams hit = 588  (99.49%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle72.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 1423 words.
Number of 3-grams hit = 1420  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle73.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 566 words.
Number of 3-grams hit = 564  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle74.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 901 words.
Number of 3-grams hit = 899  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle75.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1288 words.
Number of 3-grams hit = 1286  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle76.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 1515 words.
Number of 3-grams hit = 1512  (99.80%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle77.out
Will force inclusive back-off from OOVs.
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 322 words.
Number of 3-grams hit = 318  (98.76%)
Number of 2-grams hit = 3  (0.93%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle78.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1223 words.
Number of 3-grams hit = 1218  (99.59%)
Number of 2-grams hit = 4  (0.33%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle79.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1163 words.
Number of 3-grams hit = 1159  (99.66%)
Number of 2-grams hit = 3  (0.26%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle80.out
Will force inclusive back-off from OOVs.
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 412 words.
Number of 3-grams hit = 410  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle81.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 4418 words.
Number of 3-grams hit = 4413  (99.89%)
Number of 2-grams hit = 4  (0.09%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle82.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 510 words.
Number of 3-grams hit = 508  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle83.out
Will force inclusive back-off from OOVs.
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 476 words.
Number of 3-grams hit = 472  (99.16%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 2  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle84.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 431 words.
Number of 3-grams hit = 429  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle85.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1317 words.
Number of 3-grams hit = 1314  (99.77%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle86.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 309 words.
Number of 3-grams hit = 307  (99.35%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle87.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 676 words.
Number of 3-grams hit = 672  (99.41%)
Number of 2-grams hit = 3  (0.44%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle88.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 1583 words.
Number of 3-grams hit = 1579  (99.75%)
Number of 2-grams hit = 3  (0.19%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle89.out
Will force inclusive back-off from OOVs.
Perplexity = 4.32, Entropy = 2.11 bits
Computation based on 414 words.
Number of 3-grams hit = 411  (99.28%)
Number of 2-grams hit = 2  (0.48%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle90.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 555 words.
Number of 3-grams hit = 553  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle91.out
Will force inclusive back-off from OOVs.
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 371 words.
Number of 3-grams hit = 369  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle92.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1134 words.
Number of 3-grams hit = 1132  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle93.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 745 words.
Number of 3-grams hit = 743  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle94.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1395 words.
Number of 3-grams hit = 1391  (99.71%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle95.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 546 words.
Number of 3-grams hit = 544  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle96.out
Will force inclusive back-off from OOVs.
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 439 words.
Number of 3-grams hit = 436  (99.32%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle97.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 601 words.
Number of 3-grams hit = 598  (99.50%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle98.out
Will force inclusive back-off from OOVs.
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 391 words.
Number of 3-grams hit = 388  (99.23%)
Number of 2-grams hit = 2  (0.51%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle99.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 497 words.
Number of 3-grams hit = 495  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle100.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 513 words.
Number of 3-grams hit = 511  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle101.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 554 words.
Number of 3-grams hit = 552  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle102.out
Will force inclusive back-off from OOVs.
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 448 words.
Number of 3-grams hit = 446  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle103.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 536 words.
Number of 3-grams hit = 534  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle104.out
Will force inclusive back-off from OOVs.
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 6983 words.
Number of 3-grams hit = 6973  (99.86%)
Number of 2-grams hit = 9  (0.13%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle105.out
Will force inclusive back-off from OOVs.
Perplexity = 4.42, Entropy = 2.14 bits
Computation based on 485 words.
Number of 3-grams hit = 483  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle106.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 875 words.
Number of 3-grams hit = 873  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle107.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 529 words.
Number of 3-grams hit = 526  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle108.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 568 words.
Number of 3-grams hit = 565  (99.47%)
Number of 2-grams hit = 2  (0.35%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle109.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1223 words.
Number of 3-grams hit = 1220  (99.75%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle110.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 981 words.
Number of 3-grams hit = 979  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle111.out
Will force inclusive back-off from OOVs.
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 1359 words.
Number of 3-grams hit = 1357  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle112.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 355 words.
Number of 3-grams hit = 353  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle113.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 6382 words.
Number of 3-grams hit = 6370  (99.81%)
Number of 2-grams hit = 11  (0.17%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle114.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 552 words.
Number of 3-grams hit = 550  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle115.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 698 words.
Number of 3-grams hit = 696  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle116.out
Will force inclusive back-off from OOVs.
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 1647 words.
Number of 3-grams hit = 1641  (99.64%)
Number of 2-grams hit = 5  (0.30%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle117.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 557 words.
Number of 3-grams hit = 554  (99.46%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle118.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 667 words.
Number of 3-grams hit = 665  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle119.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 424 words.
Number of 3-grams hit = 422  (99.53%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle120.out
Will force inclusive back-off from OOVs.
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 608 words.
Number of 3-grams hit = 603  (99.18%)
Number of 2-grams hit = 4  (0.66%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle121.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 7778 words.
Number of 3-grams hit = 7772  (99.92%)
Number of 2-grams hit = 5  (0.06%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle122.out
Will force inclusive back-off from OOVs.
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 353 words.
Number of 3-grams hit = 351  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle123.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 553 words.
Number of 3-grams hit = 551  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle124.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 873 words.
Number of 3-grams hit = 871  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle125.out
Will force inclusive back-off from OOVs.
Perplexity = 3.26, Entropy = 1.71 bits
Computation based on 525 words.
Number of 3-grams hit = 523  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle126.out
Will force inclusive back-off from OOVs.
Perplexity = 3.83, Entropy = 1.94 bits
Computation based on 587 words.
Number of 3-grams hit = 585  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle127.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1001 words.
Number of 3-grams hit = 998  (99.70%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle128.out
Will force inclusive back-off from OOVs.
Perplexity = 4.29, Entropy = 2.10 bits
Computation based on 462 words.
Number of 3-grams hit = 460  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle129.out
Will force inclusive back-off from OOVs.
Perplexity = 4.15, Entropy = 2.05 bits
Computation based on 511 words.
Number of 3-grams hit = 509  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle130.out
Will force inclusive back-off from OOVs.
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 1659 words.
Number of 3-grams hit = 1657  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle131.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 7398 words.
Number of 3-grams hit = 7393  (99.93%)
Number of 2-grams hit = 4  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle132.out
Will force inclusive back-off from OOVs.
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 1222 words.
Number of 3-grams hit = 1220  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle133.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 262 words.
Number of 3-grams hit = 260  (99.24%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle134.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 539 words.
Number of 3-grams hit = 537  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle135.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 521 words.
Number of 3-grams hit = 519  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle136.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 567 words.
Number of 3-grams hit = 565  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle137.out
Will force inclusive back-off from OOVs.
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 361 words.
Number of 3-grams hit = 358  (99.17%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle138.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 342 words.
Number of 3-grams hit = 340  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle139.out
Will force inclusive back-off from OOVs.
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 298 words.
Number of 3-grams hit = 296  (99.33%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle140.out
Will force inclusive back-off from OOVs.
Perplexity = 3.85, Entropy = 1.94 bits
Computation based on 400 words.
Number of 3-grams hit = 398  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle141.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 467 words.
Number of 3-grams hit = 464  (99.36%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle142.out
Will force inclusive back-off from OOVs.
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 758 words.
Number of 3-grams hit = 756  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle143.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 412 words.
Number of 3-grams hit = 410  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle144.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 541 words.
Number of 3-grams hit = 539  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle145.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 2107 words.
Number of 3-grams hit = 2105  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle146.out
Will force inclusive back-off from OOVs.
Perplexity = 4.13, Entropy = 2.05 bits
Computation based on 262 words.
Number of 3-grams hit = 258  (98.47%)
Number of 2-grams hit = 3  (1.15%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle147.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 714 words.
Number of 3-grams hit = 711  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle148.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 1886 words.
Number of 3-grams hit = 1884  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle149.out
Will force inclusive back-off from OOVs.
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 612 words.
Number of 3-grams hit = 610  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle150.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 686 words.
Number of 3-grams hit = 683  (99.56%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle151.out
Will force inclusive back-off from OOVs.
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 188 words.
Number of 3-grams hit = 185  (98.40%)
Number of 2-grams hit = 2  (1.06%)
Number of 1-grams hit = 1  (0.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle152.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 1984 words.
Number of 3-grams hit = 1981  (99.85%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle153.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 629 words.
Number of 3-grams hit = 626  (99.52%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle154.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1160 words.
Number of 3-grams hit = 1158  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle155.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 695 words.
Number of 3-grams hit = 692  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle156.out
Will force inclusive back-off from OOVs.
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 1549 words.
Number of 3-grams hit = 1544  (99.68%)
Number of 2-grams hit = 4  (0.26%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle157.out
Will force inclusive back-off from OOVs.
Perplexity = 4.17, Entropy = 2.06 bits
Computation based on 431 words.
Number of 3-grams hit = 428  (99.30%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle158.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 623 words.
Number of 3-grams hit = 620  (99.52%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle159.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1362 words.
Number of 3-grams hit = 1355  (99.49%)
Number of 2-grams hit = 6  (0.44%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle160.out
Will force inclusive back-off from OOVs.
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 584 words.
Number of 3-grams hit = 582  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle161.out
Will force inclusive back-off from OOVs.
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 211 words.
Number of 3-grams hit = 209  (99.05%)
Number of 2-grams hit = 1  (0.47%)
Number of 1-grams hit = 1  (0.47%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle162.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1245 words.
Number of 3-grams hit = 1242  (99.76%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle163.out
Will force inclusive back-off from OOVs.
Perplexity = 2.96, Entropy = 1.56 bits
Computation based on 752 words.
Number of 3-grams hit = 750  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle164.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1948 words.
Number of 3-grams hit = 1945  (99.85%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle165.out
Will force inclusive back-off from OOVs.
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 593 words.
Number of 3-grams hit = 591  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle166.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 555 words.
Number of 3-grams hit = 551  (99.28%)
Number of 2-grams hit = 3  (0.54%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle167.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1260 words.
Number of 3-grams hit = 1256  (99.68%)
Number of 2-grams hit = 3  (0.24%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle168.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 571 words.
Number of 3-grams hit = 569  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle169.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 551 words.
Number of 3-grams hit = 549  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle170.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1235 words.
Number of 3-grams hit = 1233  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle171.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 656 words.
Number of 3-grams hit = 653  (99.54%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle172.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 734 words.
Number of 3-grams hit = 731  (99.59%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle173.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1110 words.
Number of 3-grams hit = 1106  (99.64%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle174.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 493 words.
Number of 3-grams hit = 491  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle175.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 486 words.
Number of 3-grams hit = 483  (99.38%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle176.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 800 words.
Number of 3-grams hit = 797  (99.62%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle177.out
Will force inclusive back-off from OOVs.
Perplexity = 4.03, Entropy = 2.01 bits
Computation based on 557 words.
Number of 3-grams hit = 555  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle178.out
Will force inclusive back-off from OOVs.
Perplexity = 4.04, Entropy = 2.01 bits
Computation based on 183 words.
Number of 3-grams hit = 180  (98.36%)
Number of 2-grams hit = 2  (1.09%)
Number of 1-grams hit = 1  (0.55%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle179.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 491 words.
Number of 3-grams hit = 488  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle180.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 473 words.
Number of 3-grams hit = 471  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle181.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 955 words.
Number of 3-grams hit = 953  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle182.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 407 words.
Number of 3-grams hit = 404  (99.26%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle183.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 504 words.
Number of 3-grams hit = 502  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle184.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 5958 words.
Number of 3-grams hit = 5954  (99.93%)
Number of 2-grams hit = 3  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle185.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1070 words.
Number of 3-grams hit = 1068  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle186.out
Will force inclusive back-off from OOVs.
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 529 words.
Number of 3-grams hit = 527  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle187.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 3137 words.
Number of 3-grams hit = 3135  (99.94%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle188.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 729 words.
Number of 3-grams hit = 727  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle189.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 580 words.
Number of 3-grams hit = 577  (99.48%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle190.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 421 words.
Number of 3-grams hit = 419  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle191.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 5916 words.
Number of 3-grams hit = 5909  (99.88%)
Number of 2-grams hit = 4  (0.07%)
Number of 1-grams hit = 3  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle192.out
Will force inclusive back-off from OOVs.
Perplexity = 2.78, Entropy = 1.48 bits
Computation based on 818 words.
Number of 3-grams hit = 816  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle193.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 170 words.
Number of 3-grams hit = 168  (98.82%)
Number of 2-grams hit = 1  (0.59%)
Number of 1-grams hit = 1  (0.59%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle194.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 661 words.
Number of 3-grams hit = 659  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle195.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 864 words.
Number of 3-grams hit = 862  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle196.out
Will force inclusive back-off from OOVs.
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 730 words.
Number of 3-grams hit = 728  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle197.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 251 words.
Number of 3-grams hit = 249  (99.20%)
Number of 2-grams hit = 1  (0.40%)
Number of 1-grams hit = 1  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle198.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 768 words.
Number of 3-grams hit = 765  (99.61%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle199.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 4967 words.
Number of 3-grams hit = 4963  (99.92%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle200.out
Will force inclusive back-off from OOVs.
Perplexity = 3.33, Entropy = 1.73 bits
Computation based on 702 words.
Number of 3-grams hit = 700  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle201.out
Will force inclusive back-off from OOVs.
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 507 words.
Number of 3-grams hit = 505  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle202.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 1262 words.
Number of 3-grams hit = 1259  (99.76%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle203.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 5293 words.
Number of 3-grams hit = 5289  (99.92%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle204.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 5882 words.
Number of 3-grams hit = 5878  (99.93%)
Number of 2-grams hit = 3  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle205.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 603 words.
Number of 3-grams hit = 601  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle206.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 636 words.
Number of 3-grams hit = 633  (99.53%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle207.out
Will force inclusive back-off from OOVs.
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 779 words.
Number of 3-grams hit = 775  (99.49%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle208.out
Will force inclusive back-off from OOVs.
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 951 words.
Number of 3-grams hit = 947  (99.58%)
Number of 2-grams hit = 3  (0.32%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle209.out
Will force inclusive back-off from OOVs.
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 283 words.
Number of 3-grams hit = 281  (99.29%)
Number of 2-grams hit = 1  (0.35%)
Number of 1-grams hit = 1  (0.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle210.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 886 words.
Number of 3-grams hit = 884  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle211.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 504 words.
Number of 3-grams hit = 502  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle212.out
Will force inclusive back-off from OOVs.
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 449 words.
Number of 3-grams hit = 447  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle213.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.76 bits
Computation based on 695 words.
Number of 3-grams hit = 692  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle214.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 509 words.
Number of 3-grams hit = 505  (99.21%)
Number of 2-grams hit = 3  (0.59%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle215.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 742 words.
Number of 3-grams hit = 738  (99.46%)
Number of 2-grams hit = 3  (0.40%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle216.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 525 words.
Number of 3-grams hit = 522  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle217.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 323 words.
Number of 3-grams hit = 320  (99.07%)
Number of 2-grams hit = 2  (0.62%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle218.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 792 words.
Number of 3-grams hit = 790  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle219.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 439 words.
Number of 3-grams hit = 436  (99.32%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle220.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 4706 words.
Number of 3-grams hit = 4702  (99.92%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle221.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 1255 words.
Number of 3-grams hit = 1253  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle222.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 4839 words.
Number of 3-grams hit = 4831  (99.83%)
Number of 2-grams hit = 7  (0.14%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle223.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 895 words.
Number of 3-grams hit = 893  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle224.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 406 words.
Number of 3-grams hit = 404  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle225.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 777 words.
Number of 3-grams hit = 773  (99.49%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle226.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1314 words.
Number of 3-grams hit = 1312  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle227.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 1017 words.
Number of 3-grams hit = 1014  (99.71%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle228.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 851 words.
Number of 3-grams hit = 849  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle229.out
Will force inclusive back-off from OOVs.
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 518 words.
Number of 3-grams hit = 515  (99.42%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle230.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 337 words.
Number of 3-grams hit = 335  (99.41%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle231.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 850 words.
Number of 3-grams hit = 847  (99.65%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle232.out
Will force inclusive back-off from OOVs.
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 391 words.
Number of 3-grams hit = 389  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle233.out
Will force inclusive back-off from OOVs.
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 734 words.
Number of 3-grams hit = 732  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle234.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1446 words.
Number of 3-grams hit = 1441  (99.65%)
Number of 2-grams hit = 4  (0.28%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle235.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 379 words.
Number of 3-grams hit = 376  (99.21%)
Number of 2-grams hit = 2  (0.53%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle236.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 801 words.
Number of 3-grams hit = 799  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle237.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 423 words.
Number of 3-grams hit = 420  (99.29%)
Number of 2-grams hit = 2  (0.47%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle238.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 339 words.
Number of 3-grams hit = 337  (99.41%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle239.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 530 words.
Number of 3-grams hit = 528  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle240.out
Will force inclusive back-off from OOVs.
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 350 words.
Number of 3-grams hit = 348  (99.43%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle241.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 334 words.
Number of 3-grams hit = 330  (98.80%)
Number of 2-grams hit = 3  (0.90%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle242.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.72 bits
Computation based on 319 words.
Number of 3-grams hit = 317  (99.37%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle243.out
Will force inclusive back-off from OOVs.
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 813 words.
Number of 3-grams hit = 810  (99.63%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle244.out
Will force inclusive back-off from OOVs.
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 457 words.
Number of 3-grams hit = 455  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle245.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 5533 words.
Number of 3-grams hit = 5526  (99.87%)
Number of 2-grams hit = 6  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle246.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 304 words.
Number of 3-grams hit = 302  (99.34%)
Number of 2-grams hit = 1  (0.33%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle247.out
Will force inclusive back-off from OOVs.
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 492 words.
Number of 3-grams hit = 490  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle248.out
Will force inclusive back-off from OOVs.
Perplexity = 3.96, Entropy = 1.99 bits
Computation based on 388 words.
Number of 3-grams hit = 386  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle249.out
Will force inclusive back-off from OOVs.
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 928 words.
Number of 3-grams hit = 926  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle250.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 5273 words.
Number of 3-grams hit = 5266  (99.87%)
Number of 2-grams hit = 6  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle251.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 5439 words.
Number of 3-grams hit = 5433  (99.89%)
Number of 2-grams hit = 5  (0.09%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle252.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 579 words.
Number of 3-grams hit = 576  (99.48%)
Number of 2-grams hit = 2  (0.35%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle253.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.78 bits
Computation based on 5348 words.
Number of 3-grams hit = 5339  (99.83%)
Number of 2-grams hit = 8  (0.15%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle254.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 836 words.
Number of 3-grams hit = 834  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle255.out
Will force inclusive back-off from OOVs.
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 526 words.
Number of 3-grams hit = 523  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle256.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 837 words.
Number of 3-grams hit = 831  (99.28%)
Number of 2-grams hit = 5  (0.60%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle257.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 6431 words.
Number of 3-grams hit = 6418  (99.80%)
Number of 2-grams hit = 10  (0.16%)
Number of 1-grams hit = 3  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle258.out
Will force inclusive back-off from OOVs.
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 444 words.
Number of 3-grams hit = 442  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle259.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 764 words.
Number of 3-grams hit = 762  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle260.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 7634 words.
Number of 3-grams hit = 7629  (99.93%)
Number of 2-grams hit = 4  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle261.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 717 words.
Number of 3-grams hit = 714  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle262.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 721 words.
Number of 3-grams hit = 718  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle263.out
Will force inclusive back-off from OOVs.
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 411 words.
Number of 3-grams hit = 409  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle264.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.75 bits
Computation based on 857 words.
Number of 3-grams hit = 855  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle265.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 602 words.
Number of 3-grams hit = 600  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle266.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 468 words.
Number of 3-grams hit = 466  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle267.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 6787 words.
Number of 3-grams hit = 6779  (99.88%)
Number of 2-grams hit = 7  (0.10%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle268.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 6349 words.
Number of 3-grams hit = 6341  (99.87%)
Number of 2-grams hit = 7  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle269.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 457 words.
Number of 3-grams hit = 455  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle270.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 404 words.
Number of 3-grams hit = 401  (99.26%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle271.out
Will force inclusive back-off from OOVs.
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 7909 words.
Number of 3-grams hit = 7905  (99.95%)
Number of 2-grams hit = 3  (0.04%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle272.out
Will force inclusive back-off from OOVs.
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 4304 words.
Number of 3-grams hit = 4301  (99.93%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle273.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 683 words.
Number of 3-grams hit = 680  (99.56%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle274.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 704 words.
Number of 3-grams hit = 702  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle275.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 2091 words.
Number of 3-grams hit = 2089  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle276.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 506 words.
Number of 3-grams hit = 504  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle277.out
Will force inclusive back-off from OOVs.
Perplexity = 3.17, Entropy = 1.66 bits
Computation based on 835 words.
Number of 3-grams hit = 833  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle278.out
Will force inclusive back-off from OOVs.
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 856 words.
Number of 3-grams hit = 852  (99.53%)
Number of 2-grams hit = 3  (0.35%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle279.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 446 words.
Number of 3-grams hit = 444  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle280.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1844 words.
Number of 3-grams hit = 1842  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle281.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 610 words.
Number of 3-grams hit = 608  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle282.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 779 words.
Number of 3-grams hit = 777  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle283.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1294 words.
Number of 3-grams hit = 1290  (99.69%)
Number of 2-grams hit = 3  (0.23%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle284.out
Will force inclusive back-off from OOVs.
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 1757 words.
Number of 3-grams hit = 1752  (99.72%)
Number of 2-grams hit = 4  (0.23%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle285.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1159 words.
Number of 3-grams hit = 1155  (99.65%)
Number of 2-grams hit = 3  (0.26%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle286.out
Will force inclusive back-off from OOVs.
Perplexity = 4.33, Entropy = 2.12 bits
Computation based on 485 words.
Number of 3-grams hit = 482  (99.38%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle287.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 1778 words.
Number of 3-grams hit = 1776  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle288.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 635 words.
Number of 3-grams hit = 632  (99.53%)
Number of 2-grams hit = 2  (0.31%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle289.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 774 words.
Number of 3-grams hit = 771  (99.61%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle290.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1396 words.
Number of 3-grams hit = 1392  (99.71%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle291.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 552 words.
Number of 3-grams hit = 550  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle292.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 575 words.
Number of 3-grams hit = 573  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle293.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 593 words.
Number of 3-grams hit = 591  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle294.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 2240 words.
Number of 3-grams hit = 2238  (99.91%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle295.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 644 words.
Number of 3-grams hit = 642  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle296.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1670 words.
Number of 3-grams hit = 1668  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle297.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 550 words.
Number of 3-grams hit = 547  (99.45%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle298.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 3737 words.
Number of 3-grams hit = 3733  (99.89%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle299.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 3001 words.
Number of 3-grams hit = 2999  (99.93%)
Number of 2-grams hit = 1  (0.03%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle300.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 2733 words.
Number of 3-grams hit = 2728  (99.82%)
Number of 2-grams hit = 4  (0.15%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle301.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 2305 words.
Number of 3-grams hit = 2300  (99.78%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 2  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle302.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 698 words.
Number of 3-grams hit = 695  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle303.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 941 words.
Number of 3-grams hit = 937  (99.57%)
Number of 2-grams hit = 3  (0.32%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle304.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 446 words.
Number of 3-grams hit = 443  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle305.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 331 words.
Number of 3-grams hit = 329  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle306.out
Will force inclusive back-off from OOVs.
Perplexity = 2.87, Entropy = 1.52 bits
Computation based on 635 words.
Number of 3-grams hit = 633  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle307.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 557 words.
Number of 3-grams hit = 555  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle308.out
Will force inclusive back-off from OOVs.
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 417 words.
Number of 3-grams hit = 415  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle309.out
Will force inclusive back-off from OOVs.
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 265 words.
Number of 3-grams hit = 263  (99.25%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle310.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 522 words.
Number of 3-grams hit = 520  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle311.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 497 words.
Number of 3-grams hit = 495  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle312.out
Will force inclusive back-off from OOVs.
Perplexity = 2.95, Entropy = 1.56 bits
Computation based on 651 words.
Number of 3-grams hit = 649  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle313.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 512 words.
Number of 3-grams hit = 509  (99.41%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle314.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 3615 words.
Number of 3-grams hit = 3611  (99.89%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle315.out
Will force inclusive back-off from OOVs.
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1186 words.
Number of 3-grams hit = 1184  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle316.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 363 words.
Number of 3-grams hit = 360  (99.17%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle317.out
Will force inclusive back-off from OOVs.
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 675 words.
Number of 3-grams hit = 673  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle318.out
Will force inclusive back-off from OOVs.
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 3867 words.
Number of 3-grams hit = 3862  (99.87%)
Number of 2-grams hit = 4  (0.10%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle319.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 641 words.
Number of 3-grams hit = 639  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle320.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 405 words.
Number of 3-grams hit = 403  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle321.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1172 words.
Number of 3-grams hit = 1169  (99.74%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle322.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 1851 words.
Number of 3-grams hit = 1846  (99.73%)
Number of 2-grams hit = 4  (0.22%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle323.out
Will force inclusive back-off from OOVs.
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 877 words.
Number of 3-grams hit = 874  (99.66%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle324.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1808 words.
Number of 3-grams hit = 1806  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle325.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1207 words.
Number of 3-grams hit = 1203  (99.67%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle326.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 655 words.
Number of 3-grams hit = 653  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle327.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 1223 words.
Number of 3-grams hit = 1221  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle328.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 277 words.
Number of 3-grams hit = 275  (99.28%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle329.out
Will force inclusive back-off from OOVs.
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 445 words.
Number of 3-grams hit = 442  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle330.out
Will force inclusive back-off from OOVs.
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 1141 words.
Number of 3-grams hit = 1133  (99.30%)
Number of 2-grams hit = 6  (0.53%)
Number of 1-grams hit = 2  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle331.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 2113 words.
Number of 3-grams hit = 2110  (99.86%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle332.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 626 words.
Number of 3-grams hit = 624  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle333.out
Will force inclusive back-off from OOVs.
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 1432 words.
Number of 3-grams hit = 1429  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle334.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1057 words.
Number of 3-grams hit = 1055  (99.81%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle335.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1850 words.
Number of 3-grams hit = 1847  (99.84%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle336.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 1239 words.
Number of 3-grams hit = 1235  (99.68%)
Number of 2-grams hit = 3  (0.24%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle337.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 438 words.
Number of 3-grams hit = 436  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle338.out
Will force inclusive back-off from OOVs.
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 3804 words.
Number of 3-grams hit = 3797  (99.82%)
Number of 2-grams hit = 6  (0.16%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle339.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 408 words.
Number of 3-grams hit = 405  (99.26%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle340.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1243 words.
Number of 3-grams hit = 1240  (99.76%)
Number of 2-grams hit = 2  (0.16%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle341.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1498 words.
Number of 3-grams hit = 1494  (99.73%)
Number of 2-grams hit = 3  (0.20%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle342.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 1604 words.
Number of 3-grams hit = 1600  (99.75%)
Number of 2-grams hit = 3  (0.19%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle343.out
Will force inclusive back-off from OOVs.
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 522 words.
Number of 3-grams hit = 519  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle344.out
Will force inclusive back-off from OOVs.
Perplexity = 3.05, Entropy = 1.61 bits
Computation based on 620 words.
Number of 3-grams hit = 618  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle345.out
Will force inclusive back-off from OOVs.
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 3421 words.
Number of 3-grams hit = 3417  (99.88%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle346.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 1725 words.
Number of 3-grams hit = 1723  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle347.out
Will force inclusive back-off from OOVs.
Perplexity = 3.04, Entropy = 1.60 bits
Computation based on 690 words.
Number of 3-grams hit = 688  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle348.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 2288 words.
Number of 3-grams hit = 2286  (99.91%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle349.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 2145 words.
Number of 3-grams hit = 2143  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle350.out
Will force inclusive back-off from OOVs.
Perplexity = 4.04, Entropy = 2.02 bits
Computation based on 400 words.
Number of 3-grams hit = 397  (99.25%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle351.out
Will force inclusive back-off from OOVs.
Perplexity = 3.05, Entropy = 1.61 bits
Computation based on 464 words.
Number of 3-grams hit = 462  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle352.out
Will force inclusive back-off from OOVs.
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 488 words.
Number of 3-grams hit = 486  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle353.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 471 words.
Number of 3-grams hit = 469  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle354.out
Will force inclusive back-off from OOVs.
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 551 words.
Number of 3-grams hit = 548  (99.46%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle355.out
Will force inclusive back-off from OOVs.
Perplexity = 4.02, Entropy = 2.01 bits
Computation based on 448 words.
Number of 3-grams hit = 445  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle356.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 743 words.
Number of 3-grams hit = 741  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle357.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 834 words.
Number of 3-grams hit = 830  (99.52%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle358.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1334 words.
Number of 3-grams hit = 1332  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle359.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1380 words.
Number of 3-grams hit = 1378  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle360.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 760 words.
Number of 3-grams hit = 757  (99.61%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle361.out
Will force inclusive back-off from OOVs.
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 1178 words.
Number of 3-grams hit = 1175  (99.75%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle362.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1543 words.
Number of 3-grams hit = 1540  (99.81%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle363.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1251 words.
Number of 3-grams hit = 1249  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle364.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 345 words.
Number of 3-grams hit = 343  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle365.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 430 words.
Number of 3-grams hit = 428  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle366.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 390 words.
Number of 3-grams hit = 388  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle367.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1013 words.
Number of 3-grams hit = 1010  (99.70%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle368.out
Will force inclusive back-off from OOVs.
Perplexity = 4.59, Entropy = 2.20 bits
Computation based on 345 words.
Number of 3-grams hit = 343  (99.42%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle369.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 352 words.
Number of 3-grams hit = 350  (99.43%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle370.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1781 words.
Number of 3-grams hit = 1778  (99.83%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle371.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 280 words.
Number of 3-grams hit = 278  (99.29%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle372.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 456 words.
Number of 3-grams hit = 453  (99.34%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle373.out
Will force inclusive back-off from OOVs.
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 546 words.
Number of 3-grams hit = 544  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle374.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1668 words.
Number of 3-grams hit = 1666  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle375.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1770 words.
Number of 3-grams hit = 1768  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle376.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 621 words.
Number of 3-grams hit = 618  (99.52%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle377.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 809 words.
Number of 3-grams hit = 807  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle378.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 671 words.
Number of 3-grams hit = 669  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle379.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 2784 words.
Number of 3-grams hit = 2777  (99.75%)
Number of 2-grams hit = 5  (0.18%)
Number of 1-grams hit = 2  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle380.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 377 words.
Number of 3-grams hit = 375  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle381.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 700 words.
Number of 3-grams hit = 698  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle382.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1415 words.
Number of 3-grams hit = 1408  (99.51%)
Number of 2-grams hit = 6  (0.42%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle383.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1120 words.
Number of 3-grams hit = 1118  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle384.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1098 words.
Number of 3-grams hit = 1096  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle385.out
Will force inclusive back-off from OOVs.
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 2797 words.
Number of 3-grams hit = 2794  (99.89%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle386.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 442 words.
Number of 3-grams hit = 439  (99.32%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle387.out
Will force inclusive back-off from OOVs.
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 1354 words.
Number of 3-grams hit = 1350  (99.70%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle388.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 4692 words.
Number of 3-grams hit = 4690  (99.96%)
Number of 2-grams hit = 1  (0.02%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle389.out
Will force inclusive back-off from OOVs.
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 1197 words.
Number of 3-grams hit = 1193  (99.67%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle390.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 1281 words.
Number of 3-grams hit = 1279  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle391.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 3885 words.
Number of 3-grams hit = 3878  (99.82%)
Number of 2-grams hit = 5  (0.13%)
Number of 1-grams hit = 2  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle392.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 256 words.
Number of 3-grams hit = 254  (99.22%)
Number of 2-grams hit = 1  (0.39%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle393.out
Will force inclusive back-off from OOVs.
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 508 words.
Number of 3-grams hit = 506  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle394.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 417 words.
Number of 3-grams hit = 415  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle395.out
Will force inclusive back-off from OOVs.
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 332 words.
Number of 3-grams hit = 329  (99.10%)
Number of 2-grams hit = 2  (0.60%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle396.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 3378 words.
Number of 3-grams hit = 3374  (99.88%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle397.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 4002 words.
Number of 3-grams hit = 3999  (99.93%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle398.out
Will force inclusive back-off from OOVs.
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 878 words.
Number of 3-grams hit = 876  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle399.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 3969 words.
Number of 3-grams hit = 3961  (99.80%)
Number of 2-grams hit = 6  (0.15%)
Number of 1-grams hit = 2  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle400.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 634 words.
Number of 3-grams hit = 632  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle401.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 721 words.
Number of 3-grams hit = 716  (99.31%)
Number of 2-grams hit = 4  (0.55%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle402.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.81 bits
Computation based on 3978 words.
Number of 3-grams hit = 3972  (99.85%)
Number of 2-grams hit = 5  (0.13%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle403.out
Will force inclusive back-off from OOVs.
Perplexity = 2.99, Entropy = 1.58 bits
Computation based on 668 words.
Number of 3-grams hit = 666  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle404.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 228 words.
Number of 3-grams hit = 226  (99.12%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle405.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 172 words.
Number of 3-grams hit = 170  (98.84%)
Number of 2-grams hit = 1  (0.58%)
Number of 1-grams hit = 1  (0.58%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle406.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1046 words.
Number of 3-grams hit = 1044  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle407.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 755 words.
Number of 3-grams hit = 752  (99.60%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle408.out
Will force inclusive back-off from OOVs.
Perplexity = 4.30, Entropy = 2.11 bits
Computation based on 469 words.
Number of 3-grams hit = 466  (99.36%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle409.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 595 words.
Number of 3-grams hit = 593  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle410.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 261 words.
Number of 3-grams hit = 258  (98.85%)
Number of 2-grams hit = 2  (0.77%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle411.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 465 words.
Number of 3-grams hit = 463  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle412.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 4945 words.
Number of 3-grams hit = 4936  (99.82%)
Number of 2-grams hit = 8  (0.16%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle413.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 4175 words.
Number of 3-grams hit = 4167  (99.81%)
Number of 2-grams hit = 7  (0.17%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle414.out
Will force inclusive back-off from OOVs.
Perplexity = 3.33, Entropy = 1.74 bits
Computation based on 430 words.
Number of 3-grams hit = 428  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle415.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 509 words.
Number of 3-grams hit = 507  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle416.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 635 words.
Number of 3-grams hit = 633  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle417.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 947 words.
Number of 3-grams hit = 944  (99.68%)
Number of 2-grams hit = 2  (0.21%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle418.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 863 words.
Number of 3-grams hit = 861  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle419.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 581 words.
Number of 3-grams hit = 578  (99.48%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle420.out
Will force inclusive back-off from OOVs.
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 667 words.
Number of 3-grams hit = 665  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle421.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 893 words.
Number of 3-grams hit = 891  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle422.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 602 words.
Number of 3-grams hit = 600  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle423.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 652 words.
Number of 3-grams hit = 650  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle424.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 582 words.
Number of 3-grams hit = 580  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle425.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 385 words.
Number of 3-grams hit = 383  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle426.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 537 words.
Number of 3-grams hit = 535  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle427.out
Will force inclusive back-off from OOVs.
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 844 words.
Number of 3-grams hit = 842  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle428.out
Will force inclusive back-off from OOVs.
Perplexity = 2.96, Entropy = 1.56 bits
Computation based on 680 words.
Number of 3-grams hit = 678  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle429.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 812 words.
Number of 3-grams hit = 809  (99.63%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle430.out
Will force inclusive back-off from OOVs.
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 699 words.
Number of 3-grams hit = 697  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle431.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 612 words.
Number of 3-grams hit = 607  (99.18%)
Number of 2-grams hit = 4  (0.65%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle432.out
Will force inclusive back-off from OOVs.
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 388 words.
Number of 3-grams hit = 385  (99.23%)
Number of 2-grams hit = 2  (0.52%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle433.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 1055 words.
Number of 3-grams hit = 1051  (99.62%)
Number of 2-grams hit = 3  (0.28%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle434.out
Will force inclusive back-off from OOVs.
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 414 words.
Number of 3-grams hit = 412  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle435.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 797 words.
Number of 3-grams hit = 795  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle436.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 585 words.
Number of 3-grams hit = 583  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle437.out
Will force inclusive back-off from OOVs.
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 1077 words.
Number of 3-grams hit = 1074  (99.72%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle438.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 318 words.
Number of 3-grams hit = 315  (99.06%)
Number of 2-grams hit = 2  (0.63%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle439.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 569 words.
Number of 3-grams hit = 567  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle440.out
Will force inclusive back-off from OOVs.
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 753 words.
Number of 3-grams hit = 751  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle441.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 439 words.
Number of 3-grams hit = 436  (99.32%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle442.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 11076 words.
Number of 3-grams hit = 11068  (99.93%)
Number of 2-grams hit = 7  (0.06%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle443.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 9530 words.
Number of 3-grams hit = 9524  (99.94%)
Number of 2-grams hit = 5  (0.05%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle444.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 703 words.
Number of 3-grams hit = 701  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle445.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 556 words.
Number of 3-grams hit = 553  (99.46%)
Number of 2-grams hit = 2  (0.36%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle446.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 969 words.
Number of 3-grams hit = 967  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle447.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 787 words.
Number of 3-grams hit = 785  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle448.out
Will force inclusive back-off from OOVs.
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 346 words.
Number of 3-grams hit = 343  (99.13%)
Number of 2-grams hit = 2  (0.58%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle449.out
Will force inclusive back-off from OOVs.
Perplexity = 2.89, Entropy = 1.53 bits
Computation based on 520 words.
Number of 3-grams hit = 518  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle450.out
Will force inclusive back-off from OOVs.
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 386 words.
Number of 3-grams hit = 384  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle451.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 615 words.
Number of 3-grams hit = 613  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle452.out
Will force inclusive back-off from OOVs.
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 262 words.
Number of 3-grams hit = 260  (99.24%)
Number of 2-grams hit = 1  (0.38%)
Number of 1-grams hit = 1  (0.38%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle453.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 679 words.
Number of 3-grams hit = 677  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle454.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 598 words.
Number of 3-grams hit = 592  (99.00%)
Number of 2-grams hit = 5  (0.84%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle455.out
Will force inclusive back-off from OOVs.
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 405 words.
Number of 3-grams hit = 403  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle456.out
Will force inclusive back-off from OOVs.
Perplexity = 3.02, Entropy = 1.60 bits
Computation based on 719 words.
Number of 3-grams hit = 717  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle457.out
Will force inclusive back-off from OOVs.
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 366 words.
Number of 3-grams hit = 363  (99.18%)
Number of 2-grams hit = 2  (0.55%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle458.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 299 words.
Number of 3-grams hit = 296  (99.00%)
Number of 2-grams hit = 2  (0.67%)
Number of 1-grams hit = 1  (0.33%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle459.out
Will force inclusive back-off from OOVs.
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 752 words.
Number of 3-grams hit = 750  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle460.out
Will force inclusive back-off from OOVs.
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 354 words.
Number of 3-grams hit = 352  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle461.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 440 words.
Number of 3-grams hit = 438  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle462.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1537 words.
Number of 3-grams hit = 1534  (99.80%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle463.out
Will force inclusive back-off from OOVs.
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 610 words.
Number of 3-grams hit = 608  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle464.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 466 words.
Number of 3-grams hit = 464  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle465.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1935 words.
Number of 3-grams hit = 1932  (99.84%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle466.out
Will force inclusive back-off from OOVs.
Perplexity = 4.18, Entropy = 2.07 bits
Computation based on 225 words.
Number of 3-grams hit = 222  (98.67%)
Number of 2-grams hit = 2  (0.89%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle467.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1127 words.
Number of 3-grams hit = 1125  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle468.out
Will force inclusive back-off from OOVs.
Perplexity = 3.00, Entropy = 1.59 bits
Computation based on 843 words.
Number of 3-grams hit = 837  (99.29%)
Number of 2-grams hit = 5  (0.59%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle469.out
Will force inclusive back-off from OOVs.
Perplexity = 2.91, Entropy = 1.54 bits
Computation based on 381 words.
Number of 3-grams hit = 379  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle470.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 259 words.
Number of 3-grams hit = 255  (98.46%)
Number of 2-grams hit = 3  (1.16%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle471.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 2170 words.
Number of 3-grams hit = 2163  (99.68%)
Number of 2-grams hit = 6  (0.28%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle472.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1013 words.
Number of 3-grams hit = 1010  (99.70%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle473.out
Will force inclusive back-off from OOVs.
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 247 words.
Number of 3-grams hit = 245  (99.19%)
Number of 2-grams hit = 1  (0.40%)
Number of 1-grams hit = 1  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle474.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 838 words.
Number of 3-grams hit = 834  (99.52%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle475.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 588 words.
Number of 3-grams hit = 586  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle476.out
Will force inclusive back-off from OOVs.
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 626 words.
Number of 3-grams hit = 623  (99.52%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle477.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 474 words.
Number of 3-grams hit = 472  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle478.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 547 words.
Number of 3-grams hit = 545  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle479.out
Will force inclusive back-off from OOVs.
Perplexity = 4.28, Entropy = 2.10 bits
Computation based on 458 words.
Number of 3-grams hit = 453  (98.91%)
Number of 2-grams hit = 3  (0.66%)
Number of 1-grams hit = 2  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle480.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 749 words.
Number of 3-grams hit = 746  (99.60%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle481.out
Will force inclusive back-off from OOVs.
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 379 words.
Number of 3-grams hit = 377  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle482.out
Will force inclusive back-off from OOVs.
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 542 words.
Number of 3-grams hit = 538  (99.26%)
Number of 2-grams hit = 3  (0.55%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle483.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1863 words.
Number of 3-grams hit = 1861  (99.89%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle484.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1569 words.
Number of 3-grams hit = 1566  (99.81%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle485.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 402 words.
Number of 3-grams hit = 400  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle486.out
Will force inclusive back-off from OOVs.
Perplexity = 4.25, Entropy = 2.09 bits
Computation based on 369 words.
Number of 3-grams hit = 366  (99.19%)
Number of 2-grams hit = 2  (0.54%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle487.out
Will force inclusive back-off from OOVs.
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 462 words.
Number of 3-grams hit = 460  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle488.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 770 words.
Number of 3-grams hit = 768  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle489.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 577 words.
Number of 3-grams hit = 575  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle490.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 533 words.
Number of 3-grams hit = 531  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle491.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1520 words.
Number of 3-grams hit = 1515  (99.67%)
Number of 2-grams hit = 4  (0.26%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle492.out
Will force inclusive back-off from OOVs.
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 473 words.
Number of 3-grams hit = 471  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle493.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 827 words.
Number of 3-grams hit = 825  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle494.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 540 words.
Number of 3-grams hit = 538  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle495.out
Will force inclusive back-off from OOVs.
Perplexity = 4.10, Entropy = 2.04 bits
Computation based on 454 words.
Number of 3-grams hit = 451  (99.34%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle496.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 608 words.
Number of 3-grams hit = 606  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle497.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 711 words.
Number of 3-grams hit = 709  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle498.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 1824 words.
Number of 3-grams hit = 1818  (99.67%)
Number of 2-grams hit = 5  (0.27%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle499.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 542 words.
Number of 3-grams hit = 540  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle500.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 486 words.
Number of 3-grams hit = 484  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle501.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 448 words.
Number of 3-grams hit = 445  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle502.out
Will force inclusive back-off from OOVs.
Perplexity = 3.05, Entropy = 1.61 bits
Computation based on 535 words.
Number of 3-grams hit = 533  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle503.out
Will force inclusive back-off from OOVs.
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 1109 words.
Number of 3-grams hit = 1106  (99.73%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle504.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 658 words.
Number of 3-grams hit = 656  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle505.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 768 words.
Number of 3-grams hit = 766  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle506.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 594 words.
Number of 3-grams hit = 592  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle507.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1702 words.
Number of 3-grams hit = 1699  (99.82%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle508.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 700 words.
Number of 3-grams hit = 698  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle509.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 383 words.
Number of 3-grams hit = 379  (98.96%)
Number of 2-grams hit = 3  (0.78%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle510.out
Will force inclusive back-off from OOVs.
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1032 words.
Number of 3-grams hit = 1028  (99.61%)
Number of 2-grams hit = 3  (0.29%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle511.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 677 words.
Number of 3-grams hit = 675  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle512.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 2057 words.
Number of 3-grams hit = 2054  (99.85%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle513.out
Will force inclusive back-off from OOVs.
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 448 words.
Number of 3-grams hit = 446  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle514.out
Will force inclusive back-off from OOVs.
Perplexity = 3.08, Entropy = 1.62 bits
Computation based on 422 words.
Number of 3-grams hit = 420  (99.53%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle515.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1070 words.
Number of 3-grams hit = 1067  (99.72%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle516.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 440 words.
Number of 3-grams hit = 438  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle517.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1145 words.
Number of 3-grams hit = 1143  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle518.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 639 words.
Number of 3-grams hit = 637  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle519.out
Will force inclusive back-off from OOVs.
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 592 words.
Number of 3-grams hit = 590  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle520.out
Will force inclusive back-off from OOVs.
Perplexity = 3.24, Entropy = 1.69 bits
Computation based on 663 words.
Number of 3-grams hit = 661  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle521.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 548 words.
Number of 3-grams hit = 546  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle522.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 1741 words.
Number of 3-grams hit = 1739  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle523.out
Will force inclusive back-off from OOVs.
Perplexity = 4.23, Entropy = 2.08 bits
Computation based on 354 words.
Number of 3-grams hit = 349  (98.59%)
Number of 2-grams hit = 4  (1.13%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle524.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1488 words.
Number of 3-grams hit = 1483  (99.66%)
Number of 2-grams hit = 4  (0.27%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle525.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 2121 words.
Number of 3-grams hit = 2118  (99.86%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle526.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 2067 words.
Number of 3-grams hit = 2064  (99.85%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle527.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 793 words.
Number of 3-grams hit = 791  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle528.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 788 words.
Number of 3-grams hit = 786  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle529.out
Will force inclusive back-off from OOVs.
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 957 words.
Number of 3-grams hit = 953  (99.58%)
Number of 2-grams hit = 3  (0.31%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle530.out
Will force inclusive back-off from OOVs.
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 451 words.
Number of 3-grams hit = 449  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle531.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 1046 words.
Number of 3-grams hit = 1041  (99.52%)
Number of 2-grams hit = 4  (0.38%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle532.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1019 words.
Number of 3-grams hit = 1017  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle533.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 1210 words.
Number of 3-grams hit = 1208  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle534.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 549 words.
Number of 3-grams hit = 547  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle535.out
Will force inclusive back-off from OOVs.
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 375 words.
Number of 3-grams hit = 373  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle536.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1602 words.
Number of 3-grams hit = 1600  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle537.out
Will force inclusive back-off from OOVs.
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1098 words.
Number of 3-grams hit = 1095  (99.73%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle538.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 3722 words.
Number of 3-grams hit = 3718  (99.89%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle539.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 581 words.
Number of 3-grams hit = 579  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle540.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 2359 words.
Number of 3-grams hit = 2357  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle541.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 544 words.
Number of 3-grams hit = 542  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle542.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1758 words.
Number of 3-grams hit = 1756  (99.89%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle543.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 592 words.
Number of 3-grams hit = 590  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle544.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 1399 words.
Number of 3-grams hit = 1394  (99.64%)
Number of 2-grams hit = 4  (0.29%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle545.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 802 words.
Number of 3-grams hit = 799  (99.63%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle546.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 753 words.
Number of 3-grams hit = 751  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle547.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 953 words.
Number of 3-grams hit = 949  (99.58%)
Number of 2-grams hit = 3  (0.31%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle548.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 671 words.
Number of 3-grams hit = 666  (99.25%)
Number of 2-grams hit = 3  (0.45%)
Number of 1-grams hit = 2  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle549.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 387 words.
Number of 3-grams hit = 385  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle550.out
Will force inclusive back-off from OOVs.
Perplexity = 4.18, Entropy = 2.06 bits
Computation based on 204 words.
Number of 3-grams hit = 202  (99.02%)
Number of 2-grams hit = 1  (0.49%)
Number of 1-grams hit = 1  (0.49%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle551.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 996 words.
Number of 3-grams hit = 993  (99.70%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle552.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.87 bits
Computation based on 1143 words.
Number of 3-grams hit = 1141  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle553.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 1347 words.
Number of 3-grams hit = 1345  (99.85%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle554.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 4358 words.
Number of 3-grams hit = 4349  (99.79%)
Number of 2-grams hit = 8  (0.18%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle555.out
Will force inclusive back-off from OOVs.
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 544 words.
Number of 3-grams hit = 540  (99.26%)
Number of 2-grams hit = 3  (0.55%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle556.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 605 words.
Number of 3-grams hit = 602  (99.50%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle557.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 418 words.
Number of 3-grams hit = 415  (99.28%)
Number of 2-grams hit = 2  (0.48%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle558.out
Will force inclusive back-off from OOVs.
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 1850 words.
Number of 3-grams hit = 1846  (99.78%)
Number of 2-grams hit = 3  (0.16%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle559.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 694 words.
Number of 3-grams hit = 690  (99.42%)
Number of 2-grams hit = 3  (0.43%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle560.out
Will force inclusive back-off from OOVs.
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 595 words.
Number of 3-grams hit = 593  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle561.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 362 words.
Number of 3-grams hit = 360  (99.45%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle562.out
Will force inclusive back-off from OOVs.
Perplexity = 4.23, Entropy = 2.08 bits
Computation based on 331 words.
Number of 3-grams hit = 329  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle563.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 524 words.
Number of 3-grams hit = 522  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle564.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1451 words.
Number of 3-grams hit = 1448  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle565.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1392 words.
Number of 3-grams hit = 1390  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle566.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 4543 words.
Number of 3-grams hit = 4536  (99.85%)
Number of 2-grams hit = 5  (0.11%)
Number of 1-grams hit = 2  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle567.out
Will force inclusive back-off from OOVs.
Perplexity = 4.23, Entropy = 2.08 bits
Computation based on 596 words.
Number of 3-grams hit = 594  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle568.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 355 words.
Number of 3-grams hit = 353  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle569.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.83 bits
Computation based on 4706 words.
Number of 3-grams hit = 4701  (99.89%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle570.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 897 words.
Number of 3-grams hit = 895  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle571.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 576 words.
Number of 3-grams hit = 574  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle572.out
Will force inclusive back-off from OOVs.
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 619 words.
Number of 3-grams hit = 617  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle573.out
Will force inclusive back-off from OOVs.
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 279 words.
Number of 3-grams hit = 275  (98.57%)
Number of 2-grams hit = 3  (1.08%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle574.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1179 words.
Number of 3-grams hit = 1175  (99.66%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle575.out
Will force inclusive back-off from OOVs.
Perplexity = 2.98, Entropy = 1.57 bits
Computation based on 643 words.
Number of 3-grams hit = 641  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle576.out
Will force inclusive back-off from OOVs.
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 489 words.
Number of 3-grams hit = 486  (99.39%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle577.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 405 words.
Number of 3-grams hit = 403  (99.51%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle578.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 447 words.
Number of 3-grams hit = 444  (99.33%)
Number of 2-grams hit = 2  (0.45%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle579.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 479 words.
Number of 3-grams hit = 477  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle580.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 585 words.
Number of 3-grams hit = 583  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle581.out
Will force inclusive back-off from OOVs.
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 584 words.
Number of 3-grams hit = 582  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle582.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 547 words.
Number of 3-grams hit = 545  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle583.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 722 words.
Number of 3-grams hit = 720  (99.72%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle584.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 477 words.
Number of 3-grams hit = 474  (99.37%)
Number of 2-grams hit = 2  (0.42%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle585.out
Will force inclusive back-off from OOVs.
Perplexity = 3.09, Entropy = 1.63 bits
Computation based on 1036 words.
Number of 3-grams hit = 1034  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle586.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 782 words.
Number of 3-grams hit = 779  (99.62%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle587.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 576 words.
Number of 3-grams hit = 574  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle588.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.87 bits
Computation based on 1045 words.
Number of 3-grams hit = 1043  (99.81%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle589.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 538 words.
Number of 3-grams hit = 536  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle590.out
Will force inclusive back-off from OOVs.
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 1468 words.
Number of 3-grams hit = 1466  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle591.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 749 words.
Number of 3-grams hit = 747  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle592.out
Will force inclusive back-off from OOVs.
Perplexity = 4.08, Entropy = 2.03 bits
Computation based on 367 words.
Number of 3-grams hit = 365  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle593.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 409 words.
Number of 3-grams hit = 406  (99.27%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle594.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 653 words.
Number of 3-grams hit = 651  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle595.out
Will force inclusive back-off from OOVs.
Perplexity = 4.00, Entropy = 2.00 bits
Computation based on 398 words.
Number of 3-grams hit = 395  (99.25%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle596.out
Will force inclusive back-off from OOVs.
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 1041 words.
Number of 3-grams hit = 1037  (99.62%)
Number of 2-grams hit = 3  (0.29%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle597.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 1109 words.
Number of 3-grams hit = 1107  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle598.out
Will force inclusive back-off from OOVs.
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 327 words.
Number of 3-grams hit = 324  (99.08%)
Number of 2-grams hit = 2  (0.61%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle599.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 879 words.
Number of 3-grams hit = 877  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle600.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 5209 words.
Number of 3-grams hit = 5204  (99.90%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle601.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1181 words.
Number of 3-grams hit = 1177  (99.66%)
Number of 2-grams hit = 3  (0.25%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle602.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 652 words.
Number of 3-grams hit = 650  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle603.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 94 words.
Number of 3-grams hit = 92  (97.87%)
Number of 2-grams hit = 1  (1.06%)
Number of 1-grams hit = 1  (1.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle604.out
Will force inclusive back-off from OOVs.
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 612 words.
Number of 3-grams hit = 610  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle605.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 3406 words.
Number of 3-grams hit = 3401  (99.85%)
Number of 2-grams hit = 4  (0.12%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle606.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 978 words.
Number of 3-grams hit = 976  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle607.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 1157 words.
Number of 3-grams hit = 1155  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle608.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 674 words.
Number of 3-grams hit = 671  (99.55%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle609.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 4849 words.
Number of 3-grams hit = 4845  (99.92%)
Number of 2-grams hit = 3  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle610.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 6480 words.
Number of 3-grams hit = 6475  (99.92%)
Number of 2-grams hit = 4  (0.06%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle611.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 856 words.
Number of 3-grams hit = 853  (99.65%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle612.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1757 words.
Number of 3-grams hit = 1753  (99.77%)
Number of 2-grams hit = 3  (0.17%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle613.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 4584 words.
Number of 3-grams hit = 4580  (99.91%)
Number of 2-grams hit = 3  (0.07%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle614.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1629 words.
Number of 3-grams hit = 1625  (99.75%)
Number of 2-grams hit = 3  (0.18%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle615.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 494 words.
Number of 3-grams hit = 491  (99.39%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle616.out
Will force inclusive back-off from OOVs.
Perplexity = 2.96, Entropy = 1.57 bits
Computation based on 827 words.
Number of 3-grams hit = 825  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle617.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 1418 words.
Number of 3-grams hit = 1416  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle618.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 4055 words.
Number of 3-grams hit = 4050  (99.88%)
Number of 2-grams hit = 4  (0.10%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle619.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 853 words.
Number of 3-grams hit = 847  (99.30%)
Number of 2-grams hit = 5  (0.59%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle620.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 5770 words.
Number of 3-grams hit = 5762  (99.86%)
Number of 2-grams hit = 7  (0.12%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle621.out
Will force inclusive back-off from OOVs.
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 347 words.
Number of 3-grams hit = 344  (99.14%)
Number of 2-grams hit = 2  (0.58%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle622.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 503 words.
Number of 3-grams hit = 499  (99.20%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 2  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle623.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1430 words.
Number of 3-grams hit = 1428  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle624.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 5610 words.
Number of 3-grams hit = 5601  (99.84%)
Number of 2-grams hit = 7  (0.12%)
Number of 1-grams hit = 2  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle625.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 1191 words.
Number of 3-grams hit = 1188  (99.75%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle626.out
Will force inclusive back-off from OOVs.
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 483 words.
Number of 3-grams hit = 481  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle627.out
Will force inclusive back-off from OOVs.
Perplexity = 3.00, Entropy = 1.59 bits
Computation based on 159 words.
Number of 3-grams hit = 157  (98.74%)
Number of 2-grams hit = 1  (0.63%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle628.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1142 words.
Number of 3-grams hit = 1140  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle629.out
Will force inclusive back-off from OOVs.
Perplexity = 3.51, Entropy = 1.81 bits
Computation based on 1114 words.
Number of 3-grams hit = 1111  (99.73%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle630.out
Will force inclusive back-off from OOVs.
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 349 words.
Number of 3-grams hit = 347  (99.43%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle631.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 931 words.
Number of 3-grams hit = 929  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle632.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 439 words.
Number of 3-grams hit = 436  (99.32%)
Number of 2-grams hit = 2  (0.46%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle633.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 579 words.
Number of 3-grams hit = 577  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle634.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 603 words.
Number of 3-grams hit = 601  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle635.out
Will force inclusive back-off from OOVs.
Perplexity = 3.85, Entropy = 1.95 bits
Computation based on 169 words.
Number of 3-grams hit = 167  (98.82%)
Number of 2-grams hit = 1  (0.59%)
Number of 1-grams hit = 1  (0.59%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle636.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1435 words.
Number of 3-grams hit = 1432  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle637.out
Will force inclusive back-off from OOVs.
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 1118 words.
Number of 3-grams hit = 1112  (99.46%)
Number of 2-grams hit = 5  (0.45%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle638.out
Will force inclusive back-off from OOVs.
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 586 words.
Number of 3-grams hit = 584  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle639.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 561 words.
Number of 3-grams hit = 559  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle640.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 227 words.
Number of 3-grams hit = 225  (99.12%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle641.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 888 words.
Number of 3-grams hit = 886  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle642.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 477 words.
Number of 3-grams hit = 475  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle643.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 522 words.
Number of 3-grams hit = 519  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle644.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1207 words.
Number of 3-grams hit = 1204  (99.75%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle645.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 475 words.
Number of 3-grams hit = 473  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle646.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 615 words.
Number of 3-grams hit = 613  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle647.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 653 words.
Number of 3-grams hit = 651  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle648.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 511 words.
Number of 3-grams hit = 509  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle649.out
Will force inclusive back-off from OOVs.
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 577 words.
Number of 3-grams hit = 575  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle650.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 579 words.
Number of 3-grams hit = 577  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle651.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 503 words.
Number of 3-grams hit = 500  (99.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle652.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 956 words.
Number of 3-grams hit = 951  (99.48%)
Number of 2-grams hit = 4  (0.42%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle653.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 848 words.
Number of 3-grams hit = 846  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle654.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 827 words.
Number of 3-grams hit = 824  (99.64%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle655.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 547 words.
Number of 3-grams hit = 545  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle656.out
Will force inclusive back-off from OOVs.
Perplexity = 4.30, Entropy = 2.10 bits
Computation based on 501 words.
Number of 3-grams hit = 499  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle657.out
Will force inclusive back-off from OOVs.
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 412 words.
Number of 3-grams hit = 410  (99.51%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle658.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1097 words.
Number of 3-grams hit = 1093  (99.64%)
Number of 2-grams hit = 3  (0.27%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle659.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 1082 words.
Number of 3-grams hit = 1080  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle660.out
Will force inclusive back-off from OOVs.
Perplexity = 2.45, Entropy = 1.29 bits
Computation based on 151 words.
Number of 3-grams hit = 149  (98.68%)
Number of 2-grams hit = 1  (0.66%)
Number of 1-grams hit = 1  (0.66%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle661.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1330 words.
Number of 3-grams hit = 1326  (99.70%)
Number of 2-grams hit = 3  (0.23%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle662.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 131 words.
Number of 3-grams hit = 129  (98.47%)
Number of 2-grams hit = 1  (0.76%)
Number of 1-grams hit = 1  (0.76%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle663.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 1445 words.
Number of 3-grams hit = 1440  (99.65%)
Number of 2-grams hit = 4  (0.28%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle664.out
Will force inclusive back-off from OOVs.
Perplexity = 4.22, Entropy = 2.08 bits
Computation based on 770 words.
Number of 3-grams hit = 766  (99.48%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle665.out
Will force inclusive back-off from OOVs.
Perplexity = 3.92, Entropy = 1.97 bits
Computation based on 830 words.
Number of 3-grams hit = 827  (99.64%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle666.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 413 words.
Number of 3-grams hit = 411  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle667.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 1455 words.
Number of 3-grams hit = 1451  (99.73%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle668.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 504 words.
Number of 3-grams hit = 502  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle669.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 524 words.
Number of 3-grams hit = 522  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle670.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 481 words.
Number of 3-grams hit = 479  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle671.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 693 words.
Number of 3-grams hit = 690  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle672.out
Will force inclusive back-off from OOVs.
Perplexity = 3.86, Entropy = 1.95 bits
Computation based on 316 words.
Number of 3-grams hit = 314  (99.37%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle673.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 336 words.
Number of 3-grams hit = 334  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle674.out
Will force inclusive back-off from OOVs.
Perplexity = 3.79, Entropy = 1.92 bits
Computation based on 159 words.
Number of 3-grams hit = 157  (98.74%)
Number of 2-grams hit = 1  (0.63%)
Number of 1-grams hit = 1  (0.63%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle675.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 4196 words.
Number of 3-grams hit = 4193  (99.93%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle676.out
Will force inclusive back-off from OOVs.
Perplexity = 2.86, Entropy = 1.51 bits
Computation based on 531 words.
Number of 3-grams hit = 529  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle677.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 684 words.
Number of 3-grams hit = 679  (99.27%)
Number of 2-grams hit = 4  (0.58%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle678.out
Will force inclusive back-off from OOVs.
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 373 words.
Number of 3-grams hit = 371  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle679.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.72 bits
Computation based on 1075 words.
Number of 3-grams hit = 1072  (99.72%)
Number of 2-grams hit = 2  (0.19%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle680.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 356 words.
Number of 3-grams hit = 354  (99.44%)
Number of 2-grams hit = 1  (0.28%)
Number of 1-grams hit = 1  (0.28%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle681.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 522 words.
Number of 3-grams hit = 518  (99.23%)
Number of 2-grams hit = 3  (0.57%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle682.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 4014 words.
Number of 3-grams hit = 4011  (99.93%)
Number of 2-grams hit = 2  (0.05%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle683.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 279 words.
Number of 3-grams hit = 277  (99.28%)
Number of 2-grams hit = 1  (0.36%)
Number of 1-grams hit = 1  (0.36%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle684.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 4719 words.
Number of 3-grams hit = 4714  (99.89%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle685.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 485 words.
Number of 3-grams hit = 483  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle686.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 2419 words.
Number of 3-grams hit = 2414  (99.79%)
Number of 2-grams hit = 4  (0.17%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle687.out
Will force inclusive back-off from OOVs.
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 556 words.
Number of 3-grams hit = 554  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle688.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 332 words.
Number of 3-grams hit = 330  (99.40%)
Number of 2-grams hit = 1  (0.30%)
Number of 1-grams hit = 1  (0.30%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle689.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 554 words.
Number of 3-grams hit = 552  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle690.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 824 words.
Number of 3-grams hit = 822  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle691.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 138 words.
Number of 3-grams hit = 136  (98.55%)
Number of 2-grams hit = 1  (0.72%)
Number of 1-grams hit = 1  (0.72%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle692.out
Will force inclusive back-off from OOVs.
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 460 words.
Number of 3-grams hit = 458  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle693.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1025 words.
Number of 3-grams hit = 1022  (99.71%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle694.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.78 bits
Computation based on 669 words.
Number of 3-grams hit = 667  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle695.out
Will force inclusive back-off from OOVs.
Perplexity = 3.10, Entropy = 1.63 bits
Computation based on 924 words.
Number of 3-grams hit = 922  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle696.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 541 words.
Number of 3-grams hit = 539  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle697.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1021 words.
Number of 3-grams hit = 1018  (99.71%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle698.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 630 words.
Number of 3-grams hit = 628  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle699.out
Will force inclusive back-off from OOVs.
Perplexity = 3.84, Entropy = 1.94 bits
Computation based on 377 words.
Number of 3-grams hit = 375  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle700.out
Will force inclusive back-off from OOVs.
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 3659 words.
Number of 3-grams hit = 3654  (99.86%)
Number of 2-grams hit = 4  (0.11%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle701.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 878 words.
Number of 3-grams hit = 876  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle702.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 2030 words.
Number of 3-grams hit = 2028  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle703.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 470 words.
Number of 3-grams hit = 466  (99.15%)
Number of 2-grams hit = 3  (0.64%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle704.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 341 words.
Number of 3-grams hit = 339  (99.41%)
Number of 2-grams hit = 1  (0.29%)
Number of 1-grams hit = 1  (0.29%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle705.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 1100 words.
Number of 3-grams hit = 1095  (99.55%)
Number of 2-grams hit = 4  (0.36%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle706.out
Will force inclusive back-off from OOVs.
Perplexity = 2.96, Entropy = 1.57 bits
Computation based on 509 words.
Number of 3-grams hit = 507  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle707.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 941 words.
Number of 3-grams hit = 939  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle708.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 562 words.
Number of 3-grams hit = 560  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle709.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 602 words.
Number of 3-grams hit = 599  (99.50%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle710.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 823 words.
Number of 3-grams hit = 820  (99.64%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle711.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 1017 words.
Number of 3-grams hit = 1015  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle712.out
Will force inclusive back-off from OOVs.
Perplexity = 3.94, Entropy = 1.98 bits
Computation based on 406 words.
Number of 3-grams hit = 403  (99.26%)
Number of 2-grams hit = 2  (0.49%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle713.out
Will force inclusive back-off from OOVs.
Perplexity = 4.33, Entropy = 2.11 bits
Computation based on 971 words.
Number of 3-grams hit = 966  (99.49%)
Number of 2-grams hit = 4  (0.41%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle714.out
Will force inclusive back-off from OOVs.
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 404 words.
Number of 3-grams hit = 401  (99.26%)
Number of 2-grams hit = 2  (0.50%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle715.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 525 words.
Number of 3-grams hit = 523  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle716.out
Will force inclusive back-off from OOVs.
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 521 words.
Number of 3-grams hit = 519  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle717.out
Will force inclusive back-off from OOVs.
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 403 words.
Number of 3-grams hit = 401  (99.50%)
Number of 2-grams hit = 1  (0.25%)
Number of 1-grams hit = 1  (0.25%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle718.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 7638 words.
Number of 3-grams hit = 7629  (99.88%)
Number of 2-grams hit = 7  (0.09%)
Number of 1-grams hit = 2  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle719.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 1275 words.
Number of 3-grams hit = 1273  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle720.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 872 words.
Number of 3-grams hit = 869  (99.66%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle721.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 452 words.
Number of 3-grams hit = 450  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle722.out
Will force inclusive back-off from OOVs.
Perplexity = 3.97, Entropy = 1.99 bits
Computation based on 1116 words.
Number of 3-grams hit = 1114  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle723.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 472 words.
Number of 3-grams hit = 470  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle724.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.72 bits
Computation based on 500 words.
Number of 3-grams hit = 497  (99.40%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle725.out
Will force inclusive back-off from OOVs.
Perplexity = 3.28, Entropy = 1.71 bits
Computation based on 792 words.
Number of 3-grams hit = 790  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle726.out
Will force inclusive back-off from OOVs.
Perplexity = 3.32, Entropy = 1.73 bits
Computation based on 573 words.
Number of 3-grams hit = 571  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle727.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.82 bits
Computation based on 519 words.
Number of 3-grams hit = 517  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle728.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 453 words.
Number of 3-grams hit = 450  (99.34%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle729.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 468 words.
Number of 3-grams hit = 466  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle730.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 5476 words.
Number of 3-grams hit = 5468  (99.85%)
Number of 2-grams hit = 7  (0.13%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle731.out
Will force inclusive back-off from OOVs.
Perplexity = 3.78, Entropy = 1.92 bits
Computation based on 1562 words.
Number of 3-grams hit = 1559  (99.81%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle732.out
Will force inclusive back-off from OOVs.
Perplexity = 3.98, Entropy = 1.99 bits
Computation based on 769 words.
Number of 3-grams hit = 766  (99.61%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle733.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 609 words.
Number of 3-grams hit = 606  (99.51%)
Number of 2-grams hit = 2  (0.33%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle734.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 450 words.
Number of 3-grams hit = 448  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle735.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 1274 words.
Number of 3-grams hit = 1265  (99.29%)
Number of 2-grams hit = 6  (0.47%)
Number of 1-grams hit = 3  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle736.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.92 bits
Computation based on 733 words.
Number of 3-grams hit = 729  (99.45%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle737.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 697 words.
Number of 3-grams hit = 694  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle738.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 1069 words.
Number of 3-grams hit = 1063  (99.44%)
Number of 2-grams hit = 5  (0.47%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle739.out
Will force inclusive back-off from OOVs.
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 484 words.
Number of 3-grams hit = 482  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle740.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1194 words.
Number of 3-grams hit = 1192  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle741.out
Will force inclusive back-off from OOVs.
Perplexity = 3.22, Entropy = 1.69 bits
Computation based on 824 words.
Number of 3-grams hit = 821  (99.64%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle742.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 690 words.
Number of 3-grams hit = 687  (99.57%)
Number of 2-grams hit = 2  (0.29%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle743.out
Will force inclusive back-off from OOVs.
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 2142 words.
Number of 3-grams hit = 2140  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle744.out
Will force inclusive back-off from OOVs.
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 580 words.
Number of 3-grams hit = 577  (99.48%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle745.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 740 words.
Number of 3-grams hit = 738  (99.73%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle746.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 1498 words.
Number of 3-grams hit = 1496  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle747.out
Will force inclusive back-off from OOVs.
Perplexity = 2.99, Entropy = 1.58 bits
Computation based on 571 words.
Number of 3-grams hit = 569  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle748.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 544 words.
Number of 3-grams hit = 542  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle749.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 590 words.
Number of 3-grams hit = 588  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle750.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 513 words.
Number of 3-grams hit = 511  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle751.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.81 bits
Computation based on 2179 words.
Number of 3-grams hit = 2177  (99.91%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle752.out
Will force inclusive back-off from OOVs.
Perplexity = 4.09, Entropy = 2.03 bits
Computation based on 468 words.
Number of 3-grams hit = 466  (99.57%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle753.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 572 words.
Number of 3-grams hit = 570  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle754.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 2031 words.
Number of 3-grams hit = 2028  (99.85%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle755.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 487 words.
Number of 3-grams hit = 485  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle756.out
Will force inclusive back-off from OOVs.
Perplexity = 4.13, Entropy = 2.05 bits
Computation based on 74 words.
Number of 3-grams hit = 72  (97.30%)
Number of 2-grams hit = 1  (1.35%)
Number of 1-grams hit = 1  (1.35%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle757.out
Will force inclusive back-off from OOVs.
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 557 words.
Number of 3-grams hit = 555  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle758.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 2743 words.
Number of 3-grams hit = 2738  (99.82%)
Number of 2-grams hit = 4  (0.15%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle759.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 3515 words.
Number of 3-grams hit = 3511  (99.89%)
Number of 2-grams hit = 3  (0.09%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle760.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 618 words.
Number of 3-grams hit = 614  (99.35%)
Number of 2-grams hit = 3  (0.49%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle761.out
Will force inclusive back-off from OOVs.
Perplexity = 3.99, Entropy = 2.00 bits
Computation based on 684 words.
Number of 3-grams hit = 682  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle762.out
Will force inclusive back-off from OOVs.
Perplexity = 3.40, Entropy = 1.77 bits
Computation based on 861 words.
Number of 3-grams hit = 859  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle763.out
Will force inclusive back-off from OOVs.
Perplexity = 3.24, Entropy = 1.70 bits
Computation based on 782 words.
Number of 3-grams hit = 779  (99.62%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle764.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 582 words.
Number of 3-grams hit = 580  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle765.out
Will force inclusive back-off from OOVs.
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 888 words.
Number of 3-grams hit = 883  (99.44%)
Number of 2-grams hit = 4  (0.45%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle766.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 293 words.
Number of 3-grams hit = 291  (99.32%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle767.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 4625 words.
Number of 3-grams hit = 4622  (99.94%)
Number of 2-grams hit = 2  (0.04%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle768.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle769.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 730 words.
Number of 3-grams hit = 726  (99.45%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle770.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 658 words.
Number of 3-grams hit = 656  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle771.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 501 words.
Number of 3-grams hit = 497  (99.20%)
Number of 2-grams hit = 2  (0.40%)
Number of 1-grams hit = 2  (0.40%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle772.out
Will force inclusive back-off from OOVs.
Perplexity = 3.17, Entropy = 1.67 bits
Computation based on 590 words.
Number of 3-grams hit = 588  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle773.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 3261 words.
Number of 3-grams hit = 3254  (99.79%)
Number of 2-grams hit = 6  (0.18%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle774.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 2158 words.
Number of 3-grams hit = 2153  (99.77%)
Number of 2-grams hit = 4  (0.19%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle775.out
Will force inclusive back-off from OOVs.
Perplexity = 3.81, Entropy = 1.93 bits
Computation based on 558 words.
Number of 3-grams hit = 556  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle776.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 48 words.
Number of 3-grams hit = 46  (95.83%)
Number of 2-grams hit = 1  (2.08%)
Number of 1-grams hit = 1  (2.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle777.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 10354 words.
Number of 3-grams hit = 10337  (99.84%)
Number of 2-grams hit = 16  (0.15%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle778.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 545 words.
Number of 3-grams hit = 543  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle779.out
Will force inclusive back-off from OOVs.
Perplexity = 4.15, Entropy = 2.05 bits
Computation based on 1014 words.
Number of 3-grams hit = 1010  (99.61%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle780.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 445 words.
Number of 3-grams hit = 443  (99.55%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle781.out
Will force inclusive back-off from OOVs.
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 786 words.
Number of 3-grams hit = 784  (99.75%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle782.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1485 words.
Number of 3-grams hit = 1482  (99.80%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle783.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 549 words.
Number of 3-grams hit = 547  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle784.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1881 words.
Number of 3-grams hit = 1878  (99.84%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle785.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 2868 words.
Number of 3-grams hit = 2864  (99.86%)
Number of 2-grams hit = 3  (0.10%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle786.out
Will force inclusive back-off from OOVs.
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 370 words.
Number of 3-grams hit = 368  (99.46%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle787.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 512 words.
Number of 3-grams hit = 510  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle788.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1616 words.
Number of 3-grams hit = 1613  (99.81%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle789.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 12475 words.
Number of 3-grams hit = 12463  (99.90%)
Number of 2-grams hit = 11  (0.09%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle790.out
Will force inclusive back-off from OOVs.
Perplexity = 3.13, Entropy = 1.65 bits
Computation based on 1019 words.
Number of 3-grams hit = 1017  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle791.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 749 words.
Number of 3-grams hit = 747  (99.73%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle792.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 3144 words.
Number of 3-grams hit = 3139  (99.84%)
Number of 2-grams hit = 4  (0.13%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle793.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 197 words.
Number of 3-grams hit = 195  (98.98%)
Number of 2-grams hit = 1  (0.51%)
Number of 1-grams hit = 1  (0.51%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle794.out
Will force inclusive back-off from OOVs.
Perplexity = 4.40, Entropy = 2.14 bits
Computation based on 548 words.
Number of 3-grams hit = 546  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle795.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 460 words.
Number of 3-grams hit = 458  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle796.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 1620 words.
Number of 3-grams hit = 1617  (99.81%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle797.out
Will force inclusive back-off from OOVs.
Perplexity = 3.17, Entropy = 1.67 bits
Computation based on 413 words.
Number of 3-grams hit = 411  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle798.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.77 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle799.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 816 words.
Number of 3-grams hit = 814  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle800.out
Will force inclusive back-off from OOVs.
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 473 words.
Number of 3-grams hit = 471  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle801.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 3418 words.
Number of 3-grams hit = 3415  (99.91%)
Number of 2-grams hit = 2  (0.06%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle802.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.78 bits
Computation based on 2237 words.
Number of 3-grams hit = 2233  (99.82%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle803.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 4594 words.
Number of 3-grams hit = 4584  (99.78%)
Number of 2-grams hit = 9  (0.20%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle804.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 1564 words.
Number of 3-grams hit = 1561  (99.81%)
Number of 2-grams hit = 2  (0.13%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle805.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 993 words.
Number of 3-grams hit = 988  (99.50%)
Number of 2-grams hit = 3  (0.30%)
Number of 1-grams hit = 2  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle806.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 3603 words.
Number of 3-grams hit = 3597  (99.83%)
Number of 2-grams hit = 5  (0.14%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle807.out
Will force inclusive back-off from OOVs.
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 756 words.
Number of 3-grams hit = 754  (99.74%)
Number of 2-grams hit = 1  (0.13%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle808.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 1830 words.
Number of 3-grams hit = 1827  (99.84%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle809.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 2165 words.
Number of 3-grams hit = 2161  (99.82%)
Number of 2-grams hit = 3  (0.14%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle810.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 937 words.
Number of 3-grams hit = 935  (99.79%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle811.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 4883 words.
Number of 3-grams hit = 4878  (99.90%)
Number of 2-grams hit = 4  (0.08%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle812.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 3604 words.
Number of 3-grams hit = 3598  (99.83%)
Number of 2-grams hit = 5  (0.14%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle813.out
Will force inclusive back-off from OOVs.
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 1497 words.
Number of 3-grams hit = 1495  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle814.out
Will force inclusive back-off from OOVs.
Perplexity = 3.01, Entropy = 1.59 bits
Computation based on 908 words.
Number of 3-grams hit = 906  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle815.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1390 words.
Number of 3-grams hit = 1388  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle816.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 1434 words.
Number of 3-grams hit = 1430  (99.72%)
Number of 2-grams hit = 3  (0.21%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle817.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 580 words.
Number of 3-grams hit = 577  (99.48%)
Number of 2-grams hit = 2  (0.34%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle818.out
Will force inclusive back-off from OOVs.
Perplexity = 3.76, Entropy = 1.91 bits
Computation based on 604 words.
Number of 3-grams hit = 602  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle819.out
Will force inclusive back-off from OOVs.
Perplexity = 3.18, Entropy = 1.67 bits
Computation based on 1947 words.
Number of 3-grams hit = 1945  (99.90%)
Number of 2-grams hit = 1  (0.05%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle820.out
Will force inclusive back-off from OOVs.
Perplexity = 3.16, Entropy = 1.66 bits
Computation based on 880 words.
Number of 3-grams hit = 878  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle821.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1187 words.
Number of 3-grams hit = 1185  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle822.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 2654 words.
Number of 3-grams hit = 2647  (99.74%)
Number of 2-grams hit = 5  (0.19%)
Number of 1-grams hit = 2  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle823.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1435 words.
Number of 3-grams hit = 1433  (99.86%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle824.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 533 words.
Number of 3-grams hit = 530  (99.44%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle825.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 639 words.
Number of 3-grams hit = 637  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle826.out
Will force inclusive back-off from OOVs.
Perplexity = 3.31, Entropy = 1.73 bits
Computation based on 978 words.
Number of 3-grams hit = 976  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle827.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 734 words.
Number of 3-grams hit = 731  (99.59%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle828.out
Will force inclusive back-off from OOVs.
Perplexity = 3.71, Entropy = 1.89 bits
Computation based on 13188 words.
Number of 3-grams hit = 13168  (99.85%)
Number of 2-grams hit = 17  (0.13%)
Number of 1-grams hit = 3  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle829.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 578 words.
Number of 3-grams hit = 576  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle830.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 2488 words.
Number of 3-grams hit = 2485  (99.88%)
Number of 2-grams hit = 2  (0.08%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle831.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 1106 words.
Number of 3-grams hit = 1103  (99.73%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle832.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 460 words.
Number of 3-grams hit = 458  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle833.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 888 words.
Number of 3-grams hit = 885  (99.66%)
Number of 2-grams hit = 2  (0.23%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle834.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 963 words.
Number of 3-grams hit = 961  (99.79%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle835.out
Will force inclusive back-off from OOVs.
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 491 words.
Number of 3-grams hit = 489  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle836.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.85 bits
Computation based on 528 words.
Number of 3-grams hit = 526  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle837.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 900 words.
Number of 3-grams hit = 898  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle838.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 2446 words.
Number of 3-grams hit = 2444  (99.92%)
Number of 2-grams hit = 1  (0.04%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle839.out
Will force inclusive back-off from OOVs.
Perplexity = 4.16, Entropy = 2.06 bits
Computation based on 496 words.
Number of 3-grams hit = 494  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle840.out
Will force inclusive back-off from OOVs.
Perplexity = 3.20, Entropy = 1.68 bits
Computation based on 560 words.
Number of 3-grams hit = 558  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle841.out
Will force inclusive back-off from OOVs.
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 1009 words.
Number of 3-grams hit = 1007  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle842.out
Will force inclusive back-off from OOVs.
Perplexity = 2.93, Entropy = 1.55 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle843.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 1419 words.
Number of 3-grams hit = 1413  (99.58%)
Number of 2-grams hit = 5  (0.35%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle844.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 9009 words.
Number of 3-grams hit = 8996  (99.86%)
Number of 2-grams hit = 12  (0.13%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle845.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 648 words.
Number of 3-grams hit = 646  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle846.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 609 words.
Number of 3-grams hit = 607  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle847.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 1795 words.
Number of 3-grams hit = 1790  (99.72%)
Number of 2-grams hit = 4  (0.22%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle848.out
Will force inclusive back-off from OOVs.
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 389 words.
Number of 3-grams hit = 387  (99.49%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle849.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1705 words.
Number of 3-grams hit = 1702  (99.82%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle850.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 477 words.
Number of 3-grams hit = 475  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle851.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 604 words.
Number of 3-grams hit = 602  (99.67%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle852.out
Will force inclusive back-off from OOVs.
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 684 words.
Number of 3-grams hit = 682  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle853.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 1621 words.
Number of 3-grams hit = 1616  (99.69%)
Number of 2-grams hit = 4  (0.25%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle854.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 2917 words.
Number of 3-grams hit = 2914  (99.90%)
Number of 2-grams hit = 2  (0.07%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle855.out
Will force inclusive back-off from OOVs.
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 465 words.
Number of 3-grams hit = 463  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle856.out
Will force inclusive back-off from OOVs.
Perplexity = 3.19, Entropy = 1.67 bits
Computation based on 889 words.
Number of 3-grams hit = 887  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle857.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 379 words.
Number of 3-grams hit = 377  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle858.out
Will force inclusive back-off from OOVs.
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 255 words.
Number of 3-grams hit = 253  (99.22%)
Number of 2-grams hit = 1  (0.39%)
Number of 1-grams hit = 1  (0.39%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle859.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1192 words.
Number of 3-grams hit = 1189  (99.75%)
Number of 2-grams hit = 2  (0.17%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle860.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle861.out
Will force inclusive back-off from OOVs.
Perplexity = 3.31, Entropy = 1.72 bits
Computation based on 432 words.
Number of 3-grams hit = 430  (99.54%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle862.out
Will force inclusive back-off from OOVs.
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 487 words.
Number of 3-grams hit = 484  (99.38%)
Number of 2-grams hit = 2  (0.41%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle863.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 712 words.
Number of 3-grams hit = 709  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle864.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 494 words.
Number of 3-grams hit = 489  (98.99%)
Number of 2-grams hit = 4  (0.81%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle865.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 6948 words.
Number of 3-grams hit = 6944  (99.94%)
Number of 2-grams hit = 3  (0.04%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle866.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.84 bits
Computation based on 668 words.
Number of 3-grams hit = 665  (99.55%)
Number of 2-grams hit = 2  (0.30%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle867.out
Will force inclusive back-off from OOVs.
Perplexity = 3.95, Entropy = 1.98 bits
Computation based on 377 words.
Number of 3-grams hit = 375  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle868.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1799 words.
Number of 3-grams hit = 1795  (99.78%)
Number of 2-grams hit = 3  (0.17%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle869.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 3365 words.
Number of 3-grams hit = 3360  (99.85%)
Number of 2-grams hit = 4  (0.12%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle870.out
Will force inclusive back-off from OOVs.
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 831 words.
Number of 3-grams hit = 828  (99.64%)
Number of 2-grams hit = 2  (0.24%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle871.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 444 words.
Number of 3-grams hit = 442  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle872.out
Will force inclusive back-off from OOVs.
Perplexity = 4.14, Entropy = 2.05 bits
Computation based on 316 words.
Number of 3-grams hit = 314  (99.37%)
Number of 2-grams hit = 1  (0.32%)
Number of 1-grams hit = 1  (0.32%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle873.out
Will force inclusive back-off from OOVs.
Perplexity = 3.26, Entropy = 1.70 bits
Computation based on 1015 words.
Number of 3-grams hit = 1013  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle874.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 460 words.
Number of 3-grams hit = 458  (99.57%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle875.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 855 words.
Number of 3-grams hit = 853  (99.77%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle876.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 10045 words.
Number of 3-grams hit = 10031  (99.86%)
Number of 2-grams hit = 12  (0.12%)
Number of 1-grams hit = 2  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle877.out
Will force inclusive back-off from OOVs.
Perplexity = 3.62, Entropy = 1.86 bits
Computation based on 502 words.
Number of 3-grams hit = 500  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle878.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.93 bits
Computation based on 364 words.
Number of 3-grams hit = 362  (99.45%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle879.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 591 words.
Number of 3-grams hit = 589  (99.66%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle880.out
Will force inclusive back-off from OOVs.
Perplexity = 3.34, Entropy = 1.74 bits
Computation based on 509 words.
Number of 3-grams hit = 507  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle881.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 376 words.
Number of 3-grams hit = 374  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle882.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 638 words.
Number of 3-grams hit = 636  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle883.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 638 words.
Number of 3-grams hit = 636  (99.69%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle884.out
Will force inclusive back-off from OOVs.
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 777 words.
Number of 3-grams hit = 774  (99.61%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle885.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 534 words.
Number of 3-grams hit = 532  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle886.out
Will force inclusive back-off from OOVs.
Perplexity = 3.60, Entropy = 1.85 bits
Computation based on 508 words.
Number of 3-grams hit = 506  (99.61%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle887.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 517 words.
Number of 3-grams hit = 515  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle888.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1311 words.
Number of 3-grams hit = 1309  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle889.out
Will force inclusive back-off from OOVs.
Perplexity = 3.44, Entropy = 1.78 bits
Computation based on 4700 words.
Number of 3-grams hit = 4694  (99.87%)
Number of 2-grams hit = 5  (0.11%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle890.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1148 words.
Number of 3-grams hit = 1146  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle891.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.84 bits
Computation based on 873 words.
Number of 3-grams hit = 871  (99.77%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle892.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 3489 words.
Number of 3-grams hit = 3486  (99.91%)
Number of 2-grams hit = 2  (0.06%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle893.out
Will force inclusive back-off from OOVs.
Perplexity = 4.19, Entropy = 2.07 bits
Computation based on 699 words.
Number of 3-grams hit = 697  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle894.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 805 words.
Number of 3-grams hit = 803  (99.75%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle895.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 188 words.
Number of 3-grams hit = 186  (98.94%)
Number of 2-grams hit = 1  (0.53%)
Number of 1-grams hit = 1  (0.53%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle896.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 452 words.
Number of 3-grams hit = 449  (99.34%)
Number of 2-grams hit = 2  (0.44%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle897.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 767 words.
Number of 3-grams hit = 763  (99.48%)
Number of 2-grams hit = 3  (0.39%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle898.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1746 words.
Number of 3-grams hit = 1743  (99.83%)
Number of 2-grams hit = 2  (0.11%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle899.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.79 bits
Computation based on 219 words.
Number of 3-grams hit = 217  (99.09%)
Number of 2-grams hit = 1  (0.46%)
Number of 1-grams hit = 1  (0.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle900.out
Will force inclusive back-off from OOVs.
Perplexity = 3.29, Entropy = 1.72 bits
Computation based on 1135 words.
Number of 3-grams hit = 1133  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle901.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 428 words.
Number of 3-grams hit = 426  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle902.out
Will force inclusive back-off from OOVs.
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 2155 words.
Number of 3-grams hit = 2152  (99.86%)
Number of 2-grams hit = 2  (0.09%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle903.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 1273 words.
Number of 3-grams hit = 1271  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle904.out
Will force inclusive back-off from OOVs.
Perplexity = 2.70, Entropy = 1.43 bits
Computation based on 649 words.
Number of 3-grams hit = 647  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle905.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 388 words.
Number of 3-grams hit = 386  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle906.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 476 words.
Number of 3-grams hit = 474  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle907.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 488 words.
Number of 3-grams hit = 486  (99.59%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle908.out
Will force inclusive back-off from OOVs.
Perplexity = 3.12, Entropy = 1.64 bits
Computation based on 552 words.
Number of 3-grams hit = 550  (99.64%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle909.out
Will force inclusive back-off from OOVs.
Perplexity = 3.47, Entropy = 1.80 bits
Computation based on 456 words.
Number of 3-grams hit = 454  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle910.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 737 words.
Number of 3-grams hit = 733  (99.46%)
Number of 2-grams hit = 3  (0.41%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle911.out
Will force inclusive back-off from OOVs.
Perplexity = 3.07, Entropy = 1.62 bits
Computation based on 688 words.
Number of 3-grams hit = 686  (99.71%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle912.out
Will force inclusive back-off from OOVs.
Perplexity = 4.05, Entropy = 2.02 bits
Computation based on 743 words.
Number of 3-grams hit = 740  (99.60%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle913.out
Will force inclusive back-off from OOVs.
Perplexity = 4.01, Entropy = 2.00 bits
Computation based on 743 words.
Number of 3-grams hit = 740  (99.60%)
Number of 2-grams hit = 2  (0.27%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle914.out
Will force inclusive back-off from OOVs.
Perplexity = 3.73, Entropy = 1.90 bits
Computation based on 575 words.
Number of 3-grams hit = 573  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle915.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.74 bits
Computation based on 1309 words.
Number of 3-grams hit = 1307  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle916.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 1003 words.
Number of 3-grams hit = 1001  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle917.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 1180 words.
Number of 3-grams hit = 1178  (99.83%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle918.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 535 words.
Number of 3-grams hit = 532  (99.44%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle919.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 3585 words.
Number of 3-grams hit = 3580  (99.86%)
Number of 2-grams hit = 3  (0.08%)
Number of 1-grams hit = 2  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle920.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 1001 words.
Number of 3-grams hit = 998  (99.70%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle921.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 844 words.
Number of 3-grams hit = 840  (99.53%)
Number of 2-grams hit = 3  (0.36%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle922.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 515 words.
Number of 3-grams hit = 512  (99.42%)
Number of 2-grams hit = 2  (0.39%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle923.out
Will force inclusive back-off from OOVs.
Perplexity = 3.09, Entropy = 1.63 bits
Computation based on 560 words.
Number of 3-grams hit = 556  (99.29%)
Number of 2-grams hit = 3  (0.54%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle924.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 376 words.
Number of 3-grams hit = 374  (99.47%)
Number of 2-grams hit = 1  (0.27%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle925.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 4136 words.
Number of 3-grams hit = 4129  (99.83%)
Number of 2-grams hit = 6  (0.15%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle926.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1421 words.
Number of 3-grams hit = 1418  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle927.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 440 words.
Number of 3-grams hit = 438  (99.55%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle928.out
Will force inclusive back-off from OOVs.
Perplexity = 3.69, Entropy = 1.88 bits
Computation based on 516 words.
Number of 3-grams hit = 514  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle929.out
Will force inclusive back-off from OOVs.
Perplexity = 3.17, Entropy = 1.66 bits
Computation based on 538 words.
Number of 3-grams hit = 536  (99.63%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle930.out
Will force inclusive back-off from OOVs.
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 465 words.
Number of 3-grams hit = 462  (99.35%)
Number of 2-grams hit = 2  (0.43%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle931.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 645 words.
Number of 3-grams hit = 641  (99.38%)
Number of 2-grams hit = 3  (0.47%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle932.out
Will force inclusive back-off from OOVs.
Perplexity = 2.74, Entropy = 1.45 bits
Computation based on 228 words.
Number of 3-grams hit = 226  (99.12%)
Number of 2-grams hit = 1  (0.44%)
Number of 1-grams hit = 1  (0.44%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle933.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 453 words.
Number of 3-grams hit = 451  (99.56%)
Number of 2-grams hit = 1  (0.22%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle934.out
Will force inclusive back-off from OOVs.
Perplexity = 3.90, Entropy = 1.96 bits
Computation based on 416 words.
Number of 3-grams hit = 414  (99.52%)
Number of 2-grams hit = 1  (0.24%)
Number of 1-grams hit = 1  (0.24%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle935.out
Will force inclusive back-off from OOVs.
Perplexity = 3.30, Entropy = 1.72 bits
Computation based on 1259 words.
Number of 3-grams hit = 1257  (99.84%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle936.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 483 words.
Number of 3-grams hit = 481  (99.59%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle937.out
Will force inclusive back-off from OOVs.
Perplexity = 3.36, Entropy = 1.75 bits
Computation based on 239 words.
Number of 3-grams hit = 237  (99.16%)
Number of 2-grams hit = 1  (0.42%)
Number of 1-grams hit = 1  (0.42%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle938.out
Will force inclusive back-off from OOVs.
Perplexity = 3.23, Entropy = 1.69 bits
Computation based on 1402 words.
Number of 3-grams hit = 1399  (99.79%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle939.out
Will force inclusive back-off from OOVs.
Perplexity = 3.52, Entropy = 1.81 bits
Computation based on 519 words.
Number of 3-grams hit = 517  (99.61%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle940.out
Will force inclusive back-off from OOVs.
Perplexity = 3.42, Entropy = 1.78 bits
Computation based on 2076 words.
Number of 3-grams hit = 2073  (99.86%)
Number of 2-grams hit = 2  (0.10%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle941.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.90 bits
Computation based on 1305 words.
Number of 3-grams hit = 1303  (99.85%)
Number of 2-grams hit = 1  (0.08%)
Number of 1-grams hit = 1  (0.08%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle942.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 571 words.
Number of 3-grams hit = 569  (99.65%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle943.out
Will force inclusive back-off from OOVs.
Perplexity = 3.05, Entropy = 1.61 bits
Computation based on 1491 words.
Number of 3-grams hit = 1489  (99.87%)
Number of 2-grams hit = 1  (0.07%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle944.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 477 words.
Number of 3-grams hit = 475  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle945.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 3747 words.
Number of 3-grams hit = 3740  (99.81%)
Number of 2-grams hit = 6  (0.16%)
Number of 1-grams hit = 1  (0.03%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle946.out
Will force inclusive back-off from OOVs.
Perplexity = 3.66, Entropy = 1.87 bits
Computation based on 611 words.
Number of 3-grams hit = 609  (99.67%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle947.out
Will force inclusive back-off from OOVs.
Perplexity = 3.87, Entropy = 1.95 bits
Computation based on 459 words.
Number of 3-grams hit = 455  (99.13%)
Number of 2-grams hit = 3  (0.65%)
Number of 1-grams hit = 1  (0.22%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle948.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 627 words.
Number of 3-grams hit = 624  (99.52%)
Number of 2-grams hit = 2  (0.32%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle949.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1862 words.
Number of 3-grams hit = 1857  (99.73%)
Number of 2-grams hit = 4  (0.21%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle950.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 543 words.
Number of 3-grams hit = 541  (99.63%)
Number of 2-grams hit = 1  (0.18%)
Number of 1-grams hit = 1  (0.18%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle951.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 622 words.
Number of 3-grams hit = 620  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle952.out
Will force inclusive back-off from OOVs.
Perplexity = 3.37, Entropy = 1.75 bits
Computation based on 1389 words.
Number of 3-grams hit = 1386  (99.78%)
Number of 2-grams hit = 2  (0.14%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle953.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 2294 words.
Number of 3-grams hit = 2290  (99.83%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle954.out
Will force inclusive back-off from OOVs.
Perplexity = 3.54, Entropy = 1.82 bits
Computation based on 800 words.
Number of 3-grams hit = 796  (99.50%)
Number of 2-grams hit = 3  (0.38%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle955.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 1574 words.
Number of 3-grams hit = 1572  (99.87%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle956.out
Will force inclusive back-off from OOVs.
Perplexity = 3.56, Entropy = 1.83 bits
Computation based on 1107 words.
Number of 3-grams hit = 1105  (99.82%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle957.out
Will force inclusive back-off from OOVs.
Perplexity = 3.11, Entropy = 1.64 bits
Computation based on 323 words.
Number of 3-grams hit = 321  (99.38%)
Number of 2-grams hit = 1  (0.31%)
Number of 1-grams hit = 1  (0.31%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle958.out
Will force inclusive back-off from OOVs.
Perplexity = 3.39, Entropy = 1.76 bits
Computation based on 1014 words.
Number of 3-grams hit = 1011  (99.70%)
Number of 2-grams hit = 2  (0.20%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle959.out
Will force inclusive back-off from OOVs.
Perplexity = 4.24, Entropy = 2.08 bits
Computation based on 218 words.
Number of 3-grams hit = 216  (99.08%)
Number of 2-grams hit = 1  (0.46%)
Number of 1-grams hit = 1  (0.46%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle960.out
Will force inclusive back-off from OOVs.
Perplexity = 2.86, Entropy = 1.51 bits
Computation based on 652 words.
Number of 3-grams hit = 650  (99.69%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle961.out
Will force inclusive back-off from OOVs.
Perplexity = 3.91, Entropy = 1.97 bits
Computation based on 534 words.
Number of 3-grams hit = 531  (99.44%)
Number of 2-grams hit = 2  (0.37%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle962.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 2369 words.
Number of 3-grams hit = 2365  (99.83%)
Number of 2-grams hit = 3  (0.13%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle963.out
Will force inclusive back-off from OOVs.
Perplexity = 3.45, Entropy = 1.79 bits
Computation based on 2187 words.
Number of 3-grams hit = 2182  (99.77%)
Number of 2-grams hit = 4  (0.18%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle964.out
Will force inclusive back-off from OOVs.
Perplexity = 3.88, Entropy = 1.96 bits
Computation based on 1090 words.
Number of 3-grams hit = 1087  (99.72%)
Number of 2-grams hit = 2  (0.18%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle965.out
Will force inclusive back-off from OOVs.
Perplexity = 3.27, Entropy = 1.71 bits
Computation based on 701 words.
Number of 3-grams hit = 699  (99.71%)
Number of 2-grams hit = 1  (0.14%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle966.out
Will force inclusive back-off from OOVs.
Perplexity = 3.53, Entropy = 1.82 bits
Computation based on 850 words.
Number of 3-grams hit = 848  (99.76%)
Number of 2-grams hit = 1  (0.12%)
Number of 1-grams hit = 1  (0.12%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle967.out
Will force inclusive back-off from OOVs.
Perplexity = 3.46, Entropy = 1.79 bits
Computation based on 426 words.
Number of 3-grams hit = 424  (99.53%)
Number of 2-grams hit = 1  (0.23%)
Number of 1-grams hit = 1  (0.23%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle968.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 1147 words.
Number of 3-grams hit = 1145  (99.83%)
Number of 2-grams hit = 1  (0.09%)
Number of 1-grams hit = 1  (0.09%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle969.out
Will force inclusive back-off from OOVs.
Perplexity = 3.68, Entropy = 1.88 bits
Computation based on 773 words.
Number of 3-grams hit = 770  (99.61%)
Number of 2-grams hit = 2  (0.26%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle970.out
Will force inclusive back-off from OOVs.
Perplexity = 3.59, Entropy = 1.85 bits
Computation based on 798 words.
Number of 3-grams hit = 795  (99.62%)
Number of 2-grams hit = 2  (0.25%)
Number of 1-grams hit = 1  (0.13%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle971.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 504 words.
Number of 3-grams hit = 502  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle972.out
Will force inclusive back-off from OOVs.
Perplexity = 3.57, Entropy = 1.83 bits
Computation based on 2041 words.
Number of 3-grams hit = 2034  (99.66%)
Number of 2-grams hit = 6  (0.29%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle973.out
Will force inclusive back-off from OOVs.
Perplexity = 3.75, Entropy = 1.91 bits
Computation based on 663 words.
Number of 3-grams hit = 661  (99.70%)
Number of 2-grams hit = 1  (0.15%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle974.out
Will force inclusive back-off from OOVs.
Perplexity = 2.86, Entropy = 1.51 bits
Computation based on 1022 words.
Number of 3-grams hit = 1020  (99.80%)
Number of 2-grams hit = 1  (0.10%)
Number of 1-grams hit = 1  (0.10%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle975.out
Will force inclusive back-off from OOVs.
Perplexity = 3.21, Entropy = 1.68 bits
Computation based on 380 words.
Number of 3-grams hit = 378  (99.47%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle976.out
Will force inclusive back-off from OOVs.
Perplexity = 3.74, Entropy = 1.90 bits
Computation based on 385 words.
Number of 3-grams hit = 383  (99.48%)
Number of 2-grams hit = 1  (0.26%)
Number of 1-grams hit = 1  (0.26%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle977.out
Will force inclusive back-off from OOVs.
Perplexity = 3.72, Entropy = 1.89 bits
Computation based on 917 words.
Number of 3-grams hit = 914  (99.67%)
Number of 2-grams hit = 2  (0.22%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle978.out
Will force inclusive back-off from OOVs.
Perplexity = 3.80, Entropy = 1.92 bits
Computation based on 503 words.
Number of 3-grams hit = 501  (99.60%)
Number of 2-grams hit = 1  (0.20%)
Number of 1-grams hit = 1  (0.20%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle979.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 1716 words.
Number of 3-grams hit = 1714  (99.88%)
Number of 2-grams hit = 1  (0.06%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle980.out
Will force inclusive back-off from OOVs.
Perplexity = 3.61, Entropy = 1.85 bits
Computation based on 681 words.
Number of 3-grams hit = 677  (99.41%)
Number of 2-grams hit = 3  (0.44%)
Number of 1-grams hit = 1  (0.15%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle981.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 371 words.
Number of 3-grams hit = 367  (98.92%)
Number of 2-grams hit = 3  (0.81%)
Number of 1-grams hit = 1  (0.27%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle982.out
Will force inclusive back-off from OOVs.
Perplexity = 3.55, Entropy = 1.83 bits
Computation based on 890 words.
Number of 3-grams hit = 888  (99.78%)
Number of 2-grams hit = 1  (0.11%)
Number of 1-grams hit = 1  (0.11%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle983.out
Will force inclusive back-off from OOVs.
Perplexity = 3.70, Entropy = 1.89 bits
Computation based on 520 words.
Number of 3-grams hit = 518  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle984.out
Will force inclusive back-off from OOVs.
Perplexity = 3.41, Entropy = 1.77 bits
Computation based on 479 words.
Number of 3-grams hit = 477  (99.58%)
Number of 2-grams hit = 1  (0.21%)
Number of 1-grams hit = 1  (0.21%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle985.out
Will force inclusive back-off from OOVs.
Perplexity = 3.35, Entropy = 1.75 bits
Computation based on 1351 words.
Number of 3-grams hit = 1348  (99.78%)
Number of 2-grams hit = 2  (0.15%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle986.out
Will force inclusive back-off from OOVs.
Perplexity = 3.58, Entropy = 1.84 bits
Computation based on 297 words.
Number of 3-grams hit = 295  (99.33%)
Number of 2-grams hit = 1  (0.34%)
Number of 1-grams hit = 1  (0.34%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle987.out
Will force inclusive back-off from OOVs.
Perplexity = 3.64, Entropy = 1.86 bits
Computation based on 1891 words.
Number of 3-grams hit = 1887  (99.79%)
Number of 2-grams hit = 3  (0.16%)
Number of 1-grams hit = 1  (0.05%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle988.out
Will force inclusive back-off from OOVs.
Perplexity = 3.38, Entropy = 1.76 bits
Computation based on 1353 words.
Number of 3-grams hit = 1349  (99.70%)
Number of 2-grams hit = 3  (0.22%)
Number of 1-grams hit = 1  (0.07%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle989.out
Will force inclusive back-off from OOVs.
Perplexity = 3.48, Entropy = 1.80 bits
Computation based on 2476 words.
Number of 3-grams hit = 2473  (99.88%)
Number of 2-grams hit = 2  (0.08%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle990.out
Will force inclusive back-off from OOVs.
Perplexity = 3.25, Entropy = 1.70 bits
Computation based on 710 words.
Number of 3-grams hit = 707  (99.58%)
Number of 2-grams hit = 2  (0.28%)
Number of 1-grams hit = 1  (0.14%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle991.out
Will force inclusive back-off from OOVs.
Perplexity = 3.67, Entropy = 1.88 bits
Computation based on 1688 words.
Number of 3-grams hit = 1685  (99.82%)
Number of 2-grams hit = 2  (0.12%)
Number of 1-grams hit = 1  (0.06%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle992.out
Will force inclusive back-off from OOVs.
Perplexity = 3.82, Entropy = 1.93 bits
Computation based on 578 words.
Number of 3-grams hit = 576  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle993.out
Will force inclusive back-off from OOVs.
Perplexity = 3.50, Entropy = 1.81 bits
Computation based on 2666 words.
Number of 3-grams hit = 2662  (99.85%)
Number of 2-grams hit = 3  (0.11%)
Number of 1-grams hit = 1  (0.04%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle994.out
Will force inclusive back-off from OOVs.
Perplexity = 3.89, Entropy = 1.96 bits
Computation based on 527 words.
Number of 3-grams hit = 525  (99.62%)
Number of 2-grams hit = 1  (0.19%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle995.out
Will force inclusive back-off from OOVs.
Perplexity = 3.63, Entropy = 1.86 bits
Computation based on 7968 words.
Number of 3-grams hit = 7961  (99.91%)
Number of 2-grams hit = 6  (0.08%)
Number of 1-grams hit = 1  (0.01%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle996.out
Will force inclusive back-off from OOVs.
Perplexity = 3.49, Entropy = 1.80 bits
Computation based on 627 words.
Number of 3-grams hit = 625  (99.68%)
Number of 2-grams hit = 1  (0.16%)
Number of 1-grams hit = 1  (0.16%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle997.out
Will force inclusive back-off from OOVs.
Perplexity = 3.43, Entropy = 1.78 bits
Computation based on 5785 words.
Number of 3-grams hit = 5775  (99.83%)
Number of 2-grams hit = 9  (0.16%)
Number of 1-grams hit = 1  (0.02%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle998.out
Will force inclusive back-off from OOVs.
Perplexity = 3.77, Entropy = 1.91 bits
Computation based on 572 words.
Number of 3-grams hit = 570  (99.65%)
Number of 2-grams hit = 1  (0.17%)
Number of 1-grams hit = 1  (0.17%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle999.out
Will force inclusive back-off from OOVs.
Perplexity = 3.65, Entropy = 1.87 bits
Computation based on 523 words.
Number of 3-grams hit = 520  (99.43%)
Number of 2-grams hit = 2  (0.38%)
Number of 1-grams hit = 1  (0.19%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : 