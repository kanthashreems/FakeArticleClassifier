evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article0.text
Will force inclusive back-off from OOVs.
Perplexity = 724.81, Entropy = 9.50 bits
Computation based on 1255 words.
Number of 1-grams hit = 1255  (100.00%)
7 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article1.text
Will force inclusive back-off from OOVs.
Perplexity = 552.23, Entropy = 9.11 bits
Computation based on 1499 words.
Number of 1-grams hit = 1499  (100.00%)
6 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article2.text
Will force inclusive back-off from OOVs.
Perplexity = 707.02, Entropy = 9.47 bits
Computation based on 540 words.
Number of 1-grams hit = 540  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article3.text
Will force inclusive back-off from OOVs.
Perplexity = 845.37, Entropy = 9.72 bits
Computation based on 620 words.
Number of 1-grams hit = 620  (100.00%)
5 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article4.text
Will force inclusive back-off from OOVs.
Perplexity = 883.73, Entropy = 9.79 bits
Computation based on 395 words.
Number of 1-grams hit = 395  (100.00%)
3 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article5.text
Will force inclusive back-off from OOVs.
Perplexity = 840.88, Entropy = 9.72 bits
Computation based on 888 words.
Number of 1-grams hit = 888  (100.00%)
2 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article6.text
Will force inclusive back-off from OOVs.
Perplexity = 1088.19, Entropy = 10.09 bits
Computation based on 281 words.
Number of 1-grams hit = 281  (100.00%)
6 OOVs (2.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article7.text
Will force inclusive back-off from OOVs.
Perplexity = 803.81, Entropy = 9.65 bits
Computation based on 605 words.
Number of 1-grams hit = 605  (100.00%)
6 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article8.text
Will force inclusive back-off from OOVs.
Perplexity = 1177.10, Entropy = 10.20 bits
Computation based on 496 words.
Number of 1-grams hit = 496  (100.00%)
8 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article9.text
Will force inclusive back-off from OOVs.
Perplexity = 783.86, Entropy = 9.61 bits
Computation based on 314 words.
Number of 1-grams hit = 314  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article10.text
Will force inclusive back-off from OOVs.
Perplexity = 859.88, Entropy = 9.75 bits
Computation based on 304 words.
Number of 1-grams hit = 304  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article11.text
Will force inclusive back-off from OOVs.
Perplexity = 1004.96, Entropy = 9.97 bits
Computation based on 302 words.
Number of 1-grams hit = 302  (100.00%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article12.text
Will force inclusive back-off from OOVs.
Perplexity = 548.73, Entropy = 9.10 bits
Computation based on 300 words.
Number of 1-grams hit = 300  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article13.text
Will force inclusive back-off from OOVs.
Perplexity = 1017.50, Entropy = 9.99 bits
Computation based on 453 words.
Number of 1-grams hit = 453  (100.00%)
11 OOVs (2.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article14.text
Will force inclusive back-off from OOVs.
Perplexity = 802.97, Entropy = 9.65 bits
Computation based on 468 words.
Number of 1-grams hit = 468  (100.00%)
7 OOVs (1.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article15.text
Will force inclusive back-off from OOVs.
Perplexity = 665.93, Entropy = 9.38 bits
Computation based on 493 words.
Number of 1-grams hit = 493  (100.00%)
4 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article16.text
Will force inclusive back-off from OOVs.
Perplexity = 868.11, Entropy = 9.76 bits
Computation based on 365 words.
Number of 1-grams hit = 365  (100.00%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article17.text
Will force inclusive back-off from OOVs.
Perplexity = 952.61, Entropy = 9.90 bits
Computation based on 673 words.
Number of 1-grams hit = 673  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article18.text
Will force inclusive back-off from OOVs.
Perplexity = 655.42, Entropy = 9.36 bits
Computation based on 406 words.
Number of 1-grams hit = 406  (100.00%)
6 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article19.text
Will force inclusive back-off from OOVs.
Perplexity = 975.81, Entropy = 9.93 bits
Computation based on 525 words.
Number of 1-grams hit = 525  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article20.text
Will force inclusive back-off from OOVs.
Perplexity = 859.65, Entropy = 9.75 bits
Computation based on 348 words.
Number of 1-grams hit = 348  (100.00%)
4 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article21.text
Will force inclusive back-off from OOVs.
Perplexity = 1021.81, Entropy = 10.00 bits
Computation based on 461 words.
Number of 1-grams hit = 461  (100.00%)
4 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article22.text
Will force inclusive back-off from OOVs.
Perplexity = 922.35, Entropy = 9.85 bits
Computation based on 360 words.
Number of 1-grams hit = 360  (100.00%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article23.text
Will force inclusive back-off from OOVs.
Perplexity = 1048.78, Entropy = 10.03 bits
Computation based on 345 words.
Number of 1-grams hit = 345  (100.00%)
12 OOVs (3.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article24.text
Will force inclusive back-off from OOVs.
Perplexity = 786.40, Entropy = 9.62 bits
Computation based on 561 words.
Number of 1-grams hit = 561  (100.00%)
5 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article25.text
Will force inclusive back-off from OOVs.
Perplexity = 966.61, Entropy = 9.92 bits
Computation based on 373 words.
Number of 1-grams hit = 373  (100.00%)
7 OOVs (1.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article26.text
Will force inclusive back-off from OOVs.
Perplexity = 826.08, Entropy = 9.69 bits
Computation based on 1519 words.
Number of 1-grams hit = 1519  (100.00%)
13 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article27.text
Will force inclusive back-off from OOVs.
Perplexity = 1019.86, Entropy = 9.99 bits
Computation based on 389 words.
Number of 1-grams hit = 389  (100.00%)
4 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article28.text
Will force inclusive back-off from OOVs.
Perplexity = 787.02, Entropy = 9.62 bits
Computation based on 1388 words.
Number of 1-grams hit = 1388  (100.00%)
11 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article29.text
Will force inclusive back-off from OOVs.
Perplexity = 691.08, Entropy = 9.43 bits
Computation based on 317 words.
Number of 1-grams hit = 317  (100.00%)
2 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article30.text
Will force inclusive back-off from OOVs.
Perplexity = 670.73, Entropy = 9.39 bits
Computation based on 475 words.
Number of 1-grams hit = 475  (100.00%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article31.text
Will force inclusive back-off from OOVs.
Perplexity = 808.93, Entropy = 9.66 bits
Computation based on 439 words.
Number of 1-grams hit = 439  (100.00%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article32.text
Will force inclusive back-off from OOVs.
Perplexity = 988.19, Entropy = 9.95 bits
Computation based on 475 words.
Number of 1-grams hit = 475  (100.00%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article33.text
Will force inclusive back-off from OOVs.
Perplexity = 907.52, Entropy = 9.83 bits
Computation based on 441 words.
Number of 1-grams hit = 441  (100.00%)
3 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article34.text
Will force inclusive back-off from OOVs.
Perplexity = 710.48, Entropy = 9.47 bits
Computation based on 304 words.
Number of 1-grams hit = 304  (100.00%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article35.text
Will force inclusive back-off from OOVs.
Perplexity = 980.68, Entropy = 9.94 bits
Computation based on 378 words.
Number of 1-grams hit = 378  (100.00%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article36.text
Will force inclusive back-off from OOVs.
Perplexity = 629.72, Entropy = 9.30 bits
Computation based on 1000 words.
Number of 1-grams hit = 1000  (100.00%)
11 OOVs (1.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article37.text
Will force inclusive back-off from OOVs.
Perplexity = 971.56, Entropy = 9.92 bits
Computation based on 559 words.
Number of 1-grams hit = 559  (100.00%)
10 OOVs (1.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article38.text
Will force inclusive back-off from OOVs.
Perplexity = 946.28, Entropy = 9.89 bits
Computation based on 408 words.
Number of 1-grams hit = 408  (100.00%)
4 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article39.text
Will force inclusive back-off from OOVs.
Perplexity = 628.81, Entropy = 9.30 bits
Computation based on 2623 words.
Number of 1-grams hit = 2623  (100.00%)
37 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article40.text
Will force inclusive back-off from OOVs.
Perplexity = 768.10, Entropy = 9.59 bits
Computation based on 405 words.
Number of 1-grams hit = 405  (100.00%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article41.text
Will force inclusive back-off from OOVs.
Perplexity = 678.98, Entropy = 9.41 bits
Computation based on 969 words.
Number of 1-grams hit = 969  (100.00%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article42.text
Will force inclusive back-off from OOVs.
Perplexity = 625.45, Entropy = 9.29 bits
Computation based on 547 words.
Number of 1-grams hit = 547  (100.00%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article43.text
Will force inclusive back-off from OOVs.
Perplexity = 684.92, Entropy = 9.42 bits
Computation based on 449 words.
Number of 1-grams hit = 449  (100.00%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article44.text
Will force inclusive back-off from OOVs.
Perplexity = 695.07, Entropy = 9.44 bits
Computation based on 492 words.
Number of 1-grams hit = 492  (100.00%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article45.text
Will force inclusive back-off from OOVs.
Perplexity = 781.86, Entropy = 9.61 bits
Computation based on 464 words.
Number of 1-grams hit = 464  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article46.text
Will force inclusive back-off from OOVs.
Perplexity = 1153.96, Entropy = 10.17 bits
Computation based on 401 words.
Number of 1-grams hit = 401  (100.00%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article47.text
Will force inclusive back-off from OOVs.
Perplexity = 736.81, Entropy = 9.53 bits
Computation based on 2678 words.
Number of 1-grams hit = 2678  (100.00%)
6 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article48.text
Will force inclusive back-off from OOVs.
Perplexity = 1121.41, Entropy = 10.13 bits
Computation based on 366 words.
Number of 1-grams hit = 366  (100.00%)
7 OOVs (1.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article49.text
Will force inclusive back-off from OOVs.
Perplexity = 629.78, Entropy = 9.30 bits
Computation based on 364 words.
Number of 1-grams hit = 364  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article50.text
Will force inclusive back-off from OOVs.
Perplexity = 714.75, Entropy = 9.48 bits
Computation based on 522 words.
Number of 1-grams hit = 522  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article51.text
Will force inclusive back-off from OOVs.
Perplexity = 915.90, Entropy = 9.84 bits
Computation based on 1102 words.
Number of 1-grams hit = 1102  (100.00%)
19 OOVs (1.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article52.text
Will force inclusive back-off from OOVs.
Perplexity = 640.42, Entropy = 9.32 bits
Computation based on 360 words.
Number of 1-grams hit = 360  (100.00%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article53.text
Will force inclusive back-off from OOVs.
Perplexity = 893.52, Entropy = 9.80 bits
Computation based on 472 words.
Number of 1-grams hit = 472  (100.00%)
9 OOVs (1.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article54.text
Will force inclusive back-off from OOVs.
Perplexity = 872.84, Entropy = 9.77 bits
Computation based on 1063 words.
Number of 1-grams hit = 1063  (100.00%)
21 OOVs (1.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article55.text
Will force inclusive back-off from OOVs.
Perplexity = 545.23, Entropy = 9.09 bits
Computation based on 533 words.
Number of 1-grams hit = 533  (100.00%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article56.text
Will force inclusive back-off from OOVs.
Perplexity = 616.36, Entropy = 9.27 bits
Computation based on 898 words.
Number of 1-grams hit = 898  (100.00%)
7 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article57.text
Will force inclusive back-off from OOVs.
Perplexity = 567.61, Entropy = 9.15 bits
Computation based on 1537 words.
Number of 1-grams hit = 1537  (100.00%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article58.text
Will force inclusive back-off from OOVs.
Perplexity = 725.49, Entropy = 9.50 bits
Computation based on 383 words.
Number of 1-grams hit = 383  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article59.text
Will force inclusive back-off from OOVs.
Perplexity = 576.37, Entropy = 9.17 bits
Computation based on 208 words.
Number of 1-grams hit = 208  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article60.text
Will force inclusive back-off from OOVs.
Perplexity = 770.58, Entropy = 9.59 bits
Computation based on 321 words.
Number of 1-grams hit = 321  (100.00%)
13 OOVs (3.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article61.text
Will force inclusive back-off from OOVs.
Perplexity = 740.70, Entropy = 9.53 bits
Computation based on 489 words.
Number of 1-grams hit = 489  (100.00%)
2 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article62.text
Will force inclusive back-off from OOVs.
Perplexity = 973.31, Entropy = 9.93 bits
Computation based on 1282 words.
Number of 1-grams hit = 1282  (100.00%)
11 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article63.text
Will force inclusive back-off from OOVs.
Perplexity = 741.96, Entropy = 9.54 bits
Computation based on 965 words.
Number of 1-grams hit = 965  (100.00%)
25 OOVs (2.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article64.text
Will force inclusive back-off from OOVs.
Perplexity = 830.05, Entropy = 9.70 bits
Computation based on 4162 words.
Number of 1-grams hit = 4162  (100.00%)
54 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article65.text
Will force inclusive back-off from OOVs.
Perplexity = 639.98, Entropy = 9.32 bits
Computation based on 628 words.
Number of 1-grams hit = 628  (100.00%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article66.text
Will force inclusive back-off from OOVs.
Perplexity = 667.87, Entropy = 9.38 bits
Computation based on 472 words.
Number of 1-grams hit = 472  (100.00%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article67.text
Will force inclusive back-off from OOVs.
Perplexity = 1004.16, Entropy = 9.97 bits
Computation based on 449 words.
Number of 1-grams hit = 449  (100.00%)
5 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article68.text
Will force inclusive back-off from OOVs.
Perplexity = 633.36, Entropy = 9.31 bits
Computation based on 1109 words.
Number of 1-grams hit = 1109  (100.00%)
11 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article69.text
Will force inclusive back-off from OOVs.
Perplexity = 1136.94, Entropy = 10.15 bits
Computation based on 380 words.
Number of 1-grams hit = 380  (100.00%)
3 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article70.text
Will force inclusive back-off from OOVs.
Perplexity = 593.19, Entropy = 9.21 bits
Computation based on 959 words.
Number of 1-grams hit = 959  (100.00%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article71.text
Will force inclusive back-off from OOVs.
Perplexity = 612.49, Entropy = 9.26 bits
Computation based on 550 words.
Number of 1-grams hit = 550  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article72.text
Will force inclusive back-off from OOVs.
Perplexity = 714.75, Entropy = 9.48 bits
Computation based on 1302 words.
Number of 1-grams hit = 1302  (100.00%)
6 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article73.text
Will force inclusive back-off from OOVs.
Perplexity = 533.84, Entropy = 9.06 bits
Computation based on 509 words.
Number of 1-grams hit = 509  (100.00%)
4 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article74.text
Will force inclusive back-off from OOVs.
Perplexity = 774.25, Entropy = 9.60 bits
Computation based on 818 words.
Number of 1-grams hit = 818  (100.00%)
5 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article75.text
Will force inclusive back-off from OOVs.
Perplexity = 840.58, Entropy = 9.72 bits
Computation based on 1190 words.
Number of 1-grams hit = 1190  (100.00%)
4 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article76.text
Will force inclusive back-off from OOVs.
Perplexity = 618.37, Entropy = 9.27 bits
Computation based on 1377 words.
Number of 1-grams hit = 1377  (100.00%)
5 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article77.text
Will force inclusive back-off from OOVs.
Perplexity = 1007.11, Entropy = 9.98 bits
Computation based on 288 words.
Number of 1-grams hit = 288  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article78.text
Will force inclusive back-off from OOVs.
Perplexity = 682.10, Entropy = 9.41 bits
Computation based on 1118 words.
Number of 1-grams hit = 1118  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article79.text
Will force inclusive back-off from OOVs.
Perplexity = 895.55, Entropy = 9.81 bits
Computation based on 1065 words.
Number of 1-grams hit = 1065  (100.00%)
8 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article80.text
Will force inclusive back-off from OOVs.
Perplexity = 560.63, Entropy = 9.13 bits
Computation based on 354 words.
Number of 1-grams hit = 354  (100.00%)
16 OOVs (4.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article81.text
Will force inclusive back-off from OOVs.
Perplexity = 684.78, Entropy = 9.42 bits
Computation based on 4029 words.
Number of 1-grams hit = 4029  (100.00%)
21 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article82.text
Will force inclusive back-off from OOVs.
Perplexity = 850.85, Entropy = 9.73 bits
Computation based on 467 words.
Number of 1-grams hit = 467  (100.00%)
4 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article83.text
Will force inclusive back-off from OOVs.
Perplexity = 648.82, Entropy = 9.34 bits
Computation based on 431 words.
Number of 1-grams hit = 431  (100.00%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article84.text
Will force inclusive back-off from OOVs.
Perplexity = 762.10, Entropy = 9.57 bits
Computation based on 399 words.
Number of 1-grams hit = 399  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article85.text
Will force inclusive back-off from OOVs.
Perplexity = 770.00, Entropy = 9.59 bits
Computation based on 1214 words.
Number of 1-grams hit = 1214  (100.00%)
5 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article86.text
Will force inclusive back-off from OOVs.
Perplexity = 605.69, Entropy = 9.24 bits
Computation based on 286 words.
Number of 1-grams hit = 286  (100.00%)
3 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article87.text
Will force inclusive back-off from OOVs.
Perplexity = 1284.27, Entropy = 10.33 bits
Computation based on 622 words.
Number of 1-grams hit = 622  (100.00%)
10 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article88.text
Will force inclusive back-off from OOVs.
Perplexity = 616.72, Entropy = 9.27 bits
Computation based on 1476 words.
Number of 1-grams hit = 1476  (100.00%)
23 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article89.text
Will force inclusive back-off from OOVs.
Perplexity = 919.27, Entropy = 9.84 bits
Computation based on 390 words.
Number of 1-grams hit = 390  (100.00%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article90.text
Will force inclusive back-off from OOVs.
Perplexity = 825.59, Entropy = 9.69 bits
Computation based on 506 words.
Number of 1-grams hit = 506  (100.00%)
14 OOVs (2.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article91.text
Will force inclusive back-off from OOVs.
Perplexity = 627.19, Entropy = 9.29 bits
Computation based on 342 words.
Number of 1-grams hit = 342  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article92.text
Will force inclusive back-off from OOVs.
Perplexity = 627.92, Entropy = 9.29 bits
Computation based on 1024 words.
Number of 1-grams hit = 1024  (100.00%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article93.text
Will force inclusive back-off from OOVs.
Perplexity = 791.65, Entropy = 9.63 bits
Computation based on 682 words.
Number of 1-grams hit = 682  (100.00%)
4 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article94.text
Will force inclusive back-off from OOVs.
Perplexity = 656.99, Entropy = 9.36 bits
Computation based on 1286 words.
Number of 1-grams hit = 1286  (100.00%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article95.text
Will force inclusive back-off from OOVs.
Perplexity = 610.38, Entropy = 9.25 bits
Computation based on 499 words.
Number of 1-grams hit = 499  (100.00%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article96.text
Will force inclusive back-off from OOVs.
Perplexity = 718.73, Entropy = 9.49 bits
Computation based on 397 words.
Number of 1-grams hit = 397  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article97.text
Will force inclusive back-off from OOVs.
Perplexity = 779.16, Entropy = 9.61 bits
Computation based on 544 words.
Number of 1-grams hit = 544  (100.00%)
9 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article98.text
Will force inclusive back-off from OOVs.
Perplexity = 806.34, Entropy = 9.66 bits
Computation based on 363 words.
Number of 1-grams hit = 363  (100.00%)
3 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article99.text
Will force inclusive back-off from OOVs.
Perplexity = 704.19, Entropy = 9.46 bits
Computation based on 431 words.
Number of 1-grams hit = 431  (100.00%)
25 OOVs (5.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article100.text
Will force inclusive back-off from OOVs.
Perplexity = 685.81, Entropy = 9.42 bits
Computation based on 474 words.
Number of 1-grams hit = 474  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article101.text
Will force inclusive back-off from OOVs.
Perplexity = 694.33, Entropy = 9.44 bits
Computation based on 486 words.
Number of 1-grams hit = 486  (100.00%)
21 OOVs (4.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article102.text
Will force inclusive back-off from OOVs.
Perplexity = 1025.10, Entropy = 10.00 bits
Computation based on 412 words.
Number of 1-grams hit = 412  (100.00%)
3 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article103.text
Will force inclusive back-off from OOVs.
Perplexity = 765.67, Entropy = 9.58 bits
Computation based on 476 words.
Number of 1-grams hit = 476  (100.00%)
22 OOVs (4.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article104.text
Will force inclusive back-off from OOVs.
Perplexity = 550.13, Entropy = 9.10 bits
Computation based on 6340 words.
Number of 1-grams hit = 6340  (100.00%)
19 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article105.text
Will force inclusive back-off from OOVs.
Perplexity = 711.18, Entropy = 9.47 bits
Computation based on 446 words.
Number of 1-grams hit = 446  (100.00%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article106.text
Will force inclusive back-off from OOVs.
Perplexity = 1030.00, Entropy = 10.01 bits
Computation based on 794 words.
Number of 1-grams hit = 794  (100.00%)
8 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article107.text
Will force inclusive back-off from OOVs.
Perplexity = 1279.25, Entropy = 10.32 bits
Computation based on 474 words.
Number of 1-grams hit = 474  (100.00%)
11 OOVs (2.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article108.text
Will force inclusive back-off from OOVs.
Perplexity = 652.28, Entropy = 9.35 bits
Computation based on 528 words.
Number of 1-grams hit = 528  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article109.text
Will force inclusive back-off from OOVs.
Perplexity = 700.42, Entropy = 9.45 bits
Computation based on 1123 words.
Number of 1-grams hit = 1123  (100.00%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article110.text
Will force inclusive back-off from OOVs.
Perplexity = 720.12, Entropy = 9.49 bits
Computation based on 908 words.
Number of 1-grams hit = 908  (100.00%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article111.text
Will force inclusive back-off from OOVs.
Perplexity = 638.32, Entropy = 9.32 bits
Computation based on 1223 words.
Number of 1-grams hit = 1223  (100.00%)
10 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article112.text
Will force inclusive back-off from OOVs.
Perplexity = 736.72, Entropy = 9.52 bits
Computation based on 328 words.
Number of 1-grams hit = 328  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article113.text
Will force inclusive back-off from OOVs.
Perplexity = 439.27, Entropy = 8.78 bits
Computation based on 5818 words.
Number of 1-grams hit = 5818  (100.00%)
13 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article114.text
Will force inclusive back-off from OOVs.
Perplexity = 678.01, Entropy = 9.41 bits
Computation based on 499 words.
Number of 1-grams hit = 499  (100.00%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article115.text
Will force inclusive back-off from OOVs.
Perplexity = 660.72, Entropy = 9.37 bits
Computation based on 641 words.
Number of 1-grams hit = 641  (100.00%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article116.text
Will force inclusive back-off from OOVs.
Perplexity = 634.44, Entropy = 9.31 bits
Computation based on 1526 words.
Number of 1-grams hit = 1526  (100.00%)
13 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article117.text
Will force inclusive back-off from OOVs.
Perplexity = 771.64, Entropy = 9.59 bits
Computation based on 517 words.
Number of 1-grams hit = 517  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article118.text
Will force inclusive back-off from OOVs.
Perplexity = 678.09, Entropy = 9.41 bits
Computation based on 610 words.
Number of 1-grams hit = 610  (100.00%)
5 OOVs (0.81%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article119.text
Will force inclusive back-off from OOVs.
Perplexity = 705.01, Entropy = 9.46 bits
Computation based on 390 words.
Number of 1-grams hit = 390  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article120.text
Will force inclusive back-off from OOVs.
Perplexity = 752.89, Entropy = 9.56 bits
Computation based on 557 words.
Number of 1-grams hit = 557  (100.00%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article121.text
Will force inclusive back-off from OOVs.
Perplexity = 690.70, Entropy = 9.43 bits
Computation based on 7137 words.
Number of 1-grams hit = 7137  (100.00%)
24 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article122.text
Will force inclusive back-off from OOVs.
Perplexity = 478.35, Entropy = 8.90 bits
Computation based on 325 words.
Number of 1-grams hit = 325  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article123.text
Will force inclusive back-off from OOVs.
Perplexity = 904.07, Entropy = 9.82 bits
Computation based on 524 words.
Number of 1-grams hit = 524  (100.00%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article124.text
Will force inclusive back-off from OOVs.
Perplexity = 616.18, Entropy = 9.27 bits
Computation based on 798 words.
Number of 1-grams hit = 798  (100.00%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article125.text
Will force inclusive back-off from OOVs.
Perplexity = 742.70, Entropy = 9.54 bits
Computation based on 484 words.
Number of 1-grams hit = 484  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article126.text
Will force inclusive back-off from OOVs.
Perplexity = 642.70, Entropy = 9.33 bits
Computation based on 540 words.
Number of 1-grams hit = 540  (100.00%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article127.text
Will force inclusive back-off from OOVs.
Perplexity = 635.48, Entropy = 9.31 bits
Computation based on 923 words.
Number of 1-grams hit = 923  (100.00%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article128.text
Will force inclusive back-off from OOVs.
Perplexity = 1135.06, Entropy = 10.15 bits
Computation based on 427 words.
Number of 1-grams hit = 427  (100.00%)
4 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article129.text
Will force inclusive back-off from OOVs.
Perplexity = 847.86, Entropy = 9.73 bits
Computation based on 469 words.
Number of 1-grams hit = 469  (100.00%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article130.text
Will force inclusive back-off from OOVs.
Perplexity = 752.15, Entropy = 9.55 bits
Computation based on 1527 words.
Number of 1-grams hit = 1527  (100.00%)
6 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article131.text
Will force inclusive back-off from OOVs.
Perplexity = 718.58, Entropy = 9.49 bits
Computation based on 6797 words.
Number of 1-grams hit = 6797  (100.00%)
19 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article132.text
Will force inclusive back-off from OOVs.
Perplexity = 683.01, Entropy = 9.42 bits
Computation based on 1115 words.
Number of 1-grams hit = 1115  (100.00%)
8 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article133.text
Will force inclusive back-off from OOVs.
Perplexity = 588.66, Entropy = 9.20 bits
Computation based on 243 words.
Number of 1-grams hit = 243  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article134.text
Will force inclusive back-off from OOVs.
Perplexity = 776.97, Entropy = 9.60 bits
Computation based on 500 words.
Number of 1-grams hit = 500  (100.00%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article135.text
Will force inclusive back-off from OOVs.
Perplexity = 1197.81, Entropy = 10.23 bits
Computation based on 482 words.
Number of 1-grams hit = 482  (100.00%)
9 OOVs (1.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article136.text
Will force inclusive back-off from OOVs.
Perplexity = 583.54, Entropy = 9.19 bits
Computation based on 526 words.
Number of 1-grams hit = 526  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article137.text
Will force inclusive back-off from OOVs.
Perplexity = 497.98, Entropy = 8.96 bits
Computation based on 329 words.
Number of 1-grams hit = 329  (100.00%)
4 OOVs (1.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article138.text
Will force inclusive back-off from OOVs.
Perplexity = 595.13, Entropy = 9.22 bits
Computation based on 307 words.
Number of 1-grams hit = 307  (100.00%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article139.text
Will force inclusive back-off from OOVs.
Perplexity = 644.51, Entropy = 9.33 bits
Computation based on 271 words.
Number of 1-grams hit = 271  (100.00%)
4 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article140.text
Will force inclusive back-off from OOVs.
Perplexity = 1098.53, Entropy = 10.10 bits
Computation based on 371 words.
Number of 1-grams hit = 371  (100.00%)
3 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article141.text
Will force inclusive back-off from OOVs.
Perplexity = 743.74, Entropy = 9.54 bits
Computation based on 434 words.
Number of 1-grams hit = 434  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article142.text
Will force inclusive back-off from OOVs.
Perplexity = 740.80, Entropy = 9.53 bits
Computation based on 695 words.
Number of 1-grams hit = 695  (100.00%)
5 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article143.text
Will force inclusive back-off from OOVs.
Perplexity = 1021.83, Entropy = 10.00 bits
Computation based on 377 words.
Number of 1-grams hit = 377  (100.00%)
4 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article144.text
Will force inclusive back-off from OOVs.
Perplexity = 1551.52, Entropy = 10.60 bits
Computation based on 506 words.
Number of 1-grams hit = 506  (100.00%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article145.text
Will force inclusive back-off from OOVs.
Perplexity = 693.25, Entropy = 9.44 bits
Computation based on 1941 words.
Number of 1-grams hit = 1941  (100.00%)
3 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article146.text
Will force inclusive back-off from OOVs.
Perplexity = 665.00, Entropy = 9.38 bits
Computation based on 238 words.
Number of 1-grams hit = 238  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article147.text
Will force inclusive back-off from OOVs.
Perplexity = 831.26, Entropy = 9.70 bits
Computation based on 657 words.
Number of 1-grams hit = 657  (100.00%)
2 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article148.text
Will force inclusive back-off from OOVs.
Perplexity = 868.49, Entropy = 9.76 bits
Computation based on 1733 words.
Number of 1-grams hit = 1733  (100.00%)
6 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article149.text
Will force inclusive back-off from OOVs.
Perplexity = 868.68, Entropy = 9.76 bits
Computation based on 557 words.
Number of 1-grams hit = 557  (100.00%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article150.text
Will force inclusive back-off from OOVs.
Perplexity = 759.22, Entropy = 9.57 bits
Computation based on 638 words.
Number of 1-grams hit = 638  (100.00%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article151.text
Will force inclusive back-off from OOVs.
Perplexity = 496.38, Entropy = 8.96 bits
Computation based on 173 words.
Number of 1-grams hit = 173  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article152.text
Will force inclusive back-off from OOVs.
Perplexity = 672.31, Entropy = 9.39 bits
Computation based on 1817 words.
Number of 1-grams hit = 1817  (100.00%)
2 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article153.text
Will force inclusive back-off from OOVs.
Perplexity = 800.47, Entropy = 9.64 bits
Computation based on 588 words.
Number of 1-grams hit = 588  (100.00%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article154.text
Will force inclusive back-off from OOVs.
Perplexity = 747.49, Entropy = 9.55 bits
Computation based on 1059 words.
Number of 1-grams hit = 1059  (100.00%)
5 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article155.text
Will force inclusive back-off from OOVs.
Perplexity = 730.89, Entropy = 9.51 bits
Computation based on 639 words.
Number of 1-grams hit = 639  (100.00%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article156.text
Will force inclusive back-off from OOVs.
Perplexity = 419.77, Entropy = 8.71 bits
Computation based on 1373 words.
Number of 1-grams hit = 1373  (100.00%)
5 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article157.text
Will force inclusive back-off from OOVs.
Perplexity = 870.19, Entropy = 9.77 bits
Computation based on 404 words.
Number of 1-grams hit = 404  (100.00%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article158.text
Will force inclusive back-off from OOVs.
Perplexity = 737.24, Entropy = 9.53 bits
Computation based on 574 words.
Number of 1-grams hit = 574  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article159.text
Will force inclusive back-off from OOVs.
Perplexity = 421.47, Entropy = 8.72 bits
Computation based on 1205 words.
Number of 1-grams hit = 1205  (100.00%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article160.text
Will force inclusive back-off from OOVs.
Perplexity = 1056.05, Entropy = 10.04 bits
Computation based on 534 words.
Number of 1-grams hit = 534  (100.00%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article161.text
Will force inclusive back-off from OOVs.
Perplexity = 1534.88, Entropy = 10.58 bits
Computation based on 189 words.
Number of 1-grams hit = 189  (100.00%)
10 OOVs (5.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article162.text
Will force inclusive back-off from OOVs.
Perplexity = 736.49, Entropy = 9.52 bits
Computation based on 1148 words.
Number of 1-grams hit = 1148  (100.00%)
6 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article163.text
Will force inclusive back-off from OOVs.
Perplexity = 801.41, Entropy = 9.65 bits
Computation based on 701 words.
Number of 1-grams hit = 701  (100.00%)
4 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article164.text
Will force inclusive back-off from OOVs.
Perplexity = 572.30, Entropy = 9.16 bits
Computation based on 1767 words.
Number of 1-grams hit = 1767  (100.00%)
10 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article165.text
Will force inclusive back-off from OOVs.
Perplexity = 1123.62, Entropy = 10.13 bits
Computation based on 546 words.
Number of 1-grams hit = 546  (100.00%)
11 OOVs (1.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article166.text
Will force inclusive back-off from OOVs.
Perplexity = 705.90, Entropy = 9.46 bits
Computation based on 517 words.
Number of 1-grams hit = 517  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article167.text
Will force inclusive back-off from OOVs.
Perplexity = 634.77, Entropy = 9.31 bits
Computation based on 1170 words.
Number of 1-grams hit = 1170  (100.00%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article168.text
Will force inclusive back-off from OOVs.
Perplexity = 737.20, Entropy = 9.53 bits
Computation based on 524 words.
Number of 1-grams hit = 524  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article169.text
Will force inclusive back-off from OOVs.
Perplexity = 688.39, Entropy = 9.43 bits
Computation based on 499 words.
Number of 1-grams hit = 499  (100.00%)
5 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article170.text
Will force inclusive back-off from OOVs.
Perplexity = 1121.19, Entropy = 10.13 bits
Computation based on 1128 words.
Number of 1-grams hit = 1128  (100.00%)
22 OOVs (1.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article171.text
Will force inclusive back-off from OOVs.
Perplexity = 1245.56, Entropy = 10.28 bits
Computation based on 605 words.
Number of 1-grams hit = 605  (100.00%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article172.text
Will force inclusive back-off from OOVs.
Perplexity = 742.25, Entropy = 9.54 bits
Computation based on 681 words.
Number of 1-grams hit = 681  (100.00%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article173.text
Will force inclusive back-off from OOVs.
Perplexity = 1050.06, Entropy = 10.04 bits
Computation based on 1009 words.
Number of 1-grams hit = 1009  (100.00%)
12 OOVs (1.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article174.text
Will force inclusive back-off from OOVs.
Perplexity = 605.43, Entropy = 9.24 bits
Computation based on 446 words.
Number of 1-grams hit = 446  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article175.text
Will force inclusive back-off from OOVs.
Perplexity = 738.05, Entropy = 9.53 bits
Computation based on 458 words.
Number of 1-grams hit = 458  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article176.text
Will force inclusive back-off from OOVs.
Perplexity = 786.37, Entropy = 9.62 bits
Computation based on 743 words.
Number of 1-grams hit = 743  (100.00%)
3 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article177.text
Will force inclusive back-off from OOVs.
Perplexity = 846.61, Entropy = 9.73 bits
Computation based on 511 words.
Number of 1-grams hit = 511  (100.00%)
7 OOVs (1.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article178.text
Will force inclusive back-off from OOVs.
Perplexity = 919.67, Entropy = 9.84 bits
Computation based on 168 words.
Number of 1-grams hit = 168  (100.00%)
1 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article179.text
Will force inclusive back-off from OOVs.
Perplexity = 781.39, Entropy = 9.61 bits
Computation based on 449 words.
Number of 1-grams hit = 449  (100.00%)
5 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article180.text
Will force inclusive back-off from OOVs.
Perplexity = 670.16, Entropy = 9.39 bits
Computation based on 427 words.
Number of 1-grams hit = 427  (100.00%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article181.text
Will force inclusive back-off from OOVs.
Perplexity = 494.33, Entropy = 8.95 bits
Computation based on 849 words.
Number of 1-grams hit = 849  (100.00%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article182.text
Will force inclusive back-off from OOVs.
Perplexity = 657.08, Entropy = 9.36 bits
Computation based on 366 words.
Number of 1-grams hit = 366  (100.00%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article183.text
Will force inclusive back-off from OOVs.
Perplexity = 615.77, Entropy = 9.27 bits
Computation based on 458 words.
Number of 1-grams hit = 458  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article184.text
Will force inclusive back-off from OOVs.
Perplexity = 770.28, Entropy = 9.59 bits
Computation based on 5472 words.
Number of 1-grams hit = 5472  (100.00%)
20 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article185.text
Will force inclusive back-off from OOVs.
Perplexity = 706.95, Entropy = 9.47 bits
Computation based on 975 words.
Number of 1-grams hit = 975  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article186.text
Will force inclusive back-off from OOVs.
Perplexity = 900.18, Entropy = 9.81 bits
Computation based on 487 words.
Number of 1-grams hit = 487  (100.00%)
3 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article187.text
Will force inclusive back-off from OOVs.
Perplexity = 736.35, Entropy = 9.52 bits
Computation based on 6010 words.
Number of 1-grams hit = 6010  (100.00%)
17 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article188.text
Will force inclusive back-off from OOVs.
Perplexity = 862.32, Entropy = 9.75 bits
Computation based on 684 words.
Number of 1-grams hit = 684  (100.00%)
7 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article189.text
Will force inclusive back-off from OOVs.
Perplexity = 819.37, Entropy = 9.68 bits
Computation based on 521 words.
Number of 1-grams hit = 521  (100.00%)
15 OOVs (2.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article190.text
Will force inclusive back-off from OOVs.
Perplexity = 1129.80, Entropy = 10.14 bits
Computation based on 390 words.
Number of 1-grams hit = 390  (100.00%)
6 OOVs (1.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article191.text
Will force inclusive back-off from OOVs.
Perplexity = 686.44, Entropy = 9.42 bits
Computation based on 5442 words.
Number of 1-grams hit = 5442  (100.00%)
26 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article192.text
Will force inclusive back-off from OOVs.
Perplexity = 864.86, Entropy = 9.76 bits
Computation based on 767 words.
Number of 1-grams hit = 767  (100.00%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article193.text
Will force inclusive back-off from OOVs.
Perplexity = 876.03, Entropy = 9.77 bits
Computation based on 161 words.
Number of 1-grams hit = 161  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article194.text
Will force inclusive back-off from OOVs.
Perplexity = 644.44, Entropy = 9.33 bits
Computation based on 604 words.
Number of 1-grams hit = 604  (100.00%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article195.text
Will force inclusive back-off from OOVs.
Perplexity = 680.22, Entropy = 9.41 bits
Computation based on 792 words.
Number of 1-grams hit = 792  (100.00%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article196.text
Will force inclusive back-off from OOVs.
Perplexity = 590.05, Entropy = 9.20 bits
Computation based on 685 words.
Number of 1-grams hit = 685  (100.00%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article197.text
Will force inclusive back-off from OOVs.
Perplexity = 596.70, Entropy = 9.22 bits
Computation based on 223 words.
Number of 1-grams hit = 223  (100.00%)
3 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article198.text
Will force inclusive back-off from OOVs.
Perplexity = 791.60, Entropy = 9.63 bits
Computation based on 718 words.
Number of 1-grams hit = 718  (100.00%)
3 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article199.text
Will force inclusive back-off from OOVs.
Perplexity = 615.91, Entropy = 9.27 bits
Computation based on 4477 words.
Number of 1-grams hit = 4477  (100.00%)
16 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article200.text
Will force inclusive back-off from OOVs.
Perplexity = 669.55, Entropy = 9.39 bits
Computation based on 638 words.
Number of 1-grams hit = 638  (100.00%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article201.text
Will force inclusive back-off from OOVs.
Perplexity = 757.54, Entropy = 9.57 bits
Computation based on 463 words.
Number of 1-grams hit = 463  (100.00%)
3 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article202.text
Will force inclusive back-off from OOVs.
Perplexity = 803.05, Entropy = 9.65 bits
Computation based on 1170 words.
Number of 1-grams hit = 1170  (100.00%)
5 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article203.text
Will force inclusive back-off from OOVs.
Perplexity = 513.27, Entropy = 9.00 bits
Computation based on 4808 words.
Number of 1-grams hit = 4808  (100.00%)
22 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article204.text
Will force inclusive back-off from OOVs.
Perplexity = 530.27, Entropy = 9.05 bits
Computation based on 5390 words.
Number of 1-grams hit = 5390  (100.00%)
26 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article205.text
Will force inclusive back-off from OOVs.
Perplexity = 628.46, Entropy = 9.30 bits
Computation based on 553 words.
Number of 1-grams hit = 553  (100.00%)
4 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article206.text
Will force inclusive back-off from OOVs.
Perplexity = 957.38, Entropy = 9.90 bits
Computation based on 585 words.
Number of 1-grams hit = 585  (100.00%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article207.text
Will force inclusive back-off from OOVs.
Perplexity = 759.12, Entropy = 9.57 bits
Computation based on 722 words.
Number of 1-grams hit = 722  (100.00%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article208.text
Will force inclusive back-off from OOVs.
Perplexity = 757.01, Entropy = 9.56 bits
Computation based on 886 words.
Number of 1-grams hit = 886  (100.00%)
8 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article209.text
Will force inclusive back-off from OOVs.
Perplexity = 710.98, Entropy = 9.47 bits
Computation based on 269 words.
Number of 1-grams hit = 269  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article210.text
Will force inclusive back-off from OOVs.
Perplexity = 696.33, Entropy = 9.44 bits
Computation based on 811 words.
Number of 1-grams hit = 811  (100.00%)
26 OOVs (3.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article211.text
Will force inclusive back-off from OOVs.
Perplexity = 989.01, Entropy = 9.95 bits
Computation based on 462 words.
Number of 1-grams hit = 462  (100.00%)
11 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article212.text
Will force inclusive back-off from OOVs.
Perplexity = 777.00, Entropy = 9.60 bits
Computation based on 416 words.
Number of 1-grams hit = 416  (100.00%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article213.text
Will force inclusive back-off from OOVs.
Perplexity = 656.35, Entropy = 9.36 bits
Computation based on 630 words.
Number of 1-grams hit = 630  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article214.text
Will force inclusive back-off from OOVs.
Perplexity = 827.15, Entropy = 9.69 bits
Computation based on 444 words.
Number of 1-grams hit = 444  (100.00%)
20 OOVs (4.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article215.text
Will force inclusive back-off from OOVs.
Perplexity = 1199.41, Entropy = 10.23 bits
Computation based on 685 words.
Number of 1-grams hit = 685  (100.00%)
11 OOVs (1.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article216.text
Will force inclusive back-off from OOVs.
Perplexity = 949.58, Entropy = 9.89 bits
Computation based on 481 words.
Number of 1-grams hit = 481  (100.00%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article217.text
Will force inclusive back-off from OOVs.
Perplexity = 514.04, Entropy = 9.01 bits
Computation based on 302 words.
Number of 1-grams hit = 302  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article218.text
Will force inclusive back-off from OOVs.
Perplexity = 542.30, Entropy = 9.08 bits
Computation based on 718 words.
Number of 1-grams hit = 718  (100.00%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article219.text
Will force inclusive back-off from OOVs.
Perplexity = 1349.22, Entropy = 10.40 bits
Computation based on 406 words.
Number of 1-grams hit = 406  (100.00%)
5 OOVs (1.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article220.text
Will force inclusive back-off from OOVs.
Perplexity = 466.42, Entropy = 8.87 bits
Computation based on 4293 words.
Number of 1-grams hit = 4293  (100.00%)
27 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article221.text
Will force inclusive back-off from OOVs.
Perplexity = 767.41, Entropy = 9.58 bits
Computation based on 1174 words.
Number of 1-grams hit = 1174  (100.00%)
8 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article222.text
Will force inclusive back-off from OOVs.
Perplexity = 773.84, Entropy = 9.60 bits
Computation based on 4454 words.
Number of 1-grams hit = 4454  (100.00%)
17 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article223.text
Will force inclusive back-off from OOVs.
Perplexity = 862.66, Entropy = 9.75 bits
Computation based on 825 words.
Number of 1-grams hit = 825  (100.00%)
9 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article224.text
Will force inclusive back-off from OOVs.
Perplexity = 862.32, Entropy = 9.75 bits
Computation based on 380 words.
Number of 1-grams hit = 380  (100.00%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article225.text
Will force inclusive back-off from OOVs.
Perplexity = 1180.25, Entropy = 10.20 bits
Computation based on 724 words.
Number of 1-grams hit = 724  (100.00%)
6 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article226.text
Will force inclusive back-off from OOVs.
Perplexity = 733.29, Entropy = 9.52 bits
Computation based on 1224 words.
Number of 1-grams hit = 1224  (100.00%)
12 OOVs (0.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article227.text
Will force inclusive back-off from OOVs.
Perplexity = 774.78, Entropy = 9.60 bits
Computation based on 936 words.
Number of 1-grams hit = 936  (100.00%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article228.text
Will force inclusive back-off from OOVs.
Perplexity = 869.53, Entropy = 9.76 bits
Computation based on 783 words.
Number of 1-grams hit = 783  (100.00%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article229.text
Will force inclusive back-off from OOVs.
Perplexity = 979.29, Entropy = 9.94 bits
Computation based on 471 words.
Number of 1-grams hit = 471  (100.00%)
5 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article230.text
Will force inclusive back-off from OOVs.
Perplexity = 545.92, Entropy = 9.09 bits
Computation based on 301 words.
Number of 1-grams hit = 301  (100.00%)
5 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article231.text
Will force inclusive back-off from OOVs.
Perplexity = 738.44, Entropy = 9.53 bits
Computation based on 801 words.
Number of 1-grams hit = 801  (100.00%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article232.text
Will force inclusive back-off from OOVs.
Perplexity = 797.24, Entropy = 9.64 bits
Computation based on 363 words.
Number of 1-grams hit = 363  (100.00%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article233.text
Will force inclusive back-off from OOVs.
Perplexity = 669.73, Entropy = 9.39 bits
Computation based on 668 words.
Number of 1-grams hit = 668  (100.00%)
5 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article234.text
Will force inclusive back-off from OOVs.
Perplexity = 745.02, Entropy = 9.54 bits
Computation based on 1331 words.
Number of 1-grams hit = 1331  (100.00%)
7 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article235.text
Will force inclusive back-off from OOVs.
Perplexity = 649.92, Entropy = 9.34 bits
Computation based on 349 words.
Number of 1-grams hit = 349  (100.00%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article236.text
Will force inclusive back-off from OOVs.
Perplexity = 1376.25, Entropy = 10.43 bits
Computation based on 735 words.
Number of 1-grams hit = 735  (100.00%)
18 OOVs (2.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article237.text
Will force inclusive back-off from OOVs.
Perplexity = 1037.54, Entropy = 10.02 bits
Computation based on 389 words.
Number of 1-grams hit = 389  (100.00%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article238.text
Will force inclusive back-off from OOVs.
Perplexity = 1139.55, Entropy = 10.15 bits
Computation based on 300 words.
Number of 1-grams hit = 300  (100.00%)
13 OOVs (4.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article239.text
Will force inclusive back-off from OOVs.
Perplexity = 638.63, Entropy = 9.32 bits
Computation based on 471 words.
Number of 1-grams hit = 471  (100.00%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article240.text
Will force inclusive back-off from OOVs.
Perplexity = 1068.13, Entropy = 10.06 bits
Computation based on 312 words.
Number of 1-grams hit = 312  (100.00%)
4 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article241.text
Will force inclusive back-off from OOVs.
Perplexity = 1128.51, Entropy = 10.14 bits
Computation based on 301 words.
Number of 1-grams hit = 301  (100.00%)
6 OOVs (1.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article242.text
Will force inclusive back-off from OOVs.
Perplexity = 631.90, Entropy = 9.30 bits
Computation based on 291 words.
Number of 1-grams hit = 291  (100.00%)
3 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article243.text
Will force inclusive back-off from OOVs.
Perplexity = 854.23, Entropy = 9.74 bits
Computation based on 743 words.
Number of 1-grams hit = 743  (100.00%)
6 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article244.text
Will force inclusive back-off from OOVs.
Perplexity = 829.88, Entropy = 9.70 bits
Computation based on 424 words.
Number of 1-grams hit = 424  (100.00%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article245.text
Will force inclusive back-off from OOVs.
Perplexity = 606.14, Entropy = 9.24 bits
Computation based on 5029 words.
Number of 1-grams hit = 5029  (100.00%)
19 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article246.text
Will force inclusive back-off from OOVs.
Perplexity = 663.77, Entropy = 9.37 bits
Computation based on 275 words.
Number of 1-grams hit = 275  (100.00%)
2 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article247.text
Will force inclusive back-off from OOVs.
Perplexity = 675.17, Entropy = 9.40 bits
Computation based on 449 words.
Number of 1-grams hit = 449  (100.00%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article248.text
Will force inclusive back-off from OOVs.
Perplexity = 596.79, Entropy = 9.22 bits
Computation based on 355 words.
Number of 1-grams hit = 355  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article249.text
Will force inclusive back-off from OOVs.
Perplexity = 689.41, Entropy = 9.43 bits
Computation based on 858 words.
Number of 1-grams hit = 858  (100.00%)
5 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article250.text
Will force inclusive back-off from OOVs.
Perplexity = 564.00, Entropy = 9.14 bits
Computation based on 4834 words.
Number of 1-grams hit = 4834  (100.00%)
30 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article251.text
Will force inclusive back-off from OOVs.
Perplexity = 718.43, Entropy = 9.49 bits
Computation based on 4971 words.
Number of 1-grams hit = 4971  (100.00%)
15 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article252.text
Will force inclusive back-off from OOVs.
Perplexity = 833.21, Entropy = 9.70 bits
Computation based on 525 words.
Number of 1-grams hit = 525  (100.00%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article253.text
Will force inclusive back-off from OOVs.
Perplexity = 759.49, Entropy = 9.57 bits
Computation based on 4945 words.
Number of 1-grams hit = 4945  (100.00%)
13 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article254.text
Will force inclusive back-off from OOVs.
Perplexity = 1122.71, Entropy = 10.13 bits
Computation based on 774 words.
Number of 1-grams hit = 774  (100.00%)
9 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article255.text
Will force inclusive back-off from OOVs.
Perplexity = 773.55, Entropy = 9.60 bits
Computation based on 486 words.
Number of 1-grams hit = 486  (100.00%)
7 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article256.text
Will force inclusive back-off from OOVs.
Perplexity = 815.54, Entropy = 9.67 bits
Computation based on 783 words.
Number of 1-grams hit = 783  (100.00%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article257.text
Will force inclusive back-off from OOVs.
Perplexity = 446.04, Entropy = 8.80 bits
Computation based on 5750 words.
Number of 1-grams hit = 5750  (100.00%)
31 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article258.text
Will force inclusive back-off from OOVs.
Perplexity = 701.53, Entropy = 9.45 bits
Computation based on 409 words.
Number of 1-grams hit = 409  (100.00%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article259.text
Will force inclusive back-off from OOVs.
Perplexity = 811.47, Entropy = 9.66 bits
Computation based on 685 words.
Number of 1-grams hit = 685  (100.00%)
16 OOVs (2.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article260.text
Will force inclusive back-off from OOVs.
Perplexity = 689.01, Entropy = 9.43 bits
Computation based on 6973 words.
Number of 1-grams hit = 6973  (100.00%)
22 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article261.text
Will force inclusive back-off from OOVs.
Perplexity = 636.11, Entropy = 9.31 bits
Computation based on 645 words.
Number of 1-grams hit = 645  (100.00%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article262.text
Will force inclusive back-off from OOVs.
Perplexity = 599.84, Entropy = 9.23 bits
Computation based on 661 words.
Number of 1-grams hit = 661  (100.00%)
3 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article263.text
Will force inclusive back-off from OOVs.
Perplexity = 614.04, Entropy = 9.26 bits
Computation based on 375 words.
Number of 1-grams hit = 375  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article264.text
Will force inclusive back-off from OOVs.
Perplexity = 838.48, Entropy = 9.71 bits
Computation based on 798 words.
Number of 1-grams hit = 798  (100.00%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article265.text
Will force inclusive back-off from OOVs.
Perplexity = 1146.75, Entropy = 10.16 bits
Computation based on 569 words.
Number of 1-grams hit = 569  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article266.text
Will force inclusive back-off from OOVs.
Perplexity = 637.26, Entropy = 9.32 bits
Computation based on 435 words.
Number of 1-grams hit = 435  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article267.text
Will force inclusive back-off from OOVs.
Perplexity = 705.17, Entropy = 9.46 bits
Computation based on 6221 words.
Number of 1-grams hit = 6221  (100.00%)
31 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article268.text
Will force inclusive back-off from OOVs.
Perplexity = 723.67, Entropy = 9.50 bits
Computation based on 5821 words.
Number of 1-grams hit = 5821  (100.00%)
19 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article269.text
Will force inclusive back-off from OOVs.
Perplexity = 794.57, Entropy = 9.63 bits
Computation based on 418 words.
Number of 1-grams hit = 418  (100.00%)
4 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article270.text
Will force inclusive back-off from OOVs.
Perplexity = 1137.18, Entropy = 10.15 bits
Computation based on 375 words.
Number of 1-grams hit = 375  (100.00%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article271.text
Will force inclusive back-off from OOVs.
Perplexity = 549.33, Entropy = 9.10 bits
Computation based on 7295 words.
Number of 1-grams hit = 7295  (100.00%)
52 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article272.text
Will force inclusive back-off from OOVs.
Perplexity = 545.83, Entropy = 9.09 bits
Computation based on 5195 words.
Number of 1-grams hit = 5195  (100.00%)
23 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article273.text
Will force inclusive back-off from OOVs.
Perplexity = 669.65, Entropy = 9.39 bits
Computation based on 638 words.
Number of 1-grams hit = 638  (100.00%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article274.text
Will force inclusive back-off from OOVs.
Perplexity = 723.12, Entropy = 9.50 bits
Computation based on 651 words.
Number of 1-grams hit = 651  (100.00%)
6 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article275.text
Will force inclusive back-off from OOVs.
Perplexity = 802.86, Entropy = 9.65 bits
Computation based on 1923 words.
Number of 1-grams hit = 1923  (100.00%)
4 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article276.text
Will force inclusive back-off from OOVs.
Perplexity = 926.23, Entropy = 9.86 bits
Computation based on 455 words.
Number of 1-grams hit = 455  (100.00%)
16 OOVs (3.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article277.text
Will force inclusive back-off from OOVs.
Perplexity = 720.36, Entropy = 9.49 bits
Computation based on 768 words.
Number of 1-grams hit = 768  (100.00%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article278.text
Will force inclusive back-off from OOVs.
Perplexity = 954.16, Entropy = 9.90 bits
Computation based on 806 words.
Number of 1-grams hit = 806  (100.00%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article279.text
Will force inclusive back-off from OOVs.
Perplexity = 543.47, Entropy = 9.09 bits
Computation based on 401 words.
Number of 1-grams hit = 401  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article280.text
Will force inclusive back-off from OOVs.
Perplexity = 681.47, Entropy = 9.41 bits
Computation based on 1708 words.
Number of 1-grams hit = 1708  (100.00%)
2 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article281.text
Will force inclusive back-off from OOVs.
Perplexity = 853.08, Entropy = 9.74 bits
Computation based on 546 words.
Number of 1-grams hit = 546  (100.00%)
20 OOVs (3.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article282.text
Will force inclusive back-off from OOVs.
Perplexity = 600.32, Entropy = 9.23 bits
Computation based on 708 words.
Number of 1-grams hit = 708  (100.00%)
5 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article283.text
Will force inclusive back-off from OOVs.
Perplexity = 653.59, Entropy = 9.35 bits
Computation based on 1182 words.
Number of 1-grams hit = 1182  (100.00%)
4 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article284.text
Will force inclusive back-off from OOVs.
Perplexity = 507.34, Entropy = 8.99 bits
Computation based on 1579 words.
Number of 1-grams hit = 1579  (100.00%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article285.text
Will force inclusive back-off from OOVs.
Perplexity = 690.92, Entropy = 9.43 bits
Computation based on 1094 words.
Number of 1-grams hit = 1094  (100.00%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article286.text
Will force inclusive back-off from OOVs.
Perplexity = 741.51, Entropy = 9.53 bits
Computation based on 442 words.
Number of 1-grams hit = 442  (100.00%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article287.text
Will force inclusive back-off from OOVs.
Perplexity = 805.74, Entropy = 9.65 bits
Computation based on 1652 words.
Number of 1-grams hit = 1652  (100.00%)
6 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article288.text
Will force inclusive back-off from OOVs.
Perplexity = 756.27, Entropy = 9.56 bits
Computation based on 582 words.
Number of 1-grams hit = 582  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article289.text
Will force inclusive back-off from OOVs.
Perplexity = 410.13, Entropy = 8.68 bits
Computation based on 704 words.
Number of 1-grams hit = 704  (100.00%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article290.text
Will force inclusive back-off from OOVs.
Perplexity = 622.14, Entropy = 9.28 bits
Computation based on 1280 words.
Number of 1-grams hit = 1280  (100.00%)
13 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article291.text
Will force inclusive back-off from OOVs.
Perplexity = 900.84, Entropy = 9.82 bits
Computation based on 507 words.
Number of 1-grams hit = 507  (100.00%)
9 OOVs (1.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article292.text
Will force inclusive back-off from OOVs.
Perplexity = 571.58, Entropy = 9.16 bits
Computation based on 533 words.
Number of 1-grams hit = 533  (100.00%)
5 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article293.text
Will force inclusive back-off from OOVs.
Perplexity = 960.76, Entropy = 9.91 bits
Computation based on 552 words.
Number of 1-grams hit = 552  (100.00%)
3 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article294.text
Will force inclusive back-off from OOVs.
Perplexity = 935.61, Entropy = 9.87 bits
Computation based on 2100 words.
Number of 1-grams hit = 2100  (100.00%)
20 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article295.text
Will force inclusive back-off from OOVs.
Perplexity = 960.46, Entropy = 9.91 bits
Computation based on 583 words.
Number of 1-grams hit = 583  (100.00%)
10 OOVs (1.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article296.text
Will force inclusive back-off from OOVs.
Perplexity = 646.87, Entropy = 9.34 bits
Computation based on 1523 words.
Number of 1-grams hit = 1523  (100.00%)
7 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article297.text
Will force inclusive back-off from OOVs.
Perplexity = 476.58, Entropy = 8.90 bits
Computation based on 492 words.
Number of 1-grams hit = 492  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article298.text
Will force inclusive back-off from OOVs.
Perplexity = 674.89, Entropy = 9.40 bits
Computation based on 3442 words.
Number of 1-grams hit = 3442  (100.00%)
12 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article299.text
Will force inclusive back-off from OOVs.
Perplexity = 719.29, Entropy = 9.49 bits
Computation based on 2703 words.
Number of 1-grams hit = 2703  (100.00%)
39 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article300.text
Will force inclusive back-off from OOVs.
Perplexity = 591.16, Entropy = 9.21 bits
Computation based on 2528 words.
Number of 1-grams hit = 2528  (100.00%)
11 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article301.text
Will force inclusive back-off from OOVs.
Perplexity = 770.09, Entropy = 9.59 bits
Computation based on 2136 words.
Number of 1-grams hit = 2136  (100.00%)
10 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article302.text
Will force inclusive back-off from OOVs.
Perplexity = 562.05, Entropy = 9.13 bits
Computation based on 622 words.
Number of 1-grams hit = 622  (100.00%)
4 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article303.text
Will force inclusive back-off from OOVs.
Perplexity = 942.19, Entropy = 9.88 bits
Computation based on 852 words.
Number of 1-grams hit = 852  (100.00%)
18 OOVs (2.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article304.text
Will force inclusive back-off from OOVs.
Perplexity = 1372.59, Entropy = 10.42 bits
Computation based on 392 words.
Number of 1-grams hit = 392  (100.00%)
12 OOVs (2.97%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article305.text
Will force inclusive back-off from OOVs.
Perplexity = 689.06, Entropy = 9.43 bits
Computation based on 303 words.
Number of 1-grams hit = 303  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article306.text
Will force inclusive back-off from OOVs.
Perplexity = 739.54, Entropy = 9.53 bits
Computation based on 589 words.
Number of 1-grams hit = 589  (100.00%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article307.text
Will force inclusive back-off from OOVs.
Perplexity = 778.97, Entropy = 9.61 bits
Computation based on 511 words.
Number of 1-grams hit = 511  (100.00%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article308.text
Will force inclusive back-off from OOVs.
Perplexity = 883.35, Entropy = 9.79 bits
Computation based on 392 words.
Number of 1-grams hit = 392  (100.00%)
2 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article309.text
Will force inclusive back-off from OOVs.
Perplexity = 540.64, Entropy = 9.08 bits
Computation based on 241 words.
Number of 1-grams hit = 241  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article310.text
Will force inclusive back-off from OOVs.
Perplexity = 794.26, Entropy = 9.63 bits
Computation based on 480 words.
Number of 1-grams hit = 480  (100.00%)
4 OOVs (0.83%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article311.text
Will force inclusive back-off from OOVs.
Perplexity = 757.80, Entropy = 9.57 bits
Computation based on 465 words.
Number of 1-grams hit = 465  (100.00%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article312.text
Will force inclusive back-off from OOVs.
Perplexity = 917.47, Entropy = 9.84 bits
Computation based on 594 words.
Number of 1-grams hit = 594  (100.00%)
4 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article313.text
Will force inclusive back-off from OOVs.
Perplexity = 869.24, Entropy = 9.76 bits
Computation based on 466 words.
Number of 1-grams hit = 466  (100.00%)
11 OOVs (2.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article314.text
Will force inclusive back-off from OOVs.
Perplexity = 499.14, Entropy = 8.96 bits
Computation based on 3281 words.
Number of 1-grams hit = 3281  (100.00%)
15 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article315.text
Will force inclusive back-off from OOVs.
Perplexity = 777.38, Entropy = 9.60 bits
Computation based on 1078 words.
Number of 1-grams hit = 1078  (100.00%)
14 OOVs (1.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article316.text
Will force inclusive back-off from OOVs.
Perplexity = 715.15, Entropy = 9.48 bits
Computation based on 329 words.
Number of 1-grams hit = 329  (100.00%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article317.text
Will force inclusive back-off from OOVs.
Perplexity = 750.56, Entropy = 9.55 bits
Computation based on 623 words.
Number of 1-grams hit = 623  (100.00%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article318.text
Will force inclusive back-off from OOVs.
Perplexity = 700.46, Entropy = 9.45 bits
Computation based on 3565 words.
Number of 1-grams hit = 3565  (100.00%)
9 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article319.text
Will force inclusive back-off from OOVs.
Perplexity = 1115.96, Entropy = 10.12 bits
Computation based on 589 words.
Number of 1-grams hit = 589  (100.00%)
10 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article320.text
Will force inclusive back-off from OOVs.
Perplexity = 1117.15, Entropy = 10.13 bits
Computation based on 376 words.
Number of 1-grams hit = 376  (100.00%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article321.text
Will force inclusive back-off from OOVs.
Perplexity = 699.05, Entropy = 9.45 bits
Computation based on 1074 words.
Number of 1-grams hit = 1074  (100.00%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article322.text
Will force inclusive back-off from OOVs.
Perplexity = 562.14, Entropy = 9.13 bits
Computation based on 1666 words.
Number of 1-grams hit = 1666  (100.00%)
15 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article323.text
Will force inclusive back-off from OOVs.
Perplexity = 1037.50, Entropy = 10.02 bits
Computation based on 808 words.
Number of 1-grams hit = 808  (100.00%)
12 OOVs (1.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article324.text
Will force inclusive back-off from OOVs.
Perplexity = 629.04, Entropy = 9.30 bits
Computation based on 1642 words.
Number of 1-grams hit = 1642  (100.00%)
15 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article325.text
Will force inclusive back-off from OOVs.
Perplexity = 1032.79, Entropy = 10.01 bits
Computation based on 1108 words.
Number of 1-grams hit = 1108  (100.00%)
18 OOVs (1.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article326.text
Will force inclusive back-off from OOVs.
Perplexity = 748.16, Entropy = 9.55 bits
Computation based on 606 words.
Number of 1-grams hit = 606  (100.00%)
5 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article327.text
Will force inclusive back-off from OOVs.
Perplexity = 866.93, Entropy = 9.76 bits
Computation based on 1125 words.
Number of 1-grams hit = 1125  (100.00%)
5 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article328.text
Will force inclusive back-off from OOVs.
Perplexity = 580.53, Entropy = 9.18 bits
Computation based on 250 words.
Number of 1-grams hit = 250  (100.00%)
1 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article329.text
Will force inclusive back-off from OOVs.
Perplexity = 695.48, Entropy = 9.44 bits
Computation based on 413 words.
Number of 1-grams hit = 413  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article330.text
Will force inclusive back-off from OOVs.
Perplexity = 598.72, Entropy = 9.23 bits
Computation based on 1051 words.
Number of 1-grams hit = 1051  (100.00%)
8 OOVs (0.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article331.text
Will force inclusive back-off from OOVs.
Perplexity = 651.21, Entropy = 9.35 bits
Computation based on 1948 words.
Number of 1-grams hit = 1948  (100.00%)
10 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article332.text
Will force inclusive back-off from OOVs.
Perplexity = 612.98, Entropy = 9.26 bits
Computation based on 575 words.
Number of 1-grams hit = 575  (100.00%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article333.text
Will force inclusive back-off from OOVs.
Perplexity = 822.63, Entropy = 9.68 bits
Computation based on 2146 words.
Number of 1-grams hit = 2146  (100.00%)
7 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article334.text
Will force inclusive back-off from OOVs.
Perplexity = 666.28, Entropy = 9.38 bits
Computation based on 961 words.
Number of 1-grams hit = 961  (100.00%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article335.text
Will force inclusive back-off from OOVs.
Perplexity = 679.09, Entropy = 9.41 bits
Computation based on 1753 words.
Number of 1-grams hit = 1753  (100.00%)
8 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article336.text
Will force inclusive back-off from OOVs.
Perplexity = 656.91, Entropy = 9.36 bits
Computation based on 1140 words.
Number of 1-grams hit = 1140  (100.00%)
4 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article337.text
Will force inclusive back-off from OOVs.
Perplexity = 593.34, Entropy = 9.21 bits
Computation based on 399 words.
Number of 1-grams hit = 399  (100.00%)
2 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article338.text
Will force inclusive back-off from OOVs.
Perplexity = 743.13, Entropy = 9.54 bits
Computation based on 3481 words.
Number of 1-grams hit = 3481  (100.00%)
91 OOVs (2.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article339.text
Will force inclusive back-off from OOVs.
Perplexity = 656.90, Entropy = 9.36 bits
Computation based on 373 words.
Number of 1-grams hit = 373  (100.00%)
3 OOVs (0.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article340.text
Will force inclusive back-off from OOVs.
Perplexity = 730.59, Entropy = 9.51 bits
Computation based on 1153 words.
Number of 1-grams hit = 1153  (100.00%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article341.text
Will force inclusive back-off from OOVs.
Perplexity = 668.04, Entropy = 9.38 bits
Computation based on 1370 words.
Number of 1-grams hit = 1370  (100.00%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article342.text
Will force inclusive back-off from OOVs.
Perplexity = 459.60, Entropy = 8.84 bits
Computation based on 1442 words.
Number of 1-grams hit = 1442  (100.00%)
9 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article343.text
Will force inclusive back-off from OOVs.
Perplexity = 654.06, Entropy = 9.35 bits
Computation based on 477 words.
Number of 1-grams hit = 477  (100.00%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article344.text
Will force inclusive back-off from OOVs.
Perplexity = 640.67, Entropy = 9.32 bits
Computation based on 581 words.
Number of 1-grams hit = 581  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article345.text
Will force inclusive back-off from OOVs.
Perplexity = 773.99, Entropy = 9.60 bits
Computation based on 3153 words.
Number of 1-grams hit = 3153  (100.00%)
11 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article346.text
Will force inclusive back-off from OOVs.
Perplexity = 672.51, Entropy = 9.39 bits
Computation based on 1581 words.
Number of 1-grams hit = 1581  (100.00%)
4 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article347.text
Will force inclusive back-off from OOVs.
Perplexity = 773.44, Entropy = 9.60 bits
Computation based on 647 words.
Number of 1-grams hit = 647  (100.00%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article348.text
Will force inclusive back-off from OOVs.
Perplexity = 663.09, Entropy = 9.37 bits
Computation based on 2087 words.
Number of 1-grams hit = 2087  (100.00%)
11 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article349.text
Will force inclusive back-off from OOVs.
Perplexity = 546.42, Entropy = 9.09 bits
Computation based on 1922 words.
Number of 1-grams hit = 1922  (100.00%)
19 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article350.text
Will force inclusive back-off from OOVs.
Perplexity = 727.24, Entropy = 9.51 bits
Computation based on 357 words.
Number of 1-grams hit = 357  (100.00%)
4 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article351.text
Will force inclusive back-off from OOVs.
Perplexity = 780.59, Entropy = 9.61 bits
Computation based on 432 words.
Number of 1-grams hit = 432  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article352.text
Will force inclusive back-off from OOVs.
Perplexity = 1003.13, Entropy = 9.97 bits
Computation based on 446 words.
Number of 1-grams hit = 446  (100.00%)
5 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article353.text
Will force inclusive back-off from OOVs.
Perplexity = 718.72, Entropy = 9.49 bits
Computation based on 425 words.
Number of 1-grams hit = 425  (100.00%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article354.text
Will force inclusive back-off from OOVs.
Perplexity = 759.34, Entropy = 9.57 bits
Computation based on 508 words.
Number of 1-grams hit = 508  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article355.text
Will force inclusive back-off from OOVs.
Perplexity = 925.38, Entropy = 9.85 bits
Computation based on 407 words.
Number of 1-grams hit = 407  (100.00%)
6 OOVs (1.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article356.text
Will force inclusive back-off from OOVs.
Perplexity = 1023.59, Entropy = 10.00 bits
Computation based on 691 words.
Number of 1-grams hit = 691  (100.00%)
18 OOVs (2.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article357.text
Will force inclusive back-off from OOVs.
Perplexity = 479.86, Entropy = 8.91 bits
Computation based on 747 words.
Number of 1-grams hit = 747  (100.00%)
8 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article358.text
Will force inclusive back-off from OOVs.
Perplexity = 770.35, Entropy = 9.59 bits
Computation based on 1164 words.
Number of 1-grams hit = 1164  (100.00%)
33 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article359.text
Will force inclusive back-off from OOVs.
Perplexity = 902.78, Entropy = 9.82 bits
Computation based on 1254 words.
Number of 1-grams hit = 1254  (100.00%)
16 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article360.text
Will force inclusive back-off from OOVs.
Perplexity = 694.78, Entropy = 9.44 bits
Computation based on 710 words.
Number of 1-grams hit = 710  (100.00%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article361.text
Will force inclusive back-off from OOVs.
Perplexity = 765.71, Entropy = 9.58 bits
Computation based on 1080 words.
Number of 1-grams hit = 1080  (100.00%)
5 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article362.text
Will force inclusive back-off from OOVs.
Perplexity = 582.18, Entropy = 9.19 bits
Computation based on 1417 words.
Number of 1-grams hit = 1417  (100.00%)
6 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article363.text
Will force inclusive back-off from OOVs.
Perplexity = 680.13, Entropy = 9.41 bits
Computation based on 1157 words.
Number of 1-grams hit = 1157  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article364.text
Will force inclusive back-off from OOVs.
Perplexity = 912.28, Entropy = 9.83 bits
Computation based on 326 words.
Number of 1-grams hit = 326  (100.00%)
3 OOVs (0.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article365.text
Will force inclusive back-off from OOVs.
Perplexity = 688.64, Entropy = 9.43 bits
Computation based on 405 words.
Number of 1-grams hit = 405  (100.00%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article366.text
Will force inclusive back-off from OOVs.
Perplexity = 923.87, Entropy = 9.85 bits
Computation based on 345 words.
Number of 1-grams hit = 345  (100.00%)
6 OOVs (1.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article367.text
Will force inclusive back-off from OOVs.
Perplexity = 506.14, Entropy = 8.98 bits
Computation based on 906 words.
Number of 1-grams hit = 906  (100.00%)
5 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article368.text
Will force inclusive back-off from OOVs.
Perplexity = 1284.19, Entropy = 10.33 bits
Computation based on 320 words.
Number of 1-grams hit = 320  (100.00%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article369.text
Will force inclusive back-off from OOVs.
Perplexity = 614.36, Entropy = 9.26 bits
Computation based on 316 words.
Number of 1-grams hit = 316  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article370.text
Will force inclusive back-off from OOVs.
Perplexity = 684.41, Entropy = 9.42 bits
Computation based on 1644 words.
Number of 1-grams hit = 1644  (100.00%)
3 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article371.text
Will force inclusive back-off from OOVs.
Perplexity = 602.67, Entropy = 9.24 bits
Computation based on 262 words.
Number of 1-grams hit = 262  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article372.text
Will force inclusive back-off from OOVs.
Perplexity = 611.82, Entropy = 9.26 bits
Computation based on 405 words.
Number of 1-grams hit = 405  (100.00%)
16 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article373.text
Will force inclusive back-off from OOVs.
Perplexity = 638.37, Entropy = 9.32 bits
Computation based on 505 words.
Number of 1-grams hit = 505  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article374.text
Will force inclusive back-off from OOVs.
Perplexity = 614.21, Entropy = 9.26 bits
Computation based on 1521 words.
Number of 1-grams hit = 1521  (100.00%)
2 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article375.text
Will force inclusive back-off from OOVs.
Perplexity = 497.14, Entropy = 8.96 bits
Computation based on 1608 words.
Number of 1-grams hit = 1608  (100.00%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article376.text
Will force inclusive back-off from OOVs.
Perplexity = 746.52, Entropy = 9.54 bits
Computation based on 560 words.
Number of 1-grams hit = 560  (100.00%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article377.text
Will force inclusive back-off from OOVs.
Perplexity = 691.74, Entropy = 9.43 bits
Computation based on 738 words.
Number of 1-grams hit = 738  (100.00%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article378.text
Will force inclusive back-off from OOVs.
Perplexity = 990.97, Entropy = 9.95 bits
Computation based on 609 words.
Number of 1-grams hit = 609  (100.00%)
13 OOVs (2.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article379.text
Will force inclusive back-off from OOVs.
Perplexity = 688.65, Entropy = 9.43 bits
Computation based on 2558 words.
Number of 1-grams hit = 2558  (100.00%)
10 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article380.text
Will force inclusive back-off from OOVs.
Perplexity = 766.19, Entropy = 9.58 bits
Computation based on 346 words.
Number of 1-grams hit = 346  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article381.text
Will force inclusive back-off from OOVs.
Perplexity = 758.14, Entropy = 9.57 bits
Computation based on 622 words.
Number of 1-grams hit = 622  (100.00%)
6 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article382.text
Will force inclusive back-off from OOVs.
Perplexity = 584.05, Entropy = 9.19 bits
Computation based on 1302 words.
Number of 1-grams hit = 1302  (100.00%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article383.text
Will force inclusive back-off from OOVs.
Perplexity = 728.46, Entropy = 9.51 bits
Computation based on 1031 words.
Number of 1-grams hit = 1031  (100.00%)
8 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article384.text
Will force inclusive back-off from OOVs.
Perplexity = 777.47, Entropy = 9.60 bits
Computation based on 1005 words.
Number of 1-grams hit = 1005  (100.00%)
3 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article385.text
Will force inclusive back-off from OOVs.
Perplexity = 710.25, Entropy = 9.47 bits
Computation based on 2592 words.
Number of 1-grams hit = 2592  (100.00%)
14 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article386.text
Will force inclusive back-off from OOVs.
Perplexity = 910.22, Entropy = 9.83 bits
Computation based on 405 words.
Number of 1-grams hit = 405  (100.00%)
12 OOVs (2.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article387.text
Will force inclusive back-off from OOVs.
Perplexity = 615.46, Entropy = 9.27 bits
Computation based on 1197 words.
Number of 1-grams hit = 1197  (100.00%)
18 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article388.text
Will force inclusive back-off from OOVs.
Perplexity = 711.66, Entropy = 9.48 bits
Computation based on 4328 words.
Number of 1-grams hit = 4328  (100.00%)
11 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article389.text
Will force inclusive back-off from OOVs.
Perplexity = 904.97, Entropy = 9.82 bits
Computation based on 1099 words.
Number of 1-grams hit = 1099  (100.00%)
22 OOVs (1.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article390.text
Will force inclusive back-off from OOVs.
Perplexity = 813.76, Entropy = 9.67 bits
Computation based on 1177 words.
Number of 1-grams hit = 1177  (100.00%)
23 OOVs (1.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article391.text
Will force inclusive back-off from OOVs.
Perplexity = 624.39, Entropy = 9.29 bits
Computation based on 3505 words.
Number of 1-grams hit = 3505  (100.00%)
36 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article392.text
Will force inclusive back-off from OOVs.
Perplexity = 520.43, Entropy = 9.02 bits
Computation based on 231 words.
Number of 1-grams hit = 231  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article393.text
Will force inclusive back-off from OOVs.
Perplexity = 1122.65, Entropy = 10.13 bits
Computation based on 481 words.
Number of 1-grams hit = 481  (100.00%)
5 OOVs (1.03%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article394.text
Will force inclusive back-off from OOVs.
Perplexity = 889.52, Entropy = 9.80 bits
Computation based on 390 words.
Number of 1-grams hit = 390  (100.00%)
5 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article395.text
Will force inclusive back-off from OOVs.
Perplexity = 578.63, Entropy = 9.18 bits
Computation based on 304 words.
Number of 1-grams hit = 304  (100.00%)
3 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article396.text
Will force inclusive back-off from OOVs.
Perplexity = 443.23, Entropy = 8.79 bits
Computation based on 3049 words.
Number of 1-grams hit = 3049  (100.00%)
3 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article397.text
Will force inclusive back-off from OOVs.
Perplexity = 789.31, Entropy = 9.62 bits
Computation based on 3657 words.
Number of 1-grams hit = 3657  (100.00%)
16 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article398.text
Will force inclusive back-off from OOVs.
Perplexity = 672.43, Entropy = 9.39 bits
Computation based on 815 words.
Number of 1-grams hit = 815  (100.00%)
5 OOVs (0.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article399.text
Will force inclusive back-off from OOVs.
Perplexity = 690.41, Entropy = 9.43 bits
Computation based on 3657 words.
Number of 1-grams hit = 3657  (100.00%)
15 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article400.text
Will force inclusive back-off from OOVs.
Perplexity = 745.45, Entropy = 9.54 bits
Computation based on 582 words.
Number of 1-grams hit = 582  (100.00%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article401.text
Will force inclusive back-off from OOVs.
Perplexity = 997.84, Entropy = 9.96 bits
Computation based on 652 words.
Number of 1-grams hit = 652  (100.00%)
11 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article402.text
Will force inclusive back-off from OOVs.
Perplexity = 674.24, Entropy = 9.40 bits
Computation based on 3662 words.
Number of 1-grams hit = 3662  (100.00%)
8 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article403.text
Will force inclusive back-off from OOVs.
Perplexity = 786.30, Entropy = 9.62 bits
Computation based on 621 words.
Number of 1-grams hit = 621  (100.00%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article404.text
Will force inclusive back-off from OOVs.
Perplexity = 1162.07, Entropy = 10.18 bits
Computation based on 213 words.
Number of 1-grams hit = 213  (100.00%)
1 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article405.text
Will force inclusive back-off from OOVs.
Perplexity = 1108.75, Entropy = 10.11 bits
Computation based on 159 words.
Number of 1-grams hit = 159  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article406.text
Will force inclusive back-off from OOVs.
Perplexity = 900.19, Entropy = 9.81 bits
Computation based on 982 words.
Number of 1-grams hit = 982  (100.00%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article407.text
Will force inclusive back-off from OOVs.
Perplexity = 656.29, Entropy = 9.36 bits
Computation based on 696 words.
Number of 1-grams hit = 696  (100.00%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article408.text
Will force inclusive back-off from OOVs.
Perplexity = 1148.21, Entropy = 10.17 bits
Computation based on 438 words.
Number of 1-grams hit = 438  (100.00%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article409.text
Will force inclusive back-off from OOVs.
Perplexity = 714.61, Entropy = 9.48 bits
Computation based on 541 words.
Number of 1-grams hit = 541  (100.00%)
3 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article410.text
Will force inclusive back-off from OOVs.
Perplexity = 442.32, Entropy = 8.79 bits
Computation based on 235 words.
Number of 1-grams hit = 235  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article411.text
Will force inclusive back-off from OOVs.
Perplexity = 1057.51, Entropy = 10.05 bits
Computation based on 436 words.
Number of 1-grams hit = 436  (100.00%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article412.text
Will force inclusive back-off from OOVs.
Perplexity = 539.41, Entropy = 9.08 bits
Computation based on 4559 words.
Number of 1-grams hit = 4559  (100.00%)
9 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article413.text
Will force inclusive back-off from OOVs.
Perplexity = 722.39, Entropy = 9.50 bits
Computation based on 3853 words.
Number of 1-grams hit = 3853  (100.00%)
14 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article414.text
Will force inclusive back-off from OOVs.
Perplexity = 685.84, Entropy = 9.42 bits
Computation based on 399 words.
Number of 1-grams hit = 399  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article415.text
Will force inclusive back-off from OOVs.
Perplexity = 994.69, Entropy = 9.96 bits
Computation based on 475 words.
Number of 1-grams hit = 475  (100.00%)
5 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article416.text
Will force inclusive back-off from OOVs.
Perplexity = 1017.59, Entropy = 9.99 bits
Computation based on 585 words.
Number of 1-grams hit = 585  (100.00%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article417.text
Will force inclusive back-off from OOVs.
Perplexity = 559.64, Entropy = 9.13 bits
Computation based on 850 words.
Number of 1-grams hit = 850  (100.00%)
4 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article418.text
Will force inclusive back-off from OOVs.
Perplexity = 1247.99, Entropy = 10.29 bits
Computation based on 807 words.
Number of 1-grams hit = 807  (100.00%)
5 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article419.text
Will force inclusive back-off from OOVs.
Perplexity = 1102.24, Entropy = 10.11 bits
Computation based on 549 words.
Number of 1-grams hit = 549  (100.00%)
5 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article420.text
Will force inclusive back-off from OOVs.
Perplexity = 746.17, Entropy = 9.54 bits
Computation based on 610 words.
Number of 1-grams hit = 610  (100.00%)
1 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article421.text
Will force inclusive back-off from OOVs.
Perplexity = 625.91, Entropy = 9.29 bits
Computation based on 819 words.
Number of 1-grams hit = 819  (100.00%)
2 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article422.text
Will force inclusive back-off from OOVs.
Perplexity = 900.64, Entropy = 9.81 bits
Computation based on 551 words.
Number of 1-grams hit = 551  (100.00%)
8 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article423.text
Will force inclusive back-off from OOVs.
Perplexity = 1086.93, Entropy = 10.09 bits
Computation based on 599 words.
Number of 1-grams hit = 599  (100.00%)
17 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article424.text
Will force inclusive back-off from OOVs.
Perplexity = 618.77, Entropy = 9.27 bits
Computation based on 524 words.
Number of 1-grams hit = 524  (100.00%)
5 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article425.text
Will force inclusive back-off from OOVs.
Perplexity = 711.15, Entropy = 9.47 bits
Computation based on 355 words.
Number of 1-grams hit = 355  (100.00%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article426.text
Will force inclusive back-off from OOVs.
Perplexity = 549.23, Entropy = 9.10 bits
Computation based on 484 words.
Number of 1-grams hit = 484  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article427.text
Will force inclusive back-off from OOVs.
Perplexity = 666.84, Entropy = 9.38 bits
Computation based on 787 words.
Number of 1-grams hit = 787  (100.00%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article428.text
Will force inclusive back-off from OOVs.
Perplexity = 812.36, Entropy = 9.67 bits
Computation based on 626 words.
Number of 1-grams hit = 626  (100.00%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article429.text
Will force inclusive back-off from OOVs.
Perplexity = 658.25, Entropy = 9.36 bits
Computation based on 769 words.
Number of 1-grams hit = 769  (100.00%)
3 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article430.text
Will force inclusive back-off from OOVs.
Perplexity = 683.00, Entropy = 9.42 bits
Computation based on 639 words.
Number of 1-grams hit = 639  (100.00%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article431.text
Will force inclusive back-off from OOVs.
Perplexity = 655.67, Entropy = 9.36 bits
Computation based on 569 words.
Number of 1-grams hit = 569  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article432.text
Will force inclusive back-off from OOVs.
Perplexity = 737.26, Entropy = 9.53 bits
Computation based on 351 words.
Number of 1-grams hit = 351  (100.00%)
2 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article433.text
Will force inclusive back-off from OOVs.
Perplexity = 647.62, Entropy = 9.34 bits
Computation based on 967 words.
Number of 1-grams hit = 967  (100.00%)
5 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article434.text
Will force inclusive back-off from OOVs.
Perplexity = 789.65, Entropy = 9.63 bits
Computation based on 381 words.
Number of 1-grams hit = 381  (100.00%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article435.text
Will force inclusive back-off from OOVs.
Perplexity = 552.94, Entropy = 9.11 bits
Computation based on 711 words.
Number of 1-grams hit = 711  (100.00%)
8 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article436.text
Will force inclusive back-off from OOVs.
Perplexity = 748.64, Entropy = 9.55 bits
Computation based on 531 words.
Number of 1-grams hit = 531  (100.00%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article437.text
Will force inclusive back-off from OOVs.
Perplexity = 740.69, Entropy = 9.53 bits
Computation based on 1007 words.
Number of 1-grams hit = 1007  (100.00%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article438.text
Will force inclusive back-off from OOVs.
Perplexity = 721.76, Entropy = 9.50 bits
Computation based on 290 words.
Number of 1-grams hit = 290  (100.00%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article439.text
Will force inclusive back-off from OOVs.
Perplexity = 936.49, Entropy = 9.87 bits
Computation based on 516 words.
Number of 1-grams hit = 516  (100.00%)
8 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article440.text
Will force inclusive back-off from OOVs.
Perplexity = 684.87, Entropy = 9.42 bits
Computation based on 688 words.
Number of 1-grams hit = 688  (100.00%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article441.text
Will force inclusive back-off from OOVs.
Perplexity = 1024.81, Entropy = 10.00 bits
Computation based on 400 words.
Number of 1-grams hit = 400  (100.00%)
13 OOVs (3.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article442.text
Will force inclusive back-off from OOVs.
Perplexity = 434.98, Entropy = 8.76 bits
Computation based on 10008 words.
Number of 1-grams hit = 10008  (100.00%)
23 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article443.text
Will force inclusive back-off from OOVs.
Perplexity = 700.07, Entropy = 9.45 bits
Computation based on 11815 words.
Number of 1-grams hit = 11815  (100.00%)
39 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article444.text
Will force inclusive back-off from OOVs.
Perplexity = 677.22, Entropy = 9.40 bits
Computation based on 643 words.
Number of 1-grams hit = 643  (100.00%)
8 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article445.text
Will force inclusive back-off from OOVs.
Perplexity = 605.85, Entropy = 9.24 bits
Computation based on 501 words.
Number of 1-grams hit = 501  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article446.text
Will force inclusive back-off from OOVs.
Perplexity = 813.18, Entropy = 9.67 bits
Computation based on 894 words.
Number of 1-grams hit = 894  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article447.text
Will force inclusive back-off from OOVs.
Perplexity = 642.20, Entropy = 9.33 bits
Computation based on 722 words.
Number of 1-grams hit = 722  (100.00%)
9 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article448.text
Will force inclusive back-off from OOVs.
Perplexity = 548.60, Entropy = 9.10 bits
Computation based on 323 words.
Number of 1-grams hit = 323  (100.00%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article449.text
Will force inclusive back-off from OOVs.
Perplexity = 690.24, Entropy = 9.43 bits
Computation based on 483 words.
Number of 1-grams hit = 483  (100.00%)
5 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article450.text
Will force inclusive back-off from OOVs.
Perplexity = 907.68, Entropy = 9.83 bits
Computation based on 356 words.
Number of 1-grams hit = 356  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article451.text
Will force inclusive back-off from OOVs.
Perplexity = 1041.69, Entropy = 10.02 bits
Computation based on 571 words.
Number of 1-grams hit = 571  (100.00%)
3 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article452.text
Will force inclusive back-off from OOVs.
Perplexity = 654.69, Entropy = 9.35 bits
Computation based on 237 words.
Number of 1-grams hit = 237  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article453.text
Will force inclusive back-off from OOVs.
Perplexity = 900.28, Entropy = 9.81 bits
Computation based on 625 words.
Number of 1-grams hit = 625  (100.00%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article454.text
Will force inclusive back-off from OOVs.
Perplexity = 666.37, Entropy = 9.38 bits
Computation based on 544 words.
Number of 1-grams hit = 544  (100.00%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article455.text
Will force inclusive back-off from OOVs.
Perplexity = 913.13, Entropy = 9.83 bits
Computation based on 379 words.
Number of 1-grams hit = 379  (100.00%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article456.text
Will force inclusive back-off from OOVs.
Perplexity = 755.10, Entropy = 9.56 bits
Computation based on 656 words.
Number of 1-grams hit = 656  (100.00%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article457.text
Will force inclusive back-off from OOVs.
Perplexity = 862.35, Entropy = 9.75 bits
Computation based on 337 words.
Number of 1-grams hit = 337  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article458.text
Will force inclusive back-off from OOVs.
Perplexity = 623.99, Entropy = 9.29 bits
Computation based on 276 words.
Number of 1-grams hit = 276  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article459.text
Will force inclusive back-off from OOVs.
Perplexity = 681.40, Entropy = 9.41 bits
Computation based on 700 words.
Number of 1-grams hit = 700  (100.00%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article460.text
Will force inclusive back-off from OOVs.
Perplexity = 746.90, Entropy = 9.54 bits
Computation based on 326 words.
Number of 1-grams hit = 326  (100.00%)
1 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article461.text
Will force inclusive back-off from OOVs.
Perplexity = 1268.30, Entropy = 10.31 bits
Computation based on 419 words.
Number of 1-grams hit = 419  (100.00%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article462.text
Will force inclusive back-off from OOVs.
Perplexity = 762.22, Entropy = 9.57 bits
Computation based on 1406 words.
Number of 1-grams hit = 1406  (100.00%)
4 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article463.text
Will force inclusive back-off from OOVs.
Perplexity = 890.04, Entropy = 9.80 bits
Computation based on 543 words.
Number of 1-grams hit = 543  (100.00%)
12 OOVs (2.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article464.text
Will force inclusive back-off from OOVs.
Perplexity = 838.94, Entropy = 9.71 bits
Computation based on 421 words.
Number of 1-grams hit = 421  (100.00%)
19 OOVs (4.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article465.text
Will force inclusive back-off from OOVs.
Perplexity = 515.34, Entropy = 9.01 bits
Computation based on 1805 words.
Number of 1-grams hit = 1805  (100.00%)
2 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article466.text
Will force inclusive back-off from OOVs.
Perplexity = 659.13, Entropy = 9.36 bits
Computation based on 204 words.
Number of 1-grams hit = 204  (100.00%)
1 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article467.text
Will force inclusive back-off from OOVs.
Perplexity = 637.61, Entropy = 9.32 bits
Computation based on 1028 words.
Number of 1-grams hit = 1028  (100.00%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article468.text
Will force inclusive back-off from OOVs.
Perplexity = 717.92, Entropy = 9.49 bits
Computation based on 776 words.
Number of 1-grams hit = 776  (100.00%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article469.text
Will force inclusive back-off from OOVs.
Perplexity = 807.16, Entropy = 9.66 bits
Computation based on 355 words.
Number of 1-grams hit = 355  (100.00%)
2 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article470.text
Will force inclusive back-off from OOVs.
Perplexity = 1025.59, Entropy = 10.00 bits
Computation based on 235 words.
Number of 1-grams hit = 235  (100.00%)
5 OOVs (2.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article471.text
Will force inclusive back-off from OOVs.
Perplexity = 671.19, Entropy = 9.39 bits
Computation based on 1994 words.
Number of 1-grams hit = 1994  (100.00%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article472.text
Will force inclusive back-off from OOVs.
Perplexity = 786.88, Entropy = 9.62 bits
Computation based on 930 words.
Number of 1-grams hit = 930  (100.00%)
6 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article473.text
Will force inclusive back-off from OOVs.
Perplexity = 482.11, Entropy = 8.91 bits
Computation based on 219 words.
Number of 1-grams hit = 219  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article474.text
Will force inclusive back-off from OOVs.
Perplexity = 639.41, Entropy = 9.32 bits
Computation based on 775 words.
Number of 1-grams hit = 775  (100.00%)
9 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article475.text
Will force inclusive back-off from OOVs.
Perplexity = 873.08, Entropy = 9.77 bits
Computation based on 546 words.
Number of 1-grams hit = 546  (100.00%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article476.text
Will force inclusive back-off from OOVs.
Perplexity = 886.88, Entropy = 9.79 bits
Computation based on 582 words.
Number of 1-grams hit = 582  (100.00%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article477.text
Will force inclusive back-off from OOVs.
Perplexity = 804.68, Entropy = 9.65 bits
Computation based on 444 words.
Number of 1-grams hit = 444  (100.00%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article478.text
Will force inclusive back-off from OOVs.
Perplexity = 839.97, Entropy = 9.71 bits
Computation based on 507 words.
Number of 1-grams hit = 507  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article479.text
Will force inclusive back-off from OOVs.
Perplexity = 733.84, Entropy = 9.52 bits
Computation based on 421 words.
Number of 1-grams hit = 421  (100.00%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article480.text
Will force inclusive back-off from OOVs.
Perplexity = 665.44, Entropy = 9.38 bits
Computation based on 684 words.
Number of 1-grams hit = 684  (100.00%)
5 OOVs (0.73%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article481.text
Will force inclusive back-off from OOVs.
Perplexity = 623.32, Entropy = 9.28 bits
Computation based on 349 words.
Number of 1-grams hit = 349  (100.00%)
2 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article482.text
Will force inclusive back-off from OOVs.
Perplexity = 643.31, Entropy = 9.33 bits
Computation based on 494 words.
Number of 1-grams hit = 494  (100.00%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article483.text
Will force inclusive back-off from OOVs.
Perplexity = 498.87, Entropy = 8.96 bits
Computation based on 1664 words.
Number of 1-grams hit = 1664  (100.00%)
12 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article484.text
Will force inclusive back-off from OOVs.
Perplexity = 780.36, Entropy = 9.61 bits
Computation based on 1427 words.
Number of 1-grams hit = 1427  (100.00%)
5 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article485.text
Will force inclusive back-off from OOVs.
Perplexity = 1389.72, Entropy = 10.44 bits
Computation based on 372 words.
Number of 1-grams hit = 372  (100.00%)
5 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article486.text
Will force inclusive back-off from OOVs.
Perplexity = 605.84, Entropy = 9.24 bits
Computation based on 331 words.
Number of 1-grams hit = 331  (100.00%)
2 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article487.text
Will force inclusive back-off from OOVs.
Perplexity = 658.74, Entropy = 9.36 bits
Computation based on 429 words.
Number of 1-grams hit = 429  (100.00%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article488.text
Will force inclusive back-off from OOVs.
Perplexity = 611.37, Entropy = 9.26 bits
Computation based on 710 words.
Number of 1-grams hit = 710  (100.00%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article489.text
Will force inclusive back-off from OOVs.
Perplexity = 636.73, Entropy = 9.31 bits
Computation based on 537 words.
Number of 1-grams hit = 537  (100.00%)
9 OOVs (1.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article490.text
Will force inclusive back-off from OOVs.
Perplexity = 866.86, Entropy = 9.76 bits
Computation based on 479 words.
Number of 1-grams hit = 479  (100.00%)
14 OOVs (2.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article491.text
Will force inclusive back-off from OOVs.
Perplexity = 823.49, Entropy = 9.69 bits
Computation based on 1388 words.
Number of 1-grams hit = 1388  (100.00%)
7 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article492.text
Will force inclusive back-off from OOVs.
Perplexity = 1011.44, Entropy = 9.98 bits
Computation based on 433 words.
Number of 1-grams hit = 433  (100.00%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article493.text
Will force inclusive back-off from OOVs.
Perplexity = 1145.89, Entropy = 10.16 bits
Computation based on 759 words.
Number of 1-grams hit = 759  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article494.text
Will force inclusive back-off from OOVs.
Perplexity = 678.15, Entropy = 9.41 bits
Computation based on 485 words.
Number of 1-grams hit = 485  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article495.text
Will force inclusive back-off from OOVs.
Perplexity = 776.00, Entropy = 9.60 bits
Computation based on 426 words.
Number of 1-grams hit = 426  (100.00%)
4 OOVs (0.93%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article496.text
Will force inclusive back-off from OOVs.
Perplexity = 911.86, Entropy = 9.83 bits
Computation based on 546 words.
Number of 1-grams hit = 546  (100.00%)
12 OOVs (2.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article497.text
Will force inclusive back-off from OOVs.
Perplexity = 640.05, Entropy = 9.32 bits
Computation based on 647 words.
Number of 1-grams hit = 647  (100.00%)
5 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article498.text
Will force inclusive back-off from OOVs.
Perplexity = 837.81, Entropy = 9.71 bits
Computation based on 1659 words.
Number of 1-grams hit = 1659  (100.00%)
40 OOVs (2.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article499.text
Will force inclusive back-off from OOVs.
Perplexity = 657.11, Entropy = 9.36 bits
Computation based on 495 words.
Number of 1-grams hit = 495  (100.00%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article500.text
Will force inclusive back-off from OOVs.
Perplexity = 995.15, Entropy = 9.96 bits
Computation based on 457 words.
Number of 1-grams hit = 457  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article501.text
Will force inclusive back-off from OOVs.
Perplexity = 712.64, Entropy = 9.48 bits
Computation based on 403 words.
Number of 1-grams hit = 403  (100.00%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article502.text
Will force inclusive back-off from OOVs.
Perplexity = 807.49, Entropy = 9.66 bits
Computation based on 505 words.
Number of 1-grams hit = 505  (100.00%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article503.text
Will force inclusive back-off from OOVs.
Perplexity = 673.68, Entropy = 9.40 bits
Computation based on 1013 words.
Number of 1-grams hit = 1013  (100.00%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article504.text
Will force inclusive back-off from OOVs.
Perplexity = 747.11, Entropy = 9.55 bits
Computation based on 613 words.
Number of 1-grams hit = 613  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article505.text
Will force inclusive back-off from OOVs.
Perplexity = 902.82, Entropy = 9.82 bits
Computation based on 701 words.
Number of 1-grams hit = 701  (100.00%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article506.text
Will force inclusive back-off from OOVs.
Perplexity = 767.82, Entropy = 9.58 bits
Computation based on 522 words.
Number of 1-grams hit = 522  (100.00%)
17 OOVs (3.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article507.text
Will force inclusive back-off from OOVs.
Perplexity = 757.86, Entropy = 9.57 bits
Computation based on 1584 words.
Number of 1-grams hit = 1584  (100.00%)
9 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article508.text
Will force inclusive back-off from OOVs.
Perplexity = 680.61, Entropy = 9.41 bits
Computation based on 655 words.
Number of 1-grams hit = 655  (100.00%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article509.text
Will force inclusive back-off from OOVs.
Perplexity = 775.67, Entropy = 9.60 bits
Computation based on 353 words.
Number of 1-grams hit = 353  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article510.text
Will force inclusive back-off from OOVs.
Perplexity = 803.14, Entropy = 9.65 bits
Computation based on 968 words.
Number of 1-grams hit = 968  (100.00%)
4 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article511.text
Will force inclusive back-off from OOVs.
Perplexity = 1205.00, Entropy = 10.23 bits
Computation based on 623 words.
Number of 1-grams hit = 623  (100.00%)
8 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article512.text
Will force inclusive back-off from OOVs.
Perplexity = 734.38, Entropy = 9.52 bits
Computation based on 1900 words.
Number of 1-grams hit = 1900  (100.00%)
8 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article513.text
Will force inclusive back-off from OOVs.
Perplexity = 1288.63, Entropy = 10.33 bits
Computation based on 414 words.
Number of 1-grams hit = 414  (100.00%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article514.text
Will force inclusive back-off from OOVs.
Perplexity = 879.37, Entropy = 9.78 bits
Computation based on 397 words.
Number of 1-grams hit = 397  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article515.text
Will force inclusive back-off from OOVs.
Perplexity = 885.45, Entropy = 9.79 bits
Computation based on 972 words.
Number of 1-grams hit = 972  (100.00%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article516.text
Will force inclusive back-off from OOVs.
Perplexity = 709.98, Entropy = 9.47 bits
Computation based on 412 words.
Number of 1-grams hit = 412  (100.00%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article517.text
Will force inclusive back-off from OOVs.
Perplexity = 833.63, Entropy = 9.70 bits
Computation based on 1060 words.
Number of 1-grams hit = 1060  (100.00%)
2 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article518.text
Will force inclusive back-off from OOVs.
Perplexity = 931.30, Entropy = 9.86 bits
Computation based on 584 words.
Number of 1-grams hit = 584  (100.00%)
6 OOVs (1.02%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article519.text
Will force inclusive back-off from OOVs.
Perplexity = 709.76, Entropy = 9.47 bits
Computation based on 525 words.
Number of 1-grams hit = 525  (100.00%)
6 OOVs (1.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article520.text
Will force inclusive back-off from OOVs.
Perplexity = 651.33, Entropy = 9.35 bits
Computation based on 610 words.
Number of 1-grams hit = 610  (100.00%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article521.text
Will force inclusive back-off from OOVs.
Perplexity = 933.23, Entropy = 9.87 bits
Computation based on 511 words.
Number of 1-grams hit = 511  (100.00%)
11 OOVs (2.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article522.text
Will force inclusive back-off from OOVs.
Perplexity = 642.76, Entropy = 9.33 bits
Computation based on 2319 words.
Number of 1-grams hit = 2319  (100.00%)
11 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article523.text
Will force inclusive back-off from OOVs.
Perplexity = 677.54, Entropy = 9.40 bits
Computation based on 327 words.
Number of 1-grams hit = 327  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article524.text
Will force inclusive back-off from OOVs.
Perplexity = 852.93, Entropy = 9.74 bits
Computation based on 1383 words.
Number of 1-grams hit = 1383  (100.00%)
4 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article525.text
Will force inclusive back-off from OOVs.
Perplexity = 761.71, Entropy = 9.57 bits
Computation based on 1967 words.
Number of 1-grams hit = 1967  (100.00%)
10 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article526.text
Will force inclusive back-off from OOVs.
Perplexity = 697.50, Entropy = 9.45 bits
Computation based on 1900 words.
Number of 1-grams hit = 1900  (100.00%)
9 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article527.text
Will force inclusive back-off from OOVs.
Perplexity = 843.66, Entropy = 9.72 bits
Computation based on 733 words.
Number of 1-grams hit = 733  (100.00%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article528.text
Will force inclusive back-off from OOVs.
Perplexity = 675.90, Entropy = 9.40 bits
Computation based on 723 words.
Number of 1-grams hit = 723  (100.00%)
4 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article529.text
Will force inclusive back-off from OOVs.
Perplexity = 934.96, Entropy = 9.87 bits
Computation based on 886 words.
Number of 1-grams hit = 886  (100.00%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article530.text
Will force inclusive back-off from OOVs.
Perplexity = 732.42, Entropy = 9.52 bits
Computation based on 420 words.
Number of 1-grams hit = 420  (100.00%)
6 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article531.text
Will force inclusive back-off from OOVs.
Perplexity = 737.47, Entropy = 9.53 bits
Computation based on 944 words.
Number of 1-grams hit = 944  (100.00%)
4 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article532.text
Will force inclusive back-off from OOVs.
Perplexity = 618.47, Entropy = 9.27 bits
Computation based on 933 words.
Number of 1-grams hit = 933  (100.00%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article533.text
Will force inclusive back-off from OOVs.
Perplexity = 710.86, Entropy = 9.47 bits
Computation based on 1137 words.
Number of 1-grams hit = 1137  (100.00%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article534.text
Will force inclusive back-off from OOVs.
Perplexity = 903.05, Entropy = 9.82 bits
Computation based on 494 words.
Number of 1-grams hit = 494  (100.00%)
8 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article535.text
Will force inclusive back-off from OOVs.
Perplexity = 1020.43, Entropy = 9.99 bits
Computation based on 358 words.
Number of 1-grams hit = 358  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article536.text
Will force inclusive back-off from OOVs.
Perplexity = 731.60, Entropy = 9.51 bits
Computation based on 1511 words.
Number of 1-grams hit = 1511  (100.00%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article537.text
Will force inclusive back-off from OOVs.
Perplexity = 546.75, Entropy = 9.09 bits
Computation based on 999 words.
Number of 1-grams hit = 999  (100.00%)
4 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article538.text
Will force inclusive back-off from OOVs.
Perplexity = 718.13, Entropy = 9.49 bits
Computation based on 3432 words.
Number of 1-grams hit = 3432  (100.00%)
9 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article539.text
Will force inclusive back-off from OOVs.
Perplexity = 966.05, Entropy = 9.92 bits
Computation based on 534 words.
Number of 1-grams hit = 534  (100.00%)
4 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article540.text
Will force inclusive back-off from OOVs.
Perplexity = 659.37, Entropy = 9.36 bits
Computation based on 2193 words.
Number of 1-grams hit = 2193  (100.00%)
5 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article541.text
Will force inclusive back-off from OOVs.
Perplexity = 820.67, Entropy = 9.68 bits
Computation based on 497 words.
Number of 1-grams hit = 497  (100.00%)
1 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article542.text
Will force inclusive back-off from OOVs.
Perplexity = 487.67, Entropy = 8.93 bits
Computation based on 1590 words.
Number of 1-grams hit = 1590  (100.00%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article543.text
Will force inclusive back-off from OOVs.
Perplexity = 741.74, Entropy = 9.53 bits
Computation based on 534 words.
Number of 1-grams hit = 534  (100.00%)
7 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article544.text
Will force inclusive back-off from OOVs.
Perplexity = 762.34, Entropy = 9.57 bits
Computation based on 1280 words.
Number of 1-grams hit = 1280  (100.00%)
7 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article545.text
Will force inclusive back-off from OOVs.
Perplexity = 667.43, Entropy = 9.38 bits
Computation based on 749 words.
Number of 1-grams hit = 749  (100.00%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article546.text
Will force inclusive back-off from OOVs.
Perplexity = 726.72, Entropy = 9.51 bits
Computation based on 685 words.
Number of 1-grams hit = 685  (100.00%)
7 OOVs (1.01%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article547.text
Will force inclusive back-off from OOVs.
Perplexity = 896.24, Entropy = 9.81 bits
Computation based on 880 words.
Number of 1-grams hit = 880  (100.00%)
15 OOVs (1.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article548.text
Will force inclusive back-off from OOVs.
Perplexity = 674.89, Entropy = 9.40 bits
Computation based on 623 words.
Number of 1-grams hit = 623  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article549.text
Will force inclusive back-off from OOVs.
Perplexity = 819.76, Entropy = 9.68 bits
Computation based on 358 words.
Number of 1-grams hit = 358  (100.00%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article550.text
Will force inclusive back-off from OOVs.
Perplexity = 477.63, Entropy = 8.90 bits
Computation based on 185 words.
Number of 1-grams hit = 185  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article551.text
Will force inclusive back-off from OOVs.
Perplexity = 504.85, Entropy = 8.98 bits
Computation based on 918 words.
Number of 1-grams hit = 918  (100.00%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article552.text
Will force inclusive back-off from OOVs.
Perplexity = 927.79, Entropy = 9.86 bits
Computation based on 1051 words.
Number of 1-grams hit = 1051  (100.00%)
26 OOVs (2.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article553.text
Will force inclusive back-off from OOVs.
Perplexity = 487.43, Entropy = 8.93 bits
Computation based on 1239 words.
Number of 1-grams hit = 1239  (100.00%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article554.text
Will force inclusive back-off from OOVs.
Perplexity = 701.73, Entropy = 9.45 bits
Computation based on 4043 words.
Number of 1-grams hit = 4043  (100.00%)
32 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article555.text
Will force inclusive back-off from OOVs.
Perplexity = 739.78, Entropy = 9.53 bits
Computation based on 505 words.
Number of 1-grams hit = 505  (100.00%)
3 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article556.text
Will force inclusive back-off from OOVs.
Perplexity = 987.68, Entropy = 9.95 bits
Computation based on 555 words.
Number of 1-grams hit = 555  (100.00%)
12 OOVs (2.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article557.text
Will force inclusive back-off from OOVs.
Perplexity = 648.76, Entropy = 9.34 bits
Computation based on 382 words.
Number of 1-grams hit = 382  (100.00%)
1 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article558.text
Will force inclusive back-off from OOVs.
Perplexity = 596.75, Entropy = 9.22 bits
Computation based on 1710 words.
Number of 1-grams hit = 1710  (100.00%)
19 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article559.text
Will force inclusive back-off from OOVs.
Perplexity = 989.72, Entropy = 9.95 bits
Computation based on 629 words.
Number of 1-grams hit = 629  (100.00%)
9 OOVs (1.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article560.text
Will force inclusive back-off from OOVs.
Perplexity = 709.96, Entropy = 9.47 bits
Computation based on 547 words.
Number of 1-grams hit = 547  (100.00%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article561.text
Will force inclusive back-off from OOVs.
Perplexity = 1098.68, Entropy = 10.10 bits
Computation based on 330 words.
Number of 1-grams hit = 330  (100.00%)
3 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article562.text
Will force inclusive back-off from OOVs.
Perplexity = 1136.85, Entropy = 10.15 bits
Computation based on 312 words.
Number of 1-grams hit = 312  (100.00%)
1 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article563.text
Will force inclusive back-off from OOVs.
Perplexity = 619.74, Entropy = 9.28 bits
Computation based on 492 words.
Number of 1-grams hit = 492  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article564.text
Will force inclusive back-off from OOVs.
Perplexity = 756.99, Entropy = 9.56 bits
Computation based on 1335 words.
Number of 1-grams hit = 1335  (100.00%)
4 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article565.text
Will force inclusive back-off from OOVs.
Perplexity = 904.56, Entropy = 9.82 bits
Computation based on 1286 words.
Number of 1-grams hit = 1286  (100.00%)
15 OOVs (1.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article566.text
Will force inclusive back-off from OOVs.
Perplexity = 667.67, Entropy = 9.38 bits
Computation based on 4132 words.
Number of 1-grams hit = 4132  (100.00%)
21 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article567.text
Will force inclusive back-off from OOVs.
Perplexity = 845.25, Entropy = 9.72 bits
Computation based on 547 words.
Number of 1-grams hit = 547  (100.00%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article568.text
Will force inclusive back-off from OOVs.
Perplexity = 863.79, Entropy = 9.75 bits
Computation based on 329 words.
Number of 1-grams hit = 329  (100.00%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article569.text
Will force inclusive back-off from OOVs.
Perplexity = 707.11, Entropy = 9.47 bits
Computation based on 4344 words.
Number of 1-grams hit = 4344  (100.00%)
12 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article570.text
Will force inclusive back-off from OOVs.
Perplexity = 604.89, Entropy = 9.24 bits
Computation based on 815 words.
Number of 1-grams hit = 815  (100.00%)
3 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article571.text
Will force inclusive back-off from OOVs.
Perplexity = 1090.38, Entropy = 10.09 bits
Computation based on 534 words.
Number of 1-grams hit = 534  (100.00%)
6 OOVs (1.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article572.text
Will force inclusive back-off from OOVs.
Perplexity = 722.41, Entropy = 9.50 bits
Computation based on 570 words.
Number of 1-grams hit = 570  (100.00%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article573.text
Will force inclusive back-off from OOVs.
Perplexity = 680.73, Entropy = 9.41 bits
Computation based on 251 words.
Number of 1-grams hit = 251  (100.00%)
1 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article574.text
Will force inclusive back-off from OOVs.
Perplexity = 790.79, Entropy = 9.63 bits
Computation based on 1089 words.
Number of 1-grams hit = 1089  (100.00%)
5 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article575.text
Will force inclusive back-off from OOVs.
Perplexity = 740.70, Entropy = 9.53 bits
Computation based on 595 words.
Number of 1-grams hit = 595  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article576.text
Will force inclusive back-off from OOVs.
Perplexity = 1150.04, Entropy = 10.17 bits
Computation based on 426 words.
Number of 1-grams hit = 426  (100.00%)
19 OOVs (4.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article577.text
Will force inclusive back-off from OOVs.
Perplexity = 618.55, Entropy = 9.27 bits
Computation based on 363 words.
Number of 1-grams hit = 363  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article578.text
Will force inclusive back-off from OOVs.
Perplexity = 679.50, Entropy = 9.41 bits
Computation based on 406 words.
Number of 1-grams hit = 406  (100.00%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article579.text
Will force inclusive back-off from OOVs.
Perplexity = 809.95, Entropy = 9.66 bits
Computation based on 440 words.
Number of 1-grams hit = 440  (100.00%)
2 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article580.text
Will force inclusive back-off from OOVs.
Perplexity = 630.89, Entropy = 9.30 bits
Computation based on 532 words.
Number of 1-grams hit = 532  (100.00%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article581.text
Will force inclusive back-off from OOVs.
Perplexity = 844.88, Entropy = 9.72 bits
Computation based on 533 words.
Number of 1-grams hit = 533  (100.00%)
8 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article582.text
Will force inclusive back-off from OOVs.
Perplexity = 919.34, Entropy = 9.84 bits
Computation based on 500 words.
Number of 1-grams hit = 500  (100.00%)
9 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article583.text
Will force inclusive back-off from OOVs.
Perplexity = 943.87, Entropy = 9.88 bits
Computation based on 664 words.
Number of 1-grams hit = 664  (100.00%)
11 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article584.text
Will force inclusive back-off from OOVs.
Perplexity = 702.45, Entropy = 9.46 bits
Computation based on 433 words.
Number of 1-grams hit = 433  (100.00%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article585.text
Will force inclusive back-off from OOVs.
Perplexity = 682.71, Entropy = 9.42 bits
Computation based on 946 words.
Number of 1-grams hit = 946  (100.00%)
8 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article586.text
Will force inclusive back-off from OOVs.
Perplexity = 604.32, Entropy = 9.24 bits
Computation based on 733 words.
Number of 1-grams hit = 733  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article587.text
Will force inclusive back-off from OOVs.
Perplexity = 812.63, Entropy = 9.67 bits
Computation based on 522 words.
Number of 1-grams hit = 522  (100.00%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article588.text
Will force inclusive back-off from OOVs.
Perplexity = 710.72, Entropy = 9.47 bits
Computation based on 955 words.
Number of 1-grams hit = 955  (100.00%)
5 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article589.text
Will force inclusive back-off from OOVs.
Perplexity = 742.52, Entropy = 9.54 bits
Computation based on 504 words.
Number of 1-grams hit = 504  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article590.text
Will force inclusive back-off from OOVs.
Perplexity = 769.64, Entropy = 9.59 bits
Computation based on 1387 words.
Number of 1-grams hit = 1387  (100.00%)
6 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article591.text
Will force inclusive back-off from OOVs.
Perplexity = 613.45, Entropy = 9.26 bits
Computation based on 686 words.
Number of 1-grams hit = 686  (100.00%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article592.text
Will force inclusive back-off from OOVs.
Perplexity = 828.93, Entropy = 9.70 bits
Computation based on 339 words.
Number of 1-grams hit = 339  (100.00%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article593.text
Will force inclusive back-off from OOVs.
Perplexity = 610.07, Entropy = 9.25 bits
Computation based on 376 words.
Number of 1-grams hit = 376  (100.00%)
2 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article594.text
Will force inclusive back-off from OOVs.
Perplexity = 561.31, Entropy = 9.13 bits
Computation based on 586 words.
Number of 1-grams hit = 586  (100.00%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article595.text
Will force inclusive back-off from OOVs.
Perplexity = 772.84, Entropy = 9.59 bits
Computation based on 369 words.
Number of 1-grams hit = 369  (100.00%)
2 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article596.text
Will force inclusive back-off from OOVs.
Perplexity = 745.16, Entropy = 9.54 bits
Computation based on 963 words.
Number of 1-grams hit = 963  (100.00%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article597.text
Will force inclusive back-off from OOVs.
Perplexity = 938.97, Entropy = 9.87 bits
Computation based on 1027 words.
Number of 1-grams hit = 1027  (100.00%)
7 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article598.text
Will force inclusive back-off from OOVs.
Perplexity = 736.38, Entropy = 9.52 bits
Computation based on 296 words.
Number of 1-grams hit = 296  (100.00%)
3 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article599.text
Will force inclusive back-off from OOVs.
Perplexity = 990.22, Entropy = 9.95 bits
Computation based on 802 words.
Number of 1-grams hit = 802  (100.00%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article600.text
Will force inclusive back-off from OOVs.
Perplexity = 437.70, Entropy = 8.77 bits
Computation based on 4658 words.
Number of 1-grams hit = 4658  (100.00%)
15 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article601.text
Will force inclusive back-off from OOVs.
Perplexity = 1239.24, Entropy = 10.28 bits
Computation based on 1088 words.
Number of 1-grams hit = 1088  (100.00%)
11 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article602.text
Will force inclusive back-off from OOVs.
Perplexity = 634.18, Entropy = 9.31 bits
Computation based on 590 words.
Number of 1-grams hit = 590  (100.00%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article603.text
Will force inclusive back-off from OOVs.
Perplexity = 914.49, Entropy = 9.84 bits
Computation based on 584 words.
Number of 1-grams hit = 584  (100.00%)
3 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article604.text
Will force inclusive back-off from OOVs.
Perplexity = 783.06, Entropy = 9.61 bits
Computation based on 560 words.
Number of 1-grams hit = 560  (100.00%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article605.text
Will force inclusive back-off from OOVs.
Perplexity = 598.62, Entropy = 9.23 bits
Computation based on 3116 words.
Number of 1-grams hit = 3116  (100.00%)
40 OOVs (1.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article606.text
Will force inclusive back-off from OOVs.
Perplexity = 754.77, Entropy = 9.56 bits
Computation based on 900 words.
Number of 1-grams hit = 900  (100.00%)
13 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article607.text
Will force inclusive back-off from OOVs.
Perplexity = 703.46, Entropy = 9.46 bits
Computation based on 1058 words.
Number of 1-grams hit = 1058  (100.00%)
5 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article608.text
Will force inclusive back-off from OOVs.
Perplexity = 601.45, Entropy = 9.23 bits
Computation based on 617 words.
Number of 1-grams hit = 617  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article609.text
Will force inclusive back-off from OOVs.
Perplexity = 637.65, Entropy = 9.32 bits
Computation based on 4371 words.
Number of 1-grams hit = 4371  (100.00%)
38 OOVs (0.86%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article610.text
Will force inclusive back-off from OOVs.
Perplexity = 690.21, Entropy = 9.43 bits
Computation based on 5950 words.
Number of 1-grams hit = 5950  (100.00%)
13 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article611.text
Will force inclusive back-off from OOVs.
Perplexity = 719.35, Entropy = 9.49 bits
Computation based on 776 words.
Number of 1-grams hit = 776  (100.00%)
4 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article612.text
Will force inclusive back-off from OOVs.
Perplexity = 603.00, Entropy = 9.24 bits
Computation based on 1629 words.
Number of 1-grams hit = 1629  (100.00%)
3 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article613.text
Will force inclusive back-off from OOVs.
Perplexity = 591.35, Entropy = 9.21 bits
Computation based on 4146 words.
Number of 1-grams hit = 4146  (100.00%)
42 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article614.text
Will force inclusive back-off from OOVs.
Perplexity = 670.49, Entropy = 9.39 bits
Computation based on 1524 words.
Number of 1-grams hit = 1524  (100.00%)
26 OOVs (1.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article615.text
Will force inclusive back-off from OOVs.
Perplexity = 615.82, Entropy = 9.27 bits
Computation based on 452 words.
Number of 1-grams hit = 452  (100.00%)
3 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article616.text
Will force inclusive back-off from OOVs.
Perplexity = 732.44, Entropy = 9.52 bits
Computation based on 767 words.
Number of 1-grams hit = 767  (100.00%)
5 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article617.text
Will force inclusive back-off from OOVs.
Perplexity = 741.28, Entropy = 9.53 bits
Computation based on 1319 words.
Number of 1-grams hit = 1319  (100.00%)
5 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article618.text
Will force inclusive back-off from OOVs.
Perplexity = 782.66, Entropy = 9.61 bits
Computation based on 3769 words.
Number of 1-grams hit = 3769  (100.00%)
8 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article619.text
Will force inclusive back-off from OOVs.
Perplexity = 647.69, Entropy = 9.34 bits
Computation based on 761 words.
Number of 1-grams hit = 761  (100.00%)
6 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article620.text
Will force inclusive back-off from OOVs.
Perplexity = 752.93, Entropy = 9.56 bits
Computation based on 5314 words.
Number of 1-grams hit = 5314  (100.00%)
15 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article621.text
Will force inclusive back-off from OOVs.
Perplexity = 1059.22, Entropy = 10.05 bits
Computation based on 321 words.
Number of 1-grams hit = 321  (100.00%)
2 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article622.text
Will force inclusive back-off from OOVs.
Perplexity = 749.38, Entropy = 9.55 bits
Computation based on 452 words.
Number of 1-grams hit = 452  (100.00%)
7 OOVs (1.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article623.text
Will force inclusive back-off from OOVs.
Perplexity = 658.73, Entropy = 9.36 bits
Computation based on 1306 words.
Number of 1-grams hit = 1306  (100.00%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article624.text
Will force inclusive back-off from OOVs.
Perplexity = 758.36, Entropy = 9.57 bits
Computation based on 5146 words.
Number of 1-grams hit = 5146  (100.00%)
20 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article625.text
Will force inclusive back-off from OOVs.
Perplexity = 755.36, Entropy = 9.56 bits
Computation based on 1088 words.
Number of 1-grams hit = 1088  (100.00%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article626.text
Will force inclusive back-off from OOVs.
Perplexity = 737.84, Entropy = 9.53 bits
Computation based on 446 words.
Number of 1-grams hit = 446  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article627.text
Will force inclusive back-off from OOVs.
Perplexity = 1000.08, Entropy = 9.97 bits
Computation based on 141 words.
Number of 1-grams hit = 141  (100.00%)
8 OOVs (5.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article628.text
Will force inclusive back-off from OOVs.
Perplexity = 912.40, Entropy = 9.83 bits
Computation based on 1057 words.
Number of 1-grams hit = 1057  (100.00%)
18 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article629.text
Will force inclusive back-off from OOVs.
Perplexity = 780.19, Entropy = 9.61 bits
Computation based on 1027 words.
Number of 1-grams hit = 1027  (100.00%)
10 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article630.text
Will force inclusive back-off from OOVs.
Perplexity = 1388.93, Entropy = 10.44 bits
Computation based on 328 words.
Number of 1-grams hit = 328  (100.00%)
1 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article631.text
Will force inclusive back-off from OOVs.
Perplexity = 1014.36, Entropy = 9.99 bits
Computation based on 857 words.
Number of 1-grams hit = 857  (100.00%)
19 OOVs (2.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article632.text
Will force inclusive back-off from OOVs.
Perplexity = 835.63, Entropy = 9.71 bits
Computation based on 408 words.
Number of 1-grams hit = 408  (100.00%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article633.text
Will force inclusive back-off from OOVs.
Perplexity = 734.59, Entropy = 9.52 bits
Computation based on 529 words.
Number of 1-grams hit = 529  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article634.text
Will force inclusive back-off from OOVs.
Perplexity = 633.82, Entropy = 9.31 bits
Computation based on 560 words.
Number of 1-grams hit = 560  (100.00%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article635.text
Will force inclusive back-off from OOVs.
Perplexity = 742.84, Entropy = 9.54 bits
Computation based on 151 words.
Number of 1-grams hit = 151  (100.00%)
1 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article636.text
Will force inclusive back-off from OOVs.
Perplexity = 991.75, Entropy = 9.95 bits
Computation based on 1322 words.
Number of 1-grams hit = 1322  (100.00%)
22 OOVs (1.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article637.text
Will force inclusive back-off from OOVs.
Perplexity = 754.80, Entropy = 9.56 bits
Computation based on 1034 words.
Number of 1-grams hit = 1034  (100.00%)
1 OOVs (0.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article638.text
Will force inclusive back-off from OOVs.
Perplexity = 1058.82, Entropy = 10.05 bits
Computation based on 526 words.
Number of 1-grams hit = 526  (100.00%)
7 OOVs (1.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article639.text
Will force inclusive back-off from OOVs.
Perplexity = 1011.62, Entropy = 9.98 bits
Computation based on 522 words.
Number of 1-grams hit = 522  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article640.text
Will force inclusive back-off from OOVs.
Perplexity = 679.89, Entropy = 9.41 bits
Computation based on 204 words.
Number of 1-grams hit = 204  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article641.text
Will force inclusive back-off from OOVs.
Perplexity = 871.90, Entropy = 9.77 bits
Computation based on 825 words.
Number of 1-grams hit = 825  (100.00%)
5 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article642.text
Will force inclusive back-off from OOVs.
Perplexity = 1517.73, Entropy = 10.57 bits
Computation based on 429 words.
Number of 1-grams hit = 429  (100.00%)
15 OOVs (3.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article643.text
Will force inclusive back-off from OOVs.
Perplexity = 659.66, Entropy = 9.37 bits
Computation based on 488 words.
Number of 1-grams hit = 488  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article644.text
Will force inclusive back-off from OOVs.
Perplexity = 669.48, Entropy = 9.39 bits
Computation based on 1116 words.
Number of 1-grams hit = 1116  (100.00%)
5 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article645.text
Will force inclusive back-off from OOVs.
Perplexity = 942.14, Entropy = 9.88 bits
Computation based on 440 words.
Number of 1-grams hit = 440  (100.00%)
6 OOVs (1.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article646.text
Will force inclusive back-off from OOVs.
Perplexity = 718.53, Entropy = 9.49 bits
Computation based on 573 words.
Number of 1-grams hit = 573  (100.00%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article647.text
Will force inclusive back-off from OOVs.
Perplexity = 807.57, Entropy = 9.66 bits
Computation based on 608 words.
Number of 1-grams hit = 608  (100.00%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article648.text
Will force inclusive back-off from OOVs.
Perplexity = 663.85, Entropy = 9.37 bits
Computation based on 475 words.
Number of 1-grams hit = 475  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article649.text
Will force inclusive back-off from OOVs.
Perplexity = 784.67, Entropy = 9.62 bits
Computation based on 530 words.
Number of 1-grams hit = 530  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article650.text
Will force inclusive back-off from OOVs.
Perplexity = 782.63, Entropy = 9.61 bits
Computation based on 529 words.
Number of 1-grams hit = 529  (100.00%)
5 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article651.text
Will force inclusive back-off from OOVs.
Perplexity = 656.78, Entropy = 9.36 bits
Computation based on 459 words.
Number of 1-grams hit = 459  (100.00%)
3 OOVs (0.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article652.text
Will force inclusive back-off from OOVs.
Perplexity = 492.33, Entropy = 8.94 bits
Computation based on 878 words.
Number of 1-grams hit = 878  (100.00%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article653.text
Will force inclusive back-off from OOVs.
Perplexity = 588.35, Entropy = 9.20 bits
Computation based on 765 words.
Number of 1-grams hit = 765  (100.00%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article654.text
Will force inclusive back-off from OOVs.
Perplexity = 705.61, Entropy = 9.46 bits
Computation based on 772 words.
Number of 1-grams hit = 772  (100.00%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article655.text
Will force inclusive back-off from OOVs.
Perplexity = 733.08, Entropy = 9.52 bits
Computation based on 481 words.
Number of 1-grams hit = 481  (100.00%)
6 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article656.text
Will force inclusive back-off from OOVs.
Perplexity = 1249.61, Entropy = 10.29 bits
Computation based on 461 words.
Number of 1-grams hit = 461  (100.00%)
5 OOVs (1.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article657.text
Will force inclusive back-off from OOVs.
Perplexity = 911.03, Entropy = 9.83 bits
Computation based on 376 words.
Number of 1-grams hit = 376  (100.00%)
1 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article658.text
Will force inclusive back-off from OOVs.
Perplexity = 613.11, Entropy = 9.26 bits
Computation based on 1004 words.
Number of 1-grams hit = 1004  (100.00%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article659.text
Will force inclusive back-off from OOVs.
Perplexity = 702.89, Entropy = 9.46 bits
Computation based on 964 words.
Number of 1-grams hit = 964  (100.00%)
26 OOVs (2.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article660.text
Will force inclusive back-off from OOVs.
Perplexity = 1347.50, Entropy = 10.40 bits
Computation based on 139 words.
Number of 1-grams hit = 139  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article661.text
Will force inclusive back-off from OOVs.
Perplexity = 775.19, Entropy = 9.60 bits
Computation based on 1244 words.
Number of 1-grams hit = 1244  (100.00%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article662.text
Will force inclusive back-off from OOVs.
Perplexity = 1227.51, Entropy = 10.26 bits
Computation based on 123 words.
Number of 1-grams hit = 123  (100.00%)
3 OOVs (2.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article663.text
Will force inclusive back-off from OOVs.
Perplexity = 858.09, Entropy = 9.74 bits
Computation based on 1338 words.
Number of 1-grams hit = 1338  (100.00%)
22 OOVs (1.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article664.text
Will force inclusive back-off from OOVs.
Perplexity = 553.96, Entropy = 9.11 bits
Computation based on 692 words.
Number of 1-grams hit = 692  (100.00%)
7 OOVs (1.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article665.text
Will force inclusive back-off from OOVs.
Perplexity = 602.68, Entropy = 9.24 bits
Computation based on 756 words.
Number of 1-grams hit = 756  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article666.text
Will force inclusive back-off from OOVs.
Perplexity = 807.38, Entropy = 9.66 bits
Computation based on 383 words.
Number of 1-grams hit = 383  (100.00%)
2 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article667.text
Will force inclusive back-off from OOVs.
Perplexity = 387.45, Entropy = 8.60 bits
Computation based on 1292 words.
Number of 1-grams hit = 1292  (100.00%)
9 OOVs (0.69%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article668.text
Will force inclusive back-off from OOVs.
Perplexity = 692.83, Entropy = 9.44 bits
Computation based on 466 words.
Number of 1-grams hit = 466  (100.00%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article669.text
Will force inclusive back-off from OOVs.
Perplexity = 732.64, Entropy = 9.52 bits
Computation based on 485 words.
Number of 1-grams hit = 485  (100.00%)
4 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article670.text
Will force inclusive back-off from OOVs.
Perplexity = 773.64, Entropy = 9.60 bits
Computation based on 448 words.
Number of 1-grams hit = 448  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article671.text
Will force inclusive back-off from OOVs.
Perplexity = 690.87, Entropy = 9.43 bits
Computation based on 2031 words.
Number of 1-grams hit = 2031  (100.00%)
5 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article672.text
Will force inclusive back-off from OOVs.
Perplexity = 699.74, Entropy = 9.45 bits
Computation based on 284 words.
Number of 1-grams hit = 284  (100.00%)
4 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article673.text
Will force inclusive back-off from OOVs.
Perplexity = 569.47, Entropy = 9.15 bits
Computation based on 301 words.
Number of 1-grams hit = 301  (100.00%)
3 OOVs (0.99%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article674.text
Will force inclusive back-off from OOVs.
Perplexity = 505.52, Entropy = 8.98 bits
Computation based on 141 words.
Number of 1-grams hit = 141  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article675.text
Will force inclusive back-off from OOVs.
Perplexity = 681.78, Entropy = 9.41 bits
Computation based on 3859 words.
Number of 1-grams hit = 3859  (100.00%)
15 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article676.text
Will force inclusive back-off from OOVs.
Perplexity = 717.72, Entropy = 9.49 bits
Computation based on 496 words.
Number of 1-grams hit = 496  (100.00%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article677.text
Will force inclusive back-off from OOVs.
Perplexity = 767.67, Entropy = 9.58 bits
Computation based on 641 words.
Number of 1-grams hit = 641  (100.00%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article678.text
Will force inclusive back-off from OOVs.
Perplexity = 695.50, Entropy = 9.44 bits
Computation based on 340 words.
Number of 1-grams hit = 340  (100.00%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article679.text
Will force inclusive back-off from OOVs.
Perplexity = 708.45, Entropy = 9.47 bits
Computation based on 986 words.
Number of 1-grams hit = 986  (100.00%)
3 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article680.text
Will force inclusive back-off from OOVs.
Perplexity = 576.08, Entropy = 9.17 bits
Computation based on 317 words.
Number of 1-grams hit = 317  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article681.text
Will force inclusive back-off from OOVs.
Perplexity = 672.87, Entropy = 9.39 bits
Computation based on 474 words.
Number of 1-grams hit = 474  (100.00%)
4 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article682.text
Will force inclusive back-off from OOVs.
Perplexity = 675.34, Entropy = 9.40 bits
Computation based on 3680 words.
Number of 1-grams hit = 3680  (100.00%)
10 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article683.text
Will force inclusive back-off from OOVs.
Perplexity = 924.16, Entropy = 9.85 bits
Computation based on 261 words.
Number of 1-grams hit = 261  (100.00%)
5 OOVs (1.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article684.text
Will force inclusive back-off from OOVs.
Perplexity = 642.60, Entropy = 9.33 bits
Computation based on 4339 words.
Number of 1-grams hit = 4339  (100.00%)
19 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article685.text
Will force inclusive back-off from OOVs.
Perplexity = 825.75, Entropy = 9.69 bits
Computation based on 455 words.
Number of 1-grams hit = 455  (100.00%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article686.text
Will force inclusive back-off from OOVs.
Perplexity = 761.19, Entropy = 9.57 bits
Computation based on 2213 words.
Number of 1-grams hit = 2213  (100.00%)
16 OOVs (0.72%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article687.text
Will force inclusive back-off from OOVs.
Perplexity = 873.17, Entropy = 9.77 bits
Computation based on 512 words.
Number of 1-grams hit = 512  (100.00%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article688.text
Will force inclusive back-off from OOVs.
Perplexity = 665.70, Entropy = 9.38 bits
Computation based on 303 words.
Number of 1-grams hit = 303  (100.00%)
1 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article689.text
Will force inclusive back-off from OOVs.
Perplexity = 869.58, Entropy = 9.76 bits
Computation based on 508 words.
Number of 1-grams hit = 508  (100.00%)
2 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article690.text
Will force inclusive back-off from OOVs.
Perplexity = 731.05, Entropy = 9.51 bits
Computation based on 754 words.
Number of 1-grams hit = 754  (100.00%)
1 OOVs (0.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article691.text
Will force inclusive back-off from OOVs.
Perplexity = 808.58, Entropy = 9.66 bits
Computation based on 872 words.
Number of 1-grams hit = 872  (100.00%)
6 OOVs (0.68%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article692.text
Will force inclusive back-off from OOVs.
Perplexity = 987.96, Entropy = 9.95 bits
Computation based on 427 words.
Number of 1-grams hit = 427  (100.00%)
6 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article693.text
Will force inclusive back-off from OOVs.
Perplexity = 665.35, Entropy = 9.38 bits
Computation based on 922 words.
Number of 1-grams hit = 922  (100.00%)
22 OOVs (2.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article694.text
Will force inclusive back-off from OOVs.
Perplexity = 679.33, Entropy = 9.41 bits
Computation based on 626 words.
Number of 1-grams hit = 626  (100.00%)
4 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article695.text
Will force inclusive back-off from OOVs.
Perplexity = 764.90, Entropy = 9.58 bits
Computation based on 849 words.
Number of 1-grams hit = 849  (100.00%)
5 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article696.text
Will force inclusive back-off from OOVs.
Perplexity = 811.44, Entropy = 9.66 bits
Computation based on 496 words.
Number of 1-grams hit = 496  (100.00%)
3 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article697.text
Will force inclusive back-off from OOVs.
Perplexity = 720.71, Entropy = 9.49 bits
Computation based on 942 words.
Number of 1-grams hit = 942  (100.00%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article698.text
Will force inclusive back-off from OOVs.
Perplexity = 804.78, Entropy = 9.65 bits
Computation based on 572 words.
Number of 1-grams hit = 572  (100.00%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article699.text
Will force inclusive back-off from OOVs.
Perplexity = 743.46, Entropy = 9.54 bits
Computation based on 352 words.
Number of 1-grams hit = 352  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article700.text
Will force inclusive back-off from OOVs.
Perplexity = 585.31, Entropy = 9.19 bits
Computation based on 3290 words.
Number of 1-grams hit = 3290  (100.00%)
53 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article701.text
Will force inclusive back-off from OOVs.
Perplexity = 795.30, Entropy = 9.64 bits
Computation based on 807 words.
Number of 1-grams hit = 807  (100.00%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article702.text
Will force inclusive back-off from OOVs.
Perplexity = 841.10, Entropy = 9.72 bits
Computation based on 1792 words.
Number of 1-grams hit = 1792  (100.00%)
45 OOVs (2.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article703.text
Will force inclusive back-off from OOVs.
Perplexity = 983.73, Entropy = 9.94 bits
Computation based on 421 words.
Number of 1-grams hit = 421  (100.00%)
5 OOVs (1.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article704.text
Will force inclusive back-off from OOVs.
Perplexity = 594.18, Entropy = 9.21 bits
Computation based on 311 words.
Number of 1-grams hit = 311  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article705.text
Will force inclusive back-off from OOVs.
Perplexity = 675.42, Entropy = 9.40 bits
Computation based on 1010 words.
Number of 1-grams hit = 1010  (100.00%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article706.text
Will force inclusive back-off from OOVs.
Perplexity = 714.37, Entropy = 9.48 bits
Computation based on 476 words.
Number of 1-grams hit = 476  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article707.text
Will force inclusive back-off from OOVs.
Perplexity = 983.31, Entropy = 9.94 bits
Computation based on 869 words.
Number of 1-grams hit = 869  (100.00%)
2 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article708.text
Will force inclusive back-off from OOVs.
Perplexity = 745.03, Entropy = 9.54 bits
Computation based on 523 words.
Number of 1-grams hit = 523  (100.00%)
5 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article709.text
Will force inclusive back-off from OOVs.
Perplexity = 808.45, Entropy = 9.66 bits
Computation based on 548 words.
Number of 1-grams hit = 548  (100.00%)
10 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article710.text
Will force inclusive back-off from OOVs.
Perplexity = 815.69, Entropy = 9.67 bits
Computation based on 753 words.
Number of 1-grams hit = 753  (100.00%)
21 OOVs (2.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article711.text
Will force inclusive back-off from OOVs.
Perplexity = 840.83, Entropy = 9.72 bits
Computation based on 959 words.
Number of 1-grams hit = 959  (100.00%)
3 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article712.text
Will force inclusive back-off from OOVs.
Perplexity = 882.30, Entropy = 9.79 bits
Computation based on 365 words.
Number of 1-grams hit = 365  (100.00%)
4 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article713.text
Will force inclusive back-off from OOVs.
Perplexity = 591.44, Entropy = 9.21 bits
Computation based on 887 words.
Number of 1-grams hit = 887  (100.00%)
3 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article714.text
Will force inclusive back-off from OOVs.
Perplexity = 646.47, Entropy = 9.34 bits
Computation based on 364 words.
Number of 1-grams hit = 364  (100.00%)
2 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article715.text
Will force inclusive back-off from OOVs.
Perplexity = 739.41, Entropy = 9.53 bits
Computation based on 486 words.
Number of 1-grams hit = 486  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article716.text
Will force inclusive back-off from OOVs.
Perplexity = 854.50, Entropy = 9.74 bits
Computation based on 452 words.
Number of 1-grams hit = 452  (100.00%)
16 OOVs (3.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article717.text
Will force inclusive back-off from OOVs.
Perplexity = 1105.37, Entropy = 10.11 bits
Computation based on 377 words.
Number of 1-grams hit = 377  (100.00%)
4 OOVs (1.05%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article718.text
Will force inclusive back-off from OOVs.
Perplexity = 742.39, Entropy = 9.54 bits
Computation based on 6958 words.
Number of 1-grams hit = 6958  (100.00%)
27 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article719.text
Will force inclusive back-off from OOVs.
Perplexity = 648.07, Entropy = 9.34 bits
Computation based on 1162 words.
Number of 1-grams hit = 1162  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article720.text
Will force inclusive back-off from OOVs.
Perplexity = 743.82, Entropy = 9.54 bits
Computation based on 806 words.
Number of 1-grams hit = 806  (100.00%)
8 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article721.text
Will force inclusive back-off from OOVs.
Perplexity = 715.64, Entropy = 9.48 bits
Computation based on 423 words.
Number of 1-grams hit = 423  (100.00%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article722.text
Will force inclusive back-off from OOVs.
Perplexity = 748.12, Entropy = 9.55 bits
Computation based on 1012 words.
Number of 1-grams hit = 1012  (100.00%)
8 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article723.text
Will force inclusive back-off from OOVs.
Perplexity = 1157.97, Entropy = 10.18 bits
Computation based on 427 words.
Number of 1-grams hit = 427  (100.00%)
13 OOVs (2.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article724.text
Will force inclusive back-off from OOVs.
Perplexity = 924.34, Entropy = 9.85 bits
Computation based on 458 words.
Number of 1-grams hit = 458  (100.00%)
4 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article725.text
Will force inclusive back-off from OOVs.
Perplexity = 736.01, Entropy = 9.52 bits
Computation based on 710 words.
Number of 1-grams hit = 710  (100.00%)
12 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article726.text
Will force inclusive back-off from OOVs.
Perplexity = 483.55, Entropy = 8.92 bits
Computation based on 519 words.
Number of 1-grams hit = 519  (100.00%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article727.text
Will force inclusive back-off from OOVs.
Perplexity = 1214.10, Entropy = 10.25 bits
Computation based on 482 words.
Number of 1-grams hit = 482  (100.00%)
11 OOVs (2.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article728.text
Will force inclusive back-off from OOVs.
Perplexity = 796.93, Entropy = 9.64 bits
Computation based on 416 words.
Number of 1-grams hit = 416  (100.00%)
5 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article729.text
Will force inclusive back-off from OOVs.
Perplexity = 890.70, Entropy = 9.80 bits
Computation based on 437 words.
Number of 1-grams hit = 437  (100.00%)
2 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article730.text
Will force inclusive back-off from OOVs.
Perplexity = 562.75, Entropy = 9.14 bits
Computation based on 8423 words.
Number of 1-grams hit = 8423  (100.00%)
54 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article731.text
Will force inclusive back-off from OOVs.
Perplexity = 627.88, Entropy = 9.29 bits
Computation based on 1450 words.
Number of 1-grams hit = 1450  (100.00%)
6 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article732.text
Will force inclusive back-off from OOVs.
Perplexity = 758.08, Entropy = 9.57 bits
Computation based on 702 words.
Number of 1-grams hit = 702  (100.00%)
3 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article733.text
Will force inclusive back-off from OOVs.
Perplexity = 674.18, Entropy = 9.40 bits
Computation based on 559 words.
Number of 1-grams hit = 559  (100.00%)
3 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article734.text
Will force inclusive back-off from OOVs.
Perplexity = 1262.63, Entropy = 10.30 bits
Computation based on 414 words.
Number of 1-grams hit = 414  (100.00%)
7 OOVs (1.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article735.text
Will force inclusive back-off from OOVs.
Perplexity = 626.39, Entropy = 9.29 bits
Computation based on 1178 words.
Number of 1-grams hit = 1178  (100.00%)
6 OOVs (0.51%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article736.text
Will force inclusive back-off from OOVs.
Perplexity = 715.44, Entropy = 9.48 bits
Computation based on 667 words.
Number of 1-grams hit = 667  (100.00%)
1 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article737.text
Will force inclusive back-off from OOVs.
Perplexity = 771.42, Entropy = 9.59 bits
Computation based on 633 words.
Number of 1-grams hit = 633  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article738.text
Will force inclusive back-off from OOVs.
Perplexity = 813.16, Entropy = 9.67 bits
Computation based on 1002 words.
Number of 1-grams hit = 1002  (100.00%)
9 OOVs (0.89%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article739.text
Will force inclusive back-off from OOVs.
Perplexity = 657.08, Entropy = 9.36 bits
Computation based on 433 words.
Number of 1-grams hit = 433  (100.00%)
7 OOVs (1.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article740.text
Will force inclusive back-off from OOVs.
Perplexity = 578.00, Entropy = 9.17 bits
Computation based on 1093 words.
Number of 1-grams hit = 1093  (100.00%)
3 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article741.text
Will force inclusive back-off from OOVs.
Perplexity = 760.30, Entropy = 9.57 bits
Computation based on 756 words.
Number of 1-grams hit = 756  (100.00%)
2 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article742.text
Will force inclusive back-off from OOVs.
Perplexity = 865.88, Entropy = 9.76 bits
Computation based on 609 words.
Number of 1-grams hit = 609  (100.00%)
22 OOVs (3.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article743.text
Will force inclusive back-off from OOVs.
Perplexity = 818.96, Entropy = 9.68 bits
Computation based on 1973 words.
Number of 1-grams hit = 1973  (100.00%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article744.text
Will force inclusive back-off from OOVs.
Perplexity = 645.90, Entropy = 9.34 bits
Computation based on 525 words.
Number of 1-grams hit = 525  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article745.text
Will force inclusive back-off from OOVs.
Perplexity = 789.22, Entropy = 9.62 bits
Computation based on 687 words.
Number of 1-grams hit = 687  (100.00%)
6 OOVs (0.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article746.text
Will force inclusive back-off from OOVs.
Perplexity = 429.15, Entropy = 8.75 bits
Computation based on 1313 words.
Number of 1-grams hit = 1313  (100.00%)
19 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article747.text
Will force inclusive back-off from OOVs.
Perplexity = 645.88, Entropy = 9.34 bits
Computation based on 518 words.
Number of 1-grams hit = 518  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article748.text
Will force inclusive back-off from OOVs.
Perplexity = 767.22, Entropy = 9.58 bits
Computation based on 500 words.
Number of 1-grams hit = 500  (100.00%)
9 OOVs (1.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article749.text
Will force inclusive back-off from OOVs.
Perplexity = 1019.05, Entropy = 9.99 bits
Computation based on 550 words.
Number of 1-grams hit = 550  (100.00%)
2 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article750.text
Will force inclusive back-off from OOVs.
Perplexity = 584.41, Entropy = 9.19 bits
Computation based on 467 words.
Number of 1-grams hit = 467  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article751.text
Will force inclusive back-off from OOVs.
Perplexity = 852.15, Entropy = 9.73 bits
Computation based on 2005 words.
Number of 1-grams hit = 2005  (100.00%)
11 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article752.text
Will force inclusive back-off from OOVs.
Perplexity = 611.57, Entropy = 9.26 bits
Computation based on 427 words.
Number of 1-grams hit = 427  (100.00%)
3 OOVs (0.70%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article753.text
Will force inclusive back-off from OOVs.
Perplexity = 991.13, Entropy = 9.95 bits
Computation based on 531 words.
Number of 1-grams hit = 531  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article754.text
Will force inclusive back-off from OOVs.
Perplexity = 670.99, Entropy = 9.39 bits
Computation based on 1850 words.
Number of 1-grams hit = 1850  (100.00%)
12 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article755.text
Will force inclusive back-off from OOVs.
Perplexity = 727.00, Entropy = 9.51 bits
Computation based on 448 words.
Number of 1-grams hit = 448  (100.00%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article756.text
Will force inclusive back-off from OOVs.
Perplexity = 899.11, Entropy = 9.81 bits
Computation based on 709 words.
Number of 1-grams hit = 709  (100.00%)
1 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article757.text
Will force inclusive back-off from OOVs.
Perplexity = 711.21, Entropy = 9.47 bits
Computation based on 506 words.
Number of 1-grams hit = 506  (100.00%)
4 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article758.text
Will force inclusive back-off from OOVs.
Perplexity = 730.62, Entropy = 9.51 bits
Computation based on 2506 words.
Number of 1-grams hit = 2506  (100.00%)
10 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article759.text
Will force inclusive back-off from OOVs.
Perplexity = 654.63, Entropy = 9.35 bits
Computation based on 3315 words.
Number of 1-grams hit = 3315  (100.00%)
7 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article760.text
Will force inclusive back-off from OOVs.
Perplexity = 757.22, Entropy = 9.56 bits
Computation based on 550 words.
Number of 1-grams hit = 550  (100.00%)
6 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article761.text
Will force inclusive back-off from OOVs.
Perplexity = 780.64, Entropy = 9.61 bits
Computation based on 629 words.
Number of 1-grams hit = 629  (100.00%)
6 OOVs (0.94%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article762.text
Will force inclusive back-off from OOVs.
Perplexity = 812.32, Entropy = 9.67 bits
Computation based on 790 words.
Number of 1-grams hit = 790  (100.00%)
2 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article763.text
Will force inclusive back-off from OOVs.
Perplexity = 688.57, Entropy = 9.43 bits
Computation based on 722 words.
Number of 1-grams hit = 722  (100.00%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article764.text
Will force inclusive back-off from OOVs.
Perplexity = 946.34, Entropy = 9.89 bits
Computation based on 534 words.
Number of 1-grams hit = 534  (100.00%)
7 OOVs (1.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article765.text
Will force inclusive back-off from OOVs.
Perplexity = 1101.91, Entropy = 10.11 bits
Computation based on 828 words.
Number of 1-grams hit = 828  (100.00%)
10 OOVs (1.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article766.text
Will force inclusive back-off from OOVs.
Perplexity = 684.27, Entropy = 9.42 bits
Computation based on 263 words.
Number of 1-grams hit = 263  (100.00%)
5 OOVs (1.87%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article767.text
Will force inclusive back-off from OOVs.
Perplexity = 760.35, Entropy = 9.57 bits
Computation based on 4386 words.
Number of 1-grams hit = 4386  (100.00%)
33 OOVs (0.75%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article768.text
Will force inclusive back-off from OOVs.
Perplexity = 716.82, Entropy = 9.49 bits
Computation based on 461 words.
Number of 1-grams hit = 461  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article769.text
Will force inclusive back-off from OOVs.
Perplexity = 629.43, Entropy = 9.30 bits
Computation based on 677 words.
Number of 1-grams hit = 677  (100.00%)
4 OOVs (0.59%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article770.text
Will force inclusive back-off from OOVs.
Perplexity = 770.26, Entropy = 9.59 bits
Computation based on 606 words.
Number of 1-grams hit = 606  (100.00%)
4 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article771.text
Will force inclusive back-off from OOVs.
Perplexity = 733.02, Entropy = 9.52 bits
Computation based on 468 words.
Number of 1-grams hit = 468  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article772.text
Will force inclusive back-off from OOVs.
Perplexity = 853.87, Entropy = 9.74 bits
Computation based on 541 words.
Number of 1-grams hit = 541  (100.00%)
6 OOVs (1.10%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article773.text
Will force inclusive back-off from OOVs.
Perplexity = 692.13, Entropy = 9.43 bits
Computation based on 2984 words.
Number of 1-grams hit = 2984  (100.00%)
14 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article774.text
Will force inclusive back-off from OOVs.
Perplexity = 582.76, Entropy = 9.19 bits
Computation based on 2009 words.
Number of 1-grams hit = 2009  (100.00%)
13 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article775.text
Will force inclusive back-off from OOVs.
Perplexity = 576.97, Entropy = 9.17 bits
Computation based on 498 words.
Number of 1-grams hit = 498  (100.00%)
7 OOVs (1.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article776.text
Will force inclusive back-off from OOVs.
Perplexity = 515.96, Entropy = 9.01 bits
Computation based on 1575 words.
Number of 1-grams hit = 1575  (100.00%)
9 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article777.text
Will force inclusive back-off from OOVs.
Perplexity = 498.66, Entropy = 8.96 bits
Computation based on 9319 words.
Number of 1-grams hit = 9319  (100.00%)
59 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article778.text
Will force inclusive back-off from OOVs.
Perplexity = 670.63, Entropy = 9.39 bits
Computation based on 503 words.
Number of 1-grams hit = 503  (100.00%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article779.text
Will force inclusive back-off from OOVs.
Perplexity = 608.97, Entropy = 9.25 bits
Computation based on 932 words.
Number of 1-grams hit = 932  (100.00%)
2 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article780.text
Will force inclusive back-off from OOVs.
Perplexity = 563.75, Entropy = 9.14 bits
Computation based on 401 words.
Number of 1-grams hit = 401  (100.00%)
1 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article781.text
Will force inclusive back-off from OOVs.
Perplexity = 768.80, Entropy = 9.59 bits
Computation based on 732 words.
Number of 1-grams hit = 732  (100.00%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article782.text
Will force inclusive back-off from OOVs.
Perplexity = 681.49, Entropy = 9.41 bits
Computation based on 1362 words.
Number of 1-grams hit = 1362  (100.00%)
1 OOVs (0.07%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article783.text
Will force inclusive back-off from OOVs.
Perplexity = 663.87, Entropy = 9.37 bits
Computation based on 501 words.
Number of 1-grams hit = 501  (100.00%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article784.text
Will force inclusive back-off from OOVs.
Perplexity = 646.06, Entropy = 9.34 bits
Computation based on 1721 words.
Number of 1-grams hit = 1721  (100.00%)
8 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article785.text
Will force inclusive back-off from OOVs.
Perplexity = 576.96, Entropy = 9.17 bits
Computation based on 2613 words.
Number of 1-grams hit = 2613  (100.00%)
30 OOVs (1.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article786.text
Will force inclusive back-off from OOVs.
Perplexity = 661.74, Entropy = 9.37 bits
Computation based on 353 words.
Number of 1-grams hit = 353  (100.00%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article787.text
Will force inclusive back-off from OOVs.
Perplexity = 1036.40, Entropy = 10.02 bits
Computation based on 472 words.
Number of 1-grams hit = 472  (100.00%)
6 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article788.text
Will force inclusive back-off from OOVs.
Perplexity = 427.07, Entropy = 8.74 bits
Computation based on 1453 words.
Number of 1-grams hit = 1453  (100.00%)
3 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article789.text
Will force inclusive back-off from OOVs.
Perplexity = 694.08, Entropy = 9.44 bits
Computation based on 11445 words.
Number of 1-grams hit = 11445  (100.00%)
47 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article790.text
Will force inclusive back-off from OOVs.
Perplexity = 704.80, Entropy = 9.46 bits
Computation based on 935 words.
Number of 1-grams hit = 935  (100.00%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article791.text
Will force inclusive back-off from OOVs.
Perplexity = 789.63, Entropy = 9.63 bits
Computation based on 696 words.
Number of 1-grams hit = 696  (100.00%)
2 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article792.text
Will force inclusive back-off from OOVs.
Perplexity = 711.60, Entropy = 9.47 bits
Computation based on 2871 words.
Number of 1-grams hit = 2871  (100.00%)
13 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article793.text
Will force inclusive back-off from OOVs.
Perplexity = 568.72, Entropy = 9.15 bits
Computation based on 180 words.
Number of 1-grams hit = 180  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article794.text
Will force inclusive back-off from OOVs.
Perplexity = 1083.42, Entropy = 10.08 bits
Computation based on 500 words.
Number of 1-grams hit = 500  (100.00%)
4 OOVs (0.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article795.text
Will force inclusive back-off from OOVs.
Perplexity = 1027.17, Entropy = 10.00 bits
Computation based on 426 words.
Number of 1-grams hit = 426  (100.00%)
2 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article796.text
Will force inclusive back-off from OOVs.
Perplexity = 703.05, Entropy = 9.46 bits
Computation based on 1490 words.
Number of 1-grams hit = 1490  (100.00%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article797.text
Will force inclusive back-off from OOVs.
Perplexity = 541.65, Entropy = 9.08 bits
Computation based on 375 words.
Number of 1-grams hit = 375  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article798.text
Will force inclusive back-off from OOVs.
Perplexity = 754.41, Entropy = 9.56 bits
Computation based on 458 words.
Number of 1-grams hit = 458  (100.00%)
2 OOVs (0.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article799.text
Will force inclusive back-off from OOVs.
Perplexity = 831.43, Entropy = 9.70 bits
Computation based on 741 words.
Number of 1-grams hit = 741  (100.00%)
10 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article800.text
Will force inclusive back-off from OOVs.
Perplexity = 705.51, Entropy = 9.46 bits
Computation based on 431 words.
Number of 1-grams hit = 431  (100.00%)
1 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article801.text
Will force inclusive back-off from OOVs.
Perplexity = 450.31, Entropy = 8.81 bits
Computation based on 3087 words.
Number of 1-grams hit = 3087  (100.00%)
28 OOVs (0.90%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article802.text
Will force inclusive back-off from OOVs.
Perplexity = 1046.66, Entropy = 10.03 bits
Computation based on 2082 words.
Number of 1-grams hit = 2082  (100.00%)
28 OOVs (1.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article803.text
Will force inclusive back-off from OOVs.
Perplexity = 577.39, Entropy = 9.17 bits
Computation based on 4196 words.
Number of 1-grams hit = 4196  (100.00%)
30 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article804.text
Will force inclusive back-off from OOVs.
Perplexity = 805.62, Entropy = 9.65 bits
Computation based on 1431 words.
Number of 1-grams hit = 1431  (100.00%)
37 OOVs (2.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article805.text
Will force inclusive back-off from OOVs.
Perplexity = 775.05, Entropy = 9.60 bits
Computation based on 919 words.
Number of 1-grams hit = 919  (100.00%)
5 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article806.text
Will force inclusive back-off from OOVs.
Perplexity = 668.12, Entropy = 9.38 bits
Computation based on 3287 words.
Number of 1-grams hit = 3287  (100.00%)
16 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article807.text
Will force inclusive back-off from OOVs.
Perplexity = 757.02, Entropy = 9.56 bits
Computation based on 708 words.
Number of 1-grams hit = 708  (100.00%)
2 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article808.text
Will force inclusive back-off from OOVs.
Perplexity = 786.28, Entropy = 9.62 bits
Computation based on 1682 words.
Number of 1-grams hit = 1682  (100.00%)
12 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article809.text
Will force inclusive back-off from OOVs.
Perplexity = 572.31, Entropy = 9.16 bits
Computation based on 2017 words.
Number of 1-grams hit = 2017  (100.00%)
9 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article810.text
Will force inclusive back-off from OOVs.
Perplexity = 790.33, Entropy = 9.63 bits
Computation based on 869 words.
Number of 1-grams hit = 869  (100.00%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article811.text
Will force inclusive back-off from OOVs.
Perplexity = 692.21, Entropy = 9.44 bits
Computation based on 4497 words.
Number of 1-grams hit = 4497  (100.00%)
14 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article812.text
Will force inclusive back-off from OOVs.
Perplexity = 574.08, Entropy = 9.17 bits
Computation based on 3353 words.
Number of 1-grams hit = 3353  (100.00%)
16 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article813.text
Will force inclusive back-off from OOVs.
Perplexity = 864.13, Entropy = 9.76 bits
Computation based on 1357 words.
Number of 1-grams hit = 1357  (100.00%)
23 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article814.text
Will force inclusive back-off from OOVs.
Perplexity = 800.81, Entropy = 9.65 bits
Computation based on 851 words.
Number of 1-grams hit = 851  (100.00%)
1 OOVs (0.12%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article815.text
Will force inclusive back-off from OOVs.
Perplexity = 714.33, Entropy = 9.48 bits
Computation based on 1264 words.
Number of 1-grams hit = 1264  (100.00%)
10 OOVs (0.78%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article816.text
Will force inclusive back-off from OOVs.
Perplexity = 573.50, Entropy = 9.16 bits
Computation based on 1331 words.
Number of 1-grams hit = 1331  (100.00%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article817.text
Will force inclusive back-off from OOVs.
Perplexity = 747.64, Entropy = 9.55 bits
Computation based on 533 words.
Number of 1-grams hit = 533  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article818.text
Will force inclusive back-off from OOVs.
Perplexity = 723.87, Entropy = 9.50 bits
Computation based on 532 words.
Number of 1-grams hit = 532  (100.00%)
21 OOVs (3.80%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article819.text
Will force inclusive back-off from OOVs.
Perplexity = 759.85, Entropy = 9.57 bits
Computation based on 1803 words.
Number of 1-grams hit = 1803  (100.00%)
6 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article820.text
Will force inclusive back-off from OOVs.
Perplexity = 898.39, Entropy = 9.81 bits
Computation based on 816 words.
Number of 1-grams hit = 816  (100.00%)
4 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article821.text
Will force inclusive back-off from OOVs.
Perplexity = 549.34, Entropy = 9.10 bits
Computation based on 1100 words.
Number of 1-grams hit = 1100  (100.00%)
4 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article822.text
Will force inclusive back-off from OOVs.
Perplexity = 734.21, Entropy = 9.52 bits
Computation based on 2420 words.
Number of 1-grams hit = 2420  (100.00%)
4 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article823.text
Will force inclusive back-off from OOVs.
Perplexity = 783.65, Entropy = 9.61 bits
Computation based on 1307 words.
Number of 1-grams hit = 1307  (100.00%)
3 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article824.text
Will force inclusive back-off from OOVs.
Perplexity = 1019.48, Entropy = 9.99 bits
Computation based on 486 words.
Number of 1-grams hit = 486  (100.00%)
16 OOVs (3.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article825.text
Will force inclusive back-off from OOVs.
Perplexity = 688.96, Entropy = 9.43 bits
Computation based on 571 words.
Number of 1-grams hit = 571  (100.00%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article826.text
Will force inclusive back-off from OOVs.
Perplexity = 535.31, Entropy = 9.06 bits
Computation based on 889 words.
Number of 1-grams hit = 889  (100.00%)
4 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article827.text
Will force inclusive back-off from OOVs.
Perplexity = 668.50, Entropy = 9.38 bits
Computation based on 627 words.
Number of 1-grams hit = 627  (100.00%)
21 OOVs (3.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article828.text
Will force inclusive back-off from OOVs.
Perplexity = 529.35, Entropy = 9.05 bits
Computation based on 11850 words.
Number of 1-grams hit = 11850  (100.00%)
147 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article829.text
Will force inclusive back-off from OOVs.
Perplexity = 1306.77, Entropy = 10.35 bits
Computation based on 531 words.
Number of 1-grams hit = 531  (100.00%)
9 OOVs (1.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article830.text
Will force inclusive back-off from OOVs.
Perplexity = 738.77, Entropy = 9.53 bits
Computation based on 2276 words.
Number of 1-grams hit = 2276  (100.00%)
43 OOVs (1.85%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article831.text
Will force inclusive back-off from OOVs.
Perplexity = 794.65, Entropy = 9.63 bits
Computation based on 1025 words.
Number of 1-grams hit = 1025  (100.00%)
5 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article832.text
Will force inclusive back-off from OOVs.
Perplexity = 757.88, Entropy = 9.57 bits
Computation based on 419 words.
Number of 1-grams hit = 419  (100.00%)
3 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article833.text
Will force inclusive back-off from OOVs.
Perplexity = 749.83, Entropy = 9.55 bits
Computation based on 826 words.
Number of 1-grams hit = 826  (100.00%)
3 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article834.text
Will force inclusive back-off from OOVs.
Perplexity = 705.22, Entropy = 9.46 bits
Computation based on 881 words.
Number of 1-grams hit = 881  (100.00%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article835.text
Will force inclusive back-off from OOVs.
Perplexity = 724.59, Entropy = 9.50 bits
Computation based on 460 words.
Number of 1-grams hit = 460  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article836.text
Will force inclusive back-off from OOVs.
Perplexity = 777.66, Entropy = 9.60 bits
Computation based on 483 words.
Number of 1-grams hit = 483  (100.00%)
8 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article837.text
Will force inclusive back-off from OOVs.
Perplexity = 1236.83, Entropy = 10.27 bits
Computation based on 845 words.
Number of 1-grams hit = 845  (100.00%)
7 OOVs (0.82%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article838.text
Will force inclusive back-off from OOVs.
Perplexity = 757.47, Entropy = 9.57 bits
Computation based on 2236 words.
Number of 1-grams hit = 2236  (100.00%)
35 OOVs (1.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article839.text
Will force inclusive back-off from OOVs.
Perplexity = 758.11, Entropy = 9.57 bits
Computation based on 461 words.
Number of 1-grams hit = 461  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article840.text
Will force inclusive back-off from OOVs.
Perplexity = 747.46, Entropy = 9.55 bits
Computation based on 521 words.
Number of 1-grams hit = 521  (100.00%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article841.text
Will force inclusive back-off from OOVs.
Perplexity = 735.61, Entropy = 9.52 bits
Computation based on 916 words.
Number of 1-grams hit = 916  (100.00%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article842.text
Will force inclusive back-off from OOVs.
Perplexity = 775.81, Entropy = 9.60 bits
Computation based on 473 words.
Number of 1-grams hit = 473  (100.00%)
3 OOVs (0.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article843.text
Will force inclusive back-off from OOVs.
Perplexity = 683.31, Entropy = 9.42 bits
Computation based on 1310 words.
Number of 1-grams hit = 1310  (100.00%)
2 OOVs (0.15%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article844.text
Will force inclusive back-off from OOVs.
Perplexity = 705.40, Entropy = 9.46 bits
Computation based on 15758 words.
Number of 1-grams hit = 15758  (100.00%)
53 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article845.text
Will force inclusive back-off from OOVs.
Perplexity = 720.85, Entropy = 9.49 bits
Computation based on 601 words.
Number of 1-grams hit = 601  (100.00%)
2 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article846.text
Will force inclusive back-off from OOVs.
Perplexity = 699.28, Entropy = 9.45 bits
Computation based on 556 words.
Number of 1-grams hit = 556  (100.00%)
8 OOVs (1.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article847.text
Will force inclusive back-off from OOVs.
Perplexity = 551.03, Entropy = 9.11 bits
Computation based on 1613 words.
Number of 1-grams hit = 1613  (100.00%)
8 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article848.text
Will force inclusive back-off from OOVs.
Perplexity = 722.52, Entropy = 9.50 bits
Computation based on 366 words.
Number of 1-grams hit = 366  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article849.text
Will force inclusive back-off from OOVs.
Perplexity = 599.80, Entropy = 9.23 bits
Computation based on 1550 words.
Number of 1-grams hit = 1550  (100.00%)
5 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article850.text
Will force inclusive back-off from OOVs.
Perplexity = 758.32, Entropy = 9.57 bits
Computation based on 443 words.
Number of 1-grams hit = 443  (100.00%)
3 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article851.text
Will force inclusive back-off from OOVs.
Perplexity = 760.52, Entropy = 9.57 bits
Computation based on 558 words.
Number of 1-grams hit = 558  (100.00%)
1 OOVs (0.18%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article852.text
Will force inclusive back-off from OOVs.
Perplexity = 672.72, Entropy = 9.39 bits
Computation based on 622 words.
Number of 1-grams hit = 622  (100.00%)
3 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article853.text
Will force inclusive back-off from OOVs.
Perplexity = 655.02, Entropy = 9.36 bits
Computation based on 1476 words.
Number of 1-grams hit = 1476  (100.00%)
10 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article854.text
Will force inclusive back-off from OOVs.
Perplexity = 656.58, Entropy = 9.36 bits
Computation based on 2651 words.
Number of 1-grams hit = 2651  (100.00%)
18 OOVs (0.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article855.text
Will force inclusive back-off from OOVs.
Perplexity = 552.18, Entropy = 9.11 bits
Computation based on 423 words.
Number of 1-grams hit = 423  (100.00%)
1 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article856.text
Will force inclusive back-off from OOVs.
Perplexity = 897.12, Entropy = 9.81 bits
Computation based on 824 words.
Number of 1-grams hit = 824  (100.00%)
9 OOVs (1.08%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article857.text
Will force inclusive back-off from OOVs.
Perplexity = 1256.02, Entropy = 10.29 bits
Computation based on 354 words.
Number of 1-grams hit = 354  (100.00%)
1 OOVs (0.28%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article858.text
Will force inclusive back-off from OOVs.
Perplexity = 511.62, Entropy = 9.00 bits
Computation based on 237 words.
Number of 1-grams hit = 237  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article859.text
Will force inclusive back-off from OOVs.
Perplexity = 1000.15, Entropy = 9.97 bits
Computation based on 1095 words.
Number of 1-grams hit = 1095  (100.00%)
16 OOVs (1.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article860.text
Will force inclusive back-off from OOVs.
Perplexity = 735.91, Entropy = 9.52 bits
Computation based on 450 words.
Number of 1-grams hit = 450  (100.00%)
2 OOVs (0.44%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article861.text
Will force inclusive back-off from OOVs.
Perplexity = 790.85, Entropy = 9.63 bits
Computation based on 393 words.
Number of 1-grams hit = 393  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article862.text
Will force inclusive back-off from OOVs.
Perplexity = 519.47, Entropy = 9.02 bits
Computation based on 452 words.
Number of 1-grams hit = 452  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article863.text
Will force inclusive back-off from OOVs.
Perplexity = 675.57, Entropy = 9.40 bits
Computation based on 662 words.
Number of 1-grams hit = 662  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article864.text
Will force inclusive back-off from OOVs.
Perplexity = 1246.26, Entropy = 10.28 bits
Computation based on 442 words.
Number of 1-grams hit = 442  (100.00%)
7 OOVs (1.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article865.text
Will force inclusive back-off from OOVs.
Perplexity = 386.73, Entropy = 8.60 bits
Computation based on 6140 words.
Number of 1-grams hit = 6140  (100.00%)
25 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article866.text
Will force inclusive back-off from OOVs.
Perplexity = 1068.76, Entropy = 10.06 bits
Computation based on 615 words.
Number of 1-grams hit = 615  (100.00%)
12 OOVs (1.91%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article867.text
Will force inclusive back-off from OOVs.
Perplexity = 654.73, Entropy = 9.35 bits
Computation based on 341 words.
Number of 1-grams hit = 341  (100.00%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article868.text
Will force inclusive back-off from OOVs.
Perplexity = 655.94, Entropy = 9.36 bits
Computation based on 1652 words.
Number of 1-grams hit = 1652  (100.00%)
5 OOVs (0.30%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article869.text
Will force inclusive back-off from OOVs.
Perplexity = 673.62, Entropy = 9.40 bits
Computation based on 3106 words.
Number of 1-grams hit = 3106  (100.00%)
10 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article870.text
Will force inclusive back-off from OOVs.
Perplexity = 905.15, Entropy = 9.82 bits
Computation based on 768 words.
Number of 1-grams hit = 768  (100.00%)
4 OOVs (0.52%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article871.text
Will force inclusive back-off from OOVs.
Perplexity = 763.79, Entropy = 9.58 bits
Computation based on 412 words.
Number of 1-grams hit = 412  (100.00%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article872.text
Will force inclusive back-off from OOVs.
Perplexity = 827.70, Entropy = 9.69 bits
Computation based on 294 words.
Number of 1-grams hit = 294  (100.00%)
1 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article873.text
Will force inclusive back-off from OOVs.
Perplexity = 757.68, Entropy = 9.57 bits
Computation based on 930 words.
Number of 1-grams hit = 930  (100.00%)
5 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article874.text
Will force inclusive back-off from OOVs.
Perplexity = 720.54, Entropy = 9.49 bits
Computation based on 429 words.
Number of 1-grams hit = 429  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article875.text
Will force inclusive back-off from OOVs.
Perplexity = 921.14, Entropy = 9.85 bits
Computation based on 786 words.
Number of 1-grams hit = 786  (100.00%)
7 OOVs (0.88%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article876.text
Will force inclusive back-off from OOVs.
Perplexity = 716.78, Entropy = 9.49 bits
Computation based on 9198 words.
Number of 1-grams hit = 9198  (100.00%)
33 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article877.text
Will force inclusive back-off from OOVs.
Perplexity = 542.87, Entropy = 9.08 bits
Computation based on 458 words.
Number of 1-grams hit = 458  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article878.text
Will force inclusive back-off from OOVs.
Perplexity = 655.93, Entropy = 9.36 bits
Computation based on 333 words.
Number of 1-grams hit = 333  (100.00%)
5 OOVs (1.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article879.text
Will force inclusive back-off from OOVs.
Perplexity = 634.31, Entropy = 9.31 bits
Computation based on 539 words.
Number of 1-grams hit = 539  (100.00%)
2 OOVs (0.37%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article880.text
Will force inclusive back-off from OOVs.
Perplexity = 542.93, Entropy = 9.08 bits
Computation based on 459 words.
Number of 1-grams hit = 459  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article881.text
Will force inclusive back-off from OOVs.
Perplexity = 826.37, Entropy = 9.69 bits
Computation based on 345 words.
Number of 1-grams hit = 345  (100.00%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article882.text
Will force inclusive back-off from OOVs.
Perplexity = 611.94, Entropy = 9.26 bits
Computation based on 591 words.
Number of 1-grams hit = 591  (100.00%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article883.text
Will force inclusive back-off from OOVs.
Perplexity = 607.56, Entropy = 9.25 bits
Computation based on 591 words.
Number of 1-grams hit = 591  (100.00%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article884.text
Will force inclusive back-off from OOVs.
Perplexity = 857.24, Entropy = 9.74 bits
Computation based on 727 words.
Number of 1-grams hit = 727  (100.00%)
2 OOVs (0.27%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article885.text
Will force inclusive back-off from OOVs.
Perplexity = 479.74, Entropy = 8.91 bits
Computation based on 482 words.
Number of 1-grams hit = 482  (100.00%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article886.text
Will force inclusive back-off from OOVs.
Perplexity = 705.59, Entropy = 9.46 bits
Computation based on 473 words.
Number of 1-grams hit = 473  (100.00%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article887.text
Will force inclusive back-off from OOVs.
Perplexity = 847.33, Entropy = 9.73 bits
Computation based on 481 words.
Number of 1-grams hit = 481  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article888.text
Will force inclusive back-off from OOVs.
Perplexity = 790.55, Entropy = 9.63 bits
Computation based on 1207 words.
Number of 1-grams hit = 1207  (100.00%)
2 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article889.text
Will force inclusive back-off from OOVs.
Perplexity = 731.10, Entropy = 9.51 bits
Computation based on 4303 words.
Number of 1-grams hit = 4303  (100.00%)
10 OOVs (0.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article890.text
Will force inclusive back-off from OOVs.
Perplexity = 1004.99, Entropy = 9.97 bits
Computation based on 1038 words.
Number of 1-grams hit = 1038  (100.00%)
13 OOVs (1.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article891.text
Will force inclusive back-off from OOVs.
Perplexity = 711.75, Entropy = 9.48 bits
Computation based on 804 words.
Number of 1-grams hit = 804  (100.00%)
4 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article892.text
Will force inclusive back-off from OOVs.
Perplexity = 527.13, Entropy = 9.04 bits
Computation based on 3156 words.
Number of 1-grams hit = 3156  (100.00%)
7 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article893.text
Will force inclusive back-off from OOVs.
Perplexity = 737.72, Entropy = 9.53 bits
Computation based on 625 words.
Number of 1-grams hit = 625  (100.00%)
8 OOVs (1.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article894.text
Will force inclusive back-off from OOVs.
Perplexity = 793.30, Entropy = 9.63 bits
Computation based on 743 words.
Number of 1-grams hit = 743  (100.00%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article895.text
Will force inclusive back-off from OOVs.
Perplexity = 615.36, Entropy = 9.27 bits
Computation based on 169 words.
Number of 1-grams hit = 169  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article896.text
Will force inclusive back-off from OOVs.
Perplexity = 713.66, Entropy = 9.48 bits
Computation based on 413 words.
Number of 1-grams hit = 413  (100.00%)
2 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article897.text
Will force inclusive back-off from OOVs.
Perplexity = 679.72, Entropy = 9.41 bits
Computation based on 696 words.
Number of 1-grams hit = 696  (100.00%)
4 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article898.text
Will force inclusive back-off from OOVs.
Perplexity = 694.95, Entropy = 9.44 bits
Computation based on 1592 words.
Number of 1-grams hit = 1592  (100.00%)
10 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article899.text
Will force inclusive back-off from OOVs.
Perplexity = 1113.58, Entropy = 10.12 bits
Computation based on 199 words.
Number of 1-grams hit = 199  (100.00%)
5 OOVs (2.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article900.text
Will force inclusive back-off from OOVs.
Perplexity = 784.95, Entropy = 9.62 bits
Computation based on 1046 words.
Number of 1-grams hit = 1046  (100.00%)
5 OOVs (0.48%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article901.text
Will force inclusive back-off from OOVs.
Perplexity = 869.84, Entropy = 9.76 bits
Computation based on 388 words.
Number of 1-grams hit = 388  (100.00%)
11 OOVs (2.76%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article902.text
Will force inclusive back-off from OOVs.
Perplexity = 615.19, Entropy = 9.26 bits
Computation based on 1995 words.
Number of 1-grams hit = 1995  (100.00%)
10 OOVs (0.50%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article903.text
Will force inclusive back-off from OOVs.
Perplexity = 722.70, Entropy = 9.50 bits
Computation based on 1184 words.
Number of 1-grams hit = 1184  (100.00%)
5 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article904.text
Will force inclusive back-off from OOVs.
Perplexity = 703.83, Entropy = 9.46 bits
Computation based on 623 words.
Number of 1-grams hit = 623  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article905.text
Will force inclusive back-off from OOVs.
Perplexity = 929.35, Entropy = 9.86 bits
Computation based on 354 words.
Number of 1-grams hit = 354  (100.00%)
3 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article906.text
Will force inclusive back-off from OOVs.
Perplexity = 470.65, Entropy = 8.88 bits
Computation based on 429 words.
Number of 1-grams hit = 429  (100.00%)
7 OOVs (1.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article907.text
Will force inclusive back-off from OOVs.
Perplexity = 586.46, Entropy = 9.20 bits
Computation based on 445 words.
Number of 1-grams hit = 445  (100.00%)
1 OOVs (0.22%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article908.text
Will force inclusive back-off from OOVs.
Perplexity = 1080.80, Entropy = 10.08 bits
Computation based on 524 words.
Number of 1-grams hit = 524  (100.00%)
2 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article909.text
Will force inclusive back-off from OOVs.
Perplexity = 835.43, Entropy = 9.71 bits
Computation based on 413 words.
Number of 1-grams hit = 413  (100.00%)
4 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article910.text
Will force inclusive back-off from OOVs.
Perplexity = 628.34, Entropy = 9.30 bits
Computation based on 667 words.
Number of 1-grams hit = 667  (100.00%)
4 OOVs (0.60%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article911.text
Will force inclusive back-off from OOVs.
Perplexity = 705.18, Entropy = 9.46 bits
Computation based on 635 words.
Number of 1-grams hit = 635  (100.00%)
2 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article912.text
Will force inclusive back-off from OOVs.
Perplexity = 603.25, Entropy = 9.24 bits
Computation based on 675 words.
Number of 1-grams hit = 675  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article913.text
Will force inclusive back-off from OOVs.
Perplexity = 639.18, Entropy = 9.32 bits
Computation based on 670 words.
Number of 1-grams hit = 670  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article914.text
Will force inclusive back-off from OOVs.
Perplexity = 988.69, Entropy = 9.95 bits
Computation based on 529 words.
Number of 1-grams hit = 529  (100.00%)
3 OOVs (0.56%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article915.text
Will force inclusive back-off from OOVs.
Perplexity = 701.61, Entropy = 9.45 bits
Computation based on 1206 words.
Number of 1-grams hit = 1206  (100.00%)
5 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article916.text
Will force inclusive back-off from OOVs.
Perplexity = 983.56, Entropy = 9.94 bits
Computation based on 939 words.
Number of 1-grams hit = 939  (100.00%)
1 OOVs (0.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article917.text
Will force inclusive back-off from OOVs.
Perplexity = 681.94, Entropy = 9.41 bits
Computation based on 1076 words.
Number of 1-grams hit = 1076  (100.00%)
6 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article918.text
Will force inclusive back-off from OOVs.
Perplexity = 671.96, Entropy = 9.39 bits
Computation based on 482 words.
Number of 1-grams hit = 482  (100.00%)
3 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article919.text
Will force inclusive back-off from OOVs.
Perplexity = 674.61, Entropy = 9.40 bits
Computation based on 3300 words.
Number of 1-grams hit = 3300  (100.00%)
15 OOVs (0.45%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article920.text
Will force inclusive back-off from OOVs.
Perplexity = 719.65, Entropy = 9.49 bits
Computation based on 915 words.
Number of 1-grams hit = 915  (100.00%)
3 OOVs (0.33%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article921.text
Will force inclusive back-off from OOVs.
Perplexity = 921.03, Entropy = 9.85 bits
Computation based on 795 words.
Number of 1-grams hit = 795  (100.00%)
3 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article922.text
Will force inclusive back-off from OOVs.
Perplexity = 589.03, Entropy = 9.20 bits
Computation based on 474 words.
Number of 1-grams hit = 474  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article923.text
Will force inclusive back-off from OOVs.
Perplexity = 690.12, Entropy = 9.43 bits
Computation based on 519 words.
Number of 1-grams hit = 519  (100.00%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article924.text
Will force inclusive back-off from OOVs.
Perplexity = 563.27, Entropy = 9.14 bits
Computation based on 341 words.
Number of 1-grams hit = 341  (100.00%)
2 OOVs (0.58%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article925.text
Will force inclusive back-off from OOVs.
Perplexity = 582.26, Entropy = 9.19 bits
Computation based on 3813 words.
Number of 1-grams hit = 3813  (100.00%)
21 OOVs (0.55%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article926.text
Will force inclusive back-off from OOVs.
Perplexity = 611.68, Entropy = 9.26 bits
Computation based on 1337 words.
Number of 1-grams hit = 1337  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article927.text
Will force inclusive back-off from OOVs.
Perplexity = 791.17, Entropy = 9.63 bits
Computation based on 403 words.
Number of 1-grams hit = 403  (100.00%)
4 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article928.text
Will force inclusive back-off from OOVs.
Perplexity = 866.77, Entropy = 9.76 bits
Computation based on 483 words.
Number of 1-grams hit = 483  (100.00%)
6 OOVs (1.23%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article929.text
Will force inclusive back-off from OOVs.
Perplexity = 766.87, Entropy = 9.58 bits
Computation based on 502 words.
Number of 1-grams hit = 502  (100.00%)
2 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article930.text
Will force inclusive back-off from OOVs.
Perplexity = 1048.04, Entropy = 10.03 bits
Computation based on 417 words.
Number of 1-grams hit = 417  (100.00%)
17 OOVs (3.92%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article931.text
Will force inclusive back-off from OOVs.
Perplexity = 722.13, Entropy = 9.50 bits
Computation based on 595 words.
Number of 1-grams hit = 595  (100.00%)
2 OOVs (0.34%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article932.text
Will force inclusive back-off from OOVs.
Perplexity = 890.46, Entropy = 9.80 bits
Computation based on 211 words.
Number of 1-grams hit = 211  (100.00%)
1 OOVs (0.47%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article933.text
Will force inclusive back-off from OOVs.
Perplexity = 711.87, Entropy = 9.48 bits
Computation based on 410 words.
Number of 1-grams hit = 410  (100.00%)
2 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article934.text
Will force inclusive back-off from OOVs.
Perplexity = 1005.17, Entropy = 9.97 bits
Computation based on 384 words.
Number of 1-grams hit = 384  (100.00%)
7 OOVs (1.79%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article935.text
Will force inclusive back-off from OOVs.
Perplexity = 721.39, Entropy = 9.49 bits
Computation based on 1153 words.
Number of 1-grams hit = 1153  (100.00%)
3 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article936.text
Will force inclusive back-off from OOVs.
Perplexity = 968.28, Entropy = 9.92 bits
Computation based on 435 words.
Number of 1-grams hit = 435  (100.00%)
6 OOVs (1.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article937.text
Will force inclusive back-off from OOVs.
Perplexity = 1494.47, Entropy = 10.55 bits
Computation based on 228 words.
Number of 1-grams hit = 228  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article938.text
Will force inclusive back-off from OOVs.
Perplexity = 784.65, Entropy = 9.62 bits
Computation based on 1295 words.
Number of 1-grams hit = 1295  (100.00%)
5 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article939.text
Will force inclusive back-off from OOVs.
Perplexity = 834.89, Entropy = 9.71 bits
Computation based on 476 words.
Number of 1-grams hit = 476  (100.00%)
5 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article940.text
Will force inclusive back-off from OOVs.
Perplexity = 676.38, Entropy = 9.40 bits
Computation based on 1898 words.
Number of 1-grams hit = 1898  (100.00%)
3 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article941.text
Will force inclusive back-off from OOVs.
Perplexity = 875.55, Entropy = 9.77 bits
Computation based on 1216 words.
Number of 1-grams hit = 1216  (100.00%)
2 OOVs (0.16%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article942.text
Will force inclusive back-off from OOVs.
Perplexity = 641.43, Entropy = 9.33 bits
Computation based on 524 words.
Number of 1-grams hit = 524  (100.00%)
3 OOVs (0.57%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article943.text
Will force inclusive back-off from OOVs.
Perplexity = 648.79, Entropy = 9.34 bits
Computation based on 1395 words.
Number of 1-grams hit = 1395  (100.00%)
10 OOVs (0.71%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article944.text
Will force inclusive back-off from OOVs.
Perplexity = 604.87, Entropy = 9.24 bits
Computation based on 436 words.
Number of 1-grams hit = 436  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article945.text
Will force inclusive back-off from OOVs.
Perplexity = 734.38, Entropy = 9.52 bits
Computation based on 3522 words.
Number of 1-grams hit = 3522  (100.00%)
22 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article946.text
Will force inclusive back-off from OOVs.
Perplexity = 911.85, Entropy = 9.83 bits
Computation based on 551 words.
Number of 1-grams hit = 551  (100.00%)
15 OOVs (2.65%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article947.text
Will force inclusive back-off from OOVs.
Perplexity = 765.85, Entropy = 9.58 bits
Computation based on 422 words.
Number of 1-grams hit = 422  (100.00%)
7 OOVs (1.63%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article948.text
Will force inclusive back-off from OOVs.
Perplexity = 716.58, Entropy = 9.48 bits
Computation based on 581 words.
Number of 1-grams hit = 581  (100.00%)
1 OOVs (0.17%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article949.text
Will force inclusive back-off from OOVs.
Perplexity = 772.05, Entropy = 9.59 bits
Computation based on 1701 words.
Number of 1-grams hit = 1701  (100.00%)
9 OOVs (0.53%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article950.text
Will force inclusive back-off from OOVs.
Perplexity = 1145.61, Entropy = 10.16 bits
Computation based on 500 words.
Number of 1-grams hit = 500  (100.00%)
7 OOVs (1.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article951.text
Will force inclusive back-off from OOVs.
Perplexity = 599.22, Entropy = 9.23 bits
Computation based on 564 words.
Number of 1-grams hit = 564  (100.00%)
2 OOVs (0.35%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article952.text
Will force inclusive back-off from OOVs.
Perplexity = 825.87, Entropy = 9.69 bits
Computation based on 1283 words.
Number of 1-grams hit = 1283  (100.00%)
4 OOVs (0.31%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article953.text
Will force inclusive back-off from OOVs.
Perplexity = 665.15, Entropy = 9.38 bits
Computation based on 2117 words.
Number of 1-grams hit = 2117  (100.00%)
18 OOVs (0.84%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article954.text
Will force inclusive back-off from OOVs.
Perplexity = 628.94, Entropy = 9.30 bits
Computation based on 730 words.
Number of 1-grams hit = 730  (100.00%)
3 OOVs (0.41%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article955.text
Will force inclusive back-off from OOVs.
Perplexity = 560.26, Entropy = 9.13 bits
Computation based on 1475 words.
Number of 1-grams hit = 1475  (100.00%)
2 OOVs (0.14%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article956.text
Will force inclusive back-off from OOVs.
Perplexity = 1296.50, Entropy = 10.34 bits
Computation based on 1026 words.
Number of 1-grams hit = 1026  (100.00%)
8 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article957.text
Will force inclusive back-off from OOVs.
Perplexity = 1058.29, Entropy = 10.05 bits
Computation based on 302 words.
Number of 1-grams hit = 302  (100.00%)
2 OOVs (0.66%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article958.text
Will force inclusive back-off from OOVs.
Perplexity = 670.17, Entropy = 9.39 bits
Computation based on 940 words.
Number of 1-grams hit = 940  (100.00%)
3 OOVs (0.32%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article959.text
Will force inclusive back-off from OOVs.
Perplexity = 648.60, Entropy = 9.34 bits
Computation based on 201 words.
Number of 1-grams hit = 201  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article960.text
Will force inclusive back-off from OOVs.
Perplexity = 701.30, Entropy = 9.45 bits
Computation based on 605 words.
Number of 1-grams hit = 605  (100.00%)
3 OOVs (0.49%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article961.text
Will force inclusive back-off from OOVs.
Perplexity = 870.76, Entropy = 9.77 bits
Computation based on 475 words.
Number of 1-grams hit = 475  (100.00%)
17 OOVs (3.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article962.text
Will force inclusive back-off from OOVs.
Perplexity = 555.43, Entropy = 9.12 bits
Computation based on 2235 words.
Number of 1-grams hit = 2235  (100.00%)
1 OOVs (0.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article963.text
Will force inclusive back-off from OOVs.
Perplexity = 672.78, Entropy = 9.39 bits
Computation based on 2012 words.
Number of 1-grams hit = 2012  (100.00%)
5 OOVs (0.25%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article964.text
Will force inclusive back-off from OOVs.
Perplexity = 620.65, Entropy = 9.28 bits
Computation based on 1010 words.
Number of 1-grams hit = 1010  (100.00%)
2 OOVs (0.20%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article965.text
Will force inclusive back-off from OOVs.
Perplexity = 822.31, Entropy = 9.68 bits
Computation based on 643 words.
Number of 1-grams hit = 643  (100.00%)
4 OOVs (0.62%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article966.text
Will force inclusive back-off from OOVs.
Perplexity = 748.98, Entropy = 9.55 bits
Computation based on 789 words.
Number of 1-grams hit = 789  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article967.text
Will force inclusive back-off from OOVs.
Perplexity = 596.79, Entropy = 9.22 bits
Computation based on 388 words.
Number of 1-grams hit = 388  (100.00%)
3 OOVs (0.77%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article968.text
Will force inclusive back-off from OOVs.
Perplexity = 585.11, Entropy = 9.19 bits
Computation based on 1057 words.
Number of 1-grams hit = 1057  (100.00%)
1 OOVs (0.09%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article969.text
Will force inclusive back-off from OOVs.
Perplexity = 1065.27, Entropy = 10.06 bits
Computation based on 719 words.
Number of 1-grams hit = 719  (100.00%)
7 OOVs (0.96%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article970.text
Will force inclusive back-off from OOVs.
Perplexity = 669.77, Entropy = 9.39 bits
Computation based on 739 words.
Number of 1-grams hit = 739  (100.00%)
4 OOVs (0.54%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article971.text
Will force inclusive back-off from OOVs.
Perplexity = 592.70, Entropy = 9.21 bits
Computation based on 468 words.
Number of 1-grams hit = 468  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article972.text
Will force inclusive back-off from OOVs.
Perplexity = 670.95, Entropy = 9.39 bits
Computation based on 1855 words.
Number of 1-grams hit = 1855  (100.00%)
7 OOVs (0.38%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article973.text
Will force inclusive back-off from OOVs.
Perplexity = 601.37, Entropy = 9.23 bits
Computation based on 605 words.
Number of 1-grams hit = 605  (100.00%)
6 OOVs (0.98%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article974.text
Will force inclusive back-off from OOVs.
Perplexity = 877.88, Entropy = 9.78 bits
Computation based on 942 words.
Number of 1-grams hit = 942  (100.00%)
9 OOVs (0.95%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article975.text
Will force inclusive back-off from OOVs.
Perplexity = 752.24, Entropy = 9.56 bits
Computation based on 353 words.
Number of 1-grams hit = 353  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article976.text
Will force inclusive back-off from OOVs.
Perplexity = 1137.25, Entropy = 10.15 bits
Computation based on 347 words.
Number of 1-grams hit = 347  (100.00%)
13 OOVs (3.61%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article977.text
Will force inclusive back-off from OOVs.
Perplexity = 644.62, Entropy = 9.33 bits
Computation based on 857 words.
Number of 1-grams hit = 857  (100.00%)
4 OOVs (0.46%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article978.text
Will force inclusive back-off from OOVs.
Perplexity = 822.85, Entropy = 9.68 bits
Computation based on 464 words.
Number of 1-grams hit = 464  (100.00%)
10 OOVs (2.11%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article979.text
Will force inclusive back-off from OOVs.
Perplexity = 603.12, Entropy = 9.24 bits
Computation based on 1554 words.
Number of 1-grams hit = 1554  (100.00%)
4 OOVs (0.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article980.text
Will force inclusive back-off from OOVs.
Perplexity = 797.15, Entropy = 9.64 bits
Computation based on 598 words.
Number of 1-grams hit = 598  (100.00%)
13 OOVs (2.13%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article981.text
Will force inclusive back-off from OOVs.
Perplexity = 977.12, Entropy = 9.93 bits
Computation based on 345 words.
Number of 1-grams hit = 345  (100.00%)
1 OOVs (0.29%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article982.text
Will force inclusive back-off from OOVs.
Perplexity = 905.85, Entropy = 9.82 bits
Computation based on 825 words.
Number of 1-grams hit = 825  (100.00%)
12 OOVs (1.43%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article983.text
Will force inclusive back-off from OOVs.
Perplexity = 755.11, Entropy = 9.56 bits
Computation based on 468 words.
Number of 1-grams hit = 468  (100.00%)
1 OOVs (0.21%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article984.text
Will force inclusive back-off from OOVs.
Perplexity = 774.50, Entropy = 9.60 bits
Computation based on 458 words.
Number of 1-grams hit = 458  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article985.text
Will force inclusive back-off from OOVs.
Perplexity = 797.44, Entropy = 9.64 bits
Computation based on 1233 words.
Number of 1-grams hit = 1233  (100.00%)
13 OOVs (1.04%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article986.text
Will force inclusive back-off from OOVs.
Perplexity = 714.18, Entropy = 9.48 bits
Computation based on 269 words.
Number of 1-grams hit = 269  (100.00%)
2 OOVs (0.74%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article987.text
Will force inclusive back-off from OOVs.
Perplexity = 560.34, Entropy = 9.13 bits
Computation based on 1737 words.
Number of 1-grams hit = 1737  (100.00%)
7 OOVs (0.40%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article988.text
Will force inclusive back-off from OOVs.
Perplexity = 621.40, Entropy = 9.28 bits
Computation based on 1241 words.
Number of 1-grams hit = 1241  (100.00%)
8 OOVs (0.64%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article989.text
Will force inclusive back-off from OOVs.
Perplexity = 569.14, Entropy = 9.15 bits
Computation based on 2270 words.
Number of 1-grams hit = 2270  (100.00%)
9 OOVs (0.39%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article990.text
Will force inclusive back-off from OOVs.
Perplexity = 756.03, Entropy = 9.56 bits
Computation based on 653 words.
Number of 1-grams hit = 653  (100.00%)
7 OOVs (1.06%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article991.text
Will force inclusive back-off from OOVs.
Perplexity = 679.19, Entropy = 9.41 bits
Computation based on 1540 words.
Number of 1-grams hit = 1540  (100.00%)
3 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article992.text
Will force inclusive back-off from OOVs.
Perplexity = 749.88, Entropy = 9.55 bits
Computation based on 510 words.
Number of 1-grams hit = 510  (100.00%)
25 OOVs (4.67%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article993.text
Will force inclusive back-off from OOVs.
Perplexity = 732.57, Entropy = 9.52 bits
Computation based on 2457 words.
Number of 1-grams hit = 2457  (100.00%)
9 OOVs (0.36%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article994.text
Will force inclusive back-off from OOVs.
Perplexity = 621.07, Entropy = 9.28 bits
Computation based on 477 words.
Number of 1-grams hit = 477  (100.00%)
2 OOVs (0.42%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article995.text
Will force inclusive back-off from OOVs.
Perplexity = 768.04, Entropy = 9.59 bits
Computation based on 7311 words.
Number of 1-grams hit = 7311  (100.00%)
246 OOVs (3.26%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article996.text
Will force inclusive back-off from OOVs.
Perplexity = 815.39, Entropy = 9.67 bits
Computation based on 580 words.
Number of 1-grams hit = 580  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article997.text
Will force inclusive back-off from OOVs.
Perplexity = 755.68, Entropy = 9.56 bits
Computation based on 5318 words.
Number of 1-grams hit = 5318  (100.00%)
13 OOVs (0.24%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article998.text
Will force inclusive back-off from OOVs.
Perplexity = 613.14, Entropy = 9.26 bits
Computation based on 523 words.
Number of 1-grams hit = 523  (100.00%)
1 OOVs (0.19%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/articles/article999.text
Will force inclusive back-off from OOVs.
Perplexity = 568.72, Entropy = 9.15 bits
Computation based on 468 words.
Number of 1-grams hit = 468  (100.00%)
4 OOVs (0.85%) and 0 context cues were removed from the calculation.
evallm : 