evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle0.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 1366 words.
Number of 1-grams hit = 1366  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle1.out
Perplexity = 8.23, Entropy = 3.04 bits
Computation based on 1620 words.
Number of 1-grams hit = 1620  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle2.out
Perplexity = 7.84, Entropy = 2.97 bits
Computation based on 591 words.
Number of 1-grams hit = 591  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle3.out
Perplexity = 8.23, Entropy = 3.04 bits
Computation based on 661 words.
Number of 1-grams hit = 661  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle4.out
Perplexity = 6.38, Entropy = 2.67 bits
Computation based on 425 words.
Number of 1-grams hit = 425  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle5.out
Perplexity = 6.78, Entropy = 2.76 bits
Computation based on 957 words.
Number of 1-grams hit = 957  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle6.out
Perplexity = 8.17, Entropy = 3.03 bits
Computation based on 317 words.
Number of 1-grams hit = 317  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle7.out
Perplexity = 9.46, Entropy = 3.24 bits
Computation based on 665 words.
Number of 1-grams hit = 665  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle8.out
Perplexity = 7.88, Entropy = 2.98 bits
Computation based on 528 words.
Number of 1-grams hit = 528  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle9.out
Perplexity = 9.53, Entropy = 3.25 bits
Computation based on 345 words.
Number of 1-grams hit = 345  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle10.out
Perplexity = 7.25, Entropy = 2.86 bits
Computation based on 327 words.
Number of 1-grams hit = 327  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle11.out
Perplexity = 8.92, Entropy = 3.16 bits
Computation based on 325 words.
Number of 1-grams hit = 325  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle12.out
Perplexity = 8.52, Entropy = 3.09 bits
Computation based on 326 words.
Number of 1-grams hit = 326  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle13.out
Perplexity = 7.43, Entropy = 2.89 bits
Computation based on 488 words.
Number of 1-grams hit = 488  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle14.out
Perplexity = 7.90, Entropy = 2.98 bits
Computation based on 523 words.
Number of 1-grams hit = 523  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle15.out
Perplexity = 7.49, Entropy = 2.91 bits
Computation based on 543 words.
Number of 1-grams hit = 543  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle16.out
Perplexity = 7.62, Entropy = 2.93 bits
Computation based on 400 words.
Number of 1-grams hit = 400  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle17.out
Perplexity = 7.54, Entropy = 2.92 bits
Computation based on 722 words.
Number of 1-grams hit = 722  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle18.out
Perplexity = 8.65, Entropy = 3.11 bits
Computation based on 445 words.
Number of 1-grams hit = 445  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle19.out
Perplexity = 8.53, Entropy = 3.09 bits
Computation based on 560 words.
Number of 1-grams hit = 560  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle20.out
Perplexity = 8.05, Entropy = 3.01 bits
Computation based on 386 words.
Number of 1-grams hit = 386  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle21.out
Perplexity = 8.66, Entropy = 3.11 bits
Computation based on 500 words.
Number of 1-grams hit = 500  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle22.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 387 words.
Number of 1-grams hit = 387  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle23.out
Perplexity = 7.60, Entropy = 2.93 bits
Computation based on 378 words.
Number of 1-grams hit = 378  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle24.out
Perplexity = 7.66, Entropy = 2.94 bits
Computation based on 599 words.
Number of 1-grams hit = 599  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle25.out
Perplexity = 9.31, Entropy = 3.22 bits
Computation based on 413 words.
Number of 1-grams hit = 413  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle26.out
Perplexity = 8.29, Entropy = 3.05 bits
Computation based on 1649 words.
Number of 1-grams hit = 1649  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle27.out
Perplexity = 7.52, Entropy = 2.91 bits
Computation based on 424 words.
Number of 1-grams hit = 424  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle28.out
Perplexity = 8.29, Entropy = 3.05 bits
Computation based on 1511 words.
Number of 1-grams hit = 1511  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle29.out
Perplexity = 8.11, Entropy = 3.02 bits
Computation based on 346 words.
Number of 1-grams hit = 346  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle30.out
Perplexity = 8.05, Entropy = 3.01 bits
Computation based on 523 words.
Number of 1-grams hit = 523  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle31.out
Perplexity = 8.69, Entropy = 3.12 bits
Computation based on 481 words.
Number of 1-grams hit = 481  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle32.out
Perplexity = 9.16, Entropy = 3.20 bits
Computation based on 516 words.
Number of 1-grams hit = 516  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle33.out
Perplexity = 8.31, Entropy = 3.05 bits
Computation based on 476 words.
Number of 1-grams hit = 476  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle34.out
Perplexity = 6.59, Entropy = 2.72 bits
Computation based on 326 words.
Number of 1-grams hit = 326  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle35.out
Perplexity = 8.78, Entropy = 3.13 bits
Computation based on 402 words.
Number of 1-grams hit = 402  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle36.out
Perplexity = 9.20, Entropy = 3.20 bits
Computation based on 1088 words.
Number of 1-grams hit = 1088  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle37.out
Perplexity = 7.73, Entropy = 2.95 bits
Computation based on 604 words.
Number of 1-grams hit = 604  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle38.out
Perplexity = 8.28, Entropy = 3.05 bits
Computation based on 444 words.
Number of 1-grams hit = 444  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle39.out
Perplexity = 9.18, Entropy = 3.20 bits
Computation based on 2871 words.
Number of 1-grams hit = 2871  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle40.out
Perplexity = 10.63, Entropy = 3.41 bits
Computation based on 442 words.
Number of 1-grams hit = 442  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle41.out
Perplexity = 7.95, Entropy = 2.99 bits
Computation based on 1060 words.
Number of 1-grams hit = 1060  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle42.out
Perplexity = 8.54, Entropy = 3.09 bits
Computation based on 591 words.
Number of 1-grams hit = 591  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle43.out
Perplexity = 8.20, Entropy = 3.04 bits
Computation based on 490 words.
Number of 1-grams hit = 490  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle44.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 549 words.
Number of 1-grams hit = 549  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle45.out
Perplexity = 7.65, Entropy = 2.94 bits
Computation based on 502 words.
Number of 1-grams hit = 502  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle46.out
Perplexity = 7.78, Entropy = 2.96 bits
Computation based on 436 words.
Number of 1-grams hit = 436  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle47.out
Perplexity = 8.10, Entropy = 3.02 bits
Computation based on 2914 words.
Number of 1-grams hit = 2914  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle48.out
Perplexity = 7.26, Entropy = 2.86 bits
Computation based on 400 words.
Number of 1-grams hit = 400  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle49.out
Perplexity = 8.48, Entropy = 3.08 bits
Computation based on 395 words.
Number of 1-grams hit = 395  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle50.out
Perplexity = 8.38, Entropy = 3.07 bits
Computation based on 563 words.
Number of 1-grams hit = 563  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle51.out
Perplexity = 7.07, Entropy = 2.82 bits
Computation based on 1184 words.
Number of 1-grams hit = 1184  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle52.out
Perplexity = 9.01, Entropy = 3.17 bits
Computation based on 401 words.
Number of 1-grams hit = 401  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle53.out
Perplexity = 8.72, Entropy = 3.12 bits
Computation based on 521 words.
Number of 1-grams hit = 521  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle54.out
Perplexity = 8.53, Entropy = 3.09 bits
Computation based on 1201 words.
Number of 1-grams hit = 1201  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle55.out
Perplexity = 9.07, Entropy = 3.18 bits
Computation based on 578 words.
Number of 1-grams hit = 578  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle56.out
Perplexity = 9.63, Entropy = 3.27 bits
Computation based on 988 words.
Number of 1-grams hit = 988  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle57.out
Perplexity = 9.06, Entropy = 3.18 bits
Computation based on 1691 words.
Number of 1-grams hit = 1691  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle58.out
Perplexity = 7.71, Entropy = 2.95 bits
Computation based on 416 words.
Number of 1-grams hit = 416  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle59.out
Perplexity = 10.43, Entropy = 3.38 bits
Computation based on 231 words.
Number of 1-grams hit = 231  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle60.out
Perplexity = 6.51, Entropy = 2.70 bits
Computation based on 354 words.
Number of 1-grams hit = 354  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle61.out
Perplexity = 8.47, Entropy = 3.08 bits
Computation based on 530 words.
Number of 1-grams hit = 530  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle62.out
Perplexity = 8.97, Entropy = 3.17 bits
Computation based on 1398 words.
Number of 1-grams hit = 1398  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle63.out
Perplexity = 8.80, Entropy = 3.14 bits
Computation based on 1066 words.
Number of 1-grams hit = 1066  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle64.out
Perplexity = 8.78, Entropy = 3.13 bits
Computation based on 4550 words.
Number of 1-grams hit = 4550  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle65.out
Perplexity = 9.81, Entropy = 3.29 bits
Computation based on 697 words.
Number of 1-grams hit = 697  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle66.out
Perplexity = 9.18, Entropy = 3.20 bits
Computation based on 514 words.
Number of 1-grams hit = 514  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle67.out
Perplexity = 7.47, Entropy = 2.90 bits
Computation based on 484 words.
Number of 1-grams hit = 484  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle68.out
Perplexity = 7.87, Entropy = 2.98 bits
Computation based on 1206 words.
Number of 1-grams hit = 1206  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle69.out
Perplexity = 6.53, Entropy = 2.71 bits
Computation based on 407 words.
Number of 1-grams hit = 407  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle70.out
Perplexity = 9.48, Entropy = 3.24 bits
Computation based on 1071 words.
Number of 1-grams hit = 1071  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle71.out
Perplexity = 7.03, Entropy = 2.81 bits
Computation based on 591 words.
Number of 1-grams hit = 591  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle72.out
Perplexity = 8.83, Entropy = 3.14 bits
Computation based on 1423 words.
Number of 1-grams hit = 1423  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle73.out
Perplexity = 9.35, Entropy = 3.23 bits
Computation based on 566 words.
Number of 1-grams hit = 566  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle74.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 901 words.
Number of 1-grams hit = 901  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle75.out
Perplexity = 8.46, Entropy = 3.08 bits
Computation based on 1288 words.
Number of 1-grams hit = 1288  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle76.out
Perplexity = 8.07, Entropy = 3.01 bits
Computation based on 1515 words.
Number of 1-grams hit = 1515  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle77.out
Perplexity = 8.82, Entropy = 3.14 bits
Computation based on 322 words.
Number of 1-grams hit = 322  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle78.out
Perplexity = 8.71, Entropy = 3.12 bits
Computation based on 1223 words.
Number of 1-grams hit = 1223  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle79.out
Perplexity = 7.85, Entropy = 2.97 bits
Computation based on 1163 words.
Number of 1-grams hit = 1163  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle80.out
Perplexity = 9.32, Entropy = 3.22 bits
Computation based on 412 words.
Number of 1-grams hit = 412  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle81.out
Perplexity = 8.71, Entropy = 3.12 bits
Computation based on 4418 words.
Number of 1-grams hit = 4418  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle82.out
Perplexity = 7.90, Entropy = 2.98 bits
Computation based on 510 words.
Number of 1-grams hit = 510  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle83.out
Perplexity = 9.12, Entropy = 3.19 bits
Computation based on 476 words.
Number of 1-grams hit = 476  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle84.out
Perplexity = 8.29, Entropy = 3.05 bits
Computation based on 431 words.
Number of 1-grams hit = 431  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle85.out
Perplexity = 8.25, Entropy = 3.04 bits
Computation based on 1317 words.
Number of 1-grams hit = 1317  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle86.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 309 words.
Number of 1-grams hit = 309  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle87.out
Perplexity = 7.38, Entropy = 2.88 bits
Computation based on 676 words.
Number of 1-grams hit = 676  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle88.out
Perplexity = 7.14, Entropy = 2.84 bits
Computation based on 1583 words.
Number of 1-grams hit = 1583  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle89.out
Perplexity = 9.45, Entropy = 3.24 bits
Computation based on 414 words.
Number of 1-grams hit = 414  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle90.out
Perplexity = 8.83, Entropy = 3.14 bits
Computation based on 555 words.
Number of 1-grams hit = 555  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle91.out
Perplexity = 7.39, Entropy = 2.89 bits
Computation based on 371 words.
Number of 1-grams hit = 371  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle92.out
Perplexity = 8.31, Entropy = 3.05 bits
Computation based on 1134 words.
Number of 1-grams hit = 1134  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle93.out
Perplexity = 7.86, Entropy = 2.97 bits
Computation based on 745 words.
Number of 1-grams hit = 745  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle94.out
Perplexity = 8.03, Entropy = 3.00 bits
Computation based on 1395 words.
Number of 1-grams hit = 1395  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle95.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 546 words.
Number of 1-grams hit = 546  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle96.out
Perplexity = 8.03, Entropy = 3.00 bits
Computation based on 439 words.
Number of 1-grams hit = 439  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle97.out
Perplexity = 8.52, Entropy = 3.09 bits
Computation based on 601 words.
Number of 1-grams hit = 601  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle98.out
Perplexity = 7.27, Entropy = 2.86 bits
Computation based on 391 words.
Number of 1-grams hit = 391  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle99.out
Perplexity = 8.97, Entropy = 3.16 bits
Computation based on 497 words.
Number of 1-grams hit = 497  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle100.out
Perplexity = 8.56, Entropy = 3.10 bits
Computation based on 513 words.
Number of 1-grams hit = 513  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle101.out
Perplexity = 8.97, Entropy = 3.16 bits
Computation based on 554 words.
Number of 1-grams hit = 554  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle102.out
Perplexity = 8.91, Entropy = 3.15 bits
Computation based on 448 words.
Number of 1-grams hit = 448  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle103.out
Perplexity = 7.92, Entropy = 2.99 bits
Computation based on 536 words.
Number of 1-grams hit = 536  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle104.out
Perplexity = 9.97, Entropy = 3.32 bits
Computation based on 6983 words.
Number of 1-grams hit = 6983  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle105.out
Perplexity = 10.54, Entropy = 3.40 bits
Computation based on 485 words.
Number of 1-grams hit = 485  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle106.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 875 words.
Number of 1-grams hit = 875  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle107.out
Perplexity = 7.92, Entropy = 2.98 bits
Computation based on 529 words.
Number of 1-grams hit = 529  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle108.out
Perplexity = 7.41, Entropy = 2.89 bits
Computation based on 568 words.
Number of 1-grams hit = 568  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle109.out
Perplexity = 8.51, Entropy = 3.09 bits
Computation based on 1223 words.
Number of 1-grams hit = 1223  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle110.out
Perplexity = 7.61, Entropy = 2.93 bits
Computation based on 981 words.
Number of 1-grams hit = 981  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle111.out
Perplexity = 8.05, Entropy = 3.01 bits
Computation based on 1359 words.
Number of 1-grams hit = 1359  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle112.out
Perplexity = 8.64, Entropy = 3.11 bits
Computation based on 355 words.
Number of 1-grams hit = 355  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle113.out
Perplexity = 9.25, Entropy = 3.21 bits
Computation based on 6382 words.
Number of 1-grams hit = 6382  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle114.out
Perplexity = 7.95, Entropy = 2.99 bits
Computation based on 552 words.
Number of 1-grams hit = 552  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle115.out
Perplexity = 8.62, Entropy = 3.11 bits
Computation based on 698 words.
Number of 1-grams hit = 698  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle116.out
Perplexity = 9.32, Entropy = 3.22 bits
Computation based on 1647 words.
Number of 1-grams hit = 1647  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle117.out
Perplexity = 8.04, Entropy = 3.01 bits
Computation based on 557 words.
Number of 1-grams hit = 557  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle118.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 667 words.
Number of 1-grams hit = 667  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle119.out
Perplexity = 7.85, Entropy = 2.97 bits
Computation based on 424 words.
Number of 1-grams hit = 424  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle120.out
Perplexity = 8.85, Entropy = 3.15 bits
Computation based on 608 words.
Number of 1-grams hit = 608  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle121.out
Perplexity = 8.37, Entropy = 3.06 bits
Computation based on 7778 words.
Number of 1-grams hit = 7778  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle122.out
Perplexity = 10.50, Entropy = 3.39 bits
Computation based on 353 words.
Number of 1-grams hit = 353  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle123.out
Perplexity = 7.17, Entropy = 2.84 bits
Computation based on 553 words.
Number of 1-grams hit = 553  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle124.out
Perplexity = 8.83, Entropy = 3.14 bits
Computation based on 873 words.
Number of 1-grams hit = 873  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle125.out
Perplexity = 7.35, Entropy = 2.88 bits
Computation based on 525 words.
Number of 1-grams hit = 525  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle126.out
Perplexity = 9.20, Entropy = 3.20 bits
Computation based on 587 words.
Number of 1-grams hit = 587  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle127.out
Perplexity = 8.77, Entropy = 3.13 bits
Computation based on 1001 words.
Number of 1-grams hit = 1001  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle128.out
Perplexity = 9.00, Entropy = 3.17 bits
Computation based on 462 words.
Number of 1-grams hit = 462  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle129.out
Perplexity = 9.79, Entropy = 3.29 bits
Computation based on 511 words.
Number of 1-grams hit = 511  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle130.out
Perplexity = 7.42, Entropy = 2.89 bits
Computation based on 1659 words.
Number of 1-grams hit = 1659  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle131.out
Perplexity = 7.98, Entropy = 3.00 bits
Computation based on 7398 words.
Number of 1-grams hit = 7398  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle132.out
Perplexity = 8.26, Entropy = 3.05 bits
Computation based on 1222 words.
Number of 1-grams hit = 1222  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle133.out
Perplexity = 9.21, Entropy = 3.20 bits
Computation based on 262 words.
Number of 1-grams hit = 262  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle134.out
Perplexity = 7.47, Entropy = 2.90 bits
Computation based on 539 words.
Number of 1-grams hit = 539  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle135.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 521 words.
Number of 1-grams hit = 521  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle136.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 567 words.
Number of 1-grams hit = 567  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle137.out
Perplexity = 10.11, Entropy = 3.34 bits
Computation based on 361 words.
Number of 1-grams hit = 361  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle138.out
Perplexity = 9.17, Entropy = 3.20 bits
Computation based on 342 words.
Number of 1-grams hit = 342  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle139.out
Perplexity = 7.29, Entropy = 2.87 bits
Computation based on 298 words.
Number of 1-grams hit = 298  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle140.out
Perplexity = 8.15, Entropy = 3.03 bits
Computation based on 400 words.
Number of 1-grams hit = 400  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle141.out
Perplexity = 7.49, Entropy = 2.90 bits
Computation based on 467 words.
Number of 1-grams hit = 467  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle142.out
Perplexity = 9.13, Entropy = 3.19 bits
Computation based on 758 words.
Number of 1-grams hit = 758  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle143.out
Perplexity = 7.89, Entropy = 2.98 bits
Computation based on 412 words.
Number of 1-grams hit = 412  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle144.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 541 words.
Number of 1-grams hit = 541  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle145.out
Perplexity = 7.61, Entropy = 2.93 bits
Computation based on 2107 words.
Number of 1-grams hit = 2107  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle146.out
Perplexity = 9.71, Entropy = 3.28 bits
Computation based on 262 words.
Number of 1-grams hit = 262  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle147.out
Perplexity = 7.63, Entropy = 2.93 bits
Computation based on 714 words.
Number of 1-grams hit = 714  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle148.out
Perplexity = 8.30, Entropy = 3.05 bits
Computation based on 1886 words.
Number of 1-grams hit = 1886  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle149.out
Perplexity = 7.64, Entropy = 2.93 bits
Computation based on 612 words.
Number of 1-grams hit = 612  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle150.out
Perplexity = 7.53, Entropy = 2.91 bits
Computation based on 686 words.
Number of 1-grams hit = 686  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle151.out
Perplexity = 7.98, Entropy = 3.00 bits
Computation based on 188 words.
Number of 1-grams hit = 188  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle152.out
Perplexity = 7.61, Entropy = 2.93 bits
Computation based on 1984 words.
Number of 1-grams hit = 1984  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle153.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 629 words.
Number of 1-grams hit = 629  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle154.out
Perplexity = 8.81, Entropy = 3.14 bits
Computation based on 1160 words.
Number of 1-grams hit = 1160  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle155.out
Perplexity = 7.83, Entropy = 2.97 bits
Computation based on 695 words.
Number of 1-grams hit = 695  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle156.out
Perplexity = 10.53, Entropy = 3.40 bits
Computation based on 1549 words.
Number of 1-grams hit = 1549  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle157.out
Perplexity = 9.05, Entropy = 3.18 bits
Computation based on 431 words.
Number of 1-grams hit = 431  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle158.out
Perplexity = 8.72, Entropy = 3.12 bits
Computation based on 623 words.
Number of 1-grams hit = 623  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle159.out
Perplexity = 9.72, Entropy = 3.28 bits
Computation based on 1362 words.
Number of 1-grams hit = 1362  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle160.out
Perplexity = 8.54, Entropy = 3.09 bits
Computation based on 584 words.
Number of 1-grams hit = 584  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle161.out
Perplexity = 9.39, Entropy = 3.23 bits
Computation based on 211 words.
Number of 1-grams hit = 211  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle162.out
Perplexity = 7.52, Entropy = 2.91 bits
Computation based on 1245 words.
Number of 1-grams hit = 1245  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle163.out
Perplexity = 6.07, Entropy = 2.60 bits
Computation based on 752 words.
Number of 1-grams hit = 752  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle164.out
Perplexity = 9.25, Entropy = 3.21 bits
Computation based on 1948 words.
Number of 1-grams hit = 1948  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle165.out
Perplexity = 6.94, Entropy = 2.79 bits
Computation based on 593 words.
Number of 1-grams hit = 593  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle166.out
Perplexity = 7.42, Entropy = 2.89 bits
Computation based on 555 words.
Number of 1-grams hit = 555  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle167.out
Perplexity = 8.25, Entropy = 3.04 bits
Computation based on 1260 words.
Number of 1-grams hit = 1260  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle168.out
Perplexity = 8.33, Entropy = 3.06 bits
Computation based on 571 words.
Number of 1-grams hit = 571  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle169.out
Perplexity = 8.17, Entropy = 3.03 bits
Computation based on 551 words.
Number of 1-grams hit = 551  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle170.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 1235 words.
Number of 1-grams hit = 1235  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle171.out
Perplexity = 8.06, Entropy = 3.01 bits
Computation based on 656 words.
Number of 1-grams hit = 656  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle172.out
Perplexity = 7.43, Entropy = 2.89 bits
Computation based on 734 words.
Number of 1-grams hit = 734  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle173.out
Perplexity = 7.78, Entropy = 2.96 bits
Computation based on 1110 words.
Number of 1-grams hit = 1110  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle174.out
Perplexity = 8.51, Entropy = 3.09 bits
Computation based on 493 words.
Number of 1-grams hit = 493  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle175.out
Perplexity = 7.40, Entropy = 2.89 bits
Computation based on 486 words.
Number of 1-grams hit = 486  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle176.out
Perplexity = 7.27, Entropy = 2.86 bits
Computation based on 800 words.
Number of 1-grams hit = 800  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle177.out
Perplexity = 9.20, Entropy = 3.20 bits
Computation based on 557 words.
Number of 1-grams hit = 557  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle178.out
Perplexity = 9.72, Entropy = 3.28 bits
Computation based on 183 words.
Number of 1-grams hit = 183  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle179.out
Perplexity = 8.80, Entropy = 3.14 bits
Computation based on 491 words.
Number of 1-grams hit = 491  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle180.out
Perplexity = 9.50, Entropy = 3.25 bits
Computation based on 473 words.
Number of 1-grams hit = 473  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle181.out
Perplexity = 9.69, Entropy = 3.28 bits
Computation based on 955 words.
Number of 1-grams hit = 955  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle182.out
Perplexity = 9.30, Entropy = 3.22 bits
Computation based on 407 words.
Number of 1-grams hit = 407  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle183.out
Perplexity = 8.31, Entropy = 3.05 bits
Computation based on 504 words.
Number of 1-grams hit = 504  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle184.out
Perplexity = 7.96, Entropy = 2.99 bits
Computation based on 5958 words.
Number of 1-grams hit = 5958  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle185.out
Perplexity = 8.12, Entropy = 3.02 bits
Computation based on 1070 words.
Number of 1-grams hit = 1070  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle186.out
Perplexity = 7.11, Entropy = 2.83 bits
Computation based on 529 words.
Number of 1-grams hit = 529  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle187.out
Perplexity = 8.69, Entropy = 3.12 bits
Computation based on 3137 words.
Number of 1-grams hit = 3137  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle188.out
Perplexity = 7.40, Entropy = 2.89 bits
Computation based on 729 words.
Number of 1-grams hit = 729  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle189.out
Perplexity = 7.77, Entropy = 2.96 bits
Computation based on 580 words.
Number of 1-grams hit = 580  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle190.out
Perplexity = 7.45, Entropy = 2.90 bits
Computation based on 421 words.
Number of 1-grams hit = 421  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle191.out
Perplexity = 8.51, Entropy = 3.09 bits
Computation based on 5916 words.
Number of 1-grams hit = 5916  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle192.out
Perplexity = 5.42, Entropy = 2.44 bits
Computation based on 818 words.
Number of 1-grams hit = 818  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle193.out
Perplexity = 7.83, Entropy = 2.97 bits
Computation based on 170 words.
Number of 1-grams hit = 170  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle194.out
Perplexity = 8.26, Entropy = 3.05 bits
Computation based on 661 words.
Number of 1-grams hit = 661  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle195.out
Perplexity = 8.00, Entropy = 3.00 bits
Computation based on 864 words.
Number of 1-grams hit = 864  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle196.out
Perplexity = 7.11, Entropy = 2.83 bits
Computation based on 730 words.
Number of 1-grams hit = 730  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle197.out
Perplexity = 8.19, Entropy = 3.03 bits
Computation based on 251 words.
Number of 1-grams hit = 251  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle198.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 768 words.
Number of 1-grams hit = 768  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle199.out
Perplexity = 8.89, Entropy = 3.15 bits
Computation based on 4967 words.
Number of 1-grams hit = 4967  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle200.out
Perplexity = 8.03, Entropy = 3.01 bits
Computation based on 702 words.
Number of 1-grams hit = 702  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle201.out
Perplexity = 7.21, Entropy = 2.85 bits
Computation based on 507 words.
Number of 1-grams hit = 507  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle202.out
Perplexity = 8.92, Entropy = 3.16 bits
Computation based on 1262 words.
Number of 1-grams hit = 1262  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle203.out
Perplexity = 8.60, Entropy = 3.11 bits
Computation based on 5293 words.
Number of 1-grams hit = 5293  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle204.out
Perplexity = 8.60, Entropy = 3.10 bits
Computation based on 5882 words.
Number of 1-grams hit = 5882  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle205.out
Perplexity = 8.71, Entropy = 3.12 bits
Computation based on 603 words.
Number of 1-grams hit = 603  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle206.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 636 words.
Number of 1-grams hit = 636  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle207.out
Perplexity = 8.28, Entropy = 3.05 bits
Computation based on 779 words.
Number of 1-grams hit = 779  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle208.out
Perplexity = 8.52, Entropy = 3.09 bits
Computation based on 951 words.
Number of 1-grams hit = 951  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle209.out
Perplexity = 8.30, Entropy = 3.05 bits
Computation based on 283 words.
Number of 1-grams hit = 283  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle210.out
Perplexity = 7.01, Entropy = 2.81 bits
Computation based on 886 words.
Number of 1-grams hit = 886  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle211.out
Perplexity = 8.76, Entropy = 3.13 bits
Computation based on 504 words.
Number of 1-grams hit = 504  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle212.out
Perplexity = 8.93, Entropy = 3.16 bits
Computation based on 449 words.
Number of 1-grams hit = 449  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle213.out
Perplexity = 8.36, Entropy = 3.06 bits
Computation based on 695 words.
Number of 1-grams hit = 695  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle214.out
Perplexity = 8.76, Entropy = 3.13 bits
Computation based on 509 words.
Number of 1-grams hit = 509  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle215.out
Perplexity = 8.25, Entropy = 3.04 bits
Computation based on 742 words.
Number of 1-grams hit = 742  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle216.out
Perplexity = 7.06, Entropy = 2.82 bits
Computation based on 525 words.
Number of 1-grams hit = 525  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle217.out
Perplexity = 7.68, Entropy = 2.94 bits
Computation based on 323 words.
Number of 1-grams hit = 323  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle218.out
Perplexity = 8.62, Entropy = 3.11 bits
Computation based on 792 words.
Number of 1-grams hit = 792  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle219.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 439 words.
Number of 1-grams hit = 439  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle220.out
Perplexity = 8.96, Entropy = 3.16 bits
Computation based on 4706 words.
Number of 1-grams hit = 4706  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle221.out
Perplexity = 7.12, Entropy = 2.83 bits
Computation based on 1255 words.
Number of 1-grams hit = 1255  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle222.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 4839 words.
Number of 1-grams hit = 4839  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle223.out
Perplexity = 8.27, Entropy = 3.05 bits
Computation based on 895 words.
Number of 1-grams hit = 895  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle224.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 406 words.
Number of 1-grams hit = 406  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle225.out
Perplexity = 7.18, Entropy = 2.84 bits
Computation based on 777 words.
Number of 1-grams hit = 777  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle226.out
Perplexity = 7.71, Entropy = 2.95 bits
Computation based on 1314 words.
Number of 1-grams hit = 1314  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle227.out
Perplexity = 8.36, Entropy = 3.06 bits
Computation based on 1017 words.
Number of 1-grams hit = 1017  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle228.out
Perplexity = 7.54, Entropy = 2.92 bits
Computation based on 851 words.
Number of 1-grams hit = 851  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle229.out
Perplexity = 8.33, Entropy = 3.06 bits
Computation based on 518 words.
Number of 1-grams hit = 518  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle230.out
Perplexity = 8.68, Entropy = 3.12 bits
Computation based on 337 words.
Number of 1-grams hit = 337  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle231.out
Perplexity = 7.91, Entropy = 2.98 bits
Computation based on 850 words.
Number of 1-grams hit = 850  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle232.out
Perplexity = 8.95, Entropy = 3.16 bits
Computation based on 391 words.
Number of 1-grams hit = 391  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle233.out
Perplexity = 9.74, Entropy = 3.28 bits
Computation based on 734 words.
Number of 1-grams hit = 734  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle234.out
Perplexity = 7.47, Entropy = 2.90 bits
Computation based on 1446 words.
Number of 1-grams hit = 1446  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle235.out
Perplexity = 8.90, Entropy = 3.15 bits
Computation based on 379 words.
Number of 1-grams hit = 379  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle236.out
Perplexity = 7.21, Entropy = 2.85 bits
Computation based on 801 words.
Number of 1-grams hit = 801  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle237.out
Perplexity = 7.28, Entropy = 2.86 bits
Computation based on 423 words.
Number of 1-grams hit = 423  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle238.out
Perplexity = 7.47, Entropy = 2.90 bits
Computation based on 339 words.
Number of 1-grams hit = 339  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle239.out
Perplexity = 8.76, Entropy = 3.13 bits
Computation based on 530 words.
Number of 1-grams hit = 530  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle240.out
Perplexity = 9.00, Entropy = 3.17 bits
Computation based on 350 words.
Number of 1-grams hit = 350  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle241.out
Perplexity = 8.10, Entropy = 3.02 bits
Computation based on 334 words.
Number of 1-grams hit = 334  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle242.out
Perplexity = 7.38, Entropy = 2.88 bits
Computation based on 319 words.
Number of 1-grams hit = 319  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle243.out
Perplexity = 7.38, Entropy = 2.88 bits
Computation based on 813 words.
Number of 1-grams hit = 813  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle244.out
Perplexity = 7.02, Entropy = 2.81 bits
Computation based on 457 words.
Number of 1-grams hit = 457  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle245.out
Perplexity = 8.11, Entropy = 3.02 bits
Computation based on 5533 words.
Number of 1-grams hit = 5533  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle246.out
Perplexity = 7.69, Entropy = 2.94 bits
Computation based on 304 words.
Number of 1-grams hit = 304  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle247.out
Perplexity = 8.65, Entropy = 3.11 bits
Computation based on 492 words.
Number of 1-grams hit = 492  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle248.out
Perplexity = 9.72, Entropy = 3.28 bits
Computation based on 388 words.
Number of 1-grams hit = 388  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle249.out
Perplexity = 8.39, Entropy = 3.07 bits
Computation based on 928 words.
Number of 1-grams hit = 928  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle250.out
Perplexity = 8.37, Entropy = 3.06 bits
Computation based on 5273 words.
Number of 1-grams hit = 5273  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle251.out
Perplexity = 8.16, Entropy = 3.03 bits
Computation based on 5439 words.
Number of 1-grams hit = 5439  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle252.out
Perplexity = 8.49, Entropy = 3.09 bits
Computation based on 579 words.
Number of 1-grams hit = 579  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle253.out
Perplexity = 7.98, Entropy = 3.00 bits
Computation based on 5348 words.
Number of 1-grams hit = 5348  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle254.out
Perplexity = 7.17, Entropy = 2.84 bits
Computation based on 836 words.
Number of 1-grams hit = 836  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle255.out
Perplexity = 7.40, Entropy = 2.89 bits
Computation based on 526 words.
Number of 1-grams hit = 526  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle256.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 837 words.
Number of 1-grams hit = 837  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle257.out
Perplexity = 9.89, Entropy = 3.31 bits
Computation based on 6431 words.
Number of 1-grams hit = 6431  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle258.out
Perplexity = 9.03, Entropy = 3.18 bits
Computation based on 444 words.
Number of 1-grams hit = 444  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle259.out
Perplexity = 7.96, Entropy = 2.99 bits
Computation based on 764 words.
Number of 1-grams hit = 764  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle260.out
Perplexity = 8.04, Entropy = 3.01 bits
Computation based on 7634 words.
Number of 1-grams hit = 7634  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle261.out
Perplexity = 9.09, Entropy = 3.18 bits
Computation based on 717 words.
Number of 1-grams hit = 717  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle262.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 721 words.
Number of 1-grams hit = 721  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle263.out
Perplexity = 9.21, Entropy = 3.20 bits
Computation based on 411 words.
Number of 1-grams hit = 411  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle264.out
Perplexity = 7.54, Entropy = 2.92 bits
Computation based on 857 words.
Number of 1-grams hit = 857  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle265.out
Perplexity = 7.20, Entropy = 2.85 bits
Computation based on 602 words.
Number of 1-grams hit = 602  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle266.out
Perplexity = 7.62, Entropy = 2.93 bits
Computation based on 468 words.
Number of 1-grams hit = 468  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle267.out
Perplexity = 7.87, Entropy = 2.98 bits
Computation based on 6787 words.
Number of 1-grams hit = 6787  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle268.out
Perplexity = 8.18, Entropy = 3.03 bits
Computation based on 6349 words.
Number of 1-grams hit = 6349  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle269.out
Perplexity = 7.49, Entropy = 2.90 bits
Computation based on 457 words.
Number of 1-grams hit = 457  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle270.out
Perplexity = 7.86, Entropy = 2.97 bits
Computation based on 404 words.
Number of 1-grams hit = 404  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle271.out
Perplexity = 8.60, Entropy = 3.10 bits
Computation based on 7909 words.
Number of 1-grams hit = 7909  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle272.out
Perplexity = 9.66, Entropy = 3.27 bits
Computation based on 4304 words.
Number of 1-grams hit = 4304  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle273.out
Perplexity = 8.40, Entropy = 3.07 bits
Computation based on 683 words.
Number of 1-grams hit = 683  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle274.out
Perplexity = 8.75, Entropy = 3.13 bits
Computation based on 704 words.
Number of 1-grams hit = 704  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle275.out
Perplexity = 7.75, Entropy = 2.95 bits
Computation based on 2091 words.
Number of 1-grams hit = 2091  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle276.out
Perplexity = 8.03, Entropy = 3.00 bits
Computation based on 506 words.
Number of 1-grams hit = 506  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle277.out
Perplexity = 7.45, Entropy = 2.90 bits
Computation based on 835 words.
Number of 1-grams hit = 835  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle278.out
Perplexity = 7.91, Entropy = 2.98 bits
Computation based on 856 words.
Number of 1-grams hit = 856  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle279.out
Perplexity = 8.57, Entropy = 3.10 bits
Computation based on 446 words.
Number of 1-grams hit = 446  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle280.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 1844 words.
Number of 1-grams hit = 1844  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle281.out
Perplexity = 7.63, Entropy = 2.93 bits
Computation based on 610 words.
Number of 1-grams hit = 610  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle282.out
Perplexity = 9.22, Entropy = 3.21 bits
Computation based on 779 words.
Number of 1-grams hit = 779  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle283.out
Perplexity = 8.51, Entropy = 3.09 bits
Computation based on 1294 words.
Number of 1-grams hit = 1294  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle284.out
Perplexity = 10.17, Entropy = 3.35 bits
Computation based on 1757 words.
Number of 1-grams hit = 1757  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle285.out
Perplexity = 7.07, Entropy = 2.82 bits
Computation based on 1159 words.
Number of 1-grams hit = 1159  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle286.out
Perplexity = 11.92, Entropy = 3.58 bits
Computation based on 485 words.
Number of 1-grams hit = 485  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle287.out
Perplexity = 7.45, Entropy = 2.90 bits
Computation based on 1778 words.
Number of 1-grams hit = 1778  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle288.out
Perplexity = 8.18, Entropy = 3.03 bits
Computation based on 635 words.
Number of 1-grams hit = 635  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle289.out
Perplexity = 8.34, Entropy = 3.06 bits
Computation based on 774 words.
Number of 1-grams hit = 774  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle290.out
Perplexity = 7.30, Entropy = 2.87 bits
Computation based on 1396 words.
Number of 1-grams hit = 1396  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle291.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 552 words.
Number of 1-grams hit = 552  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle292.out
Perplexity = 8.64, Entropy = 3.11 bits
Computation based on 575 words.
Number of 1-grams hit = 575  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle293.out
Perplexity = 8.22, Entropy = 3.04 bits
Computation based on 593 words.
Number of 1-grams hit = 593  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle294.out
Perplexity = 7.36, Entropy = 2.88 bits
Computation based on 2240 words.
Number of 1-grams hit = 2240  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle295.out
Perplexity = 7.28, Entropy = 2.86 bits
Computation based on 644 words.
Number of 1-grams hit = 644  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle296.out
Perplexity = 8.73, Entropy = 3.13 bits
Computation based on 1670 words.
Number of 1-grams hit = 1670  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle297.out
Perplexity = 10.16, Entropy = 3.34 bits
Computation based on 550 words.
Number of 1-grams hit = 550  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle298.out
Perplexity = 8.21, Entropy = 3.04 bits
Computation based on 3737 words.
Number of 1-grams hit = 3737  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle299.out
Perplexity = 9.33, Entropy = 3.22 bits
Computation based on 3001 words.
Number of 1-grams hit = 3001  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle300.out
Perplexity = 7.62, Entropy = 2.93 bits
Computation based on 2733 words.
Number of 1-grams hit = 2733  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle301.out
Perplexity = 7.52, Entropy = 2.91 bits
Computation based on 2305 words.
Number of 1-grams hit = 2305  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle302.out
Perplexity = 9.12, Entropy = 3.19 bits
Computation based on 698 words.
Number of 1-grams hit = 698  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle303.out
Perplexity = 7.69, Entropy = 2.94 bits
Computation based on 941 words.
Number of 1-grams hit = 941  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle304.out
Perplexity = 8.06, Entropy = 3.01 bits
Computation based on 446 words.
Number of 1-grams hit = 446  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle305.out
Perplexity = 7.92, Entropy = 2.99 bits
Computation based on 331 words.
Number of 1-grams hit = 331  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle306.out
Perplexity = 6.10, Entropy = 2.61 bits
Computation based on 635 words.
Number of 1-grams hit = 635  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle307.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 557 words.
Number of 1-grams hit = 557  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle308.out
Perplexity = 7.64, Entropy = 2.93 bits
Computation based on 417 words.
Number of 1-grams hit = 417  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle309.out
Perplexity = 8.68, Entropy = 3.12 bits
Computation based on 265 words.
Number of 1-grams hit = 265  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle310.out
Perplexity = 8.58, Entropy = 3.10 bits
Computation based on 522 words.
Number of 1-grams hit = 522  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle311.out
Perplexity = 7.49, Entropy = 2.91 bits
Computation based on 497 words.
Number of 1-grams hit = 497  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle312.out
Perplexity = 6.83, Entropy = 2.77 bits
Computation based on 651 words.
Number of 1-grams hit = 651  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle313.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 512 words.
Number of 1-grams hit = 512  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle314.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 3615 words.
Number of 1-grams hit = 3615  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle315.out
Perplexity = 7.57, Entropy = 2.92 bits
Computation based on 1186 words.
Number of 1-grams hit = 1186  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle316.out
Perplexity = 8.17, Entropy = 3.03 bits
Computation based on 363 words.
Number of 1-grams hit = 363  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle317.out
Perplexity = 6.84, Entropy = 2.77 bits
Computation based on 675 words.
Number of 1-grams hit = 675  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle318.out
Perplexity = 7.78, Entropy = 2.96 bits
Computation based on 3867 words.
Number of 1-grams hit = 3867  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle319.out
Perplexity = 7.47, Entropy = 2.90 bits
Computation based on 641 words.
Number of 1-grams hit = 641  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle320.out
Perplexity = 7.39, Entropy = 2.89 bits
Computation based on 405 words.
Number of 1-grams hit = 405  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle321.out
Perplexity = 8.52, Entropy = 3.09 bits
Computation based on 1172 words.
Number of 1-grams hit = 1172  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle322.out
Perplexity = 9.07, Entropy = 3.18 bits
Computation based on 1851 words.
Number of 1-grams hit = 1851  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle323.out
Perplexity = 6.72, Entropy = 2.75 bits
Computation based on 877 words.
Number of 1-grams hit = 877  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle324.out
Perplexity = 8.77, Entropy = 3.13 bits
Computation based on 1808 words.
Number of 1-grams hit = 1808  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle325.out
Perplexity = 8.64, Entropy = 3.11 bits
Computation based on 1207 words.
Number of 1-grams hit = 1207  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle326.out
Perplexity = 7.94, Entropy = 2.99 bits
Computation based on 655 words.
Number of 1-grams hit = 655  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle327.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 1223 words.
Number of 1-grams hit = 1223  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle328.out
Perplexity = 8.35, Entropy = 3.06 bits
Computation based on 277 words.
Number of 1-grams hit = 277  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle329.out
Perplexity = 8.92, Entropy = 3.16 bits
Computation based on 445 words.
Number of 1-grams hit = 445  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle330.out
Perplexity = 8.39, Entropy = 3.07 bits
Computation based on 1141 words.
Number of 1-grams hit = 1141  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle331.out
Perplexity = 8.47, Entropy = 3.08 bits
Computation based on 2113 words.
Number of 1-grams hit = 2113  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle332.out
Perplexity = 7.86, Entropy = 2.97 bits
Computation based on 626 words.
Number of 1-grams hit = 626  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle333.out
Perplexity = 8.57, Entropy = 3.10 bits
Computation based on 1432 words.
Number of 1-grams hit = 1432  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle334.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 1057 words.
Number of 1-grams hit = 1057  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle335.out
Perplexity = 8.01, Entropy = 3.00 bits
Computation based on 1850 words.
Number of 1-grams hit = 1850  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle336.out
Perplexity = 8.28, Entropy = 3.05 bits
Computation based on 1239 words.
Number of 1-grams hit = 1239  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle337.out
Perplexity = 9.08, Entropy = 3.18 bits
Computation based on 438 words.
Number of 1-grams hit = 438  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle338.out
Perplexity = 6.81, Entropy = 2.77 bits
Computation based on 3804 words.
Number of 1-grams hit = 3804  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle339.out
Perplexity = 8.92, Entropy = 3.16 bits
Computation based on 408 words.
Number of 1-grams hit = 408  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle340.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 1243 words.
Number of 1-grams hit = 1243  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle341.out
Perplexity = 8.01, Entropy = 3.00 bits
Computation based on 1498 words.
Number of 1-grams hit = 1498  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle342.out
Perplexity = 9.42, Entropy = 3.24 bits
Computation based on 1604 words.
Number of 1-grams hit = 1604  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle343.out
Perplexity = 8.52, Entropy = 3.09 bits
Computation based on 522 words.
Number of 1-grams hit = 522  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle344.out
Perplexity = 6.84, Entropy = 2.77 bits
Computation based on 620 words.
Number of 1-grams hit = 620  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle345.out
Perplexity = 7.29, Entropy = 2.87 bits
Computation based on 3421 words.
Number of 1-grams hit = 3421  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle346.out
Perplexity = 8.23, Entropy = 3.04 bits
Computation based on 1725 words.
Number of 1-grams hit = 1725  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle347.out
Perplexity = 6.50, Entropy = 2.70 bits
Computation based on 690 words.
Number of 1-grams hit = 690  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle348.out
Perplexity = 8.44, Entropy = 3.08 bits
Computation based on 2288 words.
Number of 1-grams hit = 2288  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle349.out
Perplexity = 8.52, Entropy = 3.09 bits
Computation based on 2145 words.
Number of 1-grams hit = 2145  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle350.out
Perplexity = 9.23, Entropy = 3.21 bits
Computation based on 400 words.
Number of 1-grams hit = 400  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle351.out
Perplexity = 6.91, Entropy = 2.79 bits
Computation based on 464 words.
Number of 1-grams hit = 464  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle352.out
Perplexity = 9.59, Entropy = 3.26 bits
Computation based on 488 words.
Number of 1-grams hit = 488  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle353.out
Perplexity = 8.46, Entropy = 3.08 bits
Computation based on 471 words.
Number of 1-grams hit = 471  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle354.out
Perplexity = 7.27, Entropy = 2.86 bits
Computation based on 551 words.
Number of 1-grams hit = 551  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle355.out
Perplexity = 9.48, Entropy = 3.24 bits
Computation based on 448 words.
Number of 1-grams hit = 448  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle356.out
Perplexity = 7.09, Entropy = 2.83 bits
Computation based on 743 words.
Number of 1-grams hit = 743  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle357.out
Perplexity = 8.43, Entropy = 3.08 bits
Computation based on 834 words.
Number of 1-grams hit = 834  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle358.out
Perplexity = 9.23, Entropy = 3.21 bits
Computation based on 1334 words.
Number of 1-grams hit = 1334  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle359.out
Perplexity = 7.90, Entropy = 2.98 bits
Computation based on 1380 words.
Number of 1-grams hit = 1380  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle360.out
Perplexity = 7.53, Entropy = 2.91 bits
Computation based on 760 words.
Number of 1-grams hit = 760  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle361.out
Perplexity = 7.43, Entropy = 2.89 bits
Computation based on 1178 words.
Number of 1-grams hit = 1178  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle362.out
Perplexity = 9.10, Entropy = 3.19 bits
Computation based on 1543 words.
Number of 1-grams hit = 1543  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle363.out
Perplexity = 8.05, Entropy = 3.01 bits
Computation based on 1251 words.
Number of 1-grams hit = 1251  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle364.out
Perplexity = 7.62, Entropy = 2.93 bits
Computation based on 345 words.
Number of 1-grams hit = 345  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle365.out
Perplexity = 6.95, Entropy = 2.80 bits
Computation based on 430 words.
Number of 1-grams hit = 430  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle366.out
Perplexity = 7.65, Entropy = 2.94 bits
Computation based on 390 words.
Number of 1-grams hit = 390  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle367.out
Perplexity = 8.66, Entropy = 3.11 bits
Computation based on 1013 words.
Number of 1-grams hit = 1013  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle368.out
Perplexity = 9.62, Entropy = 3.27 bits
Computation based on 345 words.
Number of 1-grams hit = 345  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle369.out
Perplexity = 8.38, Entropy = 3.07 bits
Computation based on 352 words.
Number of 1-grams hit = 352  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle370.out
Perplexity = 7.71, Entropy = 2.95 bits
Computation based on 1781 words.
Number of 1-grams hit = 1781  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle371.out
Perplexity = 8.93, Entropy = 3.16 bits
Computation based on 280 words.
Number of 1-grams hit = 280  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle372.out
Perplexity = 8.66, Entropy = 3.11 bits
Computation based on 456 words.
Number of 1-grams hit = 456  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle373.out
Perplexity = 7.24, Entropy = 2.86 bits
Computation based on 546 words.
Number of 1-grams hit = 546  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle374.out
Perplexity = 8.77, Entropy = 3.13 bits
Computation based on 1668 words.
Number of 1-grams hit = 1668  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle375.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 1770 words.
Number of 1-grams hit = 1770  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle376.out
Perplexity = 8.27, Entropy = 3.05 bits
Computation based on 621 words.
Number of 1-grams hit = 621  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle377.out
Perplexity = 8.12, Entropy = 3.02 bits
Computation based on 809 words.
Number of 1-grams hit = 809  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle378.out
Perplexity = 7.33, Entropy = 2.87 bits
Computation based on 671 words.
Number of 1-grams hit = 671  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle379.out
Perplexity = 8.07, Entropy = 3.01 bits
Computation based on 2784 words.
Number of 1-grams hit = 2784  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle380.out
Perplexity = 9.30, Entropy = 3.22 bits
Computation based on 377 words.
Number of 1-grams hit = 377  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle381.out
Perplexity = 9.15, Entropy = 3.19 bits
Computation based on 700 words.
Number of 1-grams hit = 700  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle382.out
Perplexity = 8.34, Entropy = 3.06 bits
Computation based on 1415 words.
Number of 1-grams hit = 1415  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle383.out
Perplexity = 7.65, Entropy = 2.93 bits
Computation based on 1120 words.
Number of 1-grams hit = 1120  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle384.out
Perplexity = 8.58, Entropy = 3.10 bits
Computation based on 1098 words.
Number of 1-grams hit = 1098  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle385.out
Perplexity = 8.53, Entropy = 3.09 bits
Computation based on 2797 words.
Number of 1-grams hit = 2797  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle386.out
Perplexity = 8.07, Entropy = 3.01 bits
Computation based on 442 words.
Number of 1-grams hit = 442  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle387.out
Perplexity = 10.40, Entropy = 3.38 bits
Computation based on 1354 words.
Number of 1-grams hit = 1354  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle388.out
Perplexity = 7.89, Entropy = 2.98 bits
Computation based on 4692 words.
Number of 1-grams hit = 4692  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle389.out
Perplexity = 8.51, Entropy = 3.09 bits
Computation based on 1197 words.
Number of 1-grams hit = 1197  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle390.out
Perplexity = 8.11, Entropy = 3.02 bits
Computation based on 1281 words.
Number of 1-grams hit = 1281  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle391.out
Perplexity = 9.12, Entropy = 3.19 bits
Computation based on 3885 words.
Number of 1-grams hit = 3885  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle392.out
Perplexity = 7.85, Entropy = 2.97 bits
Computation based on 256 words.
Number of 1-grams hit = 256  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle393.out
Perplexity = 6.77, Entropy = 2.76 bits
Computation based on 508 words.
Number of 1-grams hit = 508  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle394.out
Perplexity = 7.84, Entropy = 2.97 bits
Computation based on 417 words.
Number of 1-grams hit = 417  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle395.out
Perplexity = 8.20, Entropy = 3.04 bits
Computation based on 332 words.
Number of 1-grams hit = 332  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle396.out
Perplexity = 9.14, Entropy = 3.19 bits
Computation based on 3378 words.
Number of 1-grams hit = 3378  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle397.out
Perplexity = 8.22, Entropy = 3.04 bits
Computation based on 4002 words.
Number of 1-grams hit = 4002  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle398.out
Perplexity = 7.34, Entropy = 2.88 bits
Computation based on 878 words.
Number of 1-grams hit = 878  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle399.out
Perplexity = 8.01, Entropy = 3.00 bits
Computation based on 3969 words.
Number of 1-grams hit = 3969  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle400.out
Perplexity = 7.66, Entropy = 2.94 bits
Computation based on 634 words.
Number of 1-grams hit = 634  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle401.out
Perplexity = 7.87, Entropy = 2.98 bits
Computation based on 721 words.
Number of 1-grams hit = 721  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle402.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 3978 words.
Number of 1-grams hit = 3978  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle403.out
Perplexity = 6.29, Entropy = 2.65 bits
Computation based on 668 words.
Number of 1-grams hit = 668  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle404.out
Perplexity = 7.37, Entropy = 2.88 bits
Computation based on 228 words.
Number of 1-grams hit = 228  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle405.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 172 words.
Number of 1-grams hit = 172  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle406.out
Perplexity = 7.96, Entropy = 2.99 bits
Computation based on 1046 words.
Number of 1-grams hit = 1046  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle407.out
Perplexity = 8.29, Entropy = 3.05 bits
Computation based on 755 words.
Number of 1-grams hit = 755  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle408.out
Perplexity = 9.75, Entropy = 3.29 bits
Computation based on 469 words.
Number of 1-grams hit = 469  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle409.out
Perplexity = 7.42, Entropy = 2.89 bits
Computation based on 595 words.
Number of 1-grams hit = 595  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle410.out
Perplexity = 9.63, Entropy = 3.27 bits
Computation based on 261 words.
Number of 1-grams hit = 261  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle411.out
Perplexity = 7.90, Entropy = 2.98 bits
Computation based on 465 words.
Number of 1-grams hit = 465  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle412.out
Perplexity = 8.62, Entropy = 3.11 bits
Computation based on 4945 words.
Number of 1-grams hit = 4945  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle413.out
Perplexity = 8.08, Entropy = 3.01 bits
Computation based on 4175 words.
Number of 1-grams hit = 4175  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle414.out
Perplexity = 6.85, Entropy = 2.78 bits
Computation based on 430 words.
Number of 1-grams hit = 430  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle415.out
Perplexity = 7.29, Entropy = 2.87 bits
Computation based on 509 words.
Number of 1-grams hit = 509  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle416.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 635 words.
Number of 1-grams hit = 635  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle417.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 947 words.
Number of 1-grams hit = 947  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle418.out
Perplexity = 7.42, Entropy = 2.89 bits
Computation based on 863 words.
Number of 1-grams hit = 863  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle419.out
Perplexity = 6.91, Entropy = 2.79 bits
Computation based on 581 words.
Number of 1-grams hit = 581  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle420.out
Perplexity = 9.31, Entropy = 3.22 bits
Computation based on 667 words.
Number of 1-grams hit = 667  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle421.out
Perplexity = 8.62, Entropy = 3.11 bits
Computation based on 893 words.
Number of 1-grams hit = 893  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle422.out
Perplexity = 7.62, Entropy = 2.93 bits
Computation based on 602 words.
Number of 1-grams hit = 602  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle423.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 652 words.
Number of 1-grams hit = 652  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle424.out
Perplexity = 9.03, Entropy = 3.18 bits
Computation based on 582 words.
Number of 1-grams hit = 582  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle425.out
Perplexity = 9.01, Entropy = 3.17 bits
Computation based on 385 words.
Number of 1-grams hit = 385  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle426.out
Perplexity = 8.67, Entropy = 3.12 bits
Computation based on 537 words.
Number of 1-grams hit = 537  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle427.out
Perplexity = 8.82, Entropy = 3.14 bits
Computation based on 844 words.
Number of 1-grams hit = 844  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle428.out
Perplexity = 6.54, Entropy = 2.71 bits
Computation based on 680 words.
Number of 1-grams hit = 680  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle429.out
Perplexity = 7.57, Entropy = 2.92 bits
Computation based on 812 words.
Number of 1-grams hit = 812  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle430.out
Perplexity = 7.58, Entropy = 2.92 bits
Computation based on 699 words.
Number of 1-grams hit = 699  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle431.out
Perplexity = 8.26, Entropy = 3.05 bits
Computation based on 612 words.
Number of 1-grams hit = 612  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle432.out
Perplexity = 7.68, Entropy = 2.94 bits
Computation based on 388 words.
Number of 1-grams hit = 388  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle433.out
Perplexity = 9.13, Entropy = 3.19 bits
Computation based on 1055 words.
Number of 1-grams hit = 1055  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle434.out
Perplexity = 10.33, Entropy = 3.37 bits
Computation based on 414 words.
Number of 1-grams hit = 414  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle435.out
Perplexity = 8.63, Entropy = 3.11 bits
Computation based on 797 words.
Number of 1-grams hit = 797  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle436.out
Perplexity = 8.49, Entropy = 3.09 bits
Computation based on 585 words.
Number of 1-grams hit = 585  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle437.out
Perplexity = 6.93, Entropy = 2.79 bits
Computation based on 1077 words.
Number of 1-grams hit = 1077  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle438.out
Perplexity = 9.04, Entropy = 3.18 bits
Computation based on 318 words.
Number of 1-grams hit = 318  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle439.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 569 words.
Number of 1-grams hit = 569  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle440.out
Perplexity = 8.95, Entropy = 3.16 bits
Computation based on 753 words.
Number of 1-grams hit = 753  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle441.out
Perplexity = 7.32, Entropy = 2.87 bits
Computation based on 439 words.
Number of 1-grams hit = 439  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle442.out
Perplexity = 9.09, Entropy = 3.18 bits
Computation based on 11076 words.
Number of 1-grams hit = 11076  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle443.out
Perplexity = 8.20, Entropy = 3.04 bits
Computation based on 9530 words.
Number of 1-grams hit = 9530  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle444.out
Perplexity = 7.47, Entropy = 2.90 bits
Computation based on 703 words.
Number of 1-grams hit = 703  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle445.out
Perplexity = 9.09, Entropy = 3.18 bits
Computation based on 556 words.
Number of 1-grams hit = 556  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle446.out
Perplexity = 8.21, Entropy = 3.04 bits
Computation based on 969 words.
Number of 1-grams hit = 969  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle447.out
Perplexity = 8.47, Entropy = 3.08 bits
Computation based on 787 words.
Number of 1-grams hit = 787  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle448.out
Perplexity = 7.04, Entropy = 2.82 bits
Computation based on 346 words.
Number of 1-grams hit = 346  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle449.out
Perplexity = 5.80, Entropy = 2.54 bits
Computation based on 520 words.
Number of 1-grams hit = 520  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle450.out
Perplexity = 7.51, Entropy = 2.91 bits
Computation based on 386 words.
Number of 1-grams hit = 386  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle451.out
Perplexity = 7.90, Entropy = 2.98 bits
Computation based on 615 words.
Number of 1-grams hit = 615  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle452.out
Perplexity = 7.22, Entropy = 2.85 bits
Computation based on 262 words.
Number of 1-grams hit = 262  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle453.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 679 words.
Number of 1-grams hit = 679  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle454.out
Perplexity = 8.99, Entropy = 3.17 bits
Computation based on 598 words.
Number of 1-grams hit = 598  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle455.out
Perplexity = 6.54, Entropy = 2.71 bits
Computation based on 405 words.
Number of 1-grams hit = 405  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle456.out
Perplexity = 6.85, Entropy = 2.78 bits
Computation based on 719 words.
Number of 1-grams hit = 719  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle457.out
Perplexity = 8.05, Entropy = 3.01 bits
Computation based on 366 words.
Number of 1-grams hit = 366  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle458.out
Perplexity = 8.15, Entropy = 3.03 bits
Computation based on 299 words.
Number of 1-grams hit = 299  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle459.out
Perplexity = 7.56, Entropy = 2.92 bits
Computation based on 752 words.
Number of 1-grams hit = 752  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle460.out
Perplexity = 9.39, Entropy = 3.23 bits
Computation based on 354 words.
Number of 1-grams hit = 354  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle461.out
Perplexity = 6.98, Entropy = 2.80 bits
Computation based on 440 words.
Number of 1-grams hit = 440  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle462.out
Perplexity = 8.62, Entropy = 3.11 bits
Computation based on 1537 words.
Number of 1-grams hit = 1537  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle463.out
Perplexity = 7.97, Entropy = 3.00 bits
Computation based on 610 words.
Number of 1-grams hit = 610  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle464.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 466 words.
Number of 1-grams hit = 466  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle465.out
Perplexity = 8.17, Entropy = 3.03 bits
Computation based on 1935 words.
Number of 1-grams hit = 1935  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle466.out
Perplexity = 9.75, Entropy = 3.29 bits
Computation based on 225 words.
Number of 1-grams hit = 225  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle467.out
Perplexity = 8.74, Entropy = 3.13 bits
Computation based on 1127 words.
Number of 1-grams hit = 1127  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle468.out
Perplexity = 6.68, Entropy = 2.74 bits
Computation based on 843 words.
Number of 1-grams hit = 843  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle469.out
Perplexity = 6.60, Entropy = 2.72 bits
Computation based on 381 words.
Number of 1-grams hit = 381  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle470.out
Perplexity = 8.20, Entropy = 3.04 bits
Computation based on 259 words.
Number of 1-grams hit = 259  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle471.out
Perplexity = 8.24, Entropy = 3.04 bits
Computation based on 2170 words.
Number of 1-grams hit = 2170  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle472.out
Perplexity = 7.71, Entropy = 2.95 bits
Computation based on 1013 words.
Number of 1-grams hit = 1013  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle473.out
Perplexity = 9.24, Entropy = 3.21 bits
Computation based on 247 words.
Number of 1-grams hit = 247  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle474.out
Perplexity = 8.06, Entropy = 3.01 bits
Computation based on 838 words.
Number of 1-grams hit = 838  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle475.out
Perplexity = 7.83, Entropy = 2.97 bits
Computation based on 588 words.
Number of 1-grams hit = 588  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle476.out
Perplexity = 6.76, Entropy = 2.76 bits
Computation based on 626 words.
Number of 1-grams hit = 626  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle477.out
Perplexity = 7.00, Entropy = 2.81 bits
Computation based on 474 words.
Number of 1-grams hit = 474  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle478.out
Perplexity = 8.91, Entropy = 3.15 bits
Computation based on 547 words.
Number of 1-grams hit = 547  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle479.out
Perplexity = 9.79, Entropy = 3.29 bits
Computation based on 458 words.
Number of 1-grams hit = 458  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle480.out
Perplexity = 7.66, Entropy = 2.94 bits
Computation based on 749 words.
Number of 1-grams hit = 749  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle481.out
Perplexity = 9.35, Entropy = 3.22 bits
Computation based on 379 words.
Number of 1-grams hit = 379  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle482.out
Perplexity = 7.74, Entropy = 2.95 bits
Computation based on 542 words.
Number of 1-grams hit = 542  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle483.out
Perplexity = 9.49, Entropy = 3.25 bits
Computation based on 1863 words.
Number of 1-grams hit = 1863  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle484.out
Perplexity = 8.26, Entropy = 3.05 bits
Computation based on 1569 words.
Number of 1-grams hit = 1569  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle485.out
Perplexity = 7.69, Entropy = 2.94 bits
Computation based on 402 words.
Number of 1-grams hit = 402  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle486.out
Perplexity = 10.24, Entropy = 3.36 bits
Computation based on 369 words.
Number of 1-grams hit = 369  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle487.out
Perplexity = 8.62, Entropy = 3.11 bits
Computation based on 462 words.
Number of 1-grams hit = 462  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle488.out
Perplexity = 7.95, Entropy = 2.99 bits
Computation based on 770 words.
Number of 1-grams hit = 770  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle489.out
Perplexity = 7.88, Entropy = 2.98 bits
Computation based on 577 words.
Number of 1-grams hit = 577  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle490.out
Perplexity = 7.99, Entropy = 3.00 bits
Computation based on 533 words.
Number of 1-grams hit = 533  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle491.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 1520 words.
Number of 1-grams hit = 1520  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle492.out
Perplexity = 8.66, Entropy = 3.11 bits
Computation based on 473 words.
Number of 1-grams hit = 473  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle493.out
Perplexity = 8.08, Entropy = 3.01 bits
Computation based on 827 words.
Number of 1-grams hit = 827  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle494.out
Perplexity = 8.68, Entropy = 3.12 bits
Computation based on 540 words.
Number of 1-grams hit = 540  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle495.out
Perplexity = 9.33, Entropy = 3.22 bits
Computation based on 454 words.
Number of 1-grams hit = 454  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle496.out
Perplexity = 8.37, Entropy = 3.06 bits
Computation based on 608 words.
Number of 1-grams hit = 608  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle497.out
Perplexity = 8.55, Entropy = 3.10 bits
Computation based on 711 words.
Number of 1-grams hit = 711  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle498.out
Perplexity = 7.67, Entropy = 2.94 bits
Computation based on 1824 words.
Number of 1-grams hit = 1824  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle499.out
Perplexity = 8.07, Entropy = 3.01 bits
Computation based on 542 words.
Number of 1-grams hit = 542  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle500.out
Perplexity = 7.56, Entropy = 2.92 bits
Computation based on 486 words.
Number of 1-grams hit = 486  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle501.out
Perplexity = 9.84, Entropy = 3.30 bits
Computation based on 448 words.
Number of 1-grams hit = 448  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle502.out
Perplexity = 6.24, Entropy = 2.64 bits
Computation based on 535 words.
Number of 1-grams hit = 535  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle503.out
Perplexity = 9.56, Entropy = 3.26 bits
Computation based on 1109 words.
Number of 1-grams hit = 1109  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle504.out
Perplexity = 8.48, Entropy = 3.08 bits
Computation based on 658 words.
Number of 1-grams hit = 658  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle505.out
Perplexity = 8.11, Entropy = 3.02 bits
Computation based on 768 words.
Number of 1-grams hit = 768  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle506.out
Perplexity = 9.26, Entropy = 3.21 bits
Computation based on 594 words.
Number of 1-grams hit = 594  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle507.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 1702 words.
Number of 1-grams hit = 1702  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle508.out
Perplexity = 7.58, Entropy = 2.92 bits
Computation based on 700 words.
Number of 1-grams hit = 700  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle509.out
Perplexity = 8.83, Entropy = 3.14 bits
Computation based on 383 words.
Number of 1-grams hit = 383  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle510.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 1032 words.
Number of 1-grams hit = 1032  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle511.out
Perplexity = 7.76, Entropy = 2.96 bits
Computation based on 677 words.
Number of 1-grams hit = 677  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle512.out
Perplexity = 7.90, Entropy = 2.98 bits
Computation based on 2057 words.
Number of 1-grams hit = 2057  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle513.out
Perplexity = 7.23, Entropy = 2.85 bits
Computation based on 448 words.
Number of 1-grams hit = 448  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle514.out
Perplexity = 6.61, Entropy = 2.72 bits
Computation based on 422 words.
Number of 1-grams hit = 422  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle515.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 1070 words.
Number of 1-grams hit = 1070  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle516.out
Perplexity = 7.76, Entropy = 2.96 bits
Computation based on 440 words.
Number of 1-grams hit = 440  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle517.out
Perplexity = 7.74, Entropy = 2.95 bits
Computation based on 1145 words.
Number of 1-grams hit = 1145  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle518.out
Perplexity = 7.79, Entropy = 2.96 bits
Computation based on 639 words.
Number of 1-grams hit = 639  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle519.out
Perplexity = 10.24, Entropy = 3.36 bits
Computation based on 592 words.
Number of 1-grams hit = 592  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle520.out
Perplexity = 7.14, Entropy = 2.84 bits
Computation based on 663 words.
Number of 1-grams hit = 663  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle521.out
Perplexity = 7.35, Entropy = 2.88 bits
Computation based on 548 words.
Number of 1-grams hit = 548  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle522.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 1741 words.
Number of 1-grams hit = 1741  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle523.out
Perplexity = 9.71, Entropy = 3.28 bits
Computation based on 354 words.
Number of 1-grams hit = 354  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle524.out
Perplexity = 7.88, Entropy = 2.98 bits
Computation based on 1488 words.
Number of 1-grams hit = 1488  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle525.out
Perplexity = 7.88, Entropy = 2.98 bits
Computation based on 2121 words.
Number of 1-grams hit = 2121  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle526.out
Perplexity = 7.70, Entropy = 2.95 bits
Computation based on 2067 words.
Number of 1-grams hit = 2067  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle527.out
Perplexity = 7.64, Entropy = 2.93 bits
Computation based on 793 words.
Number of 1-grams hit = 793  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle528.out
Perplexity = 7.89, Entropy = 2.98 bits
Computation based on 788 words.
Number of 1-grams hit = 788  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle529.out
Perplexity = 6.72, Entropy = 2.75 bits
Computation based on 957 words.
Number of 1-grams hit = 957  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle530.out
Perplexity = 7.03, Entropy = 2.81 bits
Computation based on 451 words.
Number of 1-grams hit = 451  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle531.out
Perplexity = 7.84, Entropy = 2.97 bits
Computation based on 1046 words.
Number of 1-grams hit = 1046  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle532.out
Perplexity = 8.96, Entropy = 3.16 bits
Computation based on 1019 words.
Number of 1-grams hit = 1019  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle533.out
Perplexity = 8.22, Entropy = 3.04 bits
Computation based on 1210 words.
Number of 1-grams hit = 1210  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle534.out
Perplexity = 8.56, Entropy = 3.10 bits
Computation based on 549 words.
Number of 1-grams hit = 549  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle535.out
Perplexity = 6.63, Entropy = 2.73 bits
Computation based on 375 words.
Number of 1-grams hit = 375  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle536.out
Perplexity = 8.28, Entropy = 3.05 bits
Computation based on 1602 words.
Number of 1-grams hit = 1602  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle537.out
Perplexity = 9.64, Entropy = 3.27 bits
Computation based on 1098 words.
Number of 1-grams hit = 1098  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle538.out
Perplexity = 8.09, Entropy = 3.02 bits
Computation based on 3722 words.
Number of 1-grams hit = 3722  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle539.out
Perplexity = 6.84, Entropy = 2.77 bits
Computation based on 581 words.
Number of 1-grams hit = 581  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle540.out
Perplexity = 7.94, Entropy = 2.99 bits
Computation based on 2359 words.
Number of 1-grams hit = 2359  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle541.out
Perplexity = 8.29, Entropy = 3.05 bits
Computation based on 544 words.
Number of 1-grams hit = 544  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle542.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 1758 words.
Number of 1-grams hit = 1758  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle543.out
Perplexity = 8.96, Entropy = 3.16 bits
Computation based on 592 words.
Number of 1-grams hit = 592  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle544.out
Perplexity = 8.72, Entropy = 3.12 bits
Computation based on 1399 words.
Number of 1-grams hit = 1399  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle545.out
Perplexity = 8.26, Entropy = 3.05 bits
Computation based on 802 words.
Number of 1-grams hit = 802  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle546.out
Perplexity = 7.89, Entropy = 2.98 bits
Computation based on 753 words.
Number of 1-grams hit = 753  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle547.out
Perplexity = 7.29, Entropy = 2.87 bits
Computation based on 953 words.
Number of 1-grams hit = 953  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle548.out
Perplexity = 8.21, Entropy = 3.04 bits
Computation based on 671 words.
Number of 1-grams hit = 671  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle549.out
Perplexity = 7.25, Entropy = 2.86 bits
Computation based on 387 words.
Number of 1-grams hit = 387  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle550.out
Perplexity = 10.40, Entropy = 3.38 bits
Computation based on 204 words.
Number of 1-grams hit = 204  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle551.out
Perplexity = 8.53, Entropy = 3.09 bits
Computation based on 996 words.
Number of 1-grams hit = 996  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle552.out
Perplexity = 7.91, Entropy = 2.98 bits
Computation based on 1143 words.
Number of 1-grams hit = 1143  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle553.out
Perplexity = 8.43, Entropy = 3.08 bits
Computation based on 1347 words.
Number of 1-grams hit = 1347  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle554.out
Perplexity = 7.58, Entropy = 2.92 bits
Computation based on 4358 words.
Number of 1-grams hit = 4358  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle555.out
Perplexity = 9.62, Entropy = 3.27 bits
Computation based on 544 words.
Number of 1-grams hit = 544  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle556.out
Perplexity = 8.10, Entropy = 3.02 bits
Computation based on 605 words.
Number of 1-grams hit = 605  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle557.out
Perplexity = 8.28, Entropy = 3.05 bits
Computation based on 418 words.
Number of 1-grams hit = 418  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle558.out
Perplexity = 9.03, Entropy = 3.17 bits
Computation based on 1850 words.
Number of 1-grams hit = 1850  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle559.out
Perplexity = 8.57, Entropy = 3.10 bits
Computation based on 694 words.
Number of 1-grams hit = 694  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle560.out
Perplexity = 9.31, Entropy = 3.22 bits
Computation based on 595 words.
Number of 1-grams hit = 595  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle561.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 362 words.
Number of 1-grams hit = 362  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle562.out
Perplexity = 8.57, Entropy = 3.10 bits
Computation based on 331 words.
Number of 1-grams hit = 331  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle563.out
Perplexity = 7.66, Entropy = 2.94 bits
Computation based on 524 words.
Number of 1-grams hit = 524  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle564.out
Perplexity = 7.84, Entropy = 2.97 bits
Computation based on 1451 words.
Number of 1-grams hit = 1451  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle565.out
Perplexity = 8.06, Entropy = 3.01 bits
Computation based on 1392 words.
Number of 1-grams hit = 1392  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle566.out
Perplexity = 8.74, Entropy = 3.13 bits
Computation based on 4543 words.
Number of 1-grams hit = 4543  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle567.out
Perplexity = 10.88, Entropy = 3.44 bits
Computation based on 596 words.
Number of 1-grams hit = 596  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle568.out
Perplexity = 8.58, Entropy = 3.10 bits
Computation based on 355 words.
Number of 1-grams hit = 355  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle569.out
Perplexity = 7.94, Entropy = 2.99 bits
Computation based on 4706 words.
Number of 1-grams hit = 4706  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle570.out
Perplexity = 8.37, Entropy = 3.06 bits
Computation based on 897 words.
Number of 1-grams hit = 897  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle571.out
Perplexity = 7.55, Entropy = 2.92 bits
Computation based on 576 words.
Number of 1-grams hit = 576  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle572.out
Perplexity = 9.54, Entropy = 3.25 bits
Computation based on 619 words.
Number of 1-grams hit = 619  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle573.out
Perplexity = 10.14, Entropy = 3.34 bits
Computation based on 279 words.
Number of 1-grams hit = 279  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle574.out
Perplexity = 8.39, Entropy = 3.07 bits
Computation based on 1179 words.
Number of 1-grams hit = 1179  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle575.out
Perplexity = 6.84, Entropy = 2.77 bits
Computation based on 643 words.
Number of 1-grams hit = 643  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle576.out
Perplexity = 7.56, Entropy = 2.92 bits
Computation based on 489 words.
Number of 1-grams hit = 489  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle577.out
Perplexity = 9.75, Entropy = 3.28 bits
Computation based on 405 words.
Number of 1-grams hit = 405  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle578.out
Perplexity = 8.98, Entropy = 3.17 bits
Computation based on 447 words.
Number of 1-grams hit = 447  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle579.out
Perplexity = 8.24, Entropy = 3.04 bits
Computation based on 479 words.
Number of 1-grams hit = 479  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle580.out
Perplexity = 8.48, Entropy = 3.08 bits
Computation based on 585 words.
Number of 1-grams hit = 585  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle581.out
Perplexity = 8.61, Entropy = 3.11 bits
Computation based on 584 words.
Number of 1-grams hit = 584  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle582.out
Perplexity = 7.61, Entropy = 2.93 bits
Computation based on 547 words.
Number of 1-grams hit = 547  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle583.out
Perplexity = 8.03, Entropy = 3.01 bits
Computation based on 722 words.
Number of 1-grams hit = 722  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle584.out
Perplexity = 9.40, Entropy = 3.23 bits
Computation based on 477 words.
Number of 1-grams hit = 477  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle585.out
Perplexity = 6.69, Entropy = 2.74 bits
Computation based on 1036 words.
Number of 1-grams hit = 1036  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle586.out
Perplexity = 7.65, Entropy = 2.93 bits
Computation based on 782 words.
Number of 1-grams hit = 782  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle587.out
Perplexity = 9.00, Entropy = 3.17 bits
Computation based on 576 words.
Number of 1-grams hit = 576  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle588.out
Perplexity = 8.27, Entropy = 3.05 bits
Computation based on 1045 words.
Number of 1-grams hit = 1045  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle589.out
Perplexity = 7.56, Entropy = 2.92 bits
Computation based on 538 words.
Number of 1-grams hit = 538  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle590.out
Perplexity = 7.13, Entropy = 2.83 bits
Computation based on 1468 words.
Number of 1-grams hit = 1468  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle591.out
Perplexity = 9.32, Entropy = 3.22 bits
Computation based on 749 words.
Number of 1-grams hit = 749  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle592.out
Perplexity = 9.52, Entropy = 3.25 bits
Computation based on 367 words.
Number of 1-grams hit = 367  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle593.out
Perplexity = 8.24, Entropy = 3.04 bits
Computation based on 409 words.
Number of 1-grams hit = 409  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle594.out
Perplexity = 8.14, Entropy = 3.03 bits
Computation based on 653 words.
Number of 1-grams hit = 653  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle595.out
Perplexity = 9.23, Entropy = 3.21 bits
Computation based on 398 words.
Number of 1-grams hit = 398  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle596.out
Perplexity = 8.69, Entropy = 3.12 bits
Computation based on 1041 words.
Number of 1-grams hit = 1041  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle597.out
Perplexity = 7.74, Entropy = 2.95 bits
Computation based on 1109 words.
Number of 1-grams hit = 1109  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle598.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 327 words.
Number of 1-grams hit = 327  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle599.out
Perplexity = 8.30, Entropy = 3.05 bits
Computation based on 879 words.
Number of 1-grams hit = 879  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle600.out
Perplexity = 9.57, Entropy = 3.26 bits
Computation based on 5209 words.
Number of 1-grams hit = 5209  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle601.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 1181 words.
Number of 1-grams hit = 1181  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle602.out
Perplexity = 8.74, Entropy = 3.13 bits
Computation based on 652 words.
Number of 1-grams hit = 652  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle603.out
Perplexity = 8.00, Entropy = 3.00 bits
Computation based on 94 words.
Number of 1-grams hit = 94  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle604.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 612 words.
Number of 1-grams hit = 612  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle605.out
Perplexity = 7.66, Entropy = 2.94 bits
Computation based on 3406 words.
Number of 1-grams hit = 3406  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle606.out
Perplexity = 8.17, Entropy = 3.03 bits
Computation based on 978 words.
Number of 1-grams hit = 978  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle607.out
Perplexity = 8.42, Entropy = 3.07 bits
Computation based on 1157 words.
Number of 1-grams hit = 1157  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle608.out
Perplexity = 9.06, Entropy = 3.18 bits
Computation based on 674 words.
Number of 1-grams hit = 674  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle609.out
Perplexity = 8.34, Entropy = 3.06 bits
Computation based on 4849 words.
Number of 1-grams hit = 4849  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle610.out
Perplexity = 8.30, Entropy = 3.05 bits
Computation based on 6480 words.
Number of 1-grams hit = 6480  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle611.out
Perplexity = 8.80, Entropy = 3.14 bits
Computation based on 856 words.
Number of 1-grams hit = 856  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle612.out
Perplexity = 8.12, Entropy = 3.02 bits
Computation based on 1757 words.
Number of 1-grams hit = 1757  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle613.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 4584 words.
Number of 1-grams hit = 4584  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle614.out
Perplexity = 7.69, Entropy = 2.94 bits
Computation based on 1629 words.
Number of 1-grams hit = 1629  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle615.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 494 words.
Number of 1-grams hit = 494  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle616.out
Perplexity = 6.15, Entropy = 2.62 bits
Computation based on 827 words.
Number of 1-grams hit = 827  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle617.out
Perplexity = 7.13, Entropy = 2.83 bits
Computation based on 1418 words.
Number of 1-grams hit = 1418  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle618.out
Perplexity = 7.67, Entropy = 2.94 bits
Computation based on 4055 words.
Number of 1-grams hit = 4055  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle619.out
Perplexity = 8.39, Entropy = 3.07 bits
Computation based on 853 words.
Number of 1-grams hit = 853  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle620.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 5770 words.
Number of 1-grams hit = 5770  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle621.out
Perplexity = 7.02, Entropy = 2.81 bits
Computation based on 347 words.
Number of 1-grams hit = 347  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle622.out
Perplexity = 8.46, Entropy = 3.08 bits
Computation based on 503 words.
Number of 1-grams hit = 503  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle623.out
Perplexity = 8.62, Entropy = 3.11 bits
Computation based on 1430 words.
Number of 1-grams hit = 1430  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle624.out
Perplexity = 7.94, Entropy = 2.99 bits
Computation based on 5610 words.
Number of 1-grams hit = 5610  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle625.out
Perplexity = 7.96, Entropy = 2.99 bits
Computation based on 1191 words.
Number of 1-grams hit = 1191  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle626.out
Perplexity = 7.42, Entropy = 2.89 bits
Computation based on 483 words.
Number of 1-grams hit = 483  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle627.out
Perplexity = 6.84, Entropy = 2.77 bits
Computation based on 159 words.
Number of 1-grams hit = 159  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle628.out
Perplexity = 7.97, Entropy = 3.00 bits
Computation based on 1142 words.
Number of 1-grams hit = 1142  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle629.out
Perplexity = 7.77, Entropy = 2.96 bits
Computation based on 1114 words.
Number of 1-grams hit = 1114  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle630.out
Perplexity = 6.81, Entropy = 2.77 bits
Computation based on 349 words.
Number of 1-grams hit = 349  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle631.out
Perplexity = 8.34, Entropy = 3.06 bits
Computation based on 931 words.
Number of 1-grams hit = 931  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle632.out
Perplexity = 7.23, Entropy = 2.85 bits
Computation based on 439 words.
Number of 1-grams hit = 439  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle633.out
Perplexity = 9.31, Entropy = 3.22 bits
Computation based on 579 words.
Number of 1-grams hit = 579  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle634.out
Perplexity = 7.79, Entropy = 2.96 bits
Computation based on 603 words.
Number of 1-grams hit = 603  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle635.out
Perplexity = 8.31, Entropy = 3.06 bits
Computation based on 169 words.
Number of 1-grams hit = 169  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle636.out
Perplexity = 7.44, Entropy = 2.89 bits
Computation based on 1435 words.
Number of 1-grams hit = 1435  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle637.out
Perplexity = 7.06, Entropy = 2.82 bits
Computation based on 1118 words.
Number of 1-grams hit = 1118  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle638.out
Perplexity = 7.00, Entropy = 2.81 bits
Computation based on 586 words.
Number of 1-grams hit = 586  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle639.out
Perplexity = 7.25, Entropy = 2.86 bits
Computation based on 561 words.
Number of 1-grams hit = 561  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle640.out
Perplexity = 8.98, Entropy = 3.17 bits
Computation based on 227 words.
Number of 1-grams hit = 227  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle641.out
Perplexity = 7.40, Entropy = 2.89 bits
Computation based on 888 words.
Number of 1-grams hit = 888  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle642.out
Perplexity = 7.79, Entropy = 2.96 bits
Computation based on 477 words.
Number of 1-grams hit = 477  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle643.out
Perplexity = 8.09, Entropy = 3.02 bits
Computation based on 522 words.
Number of 1-grams hit = 522  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle644.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 1207 words.
Number of 1-grams hit = 1207  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle645.out
Perplexity = 7.54, Entropy = 2.91 bits
Computation based on 475 words.
Number of 1-grams hit = 475  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle646.out
Perplexity = 7.95, Entropy = 2.99 bits
Computation based on 615 words.
Number of 1-grams hit = 615  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle647.out
Perplexity = 7.95, Entropy = 2.99 bits
Computation based on 653 words.
Number of 1-grams hit = 653  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle648.out
Perplexity = 7.57, Entropy = 2.92 bits
Computation based on 511 words.
Number of 1-grams hit = 511  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle649.out
Perplexity = 6.86, Entropy = 2.78 bits
Computation based on 577 words.
Number of 1-grams hit = 577  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle650.out
Perplexity = 7.71, Entropy = 2.95 bits
Computation based on 579 words.
Number of 1-grams hit = 579  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle651.out
Perplexity = 8.11, Entropy = 3.02 bits
Computation based on 503 words.
Number of 1-grams hit = 503  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle652.out
Perplexity = 8.27, Entropy = 3.05 bits
Computation based on 956 words.
Number of 1-grams hit = 956  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle653.out
Perplexity = 8.22, Entropy = 3.04 bits
Computation based on 848 words.
Number of 1-grams hit = 848  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle654.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 827 words.
Number of 1-grams hit = 827  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle655.out
Perplexity = 8.99, Entropy = 3.17 bits
Computation based on 547 words.
Number of 1-grams hit = 547  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle656.out
Perplexity = 9.14, Entropy = 3.19 bits
Computation based on 501 words.
Number of 1-grams hit = 501  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle657.out
Perplexity = 7.31, Entropy = 2.87 bits
Computation based on 412 words.
Number of 1-grams hit = 412  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle658.out
Perplexity = 9.45, Entropy = 3.24 bits
Computation based on 1097 words.
Number of 1-grams hit = 1097  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle659.out
Perplexity = 8.85, Entropy = 3.15 bits
Computation based on 1082 words.
Number of 1-grams hit = 1082  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle660.out
Perplexity = 4.47, Entropy = 2.16 bits
Computation based on 151 words.
Number of 1-grams hit = 151  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle661.out
Perplexity = 8.01, Entropy = 3.00 bits
Computation based on 1330 words.
Number of 1-grams hit = 1330  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle662.out
Perplexity = 6.79, Entropy = 2.76 bits
Computation based on 131 words.
Number of 1-grams hit = 131  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle663.out
Perplexity = 7.14, Entropy = 2.84 bits
Computation based on 1445 words.
Number of 1-grams hit = 1445  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle664.out
Perplexity = 10.78, Entropy = 3.43 bits
Computation based on 770 words.
Number of 1-grams hit = 770  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle665.out
Perplexity = 9.42, Entropy = 3.24 bits
Computation based on 830 words.
Number of 1-grams hit = 830  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle666.out
Perplexity = 7.84, Entropy = 2.97 bits
Computation based on 413 words.
Number of 1-grams hit = 413  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle667.out
Perplexity = 9.40, Entropy = 3.23 bits
Computation based on 1455 words.
Number of 1-grams hit = 1455  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle668.out
Perplexity = 7.88, Entropy = 2.98 bits
Computation based on 504 words.
Number of 1-grams hit = 504  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle669.out
Perplexity = 8.59, Entropy = 3.10 bits
Computation based on 524 words.
Number of 1-grams hit = 524  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle670.out
Perplexity = 9.23, Entropy = 3.21 bits
Computation based on 481 words.
Number of 1-grams hit = 481  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle671.out
Perplexity = 6.81, Entropy = 2.77 bits
Computation based on 693 words.
Number of 1-grams hit = 693  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle672.out
Perplexity = 9.23, Entropy = 3.21 bits
Computation based on 316 words.
Number of 1-grams hit = 316  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle673.out
Perplexity = 9.04, Entropy = 3.18 bits
Computation based on 336 words.
Number of 1-grams hit = 336  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle674.out
Perplexity = 10.69, Entropy = 3.42 bits
Computation based on 159 words.
Number of 1-grams hit = 159  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle675.out
Perplexity = 8.19, Entropy = 3.03 bits
Computation based on 4196 words.
Number of 1-grams hit = 4196  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle676.out
Perplexity = 6.11, Entropy = 2.61 bits
Computation based on 531 words.
Number of 1-grams hit = 531  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle677.out
Perplexity = 7.56, Entropy = 2.92 bits
Computation based on 684 words.
Number of 1-grams hit = 684  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle678.out
Perplexity = 7.60, Entropy = 2.93 bits
Computation based on 373 words.
Number of 1-grams hit = 373  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle679.out
Perplexity = 7.58, Entropy = 2.92 bits
Computation based on 1075 words.
Number of 1-grams hit = 1075  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle680.out
Perplexity = 9.51, Entropy = 3.25 bits
Computation based on 356 words.
Number of 1-grams hit = 356  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle681.out
Perplexity = 8.53, Entropy = 3.09 bits
Computation based on 522 words.
Number of 1-grams hit = 522  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle682.out
Perplexity = 8.59, Entropy = 3.10 bits
Computation based on 4014 words.
Number of 1-grams hit = 4014  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle683.out
Perplexity = 6.94, Entropy = 2.80 bits
Computation based on 279 words.
Number of 1-grams hit = 279  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle684.out
Perplexity = 8.52, Entropy = 3.09 bits
Computation based on 4719 words.
Number of 1-grams hit = 4719  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle685.out
Perplexity = 7.98, Entropy = 3.00 bits
Computation based on 485 words.
Number of 1-grams hit = 485  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle686.out
Perplexity = 7.74, Entropy = 2.95 bits
Computation based on 2419 words.
Number of 1-grams hit = 2419  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle687.out
Perplexity = 9.11, Entropy = 3.19 bits
Computation based on 556 words.
Number of 1-grams hit = 556  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle688.out
Perplexity = 8.70, Entropy = 3.12 bits
Computation based on 332 words.
Number of 1-grams hit = 332  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle689.out
Perplexity = 8.12, Entropy = 3.02 bits
Computation based on 554 words.
Number of 1-grams hit = 554  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle690.out
Perplexity = 7.86, Entropy = 2.97 bits
Computation based on 824 words.
Number of 1-grams hit = 824  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle691.out
Perplexity = 9.86, Entropy = 3.30 bits
Computation based on 138 words.
Number of 1-grams hit = 138  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle692.out
Perplexity = 6.81, Entropy = 2.77 bits
Computation based on 460 words.
Number of 1-grams hit = 460  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle693.out
Perplexity = 9.22, Entropy = 3.21 bits
Computation based on 1025 words.
Number of 1-grams hit = 1025  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle694.out
Perplexity = 7.55, Entropy = 2.92 bits
Computation based on 669 words.
Number of 1-grams hit = 669  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle695.out
Perplexity = 6.81, Entropy = 2.77 bits
Computation based on 924 words.
Number of 1-grams hit = 924  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle696.out
Perplexity = 8.04, Entropy = 3.01 bits
Computation based on 541 words.
Number of 1-grams hit = 541  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle697.out
Perplexity = 8.45, Entropy = 3.08 bits
Computation based on 1021 words.
Number of 1-grams hit = 1021  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle698.out
Perplexity = 8.43, Entropy = 3.08 bits
Computation based on 630 words.
Number of 1-grams hit = 630  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle699.out
Perplexity = 8.16, Entropy = 3.03 bits
Computation based on 377 words.
Number of 1-grams hit = 377  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle700.out
Perplexity = 9.70, Entropy = 3.28 bits
Computation based on 3659 words.
Number of 1-grams hit = 3659  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle701.out
Perplexity = 8.10, Entropy = 3.02 bits
Computation based on 878 words.
Number of 1-grams hit = 878  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle702.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 2030 words.
Number of 1-grams hit = 2030  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle703.out
Perplexity = 8.38, Entropy = 3.07 bits
Computation based on 470 words.
Number of 1-grams hit = 470  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle704.out
Perplexity = 9.04, Entropy = 3.18 bits
Computation based on 341 words.
Number of 1-grams hit = 341  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle705.out
Perplexity = 9.14, Entropy = 3.19 bits
Computation based on 1100 words.
Number of 1-grams hit = 1100  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle706.out
Perplexity = 6.62, Entropy = 2.73 bits
Computation based on 509 words.
Number of 1-grams hit = 509  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle707.out
Perplexity = 7.50, Entropy = 2.91 bits
Computation based on 941 words.
Number of 1-grams hit = 941  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle708.out
Perplexity = 8.35, Entropy = 3.06 bits
Computation based on 562 words.
Number of 1-grams hit = 562  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle709.out
Perplexity = 8.69, Entropy = 3.12 bits
Computation based on 602 words.
Number of 1-grams hit = 602  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle710.out
Perplexity = 8.22, Entropy = 3.04 bits
Computation based on 823 words.
Number of 1-grams hit = 823  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle711.out
Perplexity = 8.38, Entropy = 3.07 bits
Computation based on 1017 words.
Number of 1-grams hit = 1017  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle712.out
Perplexity = 9.91, Entropy = 3.31 bits
Computation based on 406 words.
Number of 1-grams hit = 406  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle713.out
Perplexity = 10.22, Entropy = 3.35 bits
Computation based on 971 words.
Number of 1-grams hit = 971  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle714.out
Perplexity = 9.28, Entropy = 3.21 bits
Computation based on 404 words.
Number of 1-grams hit = 404  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle715.out
Perplexity = 8.51, Entropy = 3.09 bits
Computation based on 525 words.
Number of 1-grams hit = 525  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle716.out
Perplexity = 7.74, Entropy = 2.95 bits
Computation based on 521 words.
Number of 1-grams hit = 521  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle717.out
Perplexity = 8.39, Entropy = 3.07 bits
Computation based on 403 words.
Number of 1-grams hit = 403  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle718.out
Perplexity = 7.88, Entropy = 2.98 bits
Computation based on 7638 words.
Number of 1-grams hit = 7638  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle719.out
Perplexity = 8.72, Entropy = 3.12 bits
Computation based on 1275 words.
Number of 1-grams hit = 1275  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle720.out
Perplexity = 7.57, Entropy = 2.92 bits
Computation based on 872 words.
Number of 1-grams hit = 872  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle721.out
Perplexity = 7.51, Entropy = 2.91 bits
Computation based on 452 words.
Number of 1-grams hit = 452  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle722.out
Perplexity = 9.90, Entropy = 3.31 bits
Computation based on 1116 words.
Number of 1-grams hit = 1116  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle723.out
Perplexity = 7.53, Entropy = 2.91 bits
Computation based on 472 words.
Number of 1-grams hit = 472  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle724.out
Perplexity = 6.71, Entropy = 2.75 bits
Computation based on 500 words.
Number of 1-grams hit = 500  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle725.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 792 words.
Number of 1-grams hit = 792  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle726.out
Perplexity = 7.91, Entropy = 2.98 bits
Computation based on 573 words.
Number of 1-grams hit = 573  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle727.out
Perplexity = 7.43, Entropy = 2.89 bits
Computation based on 519 words.
Number of 1-grams hit = 519  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle728.out
Perplexity = 7.72, Entropy = 2.95 bits
Computation based on 453 words.
Number of 1-grams hit = 453  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle729.out
Perplexity = 8.45, Entropy = 3.08 bits
Computation based on 468 words.
Number of 1-grams hit = 468  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle730.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 5476 words.
Number of 1-grams hit = 5476  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle731.out
Perplexity = 9.00, Entropy = 3.17 bits
Computation based on 1562 words.
Number of 1-grams hit = 1562  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle732.out
Perplexity = 9.24, Entropy = 3.21 bits
Computation based on 769 words.
Number of 1-grams hit = 769  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle733.out
Perplexity = 8.29, Entropy = 3.05 bits
Computation based on 609 words.
Number of 1-grams hit = 609  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle734.out
Perplexity = 8.04, Entropy = 3.01 bits
Computation based on 450 words.
Number of 1-grams hit = 450  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle735.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 1274 words.
Number of 1-grams hit = 1274  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle736.out
Perplexity = 9.25, Entropy = 3.21 bits
Computation based on 733 words.
Number of 1-grams hit = 733  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle737.out
Perplexity = 9.20, Entropy = 3.20 bits
Computation based on 697 words.
Number of 1-grams hit = 697  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle738.out
Perplexity = 6.82, Entropy = 2.77 bits
Computation based on 1069 words.
Number of 1-grams hit = 1069  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle739.out
Perplexity = 10.21, Entropy = 3.35 bits
Computation based on 484 words.
Number of 1-grams hit = 484  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle740.out
Perplexity = 8.71, Entropy = 3.12 bits
Computation based on 1194 words.
Number of 1-grams hit = 1194  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle741.out
Perplexity = 7.28, Entropy = 2.86 bits
Computation based on 824 words.
Number of 1-grams hit = 824  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle742.out
Perplexity = 8.37, Entropy = 3.06 bits
Computation based on 690 words.
Number of 1-grams hit = 690  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle743.out
Perplexity = 7.14, Entropy = 2.84 bits
Computation based on 2142 words.
Number of 1-grams hit = 2142  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle744.out
Perplexity = 7.66, Entropy = 2.94 bits
Computation based on 580 words.
Number of 1-grams hit = 580  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle745.out
Perplexity = 8.32, Entropy = 3.06 bits
Computation based on 740 words.
Number of 1-grams hit = 740  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle746.out
Perplexity = 9.68, Entropy = 3.28 bits
Computation based on 1498 words.
Number of 1-grams hit = 1498  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle747.out
Perplexity = 6.96, Entropy = 2.80 bits
Computation based on 571 words.
Number of 1-grams hit = 571  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle748.out
Perplexity = 7.89, Entropy = 2.98 bits
Computation based on 544 words.
Number of 1-grams hit = 544  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle749.out
Perplexity = 8.07, Entropy = 3.01 bits
Computation based on 590 words.
Number of 1-grams hit = 590  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle750.out
Perplexity = 8.10, Entropy = 3.02 bits
Computation based on 513 words.
Number of 1-grams hit = 513  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle751.out
Perplexity = 7.75, Entropy = 2.95 bits
Computation based on 2179 words.
Number of 1-grams hit = 2179  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle752.out
Perplexity = 9.66, Entropy = 3.27 bits
Computation based on 468 words.
Number of 1-grams hit = 468  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle753.out
Perplexity = 7.76, Entropy = 2.96 bits
Computation based on 572 words.
Number of 1-grams hit = 572  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle754.out
Perplexity = 8.70, Entropy = 3.12 bits
Computation based on 2031 words.
Number of 1-grams hit = 2031  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle755.out
Perplexity = 8.59, Entropy = 3.10 bits
Computation based on 487 words.
Number of 1-grams hit = 487  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle756.out
Perplexity = 13.35, Entropy = 3.74 bits
Computation based on 74 words.
Number of 1-grams hit = 74  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle757.out
Perplexity = 9.68, Entropy = 3.28 bits
Computation based on 557 words.
Number of 1-grams hit = 557  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle758.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 2743 words.
Number of 1-grams hit = 2743  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle759.out
Perplexity = 8.25, Entropy = 3.04 bits
Computation based on 3515 words.
Number of 1-grams hit = 3515  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle760.out
Perplexity = 8.88, Entropy = 3.15 bits
Computation based on 618 words.
Number of 1-grams hit = 618  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle761.out
Perplexity = 9.83, Entropy = 3.30 bits
Computation based on 684 words.
Number of 1-grams hit = 684  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle762.out
Perplexity = 7.87, Entropy = 2.98 bits
Computation based on 861 words.
Number of 1-grams hit = 861  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle763.out
Perplexity = 7.28, Entropy = 2.86 bits
Computation based on 782 words.
Number of 1-grams hit = 782  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle764.out
Perplexity = 8.75, Entropy = 3.13 bits
Computation based on 582 words.
Number of 1-grams hit = 582  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle765.out
Perplexity = 7.68, Entropy = 2.94 bits
Computation based on 888 words.
Number of 1-grams hit = 888  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle766.out
Perplexity = 9.12, Entropy = 3.19 bits
Computation based on 293 words.
Number of 1-grams hit = 293  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle767.out
Perplexity = 8.08, Entropy = 3.01 bits
Computation based on 4625 words.
Number of 1-grams hit = 4625  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle768.out
Perplexity = 8.49, Entropy = 3.09 bits
Computation based on 503 words.
Number of 1-grams hit = 503  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle769.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 730 words.
Number of 1-grams hit = 730  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle770.out
Perplexity = 8.35, Entropy = 3.06 bits
Computation based on 658 words.
Number of 1-grams hit = 658  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle771.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 501 words.
Number of 1-grams hit = 501  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle772.out
Perplexity = 6.62, Entropy = 2.73 bits
Computation based on 590 words.
Number of 1-grams hit = 590  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle773.out
Perplexity = 8.72, Entropy = 3.12 bits
Computation based on 3261 words.
Number of 1-grams hit = 3261  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle774.out
Perplexity = 7.60, Entropy = 2.93 bits
Computation based on 2158 words.
Number of 1-grams hit = 2158  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle775.out
Perplexity = 8.16, Entropy = 3.03 bits
Computation based on 558 words.
Number of 1-grams hit = 558  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle776.out
Perplexity = 7.58, Entropy = 2.92 bits
Computation based on 48 words.
Number of 1-grams hit = 48  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle777.out
Perplexity = 9.07, Entropy = 3.18 bits
Computation based on 10354 words.
Number of 1-grams hit = 10354  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle778.out
Perplexity = 8.66, Entropy = 3.11 bits
Computation based on 545 words.
Number of 1-grams hit = 545  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle779.out
Perplexity = 9.39, Entropy = 3.23 bits
Computation based on 1014 words.
Number of 1-grams hit = 1014  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle780.out
Perplexity = 9.14, Entropy = 3.19 bits
Computation based on 445 words.
Number of 1-grams hit = 445  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle781.out
Perplexity = 8.47, Entropy = 3.08 bits
Computation based on 786 words.
Number of 1-grams hit = 786  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle782.out
Perplexity = 8.16, Entropy = 3.03 bits
Computation based on 1485 words.
Number of 1-grams hit = 1485  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle783.out
Perplexity = 8.72, Entropy = 3.12 bits
Computation based on 549 words.
Number of 1-grams hit = 549  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle784.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 1881 words.
Number of 1-grams hit = 1881  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle785.out
Perplexity = 9.81, Entropy = 3.29 bits
Computation based on 2868 words.
Number of 1-grams hit = 2868  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle786.out
Perplexity = 8.83, Entropy = 3.14 bits
Computation based on 370 words.
Number of 1-grams hit = 370  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle787.out
Perplexity = 8.00, Entropy = 3.00 bits
Computation based on 512 words.
Number of 1-grams hit = 512  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle788.out
Perplexity = 9.18, Entropy = 3.20 bits
Computation based on 1616 words.
Number of 1-grams hit = 1616  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle789.out
Perplexity = 8.14, Entropy = 3.02 bits
Computation based on 12475 words.
Number of 1-grams hit = 12475  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle790.out
Perplexity = 7.33, Entropy = 2.87 bits
Computation based on 1019 words.
Number of 1-grams hit = 1019  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle791.out
Perplexity = 7.87, Entropy = 2.98 bits
Computation based on 749 words.
Number of 1-grams hit = 749  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle792.out
Perplexity = 8.55, Entropy = 3.10 bits
Computation based on 3144 words.
Number of 1-grams hit = 3144  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle793.out
Perplexity = 8.42, Entropy = 3.07 bits
Computation based on 197 words.
Number of 1-grams hit = 197  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle794.out
Perplexity = 9.42, Entropy = 3.24 bits
Computation based on 548 words.
Number of 1-grams hit = 548  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle795.out
Perplexity = 8.43, Entropy = 3.08 bits
Computation based on 460 words.
Number of 1-grams hit = 460  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle796.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 1620 words.
Number of 1-grams hit = 1620  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle797.out
Perplexity = 7.73, Entropy = 2.95 bits
Computation based on 413 words.
Number of 1-grams hit = 413  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle798.out
Perplexity = 8.24, Entropy = 3.04 bits
Computation based on 503 words.
Number of 1-grams hit = 503  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle799.out
Perplexity = 8.48, Entropy = 3.08 bits
Computation based on 816 words.
Number of 1-grams hit = 816  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle800.out
Perplexity = 7.78, Entropy = 2.96 bits
Computation based on 473 words.
Number of 1-grams hit = 473  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle801.out
Perplexity = 8.87, Entropy = 3.15 bits
Computation based on 3418 words.
Number of 1-grams hit = 3418  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle802.out
Perplexity = 7.07, Entropy = 2.82 bits
Computation based on 2237 words.
Number of 1-grams hit = 2237  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle803.out
Perplexity = 9.19, Entropy = 3.20 bits
Computation based on 4594 words.
Number of 1-grams hit = 4594  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle804.out
Perplexity = 7.75, Entropy = 2.95 bits
Computation based on 1564 words.
Number of 1-grams hit = 1564  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle805.out
Perplexity = 7.92, Entropy = 2.98 bits
Computation based on 993 words.
Number of 1-grams hit = 993  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle806.out
Perplexity = 8.17, Entropy = 3.03 bits
Computation based on 3603 words.
Number of 1-grams hit = 3603  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle807.out
Perplexity = 6.86, Entropy = 2.78 bits
Computation based on 756 words.
Number of 1-grams hit = 756  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle808.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 1830 words.
Number of 1-grams hit = 1830  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle809.out
Perplexity = 8.91, Entropy = 3.16 bits
Computation based on 2165 words.
Number of 1-grams hit = 2165  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle810.out
Perplexity = 8.55, Entropy = 3.10 bits
Computation based on 937 words.
Number of 1-grams hit = 937  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle811.out
Perplexity = 8.51, Entropy = 3.09 bits
Computation based on 4883 words.
Number of 1-grams hit = 4883  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle812.out
Perplexity = 7.99, Entropy = 3.00 bits
Computation based on 3604 words.
Number of 1-grams hit = 3604  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle813.out
Perplexity = 7.23, Entropy = 2.85 bits
Computation based on 1497 words.
Number of 1-grams hit = 1497  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle814.out
Perplexity = 6.21, Entropy = 2.64 bits
Computation based on 908 words.
Number of 1-grams hit = 908  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle815.out
Perplexity = 8.53, Entropy = 3.09 bits
Computation based on 1390 words.
Number of 1-grams hit = 1390  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle816.out
Perplexity = 8.75, Entropy = 3.13 bits
Computation based on 1434 words.
Number of 1-grams hit = 1434  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle817.out
Perplexity = 9.04, Entropy = 3.18 bits
Computation based on 580 words.
Number of 1-grams hit = 580  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle818.out
Perplexity = 10.55, Entropy = 3.40 bits
Computation based on 604 words.
Number of 1-grams hit = 604  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle819.out
Perplexity = 7.11, Entropy = 2.83 bits
Computation based on 1947 words.
Number of 1-grams hit = 1947  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle820.out
Perplexity = 6.84, Entropy = 2.77 bits
Computation based on 880 words.
Number of 1-grams hit = 880  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle821.out
Perplexity = 7.64, Entropy = 2.93 bits
Computation based on 1187 words.
Number of 1-grams hit = 1187  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle822.out
Perplexity = 8.09, Entropy = 3.02 bits
Computation based on 2654 words.
Number of 1-grams hit = 2654  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle823.out
Perplexity = 8.81, Entropy = 3.14 bits
Computation based on 1435 words.
Number of 1-grams hit = 1435  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle824.out
Perplexity = 7.55, Entropy = 2.92 bits
Computation based on 533 words.
Number of 1-grams hit = 533  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle825.out
Perplexity = 8.76, Entropy = 3.13 bits
Computation based on 639 words.
Number of 1-grams hit = 639  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle826.out
Perplexity = 7.86, Entropy = 2.98 bits
Computation based on 978 words.
Number of 1-grams hit = 978  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle827.out
Perplexity = 9.76, Entropy = 3.29 bits
Computation based on 734 words.
Number of 1-grams hit = 734  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle828.out
Perplexity = 9.64, Entropy = 3.27 bits
Computation based on 13188 words.
Number of 1-grams hit = 13188  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle829.out
Perplexity = 8.11, Entropy = 3.02 bits
Computation based on 578 words.
Number of 1-grams hit = 578  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle830.out
Perplexity = 7.63, Entropy = 2.93 bits
Computation based on 2488 words.
Number of 1-grams hit = 2488  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle831.out
Perplexity = 8.76, Entropy = 3.13 bits
Computation based on 1106 words.
Number of 1-grams hit = 1106  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle832.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 460 words.
Number of 1-grams hit = 460  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle833.out
Perplexity = 7.55, Entropy = 2.92 bits
Computation based on 888 words.
Number of 1-grams hit = 888  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle834.out
Perplexity = 8.11, Entropy = 3.02 bits
Computation based on 963 words.
Number of 1-grams hit = 963  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle835.out
Perplexity = 6.84, Entropy = 2.77 bits
Computation based on 491 words.
Number of 1-grams hit = 491  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle836.out
Perplexity = 8.18, Entropy = 3.03 bits
Computation based on 528 words.
Number of 1-grams hit = 528  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle837.out
Perplexity = 7.69, Entropy = 2.94 bits
Computation based on 900 words.
Number of 1-grams hit = 900  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle838.out
Perplexity = 7.82, Entropy = 2.97 bits
Computation based on 2446 words.
Number of 1-grams hit = 2446  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle839.out
Perplexity = 9.60, Entropy = 3.26 bits
Computation based on 496 words.
Number of 1-grams hit = 496  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle840.out
Perplexity = 6.98, Entropy = 2.80 bits
Computation based on 560 words.
Number of 1-grams hit = 560  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle841.out
Perplexity = 7.49, Entropy = 2.90 bits
Computation based on 1009 words.
Number of 1-grams hit = 1009  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle842.out
Perplexity = 6.02, Entropy = 2.59 bits
Computation based on 503 words.
Number of 1-grams hit = 503  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle843.out
Perplexity = 8.28, Entropy = 3.05 bits
Computation based on 1419 words.
Number of 1-grams hit = 1419  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle844.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 9009 words.
Number of 1-grams hit = 9009  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle845.out
Perplexity = 8.12, Entropy = 3.02 bits
Computation based on 648 words.
Number of 1-grams hit = 648  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle846.out
Perplexity = 8.49, Entropy = 3.09 bits
Computation based on 609 words.
Number of 1-grams hit = 609  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle847.out
Perplexity = 9.32, Entropy = 3.22 bits
Computation based on 1795 words.
Number of 1-grams hit = 1795  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle848.out
Perplexity = 6.90, Entropy = 2.79 bits
Computation based on 389 words.
Number of 1-grams hit = 389  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle849.out
Perplexity = 7.96, Entropy = 2.99 bits
Computation based on 1705 words.
Number of 1-grams hit = 1705  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle850.out
Perplexity = 8.23, Entropy = 3.04 bits
Computation based on 477 words.
Number of 1-grams hit = 477  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle851.out
Perplexity = 8.58, Entropy = 3.10 bits
Computation based on 604 words.
Number of 1-grams hit = 604  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle852.out
Perplexity = 8.33, Entropy = 3.06 bits
Computation based on 684 words.
Number of 1-grams hit = 684  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle853.out
Perplexity = 8.93, Entropy = 3.16 bits
Computation based on 1621 words.
Number of 1-grams hit = 1621  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle854.out
Perplexity = 8.80, Entropy = 3.14 bits
Computation based on 2917 words.
Number of 1-grams hit = 2917  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle855.out
Perplexity = 7.98, Entropy = 3.00 bits
Computation based on 465 words.
Number of 1-grams hit = 465  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle856.out
Perplexity = 7.07, Entropy = 2.82 bits
Computation based on 889 words.
Number of 1-grams hit = 889  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle857.out
Perplexity = 8.16, Entropy = 3.03 bits
Computation based on 379 words.
Number of 1-grams hit = 379  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle858.out
Perplexity = 7.50, Entropy = 2.91 bits
Computation based on 255 words.
Number of 1-grams hit = 255  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle859.out
Perplexity = 7.74, Entropy = 2.95 bits
Computation based on 1192 words.
Number of 1-grams hit = 1192  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle860.out
Perplexity = 7.22, Entropy = 2.85 bits
Computation based on 503 words.
Number of 1-grams hit = 503  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle861.out
Perplexity = 8.41, Entropy = 3.07 bits
Computation based on 432 words.
Number of 1-grams hit = 432  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle862.out
Perplexity = 8.70, Entropy = 3.12 bits
Computation based on 487 words.
Number of 1-grams hit = 487  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle863.out
Perplexity = 8.14, Entropy = 3.03 bits
Computation based on 712 words.
Number of 1-grams hit = 712  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle864.out
Perplexity = 8.18, Entropy = 3.03 bits
Computation based on 494 words.
Number of 1-grams hit = 494  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle865.out
Perplexity = 9.74, Entropy = 3.28 bits
Computation based on 6948 words.
Number of 1-grams hit = 6948  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle866.out
Perplexity = 7.82, Entropy = 2.97 bits
Computation based on 668 words.
Number of 1-grams hit = 668  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle867.out
Perplexity = 9.21, Entropy = 3.20 bits
Computation based on 377 words.
Number of 1-grams hit = 377  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle868.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 1799 words.
Number of 1-grams hit = 1799  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle869.out
Perplexity = 8.03, Entropy = 3.00 bits
Computation based on 3365 words.
Number of 1-grams hit = 3365  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle870.out
Perplexity = 7.19, Entropy = 2.85 bits
Computation based on 831 words.
Number of 1-grams hit = 831  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle871.out
Perplexity = 6.74, Entropy = 2.75 bits
Computation based on 444 words.
Number of 1-grams hit = 444  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle872.out
Perplexity = 8.86, Entropy = 3.15 bits
Computation based on 316 words.
Number of 1-grams hit = 316  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle873.out
Perplexity = 7.51, Entropy = 2.91 bits
Computation based on 1015 words.
Number of 1-grams hit = 1015  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle874.out
Perplexity = 8.36, Entropy = 3.06 bits
Computation based on 460 words.
Number of 1-grams hit = 460  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle875.out
Perplexity = 7.57, Entropy = 2.92 bits
Computation based on 855 words.
Number of 1-grams hit = 855  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle876.out
Perplexity = 7.99, Entropy = 3.00 bits
Computation based on 10045 words.
Number of 1-grams hit = 10045  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle877.out
Perplexity = 9.29, Entropy = 3.22 bits
Computation based on 502 words.
Number of 1-grams hit = 502  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle878.out
Perplexity = 8.32, Entropy = 3.06 bits
Computation based on 364 words.
Number of 1-grams hit = 364  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle879.out
Perplexity = 8.36, Entropy = 3.06 bits
Computation based on 591 words.
Number of 1-grams hit = 591  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle880.out
Perplexity = 7.98, Entropy = 3.00 bits
Computation based on 509 words.
Number of 1-grams hit = 509  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle881.out
Perplexity = 7.02, Entropy = 2.81 bits
Computation based on 376 words.
Number of 1-grams hit = 376  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle882.out
Perplexity = 8.77, Entropy = 3.13 bits
Computation based on 638 words.
Number of 1-grams hit = 638  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle883.out
Perplexity = 7.88, Entropy = 2.98 bits
Computation based on 638 words.
Number of 1-grams hit = 638  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle884.out
Perplexity = 8.13, Entropy = 3.02 bits
Computation based on 777 words.
Number of 1-grams hit = 777  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle885.out
Perplexity = 8.65, Entropy = 3.11 bits
Computation based on 534 words.
Number of 1-grams hit = 534  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle886.out
Perplexity = 8.12, Entropy = 3.02 bits
Computation based on 508 words.
Number of 1-grams hit = 508  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle887.out
Perplexity = 8.66, Entropy = 3.11 bits
Computation based on 517 words.
Number of 1-grams hit = 517  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle888.out
Perplexity = 8.47, Entropy = 3.08 bits
Computation based on 1311 words.
Number of 1-grams hit = 1311  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle889.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 4700 words.
Number of 1-grams hit = 4700  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle890.out
Perplexity = 7.73, Entropy = 2.95 bits
Computation based on 1148 words.
Number of 1-grams hit = 1148  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle891.out
Perplexity = 8.15, Entropy = 3.03 bits
Computation based on 873 words.
Number of 1-grams hit = 873  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle892.out
Perplexity = 8.62, Entropy = 3.11 bits
Computation based on 3489 words.
Number of 1-grams hit = 3489  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle893.out
Perplexity = 10.27, Entropy = 3.36 bits
Computation based on 699 words.
Number of 1-grams hit = 699  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle894.out
Perplexity = 7.89, Entropy = 2.98 bits
Computation based on 805 words.
Number of 1-grams hit = 805  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle895.out
Perplexity = 9.30, Entropy = 3.22 bits
Computation based on 188 words.
Number of 1-grams hit = 188  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle896.out
Perplexity = 8.32, Entropy = 3.06 bits
Computation based on 452 words.
Number of 1-grams hit = 452  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle897.out
Perplexity = 8.26, Entropy = 3.05 bits
Computation based on 767 words.
Number of 1-grams hit = 767  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle898.out
Perplexity = 8.35, Entropy = 3.06 bits
Computation based on 1746 words.
Number of 1-grams hit = 1746  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle899.out
Perplexity = 7.60, Entropy = 2.93 bits
Computation based on 219 words.
Number of 1-grams hit = 219  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle900.out
Perplexity = 7.50, Entropy = 2.91 bits
Computation based on 1135 words.
Number of 1-grams hit = 1135  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle901.out
Perplexity = 7.85, Entropy = 2.97 bits
Computation based on 428 words.
Number of 1-grams hit = 428  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle902.out
Perplexity = 8.57, Entropy = 3.10 bits
Computation based on 2155 words.
Number of 1-grams hit = 2155  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle903.out
Perplexity = 8.18, Entropy = 3.03 bits
Computation based on 1273 words.
Number of 1-grams hit = 1273  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle904.out
Perplexity = 5.24, Entropy = 2.39 bits
Computation based on 649 words.
Number of 1-grams hit = 649  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle905.out
Perplexity = 8.45, Entropy = 3.08 bits
Computation based on 388 words.
Number of 1-grams hit = 388  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle906.out
Perplexity = 8.01, Entropy = 3.00 bits
Computation based on 476 words.
Number of 1-grams hit = 476  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle907.out
Perplexity = 9.21, Entropy = 3.20 bits
Computation based on 488 words.
Number of 1-grams hit = 488  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle908.out
Perplexity = 6.28, Entropy = 2.65 bits
Computation based on 552 words.
Number of 1-grams hit = 552  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle909.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 456 words.
Number of 1-grams hit = 456  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle910.out
Perplexity = 8.85, Entropy = 3.15 bits
Computation based on 737 words.
Number of 1-grams hit = 737  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle911.out
Perplexity = 6.70, Entropy = 2.75 bits
Computation based on 688 words.
Number of 1-grams hit = 688  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle912.out
Perplexity = 10.00, Entropy = 3.32 bits
Computation based on 743 words.
Number of 1-grams hit = 743  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle913.out
Perplexity = 10.12, Entropy = 3.34 bits
Computation based on 743 words.
Number of 1-grams hit = 743  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle914.out
Perplexity = 8.36, Entropy = 3.06 bits
Computation based on 575 words.
Number of 1-grams hit = 575  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle915.out
Perplexity = 7.26, Entropy = 2.86 bits
Computation based on 1309 words.
Number of 1-grams hit = 1309  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle916.out
Perplexity = 7.77, Entropy = 2.96 bits
Computation based on 1003 words.
Number of 1-grams hit = 1003  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle917.out
Perplexity = 8.49, Entropy = 3.09 bits
Computation based on 1180 words.
Number of 1-grams hit = 1180  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle918.out
Perplexity = 8.49, Entropy = 3.09 bits
Computation based on 535 words.
Number of 1-grams hit = 535  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle919.out
Perplexity = 7.99, Entropy = 3.00 bits
Computation based on 3585 words.
Number of 1-grams hit = 3585  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle920.out
Perplexity = 8.08, Entropy = 3.01 bits
Computation based on 1001 words.
Number of 1-grams hit = 1001  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle921.out
Perplexity = 7.81, Entropy = 2.96 bits
Computation based on 844 words.
Number of 1-grams hit = 844  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle922.out
Perplexity = 8.74, Entropy = 3.13 bits
Computation based on 515 words.
Number of 1-grams hit = 515  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle923.out
Perplexity = 6.72, Entropy = 2.75 bits
Computation based on 560 words.
Number of 1-grams hit = 560  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle924.out
Perplexity = 8.70, Entropy = 3.12 bits
Computation based on 376 words.
Number of 1-grams hit = 376  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle925.out
Perplexity = 7.99, Entropy = 3.00 bits
Computation based on 4136 words.
Number of 1-grams hit = 4136  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle926.out
Perplexity = 7.82, Entropy = 2.97 bits
Computation based on 1421 words.
Number of 1-grams hit = 1421  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle927.out
Perplexity = 8.57, Entropy = 3.10 bits
Computation based on 440 words.
Number of 1-grams hit = 440  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle928.out
Perplexity = 7.87, Entropy = 2.98 bits
Computation based on 516 words.
Number of 1-grams hit = 516  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle929.out
Perplexity = 6.44, Entropy = 2.69 bits
Computation based on 538 words.
Number of 1-grams hit = 538  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle930.out
Perplexity = 7.24, Entropy = 2.86 bits
Computation based on 465 words.
Number of 1-grams hit = 465  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle931.out
Perplexity = 7.70, Entropy = 2.95 bits
Computation based on 645 words.
Number of 1-grams hit = 645  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle932.out
Perplexity = 6.74, Entropy = 2.75 bits
Computation based on 228 words.
Number of 1-grams hit = 228  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle933.out
Perplexity = 7.76, Entropy = 2.96 bits
Computation based on 453 words.
Number of 1-grams hit = 453  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle934.out
Perplexity = 8.84, Entropy = 3.14 bits
Computation based on 416 words.
Number of 1-grams hit = 416  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle935.out
Perplexity = 7.59, Entropy = 2.92 bits
Computation based on 1259 words.
Number of 1-grams hit = 1259  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle936.out
Perplexity = 8.23, Entropy = 3.04 bits
Computation based on 483 words.
Number of 1-grams hit = 483  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle937.out
Perplexity = 7.09, Entropy = 2.83 bits
Computation based on 239 words.
Number of 1-grams hit = 239  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle938.out
Perplexity = 7.38, Entropy = 2.88 bits
Computation based on 1402 words.
Number of 1-grams hit = 1402  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle939.out
Perplexity = 8.05, Entropy = 3.01 bits
Computation based on 519 words.
Number of 1-grams hit = 519  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle940.out
Perplexity = 8.50, Entropy = 3.09 bits
Computation based on 2076 words.
Number of 1-grams hit = 2076  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle941.out
Perplexity = 8.15, Entropy = 3.03 bits
Computation based on 1305 words.
Number of 1-grams hit = 1305  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle942.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 571 words.
Number of 1-grams hit = 571  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle943.out
Perplexity = 6.67, Entropy = 2.74 bits
Computation based on 1491 words.
Number of 1-grams hit = 1491  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle944.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 477 words.
Number of 1-grams hit = 477  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle945.out
Perplexity = 6.92, Entropy = 2.79 bits
Computation based on 3747 words.
Number of 1-grams hit = 3747  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle946.out
Perplexity = 8.49, Entropy = 3.09 bits
Computation based on 611 words.
Number of 1-grams hit = 611  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle947.out
Perplexity = 8.56, Entropy = 3.10 bits
Computation based on 459 words.
Number of 1-grams hit = 459  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle948.out
Perplexity = 7.70, Entropy = 2.94 bits
Computation based on 627 words.
Number of 1-grams hit = 627  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle949.out
Perplexity = 7.96, Entropy = 2.99 bits
Computation based on 1862 words.
Number of 1-grams hit = 1862  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle950.out
Perplexity = 7.97, Entropy = 2.99 bits
Computation based on 543 words.
Number of 1-grams hit = 543  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle951.out
Perplexity = 8.70, Entropy = 3.12 bits
Computation based on 622 words.
Number of 1-grams hit = 622  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle952.out
Perplexity = 7.66, Entropy = 2.94 bits
Computation based on 1389 words.
Number of 1-grams hit = 1389  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle953.out
Perplexity = 7.61, Entropy = 2.93 bits
Computation based on 2294 words.
Number of 1-grams hit = 2294  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle954.out
Perplexity = 8.53, Entropy = 3.09 bits
Computation based on 800 words.
Number of 1-grams hit = 800  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle955.out
Perplexity = 7.29, Entropy = 2.87 bits
Computation based on 1574 words.
Number of 1-grams hit = 1574  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle956.out
Perplexity = 7.65, Entropy = 2.94 bits
Computation based on 1107 words.
Number of 1-grams hit = 1107  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle957.out
Perplexity = 6.35, Entropy = 2.67 bits
Computation based on 323 words.
Number of 1-grams hit = 323  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle958.out
Perplexity = 7.70, Entropy = 2.95 bits
Computation based on 1014 words.
Number of 1-grams hit = 1014  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle959.out
Perplexity = 11.09, Entropy = 3.47 bits
Computation based on 218 words.
Number of 1-grams hit = 218  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle960.out
Perplexity = 6.18, Entropy = 2.63 bits
Computation based on 652 words.
Number of 1-grams hit = 652  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle961.out
Perplexity = 9.43, Entropy = 3.24 bits
Computation based on 534 words.
Number of 1-grams hit = 534  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle962.out
Perplexity = 7.84, Entropy = 2.97 bits
Computation based on 2369 words.
Number of 1-grams hit = 2369  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle963.out
Perplexity = 7.76, Entropy = 2.96 bits
Computation based on 2187 words.
Number of 1-grams hit = 2187  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle964.out
Perplexity = 9.01, Entropy = 3.17 bits
Computation based on 1090 words.
Number of 1-grams hit = 1090  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle965.out
Perplexity = 7.79, Entropy = 2.96 bits
Computation based on 701 words.
Number of 1-grams hit = 701  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle966.out
Perplexity = 7.87, Entropy = 2.98 bits
Computation based on 850 words.
Number of 1-grams hit = 850  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle967.out
Perplexity = 8.64, Entropy = 3.11 bits
Computation based on 426 words.
Number of 1-grams hit = 426  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle968.out
Perplexity = 7.81, Entropy = 2.97 bits
Computation based on 1147 words.
Number of 1-grams hit = 1147  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle969.out
Perplexity = 7.96, Entropy = 2.99 bits
Computation based on 773 words.
Number of 1-grams hit = 773  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle970.out
Perplexity = 8.18, Entropy = 3.03 bits
Computation based on 798 words.
Number of 1-grams hit = 798  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle971.out
Perplexity = 8.23, Entropy = 3.04 bits
Computation based on 504 words.
Number of 1-grams hit = 504  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle972.out
Perplexity = 8.43, Entropy = 3.08 bits
Computation based on 2041 words.
Number of 1-grams hit = 2041  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle973.out
Perplexity = 9.23, Entropy = 3.21 bits
Computation based on 663 words.
Number of 1-grams hit = 663  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle974.out
Perplexity = 6.22, Entropy = 2.64 bits
Computation based on 1022 words.
Number of 1-grams hit = 1022  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle975.out
Perplexity = 7.02, Entropy = 2.81 bits
Computation based on 380 words.
Number of 1-grams hit = 380  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle976.out
Perplexity = 8.27, Entropy = 3.05 bits
Computation based on 385 words.
Number of 1-grams hit = 385  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle977.out
Perplexity = 8.39, Entropy = 3.07 bits
Computation based on 917 words.
Number of 1-grams hit = 917  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle978.out
Perplexity = 8.51, Entropy = 3.09 bits
Computation based on 503 words.
Number of 1-grams hit = 503  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle979.out
Perplexity = 9.23, Entropy = 3.21 bits
Computation based on 1716 words.
Number of 1-grams hit = 1716  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle980.out
Perplexity = 8.96, Entropy = 3.16 bits
Computation based on 681 words.
Number of 1-grams hit = 681  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle981.out
Perplexity = 8.18, Entropy = 3.03 bits
Computation based on 371 words.
Number of 1-grams hit = 371  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle982.out
Perplexity = 7.76, Entropy = 2.96 bits
Computation based on 890 words.
Number of 1-grams hit = 890  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle983.out
Perplexity = 8.68, Entropy = 3.12 bits
Computation based on 520 words.
Number of 1-grams hit = 520  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle984.out
Perplexity = 7.07, Entropy = 2.82 bits
Computation based on 479 words.
Number of 1-grams hit = 479  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle985.out
Perplexity = 7.80, Entropy = 2.96 bits
Computation based on 1351 words.
Number of 1-grams hit = 1351  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle986.out
Perplexity = 8.33, Entropy = 3.06 bits
Computation based on 297 words.
Number of 1-grams hit = 297  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle987.out
Perplexity = 8.70, Entropy = 3.12 bits
Computation based on 1891 words.
Number of 1-grams hit = 1891  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle988.out
Perplexity = 7.83, Entropy = 2.97 bits
Computation based on 1353 words.
Number of 1-grams hit = 1353  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle989.out
Perplexity = 8.20, Entropy = 3.03 bits
Computation based on 2476 words.
Number of 1-grams hit = 2476  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle990.out
Perplexity = 7.19, Entropy = 2.85 bits
Computation based on 710 words.
Number of 1-grams hit = 710  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle991.out
Perplexity = 8.38, Entropy = 3.07 bits
Computation based on 1688 words.
Number of 1-grams hit = 1688  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle992.out
Perplexity = 9.42, Entropy = 3.24 bits
Computation based on 578 words.
Number of 1-grams hit = 578  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle993.out
Perplexity = 8.11, Entropy = 3.02 bits
Computation based on 2666 words.
Number of 1-grams hit = 2666  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle994.out
Perplexity = 10.17, Entropy = 3.35 bits
Computation based on 527 words.
Number of 1-grams hit = 527  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle995.out
Perplexity = 7.73, Entropy = 2.95 bits
Computation based on 7968 words.
Number of 1-grams hit = 7968  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle996.out
Perplexity = 8.02, Entropy = 3.00 bits
Computation based on 627 words.
Number of 1-grams hit = 627  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle997.out
Perplexity = 7.93, Entropy = 2.99 bits
Computation based on 5785 words.
Number of 1-grams hit = 5785  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle998.out
Perplexity = 9.78, Entropy = 3.29 bits
Computation based on 572 words.
Number of 1-grams hit = 572  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : Computing perplexity of the language model with respect
   to the text ../../data/posTraining/posarticle999.out
Perplexity = 9.72, Entropy = 3.28 bits
Computation based on 523 words.
Number of 1-grams hit = 523  (100.00%)
0 OOVs (0.00%) and 0 context cues were removed from the calculation.
evallm : 